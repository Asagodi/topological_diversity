@ARTICLE{Fusi2007-yg,
  title     = "Limits on the memory storage capacity of bounded synapses",
  author    = "Fusi, Stefano and Abbott, L F",
  journal   = "Nature neuroscience",
  publisher = "Springer Science and Business Media LLC",
  volume    =  10,
  number    =  4,
  pages     = "485--493",
  abstract  = "Memories maintained in patterns of synaptic connectivity are
               rapidly overwritten and destroyed by ongoing plasticity related
               to the storage of new memories. Short memory lifetimes arise from
               the bounds that must be imposed on synaptic efficacy in any
               realistic model. We explored whether memory performance can be
               improved by allowing synapses to traverse a large number of
               states before reaching their bounds, or by changing the way these
               bounds are imposed. In the case of hard bounds, memory lifetimes
               grow proportional to the square of the number of synaptic states,
               but only if potentiation and depression are precisely balanced.
               Improved performance can be obtained without fine tuning by
               imposing soft bounds, but this improvement is only linear with
               respect to the number of synaptic states. We explored several
               other possibilities and conclude that improving memory
               performance requires a more radical modification of the standard
               model of memory storage.",
  month     =  apr,
  year      =  2007,
  url       = "https://www.nature.com/articles/nn1859",
  doi       = "10.1038/nn1859",
  pmid      =  17351638,
  issn      = "1097-6256,1546-1726",
  language  = "en"
}

@ARTICLE{vyas2020,
  title     = "Computation Through Neural Population Dynamics",
  author    = "Vyas, Saurabh and Golub, Matthew D and Sussillo, David and
               Shenoy, Krishna V",
  journal   = "Annual review of neuroscience",
  publisher = "Annual Reviews",
  volume    =  43,
  number    =  1,
  pages     = "249--275",
  abstract  = "Significant experimental, computational, and theoretical work has
               identified rich structure within the coordinated activity of
               interconnected neural populations. An emerging challenge now is
               to uncover the nature of the associated computations, how they
               are implemented, and what role they play in driving behavior. We
               term this computation through neural population dynamics. If
               successful, this framework will reveal general motifs of neural
               population activity and quantitatively describe how neural
               population dynamics implement computations necessary for driving
               goal-directed behavior. Here, we start with a mathematical primer
               on dynamical systems theory and analytical tools necessary to
               apply this perspective to experimental data. Next, we highlight
               some recent discoveries resulting from successful application of
               dynamical systems. We focus on studies spanning motor control,
               timing, decision-making, and working memory. Finally, we briefly
               discuss promising recent lines of investigation and future
               directions for the computation through neural population dynamics
               framework.",
  month     =  jul,
  year      =  2020,
  url       = "https://doi.org/10.1146/annurev-neuro-092619-094115",
  doi       = "10.1146/annurev-neuro-092619-094115",
  issn      = "0147-006X",
  annote    = "doi: 10.1146/annurev-neuro-092619-094115"
}

@book{leonelli2016,
  title={Data-centric biology: A philosophical study},
  author={Leonelli, Sabina},
  year={2016},
  publisher={University of Chicago Press}
}

@article{mrozek1999,
  title={An algorithmic approach to the {C}onley index theory},
  author={Mrozek, Marian},
  journal={Journal of Dynamics and Differential Equations},
  volume={11},
  number={4},
  pages={711--734},
  year={1999},
  publisher={Springer}
}

@article{ermentrout1979,
  title={Temporal oscillations in neuronal nets},
  author={Ermentrout, GB and Cowan, Jack D},
  journal={Journal of mathematical biology},
  volume={7},
  number={3},
  pages={265--280},
  year={1979},
  publisher={Springer}
}

@article{whitelaw1981,
  title={Specificity and plasticity of retinotectal connections: A computational model},
  author={Whitelaw, Virginia A and Cowan, Jack D},
  journal={Journal of Neuroscience},
  volume={1},
  number={12},
  pages={1369--1387},
  year={1981},
  publisher={Soc Neuroscience}
}

@article{xuan2017,
  title={Social synchrony on complex networks},
  author={Xuan, Qi and Zhang, Zhi-Yuan and Fu, Chenbo and Hu, Hong-Xiang and Filkov, Vladimir},
  journal={IEEE transactions on cybernetics},
  volume={48},
  number={5},
  pages={1420--1431},
  year={2017},
  publisher={IEEE}
}

@article{steriade1993,
  title={Thalamocortical oscillations in the sleeping and aroused brain},
  author={Steriade, Mircea and McCormick, David A and Sejnowski, Terrence J},
  journal={Science},
  volume={262},
  number={5134},
  pages={679--685},
  year={1993},
  publisher={American Association for the Advancement of Science}
}

@book{buzsaki2006,
  title={Rhythms of the Brain},
  author={Buzsaki, Gyorgy},
  year={2006},
  publisher={Oxford University Press}
}

@article{buzsaki2012,
  title={Mechanisms of gamma oscillations},
  author={Buzs{\'a}ki, Gy{\"o}rgy and Wang, Xiao-Jing},
  journal={Annual review of neuroscience},
  volume={35},
  pages={203--225},
  year={2012},
  publisher={Annual Reviews}
}

@article{lisman2013,
  title={The theta-gamma neural code},
  author={Lisman, John E and Jensen, Ole},
  journal={Neuron},
  volume={77},
  number={6},
  pages={1002--1016},
  year={2013},
  publisher={Elsevier}
}

@book{akin2010,
  title={The general topology of dynamical systems},
  author={Akin, Ethan},
  volume={1},
  year={2010},
  publisher={American Mathematical Soc.}
}

@inproceedings{kim2021,
  title={Inferring latent dynamics underlying neural population activity via neural differential equations},
  author={Kim, Timothy D and Luo, Thomas Z and Pillow, Jonathan W and Brody, Carlos D},
  booktitle={International Conference on Machine Learning},
  pages={5551--5561},
  year={2021},
  organization={PMLR}
}

@article{ching2012,
  title={Distributed control in a mean-field cortical network model: Implications for seizure suppression},
  author={Ching, ShiNung and Brown, Emery N and Kramer, Mark A},
  journal={Physical Review E},
  volume={86},
  number={2},
  pages={021920},
  year={2012},
  publisher={APS}
}

@article{bondanelli2020,
  title={Coding with transient trajectories in recurrent neural networks},
  author={Bondanelli, Giulio and Ostojic, Srdjan},
  journal={PLoS computational biology},
  volume={16},
  number={2},
  pages={e1007655},
  year={2020},
  publisher={Public Library of Science}
}

@article{curto2012,
  title={Flexible memory networks},
  author={Curto, Carina and Degeratu, Anda and Itskov, Vladimir},
  journal={Bulletin of mathematical biology},
  volume={74},
  number={3},
  pages={590--614},
  year={2012},
  publisher={Springer}
}

@inproceedings{hahnloser2001,
  title={Permitted and forbidden sets in symmetric threshold-linear networks},
  author={Hahnloser, Richard HR and Seung, H Sebastian},
  booktitle={Advances in neural information processing systems},
  pages={217--223},
  year={2001}
}

@article{zhang2008,
  title={Transient dynamics of sparsely connected {H}opfield neural networks with arbitrary degree distributions},
  author={Zhang, Pan and Chen, Yong},
  journal={Physica A: Statistical Mechanics and its Applications},
  volume={387},
  number={4},
  pages={1009--1015},
  year={2008},
  publisher={Elsevier}
}

@article{nachstedt2017,
  title={Working memory requires a combination of transient and attractor-dominated dynamics to process unreliably timed inputs},
  author={Nachstedt, Timo and Tetzlaff, Christian},
  journal={Scientific reports},
  volume={7},
  number={1},
  pages={1--14},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{gudowska2020,
  title={From synaptic interactions to collective dynamics in random neuronal networks models: Critical role of eigenvectors and transient behavior},
  author={Gudowska-Nowak, Ewa and Nowak, Maciej A and Chialvo, Dante R and Ochab, Jeremi K and Tarnowski, Wojciech},
  journal={Neural Computation},
  volume={32},
  number={2},
  pages={395--423},
  year={2020},
  publisher={MIT Press}
}

@article{may1974,
  title={Biological populations with nonoverlapping generations: stable points, stable cycles, and chaos},
  author={May, Robert M},
  journal={Science},
  volume={186},
  number={4164},
  pages={645--647},
  year={1974},
  publisher={American Association for the Advancement of Science}
}

@article{silva2003,
  title={Dynamical diseases of brain systems: Different routes to epileptic seizures},
  author={da Silva, Fernando H Lopes and Blanes, Wouter and Kalitzin, Stiliyan N and Parra, Jaime and Suffczynski, Piotr and Velis, Demetrios N},
  journal={IEEE transactions on biomedical engineering},
  volume={50},
  number={5},
  pages={540--548},
  year={2003},
  publisher={IEEE}
}

@inproceedings{zhao2007,
  title={Visual selection and shifting mechanisms based on a network of chaotic {W}ilson-{C}owan oscillators},
  author={Zhao, Liang and Breve, Fabricio A and Quiles, Marcos G and Romero, Roseli AF},
  booktitle={Third International Conference on Natural Computation (ICNC 2007)},
  volume={5},
  pages={754--762},
  year={2007},
  organization={IEEE}
}

@book{degn2013chaos,
  title={Chaos in biological systems},
  author={Degn, Hans and Holden, Arunn V and Olsen, Lars Folke},
  volume={138},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@phdthesis{pina2019,
  title={Evoked Patterns of Oscillatory Activity in Mean-Field Neuronal Networks},
  author={Pina, Jason},
  year={2019},
  school={University of Pittsburgh}
}

@article{pina2018,
  title={Oscillations in working memory and neural binding: A mechanism for multiple memories and their interactions},
  author={Pina, Jason E and Bodner, Mark and Ermentrout, Bard},
  journal={PLoS computational biology},
  volume={14},
  number={11},
  pages={e1006517},
  year={2018},
  publisher={Public Library of Science}
}

@article{wuensche1998,
  title={Discrete dynamical networks and their attractor basins},
  author={Wuensche, Andrew and others},
  journal={Complexity International},
  volume={6},
  pages={3--21},
  year={1998}
}

@inproceedings{ganguly2001,
  title={Evolving cellular automata based associative memory for pattern recognition},
  author={Ganguly, Niloy and Das, Arijit and Maji, Pradipta and Sikdar, Biplab K and Chaudhuri, P Pal},
  booktitle={International Conference on High-Performance Computing},
  pages={115--124},
  year={2001},
  organization={Springer}
}

@article{marder2011,
  title={Multiple models to capture the variability in biological neurons and networks},
  author={Marder, Eve and Taylor, Adam L},
  journal={Nature neuroscience},
  volume={14},
  number={2},
  pages={133--138},
  year={2011},
  publisher={Nature Publishing Group}
}

@article{frascoli2011,
  title={Metabifurcation analysis of a mean field model of the cortex},
  author={Frascoli, Federico and Van Veen, Lennaert and Bojak, Ingo and Liley, David TJ},
  journal={Physica D: Nonlinear Phenomena},
  volume={240},
  number={11},
  pages={949--962},
  year={2011},
  publisher={Elsevier}
}

@article{wang2012,
  title={Phase space approach for modeling of epileptic dynamics},
  author={Wang, Yujiang and Goodfellow, Marc and Taylor, Peter Neal and Baier, Gerold},
  journal={Physical Review E},
  volume={85},
  number={6},
  pages={061918},
  year={2012},
  publisher={APS}
}

@article{lepine2013,
  title={Delay-induced oscillations in {Wilson and Cowan}’s model: An analysis of the subthalamo-pallidal feedback loop in healthy and parkinsonian subjects},
  author={Pasillas-L{\'e}pine, William},
  journal={Biological cybernetics},
  volume={107},
  number={3},
  pages={289--308},
  year={2013},
  publisher={Springer}
}

@article{li2019,
  title={Control principles for complex biological networks},
  author={Li, Min and Gao, Hao and Wang, Jianxin and Wu, Fang-Xiang},
  journal={Briefings in bioinformatics},
  volume={20},
  number={6},
  pages={2253--2266},
  year={2019},
  publisher={Oxford University Press}
}

@article{fisher2014,
  title={{ILAE} official report: {A} practical clinical definition of epilepsy},
  author={Fisher, Robert S and Acevedo, Carlos and Arzimanoglou, Alexis and Bogacz, Alicia and Cross, J Helen and Elger, Christian E and Engel Jr, Jerome and Forsgren, Lars and French, Jacqueline A and Glynn, Mike and others},
  journal={Epilepsia},
  volume={55},
  number={4},
  pages={475--482},
  year={2014},
  publisher={Wiley Online Library}
}

@article{dey2020,
  title={Persistence of the {C}onley Index in Combinatorial Dynamical Systems},
  author={Dey, Tamal K and Mrozek, Marian and Slechta, Ryan},
  journal={arXiv preprint arXiv:2003.05579},
  year={2020}
}

@book{lorenz1993,
  title={Nonlinear dynamical economics and chaotic motion},
  author={Lorenz, Hans-Walter},
  volume={334},
  year={1993},
  publisher={Springer}
}

@article{broyden1965,
  title={A class of methods for solving nonlinear simultaneous equations},
  author={Broyden, Charles G},
  journal={Mathematics of computation},
  volume={19},
  number={92},
  pages={577--593},
  year={1965},
  publisher={JSTOR}
}

@article{nagumo1962,
  title={An active pulse transmission line simulating nerve axon},
  author={Nagumo, Jinichi and Arimoto, Suguru and Yoshizawa, Shuji},
  journal={Proceedings of the IRE},
  volume={50},
  number={10},
  pages={2061--2070},
  year={1962},
  publisher={IEEE}
}

@book{dold2012,
  title={Lectures on algebraic topology},
  author={Dold, Albrecht},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{fitzhugh1961,
  title={Impulses and physiological states in theoretical models of nerve membrane},
  author={FitzHugh, Richard},
  journal={Biophysical journal},
  volume={1},
  number={6},
  pages={445},
  year={1961},
  publisher={The Biophysical Society}
}

@article{eliasmith2010,
  title={How we ought to describe computation in the brain},
  author={Eliasmith, Chris},
  journal={Studies in History and Philosophy of Science Part A},
  volume={41},
  number={3},
  pages={313--320},
  year={2010},
  publisher={Elsevier}
}

@article{hochstein2017,
  title={Why one model is never enough: A defense of explanatory holism},
  author={Hochstein, Eric},
  journal={Biology \& Philosophy},
  volume={32},
  number={6},
  pages={1105--1125},
  year={2017},
  publisher={Springer}
}

@incollection{clarke2004,
  title={Lyapunov functions and feedback in nonlinear control},
  author={Clarke, Francis},
  booktitle={Optimal control, stabilization and nonsmooth analysis},
  pages={267--282},
  year={2004},
  publisher={Springer}
}

@article{koditschek1989,
  title={The application of total energy as a {L}yapunov function for mechanical control systems},
  author={Koditschek, Daniel E},
  journal={Contemporary mathematics},
  volume={97},
  pages={131},
  year={1989}
}

@article{schreiber2006,
  title={Persistence despite perturbations for interacting populations},
  author={Schreiber, Sebastian J},
  journal={Journal of Theoretical Biology},
  volume={242},
  number={4},
  pages={844--852},
  year={2006},
  publisher={Elsevier}
}

@article{silveira2011,
  title={Cell types, circuits, computation},
  author={da Silveira, Rava Azeredo and Roska, Botond},
  journal={Current opinion in neurobiology},
  volume={21},
  number={5},
  pages={664--671},
  year={2011},
  publisher={Elsevier}
}

@book{guckenheimer2013,
  title={Nonlinear oscillations, dynamical systems, and bifurcations of vector fields},
  author={Guckenheimer, John and Holmes, Philip},
  volume={42},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@article{liu2005,
  title={The random case of {C}onley's theorem},
  author={Liu, Zhenxin},
  journal={Nonlinearity},
  volume={19},
  number={2},
  pages={277},
  year={2005},
  publisher={IOP Publishing}
}

@article{liu2007ii,
  title={The random case of {C}onley's theorem: {II. T}he complete {L}yapunov function},
  author={Liu, Zhenxin},
  journal={Nonlinearity},
  volume={20},
  number={4},
  pages={1017},
  year={2007},
  publisher={IOP Publishing}
}

@article{liu2007iii,
  title={The random case of {C}onley's theorem: {III}. {R}andom semiflow case and {M}orse decomposition},
  author={Liu, Zhenxin},
  journal={Nonlinearity},
  volume={20},
  number={12},
  pages={2773},
  year={2007},
  publisher={IOP Publishing}
}

@book{reineck1985,
  title={The connection matrix and the classification of flows arising from ecological models},
  author={Reineck, James Francis},
  year={1985},
  publisher={University of Wisconsin--Madison}
}

@article{reineck1991,
  title={A connection matrix analysis of ecological models},
  author={Reineck, James F},
  journal={Nonlinear Analysis: Theory, Methods \& Applications},
  volume={17},
  number={4},
  pages={361--384},
  year={1991},
  publisher={Elsevier}
}

@article{kalies2013,
  title={Lattice structures for attractors {I}},
  author={Kalies, William D and Mischaikow, Konstantin and Vandervorst, Robert CAM},
  journal={arXiv preprint arXiv:1307.1737},
  year={2013}
}

@article{kurland1982,
  title={Homotopy invariants of repeller-attractor pairs. {I}. The {P\"u}ppe sequence of an {R-A} pair},
  author={Kurland, Henry L},
  journal={Journal of Differential Equations},
  volume={46},
  number={1},
  pages={1--31},
  year={1982},
  publisher={Elsevier}
}

@article{kurland1983,
  title={Homotopy invariants of repeller-attractor pairs, {II}. Continuation of {R-A} pairs},
  author={Kurland, Henry L},
  journal={Journal of differential equations},
  volume={49},
  number={2},
  pages={281--329},
  year={1983},
  publisher={Academic Press}
}

@article{batko2017,
  title={Linking combinatorial and classical dynamics: {C}onley index and {M}orse decompositions},
  author={Batko, Bogdan and Kaczynski, Tomasz and Mrozek, Marian and Wanner, Thomas},
  journal={arXiv preprint arXiv:1710.05802},
  year={2017}
}

@article{floer1987,
  title={A refinement of the {C}onley index and an application to the stability of hyperbolic invariant sets},
  author={Floer, Andreas},
  journal={Ergodic Theory and Dynamical Systems},
  volume={7},
  number={1},
  pages={93--103},
  year={1987},
  publisher={Cambridge University Press}
}

@article{petri2018,
  title={Simplicial activity driven model},
  author={Petri, Giovanni and Barrat, Alain},
  journal={Physical review letters},
  volume={121},
  number={22},
  pages={228301},
  year={2018},
  publisher={APS}
}

@article{reimann2017,
  title={Cliques of neurons bound into cavities provide a missing link between structure and function},
  author={Reimann, Michael W and Nolte, Max and Scolamiero, Martina and Turner, Katharine and Perin, Rodrigo and Chindemi, Giuseppe and D{\l}otko, Pawe{\l} and Levi, Ran and Hess, Kathryn and Markram, Henry},
  journal={Frontiers in computational neuroscience},
  volume={11},
  pages={48},
  year={2017},
  publisher={Frontiers}
}

@article{ghrist2008,
  title={Barcodes: The persistent topology of data},
  author={Ghrist, Robert},
  journal={Bulletin of the American Mathematical Society},
  volume={45},
  number={1},
  pages={61--75},
  year={2008}
}

@article{muezzinoglu2010,
  title={Transients versus attractors in complex networks},
  author={Muezzinoglu, Mehmet K and Tristan, Irma and Huerta, Ramon and Afraimovich, Valentin S and Rabinovich, Mikhail I},
  journal={International Journal of Bifurcation and Chaos},
  volume={20},
  number={06},
  pages={1653--1675},
  year={2010},
  publisher={World Scientific}
}

@article{mrozek1990,
  title={Leray functor and cohomological {C}onley index for discrete dynamical systems},
  author={Mrozek, Marian},
  journal={Transactions of the American Mathematical Society},
  volume={318},
  number={1},
  pages={149--178},
  year={1990}
}

@article{franks2000,
  title={Shift equivalence and the {C}onley index},
  author={Franks, John and Richeson, David},
  journal={Transactions of the American Mathematical Society},
  volume={352},
  number={7},
  pages={3305--3322},
  year={2000}
}

@article{batko2019,
  title={{C}onley index approach to sampled dynamics},
  author={Batko, Bogdan and Mischaikow, Konstantin and Mrozek, Marian and Przybylski, Mateusz},
  journal={arXiv preprint arXiv:1904.03757},
  year={2019}
}

@article{friedman1974,
  title={Explanation and scientific understanding},
  author={Friedman, Michael},
  journal={The Journal of Philosophy},
  volume={71},
  number={1},
  pages={5--19},
  year={1974},
  publisher={JSTOR}
}

@article{cummins2018a,
  title={DSGRN: Examining the dynamics of families of logical models},
  author={Cummins, Bree and Gedeon, Tomas and Harker, Shaun and Mischaikow, Konstantin},
  journal={Frontiers in physiology},
  volume={9},
  pages={549},
  year={2018},
  publisher={Frontiers}
}

@article{cummins2018b,
  title={Model rejection and parameter reduction via time series},
  author={Cummins, Bree and Gedeon, Tomas and Harker, Shaun and Mischaikow, Konstantin},
  journal={SIAM journal on applied dynamical systems},
  volume={17},
  number={2},
  pages={1589--1616},
  year={2018},
  publisher={SIAM}
}

@article{kalies2016,
  title={Lattice structures for attractors {II}},
  author={Kalies, William D and Mischaikow, Konstantin and Vandervorst, Robert CA M},
  journal={Foundations of Computational Mathematics},
  volume={16},
  number={5},
  pages={1151--1191},
  year={2016},
  publisher={Springer}
}

@article{ban2006,
  title={A computational approach to {C}onley’s decomposition theorem},
  author={Ban, Hyunju and Kalies, William D},
  year={2006}
}

@incollection{galan2009,
  title={The phase oscillator approximation in neuroscience: An analytical framework to study coherent activity in neural networks},
  author={Gal{\'a}n, Roberto F},
  booktitle={Coordinated Activity in the Brain},
  pages={65--89},
  year={2009},
  publisher={Springer}
}

@book{winfree2001,
  title={The geometry of biological time},
  author={Winfree, Arthur T},
  volume={12},
  year={2001},
  publisher={Springer Science \& Business Media}
}

@article{hodgkin1952,
  title={A quantitative description of membrane current and its application to conduction and excitation in nerve},
  author={Hodgkin, Alan L and Huxley, Andrew F},
  journal={The Journal of physiology},
  volume={117},
  number={4},
  pages={500--544},
  year={1952},
  publisher={Wiley Online Library}
}

@book{izhikevich2007,
  title={Dynamical systems in neuroscience},
  author={Izhikevich, Eugene M},
  year={2007},
  publisher={MIT press}
}

@article{ma2007,
  title={Multistability in spiking neuron models of delayed recurrent inhibitory loops},
  author={Ma, Jianfu and Wu, Jianhong},
  journal={Neural computation},
  volume={19},
  number={8},
  pages={2124--2148},
  year={2007},
  publisher={MIT Press}
} 

@article{schuster1990,
  title={A model for neuronal oscillations in the visual cortex},
  author={Schuster, HG and Wagner, P},
  journal={Biological cybernetics},
  volume={64},
  number={1},
  pages={77--82},
  year={1990},
  publisher={Springer}
}

@article{shusterman2008,
  title={From baseline to epileptiform activity: {A} path to synchronized rhythmicity in large-scale neural networks},
  author={Shusterman, Vladimir and Troy, William C},
  journal={Physical Review E},
  volume={77},
  number={6},
  pages={061911},
  year={2008},
  publisher={APS}
}

@article{lipinski2019,
  title={{C}onley-{M}orse-{F}orman theory for generalized combinatorial multivector fields on finite topological spaces},
  author={Lipi{\'n}ski, Micha{\l} and Kubica, Jacek and Mrozek, Marian and Wanner, Thomas},
  journal={arXiv preprint arXiv:1911.12698},
  year={2019}
}

@book{munkres1984,
  title={Elements of algebraic topology},
  author={Munkres, James Raymond},
  year={1984},
  publisher={Addison-Wesley}
}

@article{forman2002,
  title={A user’s guide to discrete {M}orse theory},
  author={Forman, Robin},
  journal={S{\'e}m. Lothar. Combin},
  volume={48},
  pages={35pp},
  year={2002}
}

@book{ermentrout2010,
  title={Mathematical foundations of neuroscience},
  author={Ermentrout, G Bard and Terman, David H},
  volume={35},
  year={2010},
  publisher={Springer Science \& Business Media}
}

@book{vick1973,
  title={Homology theory: An introduction to algebraic topology},
  author={Vick, James W},
  volume={145},
  year={1973},
  publisher={Springer Science \& Business Media}
}

@article{haider2006,
  title={Neocortical network activity in vivo is generated through a dynamic balance of excitation and inhibition},
  author={Haider, Bilal and Duque, Alvaro and Hasenstaub, Andrea R and McCormick, David A},
  journal={Journal of Neuroscience},
  volume={26},
  number={17},
  pages={4535--4545},
  year={2006},
  publisher={Soc Neuroscience}
}



@article{dey2019,
  title={Persistent homology of {M}orse decompositions in combinatorial dynamics},
  author={Dey, Tamal K and Juda, Mateusz and Kapela, Tomasz and Kubica, Jacek and Lipiński, Micha{\l} and Mrozek, Marian},
  journal={SIAM Journal on Applied Dynamical Systems},
  volume={18},
  number={1},
  pages={510--530},
  year={2019},
  publisher={SIAM}
}

@article{chow2019,
  title={Before and beyond the {W}ilson-{C}owan equations},
  author={Chow, Carson C and Karimipanah, Yahya},
  journal={arXiv preprint arXiv:1907.07821},
  year={2019}
}

@article{grossberg1988,
  title={Nonlinear neural networks: Principles, mechanisms, and architectures},
  author={Grossberg, Stephen},
  journal={Neural networks},
  volume={1},
  number={1},
  pages={17--61},
  year={1988},
  publisher={Elsevier}
}

@article{mizutani1998,
  title={Controlling chaos in chaotic neural networks},
  author={Mizutani, Shin and Sano, Takuya and Uchiyama, Tadasu and Sonehara, Noboru},
  journal={Electronics and Communications in Japan (Part III: Fundamental Electronic Science)},
  volume={81},
  number={8},
  pages={73--82},
  year={1998},
  publisher={Wiley Online Library}
}

@book{hoppensteadt1997,
  title={Weakly connected neural networks},
  author={Hoppensteadt, Frank C and Izhikevich, Eugene M},
  volume={126},
  year={1997},
  publisher={Springer Science \& Business Media}
}

@article{deco2012,
  title={Ongoing cortical activity at rest: Criticality, multistability, and ghost attractors},
  author={Deco, Gustavo and Jirsa, Viktor K},
  journal={Journal of Neuroscience},
  volume={32},
  number={10},
  pages={3366--3375},
  year={2012},
  publisher={Soc Neuroscience}
}

@article{wilson1972,
  title={Excitatory and inhibitory interactions in localized populations of model neurons},
  author={Wilson, Hugh R and Cowan, Jack D},
  journal={Biophysical journal},
  volume={12},
  number={1},
  pages={1--24},
  year={1972},
  publisher={Elsevier}
}

@article{kilpatrick2014,
  title={Wilson-{C}owan model},
  author={Kilpatrick, Zachary P},
  year={2014},
  publisher={Citeseer}
}

@article{destexhe2009,
  title={The {W}ilson--{C}owan model, 36 years later},
  author={Destexhe, Alain and Sejnowski, Terrence J},
  journal={Biological cybernetics},
  volume={101},
  number={1},
  pages={1--2},
  year={2009},
  publisher={Springer}
}

@article{tang2018,
  title={Colloquium: Control of dynamics in brain networks},
  author={Tang, Evelyn and Bassett, Danielle S},
  journal={Reviews of modern physics},
  volume={90},
  number={3},
  pages={031003},
  year={2018},
  publisher={APS}
}

@article{talathi2009,
  title={Circadian control of neural excitability in an animal model of temporal lobe epilepsy},
  author={Talathi, Sachin S and Hwang, Dong-Uk and Ditto, William L and Mareci, Tom and Sepulveda, Hector and Spano, Mark and Carney, Paul R},
  journal={Neuroscience letters},
  volume={455},
  number={2},
  pages={145--149},
  year={2009},
  publisher={Elsevier}
}

@article{holt2014,
  title={Origins and suppression of oscillations in a computational model of Parkinson’s disease},
  author={Holt, Abbey B and Netoff, Theoden I},
  journal={Journal of computational neuroscience},
  volume={37},
  number={3},
  pages={505--521},
  year={2014},
  publisher={Springer}
}

@article{coombes2018,
  title={Networks of piecewise linear neural mass models},
  author={Coombes, Stephen and Lai, Yi Ming and {\c{S}}ayli, Mustafa and Thul, Ruediger},
  journal={European Journal of Applied Mathematics},
  volume={29},
  number={5},
  pages={869--890},
  year={2018},
  publisher={Cambridge University Press}
}

@article{buendia2020,
  title={Self-organized bistability and its possible relevance for brain dynamics},
  author={Buend{\'\i}a, Victor and di Santo, Serena and Villegas, Pablo and Burioni, Raffaella and Mu{\~n}oz, Miguel A},
  journal={Physical Review Research},
  volume={2},
  number={1},
  pages={013318},
  year={2020},
  publisher={APS}
}

@article{cowan2016,
  title={{W}ilson--{C}owan equations for neocortical dynamics},
  author={Cowan, Jack D and Neuman, Jeremy and van Drongelen, Wim},
  journal={The Journal of Mathematical Neuroscience},
  volume={6},
  number={1},
  pages={1--24},
  year={2016},
  publisher={SpringerOpen}
}

@article{morrison2016a,
  title={Emergent dynamics from network connectivity: A minimal model},
  author={Morrison, K and Geneson, J and Langdon, C and Degeratu, A and Itskov, V and Curto, C},
  journal={preparation. Earlier version available online at https://arxiv. org/abs/1605.04463}
}

@article{morrison2016b,
  title={Diversity of emergent dynamics in competitive threshold-linear networks: A preliminary report},
  author={Morrison, Katherine and Degeratu, Anda and Itskov, Vladimir and Curto, Carina},
  journal={arXiv preprint arXiv:1605.04463},
  year={2016}
}

@article{campbell1996,
  title={Synchronization and desynchronization in a network of locally coupled {Wilson-Cowan} oscillators},
  author={Campbell, Shannon and Wang, DeLiang},
  journal={IEEE transactions on neural networks},
  volume={7},
  number={3},
  pages={541--554},
  year={1996},
  publisher={IEEE}
}

@article{wang2014,
  title={Dynamic mechanisms of neocortical focal seizure onset},
  author={Wang, Yujiang and Goodfellow, Marc and Taylor, Peter Neal and Baier, Gerold},
  journal={PLoS computational biology},
  volume={10},
  number={8},
  year={2014},
  publisher={Public Library of Science}
}

@article{conti2019,
  title={The role of network structure and time delay in a metapopulation {W}ilson--{C}owan model},
  author={Conti, Federica and Van Gorder, Robert A},
  journal={Journal of theoretical biology},
  volume={477},
  pages={1--13},
  year={2019},
  publisher={Elsevier}
}

@article{ahmadizadeh2016,
  title={On synchronization of networks of {W}ilson--{C}owan oscillators with diffusive coupling},
  author={Ahmadizadeh, Saeed and Ne{\v{s}}i{\'c}, Dragan and Freestone, Dean R and Grayden, David B},
  journal={Automatica},
  volume={71},
  pages={169--178},
  year={2016},
  publisher={Elsevier}
}

@article{liou2019,
  title={A theoretical model for focal seizure initiation, propagation, termination, and progression},
  author={Liou, Jyun-you and Smith, Elliot H and Bateman, Lisa M and Bruce, Samuel L and McKhann, Guy M and Goodman, Robert R and Emerson, Ronald G and Schevon, Catherine A and Abbott, LF},
  journal={bioRxiv},
  pages={724088},
  year={2019},
  publisher={Cold Spring Harbor Laboratory}
}

@article{meijer2015,
  title={Modeling focal epileptic activity in the {W}ilson--{C}owan model with depolarization block},
  author={Meijer, Hil GE and Eissa, Tahra L and Kiewiet, Bert and Neuman, Jeremy F and Schevon, Catherine A and Emerson, Ronald G and Goodman, Robert R and McKhann, Guy M and Marcuccilli, Charles J and Tryba, Andrew K and others},
  journal={The Journal of Mathematical Neuroscience (JMN)},
  volume={5},
  number={1},
  pages={1--17},
  year={2015},
  publisher={SpringerOpen}
}

@article{neves2016,
  title={A linear analysis of coupled {W}ilson-{C}owan neuronal populations},
  author={Neves, Lucas L and Monteiro, Luiz Henrique Alves},
  journal={Computational intelligence and neuroscience},
  volume={2016},
  year={2016},
  publisher={Hindawi}
}

@article{baird1986,
  title={Nonlinear dynamics of pattern formation and pattern recognition in the rabbit olfactory bulb},
  author={Baird, Bill},
  journal={Physica D: Nonlinear Phenomena},
  volume={22},
  number={1-3},
  pages={150--175},
  year={1986},
  publisher={Elsevier}
}

@article{yu2013b,
  title={A survey on CPG-inspired control models and system implementation},
  author={Yu, Junzhi and Tan, Min and Chen, Jian and Zhang, Jianwei},
  journal={IEEE transactions on neural networks and learning systems},
  volume={25},
  number={3},
  pages={441--456},
  year={2013},
  publisher={IEEE}
}

@inproceedings{cui2009,
  title={Chinese character learning by synchronization in {Wilson-Cowan} oscillatory neural networks},
  author={Cui, Jiaxin and Liu, Yan and Chen, Jiawei and Chen, Liujun and Fang, Fukang},
  booktitle={2009 Fifth International Conference on Natural Computation},
  volume={1},
  pages={105--109},
  year={2009},
  organization={IEEE}
}

@article{sarti2003,
  title={From neural oscillations to variational problems in the visual cortex},
  author={Sarti, Alessandro and Citti, Giovanna and Manfredini, Maria},
  journal={Journal of Physiology-Paris},
  volume={97},
  number={2-3},
  pages={379--385},
  year={2003},
  publisher={Elsevier}
}

@article{monteiro2002,
  title={Analytical results on a {Wilson-Cowan} neuronal network modified model},
  author={Monteiro, LHA and Bussab, MA and Berlinck, JG Chaui},
  journal={Journal of theoretical biology},
  volume={219},
  number={1},
  pages={83--91},
  year={2002},
  publisher={Elsevier}
}

@inproceedings{schiff2009,
  title={Kalman meets neuron: The emerging intersection of control theory with neuroscience},
  author={Schiff, Steven J},
  booktitle={2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
  pages={3318--3321},
  year={2009},
  organization={IEEE}
}

@article{selverston2006,
  title={Oscillations and oscillatory behavior in small neural circuits},
  author={Selverston, Allen I and Ayers, Joseph},
  journal={Biological cybernetics},
  volume={95},
  number={6},
  pages={537},
  year={2006},
  publisher={Springer}
}

@article{cervera2020,
  title={Phase-locked states in oscillating neural networks and their role in neural communication},
  author={P{\'e}rez-Cervera, Alberto and Seara, Tere M and Huguet, Gemma},
  journal={Communications in Nonlinear Science and Numerical Simulation},
  volume={80},
  pages={104992},
  year={2020},
  publisher={Elsevier}
}

@article{moeckel1988,
  title={{M}orse decompositions and connection matrices},
  author={Moeckel, Richard},
  journal={Ergodic theory and dynamical systems},
  volume={8},
  number={8*},
  pages={227--249},
  year={1988},
  publisher={Cambridge University Press}
}

@article{szymczak1997,
  title={A combinatorial procedure for finding isolating neighbourhoods and index pairs},
  author={Szymczak, Andrzej},
  journal={Proceedings of the Royal Society of Edinburgh Section A: Mathematics},
  volume={127},
  number={5},
  pages={1075--1088},
  year={1997},
  publisher={Royal Society of Edinburgh Scotland Foundation}
}

@article{alexander2015,
  title={Simplicial multivalued maps and the witness complex for dynamical analysis of time series},
  author={Alexander, Zachary and Bradley, Elizabeth and Meiss, James D and Sanderson, Nicole F},
  journal={SIAM Journal on applied dynamical systems},
  volume={14},
  number={3},
  pages={1278--1307},
  year={2015},
  publisher={SIAM}
}

@article{jovanovic2016,
  title={Interplay between graph topology and correlations of third order in spiking neuronal networks},
  author={Jovanovi{\'c}, Stojan and Rotter, Stefan},
  journal={PLoS computational biology},
  volume={12},
  number={6},
  year={2016},
  publisher={Public Library of Science}
}

@article{morrison2020,
  title={Nonlinear control in the nematode {C}. elegans},
  author={Morrison, Megan and Fieseler, Charles and Kutz, J Nathan},
  journal={arXiv preprint arXiv:2001.08332},
  year={2020}
}

@book{robeva2018,
  title={Algebraic and Combinatorial Computational Biology},
  author={Robeva, Raina and Macauley, Matthew},
  year={2018},
  publisher={Academic Press}
}

@book{jost2014,
  title={Mathematical methods in biology and neurobiology},
  author={Jost, J{\"u}rgen},
  year={2014},
  publisher={Springer}
}

@article{ocker2017,
  title={Linking structure and activity in nonlinear spiking networks},
  author={Ocker, Gabriel Koch and Josi{\'c}, Kre{\v{s}}imir and Shea-Brown, Eric and Buice, Michael A},
  journal={PLoS computational biology},
  volume={13},
  number={6},
  pages={e1005583},
  year={2017},
  publisher={Public Library of Science}
}

@article{mischaikow2015,
  title={Discretization strategies for computing {C}onley indices and {M}orse decompositions of flows},
  author={Mischaikow, Konstantin and Mrozek, Marian and Weilandt, Frank},
  journal={arXiv preprint arXiv:1511.04426},
  year={2015}
}

@article{arai2009b,
  title={Recent development in rigorous computational methods in dynamical systems},
  author={Arai, Zin and Kokubu, Hiroshi and Pilarczyk, Pawe{\l}},
  journal={Japan journal of industrial and applied mathematics},
  volume={26},
  number={2-3},
  pages={393--417},
  year={2009},
  publisher={Springer}
}

@incollection{arai2013,
  title={Capturing the Global Behavior of Dynamical Systems with {Conley-M}orse Graphs},
  author={Arai, Zin and Kokubu, Hiroshi and Obayashi, Ippei},
  booktitle={Advances in Cognitive Neurodynamics (III)},
  pages={665--672},
  year={2013},
  publisher={Springer}
}

@article{brody2003,
  title={Basic mechanisms for graded persistent activity: {D}iscrete attractors, continuous attractors, and dynamic representations},
  author={Brody, Carlos D and Romo, Ranulfo and Kepecs, Adam},
  journal={Current opinion in neurobiology},
  volume={13},
  number={2},
  pages={204--211},
  year={2003},
  publisher={Elsevier}
}

@article{curto2019a,
  title={Fixed points of competitive threshold-linear networks},
  author={Curto, Carina and Geneson, Jesse and Morrison, Katherine},
  journal={Neural computation},
  volume={31},
  number={1},
  pages={94--155},
  year={2019},
  publisher={MIT Press}
}

@article{curto2019b,
  title={Relating network connectivity to dynamics: Opportunities and challenges for theoretical neuroscience},
  author={Curto, Carina and Morrison, Katherine},
  journal={Current opinion in neurobiology},
  volume={58},
  pages={11--20},
  year={2019},
  publisher={Elsevier}
}

@article{sompolinsky1988,
  title={Chaos in random neural networks},
  author={Sompolinsky, Haim and Crisanti, Andrea and Sommers, Hans-Jurgen},
  journal={Physical review letters},
  volume={61},
  number={3},
  pages={259},
  year={1988},
  publisher={APS}
}

@article{curto2019c,
  title={Stable fixed points of combinatorial threshold-linear networks},
  author={Curto, Carina and Geneson, Jesse and Morrison, Katherine},
  journal={arXiv preprint arXiv:1909.02947},
  year={2019}
}

@article{curto2019d,
  title={Robust motifs of threshold-linear networks},
  author={Curto, Carina and Langdon, Christopher and Morrison, Katherine},
  journal={arXiv preprint arXiv:1902.10270},
  year={2019}
}

@article{arai2009a,
  title={A database schema for the analysis of global dynamics of multiparameter systems},
  author={Arai, Zin and Kalies, William and Kokubu, Hiroshi and Mischaikow, Konstantin and Oka, Hiroe and Pilarczyk, Pawel},
  journal={SIAM Journal on Applied Dynamical Systems},
  volume={8},
  number={3},
  pages={757--789},
  year={2009},
  publisher={SIAM}
}

@article{arthur2010,
  title={Silicon-neuron design: A dynamical systems approach},
  author={Arthur, John V and Boahen, Kwabena A},
  journal={IEEE Transactions on Circuits and Systems I: Regular Papers},
  volume={58},
  number={5},
  pages={1034--1043},
  year={2010},
  publisher={IEEE}
}

@article{borisyuk1992,
  title={Bifurcation analysis of a neural network model},
  author={Borisyuk, Roman M and Kirillov, Alexandr B},
  journal={Biological Cybernetics},
  volume={66},
  number={4},
  pages={319--325},
  year={1992},
  publisher={Springer}
}

@article{bush2012,
  title={Combinatorial-topological framework for the analysis of global dynamics},
  author={Bush, Justin and Gameiro, Marcio and Harker, Shaun and Kokubu, Hiroshi and Mischaikow, Konstantin and Obayashi, Ippei and Pilarczyk, Pawe{\l}},
  journal={Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume={22},
  number={4},
  pages={047508},
  year={2012},
  publisher={American Institute of Physics}
}

@article{bush2014,
  title={Coarse dynamics for coarse modeling: An example from population biology},
  author={Bush, Justin and Mischaikow, Konstantin},
  journal={Entropy},
  volume={16},
  number={6},
  pages={3379--3400},
  year={2014},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{bauer2020,
  title={Combinatorial models of global dynamics: {L}earning cycling motion from data},
  author={Bauer, Ulrich and Hien, David and Junge, Oliver and Mischaikow, Konstantin and Snijders, Max},
  journal={arXiv preprint arXiv:2001.07066},
  year={2020}
}

@phdthesis{weilandt2015,
  title={Rigorous numerical computation of the {C}onley index for flows},
  author={Weilandt, Frank},
  year={2015},
  school={PhD thesis}
}

@article{mrozek2015,
  title={A topological approach to the algorithmic computation of the {C}onley index for {P}oincar{\'e} maps},
  author={Mrozek, Marian and Srzednicki, Roman and Weilandt, Frank},
  journal={SIAM Journal on Applied Dynamical Systems},
  volume={14},
  number={3},
  pages={1348--1386},
  year={2015},
  publisher={SIAM}
}

@article{bondanelli2018,
  title={Coding with transient trajectories in recurrent neural networks},
  author={Bondanelli, Giulio and Ostojic, Srdjan},
  journal={arXiv preprint arXiv:1811.07592},
  year={2018}
}

@book{eilenberg1952,
  title={Foundations of algebraic topology},
  author={Eilenberg, Samuel and Steenrod, Norman},
  volume={2193},
  year={1952},
  publisher={Princeton University Press}
}

@book{fiedler2002,
  title={Handbook of dynamical systems},
  author={Fiedler, Bernold},
  year={2002},
  publisher={Gulf Professional Publishing}
}

@article{seifert1934,
  title={A textbook of topology},
  author={Seifert, Herbert and Threlfall, William},
  year={1934},
  publisher={Academic Press}
}

@article{kao2019,
  title={Neuroscience out of control: Control-theoretic perspectives on neural circuit dynamics},
  author={Kao, Ta-Chu and Hennequin, Guillaume},
  journal={Current Opinion in Neurobiology},
  volume={58},
  pages={122--129},
  year={2019},
  publisher={Elsevier}
}

@article{hennequin2014,
  title={Optimal control of transient dynamics in balanced networks supports generation of complex movements},
  author={Hennequin, Guillaume and Vogels, Tim P and Gerstner, Wulfram},
  journal={Neuron},
  volume={82},
  number={6},
  pages={1394--1406},
  year={2014},
  publisher={Elsevier}
}

@article{kumar2016,
  title={Synchronization properties of coupled chaotic neurons: The role of random shared input},
  author={Kumar, Rupesh and Bilal, Shakir and Ramaswamy, Ram},
  journal={Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume={26},
  number={6},
  pages={063118},
  year={2016},
  publisher={AIP Publishing LLC}
}

@article{gauthier2019,
  title={From mental representations to neural codes: A multilevel approach},
  author={Gauthier, Jon and Loula, Jo{\~a}o and Pollock, Eli and Wilson, Tyler Brooke and Wong, Catherine},
  journal={Behavioral and Brain Sciences},
  volume={42},
  year={2019},
  publisher={Cambridge University Press}
}

@article{recanatesi2020,
  title={Metastable attractors explain the variable timing of stable behavioral action sequences},
  author={Recanatesi, Stefano and Pereira, Ulises and Murakami, Masayoshi and Mainen, Zachary and Mazzucato, Luca},
  journal={arXiv preprint arXiv:2001.09600},
  year={2020}
}

@article{gedeon2017,
  title={Global dynamics for steep nonlinearities in two dimensions},
  author={Gedeon, Tom{\'a}{\v{s}} and Harker, Shaun and Kokubu, Hiroshi and Mischaikow, Konstantin and Oka, Hiroe},
  journal={Physica D: Nonlinear Phenomena},
  volume={339},
  pages={18--38},
  year={2017},
  publisher={Elsevier}
}

@book{traub1991,
  title={Neuronal networks of the hippocampus},
  author={Traub, Roger D and Miles, Richard},
  volume={777},
  year={1991},
  publisher={Cambridge University Press}
}

@article{foss1996,
  title={Multistability and delayed recurrent loops},
  author={Foss, Jennifer and Longtin, Andr{\'e} and Mensour, Boualem and Milton, John},
  journal={Physical Review Letters},
  volume={76},
  number={4},
  pages={708},
  year={1996},
  publisher={APS}
}

@article{mischaikow1989,
  title={Transition systems},
  author={Mischaikow, Konstantin},
  journal={Proceedings of the Royal Society of Edinburgh Section A: Mathematics},
  volume={112},
  number={1-2},
  pages={155--175},
  year={1989},
  publisher={Royal Society of Edinburgh Scotland Foundation}
}

@article{mccord1988,
  title={The connection map for attractor-repeller pairs},
  author={McCord, Christopher},
  journal={Transactions of the American Mathematical Society},
  volume={307},
  number={1},
  pages={195--203},
  year={1988}
}

@article{strogatz2000,
  title={From {K}uramoto to {C}rawford: {E}xploring the onset of synchronization in populations of coupled oscillators},
  author={Strogatz, Steven H},
  journal={Physica D: Nonlinear Phenomena},
  volume={143},
  number={1-4},
  pages={1--20},
  year={2000},
  publisher={Elsevier}
}

@article{strogatz2001,
  title={Nonlinear dynamics and chaos: With applications to physics, biology, chemistry, and engineering (studies in nonlinearity)},
  author={Strogatz, Stephen},
  year={2001}
}

@book{strogatz2015,
  title={Nonlinear dynamics and chaos with student solutions manual: With applications to physics, biology, chemistry, and engineering},
  author={Strogatz, Steven H},
  year={2015},
  publisher={CRC press}
}

@phdthesis{mastrogiuseppe2017,
  title={From dynamics to computations in recurrent neural networks},
  author={Mastrogiuseppe, Francesca},
  year={2017},
  school={Paris Sciences et Lettres}
}

@article{mastrogiuseppe2018,
  title={Linking connectivity, dynamics, and computations in low-rank recurrent neural networks},
  author={Mastrogiuseppe, Francesca and Ostojic, Srdjan},
  journal={Neuron},
  volume={99},
  number={3},
  pages={609--623},
  year={2018},
  publisher={Elsevier}
}

@article{chand2016a,
  title={The salience network dynamics in perceptual decision-making},
  author={Chand, Ganesh B and Dhamala, Mukesh},
  journal={Neuroimage},
  volume={134},
  pages={85--93},
  year={2016},
  publisher={Elsevier}
}

@article{chand2016b,
  title={Face or house image perception: beta and gamma bands of oscillations in brain networks carry out decision-making},
  author={Chand, Ganesh B and Lamichhane, Bidhan and Dhamala, Mukesh},
  journal={Brain connectivity},
  volume={6},
  number={8},
  pages={621--631},
  year={2016},
  publisher={Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA}
}

@article{khambhati2015,
  title={Dynamic network drivers of seizure generation, propagation and termination in human neocortical epilepsy},
  author={Khambhati, Ankit N and Davis, Kathryn A and Oommen, Brian S and Chen, Stephanie H and Lucas, Timothy H and Litt, Brian and Bassett, Danielle S},
  journal={PLoS computational biology},
  volume={11},
  number={12},
  year={2015},
  publisher={Public Library of Science}
}

@article{whalen2015,
  title={Observability and controllability of nonlinear networks: The role of symmetry},
  author={Whalen, Andrew J and Brennan, Sean N and Sauer, Timothy D and Schiff, Steven J},
  journal={Physical Review X},
  volume={5},
  number={1},
  pages={011005},
  year={2015},
  publisher={APS}
}

@article{schiff2012,
  title={Neural control engineering},
  author={Schiff, Steven J},
  journal={Computational Neuroscience ed TJ Sejnowski and TA Poggio (Cambridge, MA: MIT Press)},
  year={2012}
}

@article{tang2017,
  title={Developmental increases in white matter network controllability support a growing diversity of brain dynamics},
  author={Tang, Evelyn and Giusti, Chad and Baum, Graham L and Gu, Shi and Pollock, Eli and Kahn, Ari E and Roalf, David R and Moore, Tyler M and Ruparel, Kosha and Gur, Ruben C and others},
  journal={Nature communications},
  volume={8},
  number={1},
  pages={1--16},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{gu2015,
  title={Controllability of structural brain networks},
  author={Gu, Shi and Pasqualetti, Fabio and Cieslak, Matthew and Telesford, Qawi K and Alfred, B Yu and Kahn, Ari E and Medaglia, John D and Vettel, Jean M and Miller, Michael B and Grafton, Scott T and others},
  journal={Nature communications},
  volume={6},
  number={1},
  pages={1--10},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{day2004,
  title={A rigorous numerical method for the global analysis of infinite-dimensional discrete dynamical systems},
  author={Day, Sarah and Junge, Oliver and Mischaikow, Konstantin},
  journal={SIAM Journal on Applied Dynamical Systems},
  volume={3},
  number={2},
  pages={117--160},
  year={2004},
  publisher={SIAM}
}

@article{day2005,
  title={Rigorous numerics for global dynamics: A study of the {S}wift--{H}ohenberg equation},
  author={Day, Sarah and Hiraoka, Yasuaki and Mischaikow, Konstantin and Ogawa, Toshiyuki},
  journal={SIAM Journal on Applied Dynamical Systems},
  volume={4},
  number={1},
  pages={1--31},
  year={2005},
  publisher={SIAM}
}

@misc{day2016,
  title={Topology in dynamics, differential equations, and data},
  author={Day, Sarah and Vandervorst, Robertus CAM and Wanner, Thomas},
  year={2016},
  publisher={Elsevier}
}

@article{day2019,
  title={Sofic shifts via {C}onley index theory: Computing lower bounds on recurrent dynamics for maps},
  author={Day, Sarah and Frongillo, Rafael},
  journal={SIAM Journal on Applied Dynamical Systems},
  volume={18},
  number={3},
  pages={1610--1642},
  year={2019},
  publisher={SIAM}
}

@article{maruyama2014,
  title={Analysis of chaotic oscillations induced in two coupled {W}ilson--{C}owan models},
  author={Maruyama, Yuya and Kakimoto, Yuta and Araki, Osamu},
  journal={Biological cybernetics},
  volume={108},
  number={3},
  pages={355--363},
  year={2014},
  publisher={Springer}
}

@article{guckenheimer2002,
  title={Chaos in the {H}odgkin--{H}uxley Model},
  author={Guckenheimer, John and Oliva, Ricardo A},
  journal={SIAM Journal on Applied Dynamical Systems},
  volume={1},
  number={1},
  pages={105--114},
  year={2002},
  publisher={SIAM}
}

@article{akhmet2017,
  title={Bifurcation analysis of {W}ilson-{C}owan model with singular impulses},
  author={Akhmet, Marat and {\c{C}}a{\u{g}}, Sabahattin},
  journal={arXiv preprint arXiv:1701.04015},
  year={2017}
}

@article{gils2015,
  title={Epilepsy: Computational Models},
  author={van Gils, Stephan and van Drongelen, Wim},
  journal={Encyclopedia of Computational Neuroscience},
  pages={1121--1134},
  year={2015},
  publisher={Springer}
}

@article{rahi2017,
  title={Oscillatory stimuli differentiate adapting circuit topologies},
  author={Rahi, Sahand Jamal and Larsch, Johannes and Pecani, Kresti and Katsov, Alexander Y and Mansouri, Nahal and Tsaneva-Atanasova, Krasimira and Sontag, Eduardo D and Cross, Frederick R},
  journal={Nature methods},
  volume={14},
  number={10},
  pages={1010},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{naud2008,
  title={Firing patterns in the adaptive exponential integrate-and-fire model},
  author={Naud, Richard and Marcille, Nicolas and Clopath, Claudia and Gerstner, Wulfram},
  journal={Biological cybernetics},
  volume={99},
  number={4-5},
  pages={335},
  year={2008},
  publisher={Springer}
}

@book{conley1978,
  title={Isolated invariant sets and the {M}orse index},
  author={Conley, Charles C},
  number={38},
  year={1978},
  publisher={American Mathematical Soc.}
}

@incollection{forman1995,
  title={A discrete {M}orse theory for cell complexes},
  author={Forman, Robin},
  booktitle={Geometry, Topology \& Physics for Raoul Bott},
  year={1995},
  editor={Yau, S. T.},
  publisher={International Press}
}

@misc{forman1998,
  title={{M}orse theory for cell complexes},
  author={Forman, Robin},
  year={1998}
}

@book{gedeon1998,
  title={Cyclic feedback systems},
  author={Gedeon, Tom{\'a}{\v{s}}},
  volume={637},
  year={1998},
  publisher={American Mathematical Soc.}
}

@book{kaczynski2006,
  title={Computational homology},
  author={Kaczynski, Tomasz and Mischaikow, Konstantin and Mrozek, Marian},
  volume={157},
  year={2006},
  publisher={Springer Science \& Business Media}
}

@misc{kalies2006,
  title={Computational homology project},
  author={Kalies, W and Mrozek, M and Pilarczyk, P},
  year={2006}
}

@article{kokubu2014,
    year={2014},
  title={A User’s Guide to the {C}onley-{M}orse Database},
  author={Kokubu, Hiroshi and Spendlove, Kelly}
}

@article{ashwin2016,
  title={Mathematical frameworks for oscillatory network dynamics in neuroscience},
  author={Ashwin, Peter and Coombes, Stephen and Nicks, Rachel},
  journal={The Journal of Mathematical Neuroscience},
  volume={6},
  number={1},
  pages={2},
  year={2016},
  publisher={SpringerOpen}
}

@article{korn2003,
  title={Is there chaos in the brain? {II}. {E}xperimental evidence and related models},
  author={Korn, Henri and Faure, Philippe},
  journal={Comptes rendus biologies},
  volume={326},
  number={9},
  pages={787--840},
  year={2003},
  publisher={Elsevier}
}

@article{rabinovich1998,
  title={The role of chaos in neural systems},
  author={Rabinovich, MI and Abarbanel, HDI},
  journal={Neuroscience},
  volume={87},
  number={1},
  pages={5--14},
  year={1998},
  publisher={Elsevier}
}

@article{rabinovich2006,
  title={Dynamical principles in neuroscience},
  author={Rabinovich, Mikhail I and Varona, Pablo and Selverston, Allen I and Abarbanel, Henry DI},
  journal={Reviews of modern physics},
  volume={78},
  number={4},
  pages={1213},
  year={2006},
  publisher={APS}
}

@article{rabinovich2008,
  title={Transient cognitive dynamics, metastability, and decision making},
  author={Rabinovich, Mikhail I and Huerta, Ram{\'o}n and Varona, Pablo and Afraimovich, Valentin S},
  journal={PLoS computational biology},
  volume={4},
  number={5},
  pages={e1000072},
  year={2008},
  publisher={Public Library of Science}
}

@article{rabinovich2011,
  title={Robust transient dynamics and brain functions},
  author={Rabinovich, Mikhail I and Varona, Pablo},
  journal={Frontiers in computational neuroscience},
  volume={5},
  pages={24},
  year={2011},
  publisher={Frontiers}
}

@book{kantz2004,
  title={Nonlinear time series analysis},
  author={Kantz, Holger and Schreiber, Thomas},
  volume={7},
  year={2004},
  publisher={Cambridge university press}
}

@article{jin2002,
  title={Fast convergence of spike sequences to periodic patterns in recurrent networks},
  author={Jin, Dezhe Z},
  journal={Physical review letters},
  volume={89},
  number={20},
  pages={208102},
  year={2002},
  publisher={APS}
}

@article{huerta2004,
  title={Reproducible sequence generation in random neural ensembles},
  author={Huerta, Ram{\'o}n and Rabinovich, Mikhail},
  journal={Physical review letters},
  volume={93},
  number={23},
  pages={238104},
  year={2004},
  publisher={APS}
}

@book{hebb1949,
  title={The organization of behavior: A Neuropsychological Theory},
  author={Hebb, Donald O},
  year={1949},
  publisher={New York: Wiley and Sons}
}

@article{szekely1965,
  title={LOGICAL NETWORK FOR CONTROLLING LIMB MOVEMENTS IN {U}RODELA},
  author={Sz{\'e}kely, G},
  journal={Acta physiologica Academiae Scientiarum Hungaricae},
  volume={27},
  pages={285},
  year={1965}
}

@book{wiggins1994,
  title={Normally hyperbolic invariant manifolds in dynamical systems},
  author={Wiggins, Stephen},
  volume={105},
  year={1994},
  publisher={Springer Science \& Business Media}
}

@book{wiggins2003,
  title={Introduction to applied nonlinear dynamical systems and chaos},
  author={Wiggins, Stephen},
  volume={2},
  year={2003},
  publisher={Springer Science \& Business Media}
}

@article{salomon1985,
  title={Connected simple systems and {C}onley index of isolated invariant set},
  author={Salomon, D},
  journal={Trans. Am. Math. Soc.},
  volume={291},
  pages={1--41},
  year={1985}
}

@article{eilenberg1953,
  title={On products of complexes},
  author={Eilenberg, Samuel and Zilber, Joseph A},
  journal={American Journal of Mathematics},
  volume={75},
  number={1},
  pages={200--204},
  year={1953},
  publisher={JSTOR}
}

@article{franzosa1986,
  title={Index filtrations and the homology index braid for partially ordered {M}orse decompositions},
  author={Franzosa, Robert},
  journal={Transactions of the American Mathematical Society},
  volume={298},
  number={1},
  pages={193--213},
  year={1986}
}

@article{franzosa1988a,
  title={The continuation theory for {M}orse decompositions and connection matrices},
  author={Franzosa, Robert D},
  journal={Transactions of the American Mathematical Society},
  volume={310},
  number={2},
  pages={781--803},
  year={1988}
}

@article{franzosa1988b,
  title={The connection matrix theory for semiflows on (not necessarily locally compact) metric spaces},
  author={Franzosa, Robert D and Mischaikow, Konstantin},
  journal={Journal of Differential Equations},
  volume={71},
  number={2},
  pages={270--287},
  year={1988},
  publisher={Elsevier}
}

@article{franzosa1989,
  title={The connection matrix theory for {M}orse decompositions},
  author={Franzosa, Robert D},
  journal={Transactions of the American Mathematical Society},
  volume={311},
  number={2},
  pages={561--592},
  year={1989}
}

@article{mccord1992,
  title={Connected simple systems, transition matrices, and heteroclinic bifurcations},
  author={McCord, Christopher and Mischaikow, Konstantin},
  journal={Transactions of the American Mathematical Society},
  volume={333},
  number={1},
  pages={397--422},
  year={1992}
}

@article{sompolinsky1986,
  title={Temporal association in asymmetric neural networks},
  author={Sompolinsky, Haim and Kanter, I},
  journal={Physical review letters},
  volume={57},
  number={22},
  pages={2861},
  year={1986},
  publisher={APS}
}

@article{litwin2019,
  title={Constraining computational models using electron microscopy wiring diagrams},
  author={Litwin-Kumar, Ashok and Turaga, Srinivas C},
  journal={Current opinion in neurobiology},
  volume={58},
  pages={94--100},
  year={2019},
  publisher={Elsevier}
}

@article{stiefel2016,
  title={Neurons as oscillators},
  author={Stiefel, Klaus M and Ermentrout, G Bard},
  journal={Journal of neurophysiology},
  volume={116},
  number={6},
  pages={2950--2960},
  year={2016},
  publisher={American Physiological Society Bethesda, MD}
}

@article{sauer1994,
  title={Reconstruction of dynamical systems from interspike intervals},
  author={Sauer, Tim},
  journal={Physical Review Letters},
  volume={72},
  number={24},
  pages={3811},
  year={1994},
  publisher={APS}
}

@article{so1998,
  title={Periodic orbits: A new language for neuronal dynamics},
  author={So, Paul and Francis, Joseph T and Netoff, Theoden I and Gluckman, Bruce J and Schiff, Steven J},
  journal={Biophysical journal},
  volume={74},
  number={6},
  pages={2776--2785},
  year={1998},
  publisher={Elsevier}
}

@incollection{venkadesh2017,
  title={Computational Modeling as a Means to Defining Neuronal Spike Pattern Behaviors},
  author={Venkadesh, Siva and Ascoli, Giorgio A},
  booktitle={Mathematical and Theoretical Neuroscience},
  pages={25--43},
  year={2017},
  publisher={Springer}
}

@inproceedings{huang2009,
  title={Cancer attractors: A systems view of tumors from a gene network dynamics and developmental perspective},
  author={Huang, Sui and Ernberg, Ingemar and Kauffman, Stuart},
  booktitle={Seminars in cell \& developmental biology},
  volume={20},
  number={7},
  pages={869--876},
  year={2009},
  organization={Elsevier}
}

@article{touboul2008,
  title={Dynamics and bifurcations of the adaptive exponential integrate-and-fire model},
  author={Touboul, Jonathan and Brette, Romain},
  journal={Biological cybernetics},
  volume={99},
  number={4-5},
  pages={319},
  year={2008},
  publisher={Springer}
}

@article{ueta2003,
  title={On synchronization and control of coupled {W}ilson--{C}owan neural oscillators},
  author={Ueta, Tetsushi and Chen, Guanrong},
  journal={International Journal of Bifurcation and Chaos},
  volume={13},
  number={01},
  pages={163--175},
  year={2003},
  publisher={World Scientific}
}

@article{haken1983,
  title={At least one {L}yapunov exponent vanishes if the trajectory of an attractor does not contain a fixed point},
  author={Haken, H},
  journal={Physics Letters A},
  volume={94},
  number={2},
  pages={71--72},
  year={1983},
  publisher={Elsevier}
}

@article{jawarneh2019,
  title={{C}onley index methods detecting bifurcations in a modified {V}an der {Pol} oscillator appearing in heart action models},
  author={Jawarneh, Ibrahim and Staffeldt, Ross},
  journal={arXiv preprint arXiv:1901.11180},
  year={2019}
}

@article{mccord1995,
  title={Zeta functions, periodic trajectories, and the {C}onley index},
  author={McCord, Christopher and Mischaikow, Konstantin and Mrozek, Marian},
  journal={Journal of differential equations},
  volume={121},
  number={2},
  pages={258--292},
  year={1995},
  publisher={Elsevier}
}

@article{conley1971,
  title={Isolated invariant sets and isolating blocks},
  author={Conley, Charles C and Easton, Robert},
  journal={Transactions of the American Mathematical Society},
  volume={158},
  number={1},
  pages={35--61},
  year={1971},
  publisher={JSTOR}
}

@inproceedings{pilarczyk2003,
  title={Topological-numerical approach to the existence of periodic trajectories in {ODE}'s},
  author={Pilarczyk, Pawe{\l}},
  booktitle={Conference Publications},
  volume={2003},
  number={Special},
  pages={701},
  year={2003},
  organization={American Institute of Mathematical Sciences}
}

@article{wolf1985,
  title={Determining {L}yapunov exponents from a time series},
  author={Wolf, Alan and Swift, Jack B and Swinney, Harry L and Vastano, John A},
  journal={Physica D: Nonlinear Phenomena},
  volume={16},
  number={3},
  pages={285--317},
  year={1985},
  publisher={Elsevier}
}

@article{henon1982,
  title={On the numerical computation of Poincar{\'e} maps},
  author={H{\'e}non, Michel and others},
  year={1982}
}

@article{oseledec1968,
  title={A Multiplicative Ergodic Theorem, Characteristic {L}yapnov Exponents of Dynamical Systems (Transactions of the {M}oscow Mathematical Society, 19)},
  author={Oseledec, V},
  journal={American Mathematical Society, Providence, RI},
  year={1968}
}

@article{tucker2002,
  title={Computing accurate Poincar{\'e} maps},
  author={Tucker, Warwick},
  journal={Physica D: Nonlinear Phenomena},
  volume={171},
  number={3},
  pages={127--137},
  year={2002},
  publisher={Elsevier}
}

@incollection{manneville1995,
  title={Dissipative structures and weak turbulence},
  author={Manneville, Paul},
  booktitle={Chaos—The Interplay Between Stochastic and Deterministic Behaviour},
  pages={257--272},
  year={1995},
  publisher={Springer}
}

@article{wilson2019a,
  title={Hyperchaos in {W}ilson--{C}owan oscillator circuits},
  author={Wilson, Hugh R},
  journal={Journal of neurophysiology},
  volume={122},
  number={6},
  pages={2449--2457},
  year={2019},
  publisher={American Physiological Society Bethesda, MD}
}

@article{wilson2019b,
  title={Voluntary generation of hyperchaotic visuo-motor patterns},
  author={Wilson, Hugh R},
  journal={Scientific reports},
  volume={9},
  number={1},
  pages={1--6},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{muldoon2016,
  title={Stimulation-based control of dynamic brain networks},
  author={Muldoon, Sarah Feldt and Pasqualetti, Fabio and Gu, Shi and Cieslak, Matthew and Grafton, Scott T and Vettel, Jean M and Bassett, Danielle S},
  journal={PLoS computational biology},
  volume={12},
  number={9},
  year={2016},
  publisher={Public Library of Science}
}

@article{aljadeff2016,
  title={Low-dimensional dynamics of structured random networks},
  author={Aljadeff, Johnatan and Renfrew, David and Vegu{\'e}, Marina and Sharpee, Tatyana O},
  journal={Physical Review E},
  volume={93},
  number={2},
  pages={022302},
  year={2016},
  publisher={APS}
}

@article{amit1985,
  title={Spin-glass models of neural networks},
  author={Amit, Daniel J and Gutfreund, Hanoch and Sompolinsky, Haim},
  journal={Physical Review A},
  volume={32},
  number={2},
  pages={1007},
  year={1985},
  publisher={APS}
}

@inproceedings{ritt2015,
  title={Neurocontrol: Methods, models and technologies for manipulating dynamics in the brain},
  author={Ritt, Jason T and Ching, ShiNung},
  booktitle={2015 American Control Conference (ACC)},
  pages={3765--3780},
  year={2015},
  organization={IEEE}
}

@article{amit1987,
  title={Information storage in neural networks with low levels of activity},
  author={Amit, Daniel J and Gutfreund, Hanoch and Sompolinsky, Haim},
  journal={Physical Review A},
  volume={35},
  number={5},
  pages={2293},
  year={1987},
  publisher={APS}
}

@book{amit1992,
  title={Modeling brain function: The world of attractor neural networks},
  author={Amit, Daniel J and Amit, Daniel J},
  year={1992},
  publisher={Cambridge university press}
}

@article{deneve2016,
  title={Efficient codes and balanced networks},
  author={Den{\`e}ve, Sophie and Machens, Christian K},
  journal={Nature neuroscience},
  volume={19},
  number={3},
  pages={375},
  year={2016},
  publisher={Nature Publishing Group}
}

@inproceedings{nozari2019,
  title={Oscillations and coupling in interconnections of two-dimensional brain networks},
  author={Nozari, Erfan and Cort{\'e}s, Jorge},
  booktitle={2019 American Control Conference (ACC)},
  pages={193--198},
  year={2019},
  organization={IEEE}
}

@article{spendlove2019,
  title={A COMPUTATIONAL FRAMEWORK FOR CONNECTION MATRIX THEORY},
  author={Spendlove, Kelly and Harker, Shaun and Mischaikow, Konstantin and Vandervorst, Rob},
  year={2019}
}

@article{harker2018,
  title={A Computational Framework for the Connection Matrix Theory},
  author={Harker, Shaun and Mischaikow, Konstantin and Spendlove, Kelly},
  journal={arXiv preprint arXiv:1810.04552},
  year={2018}
}

@article{milnor1975,
  title={Characteristic classes},
  author={Milnor, John W and Stasheff, James D},
  journal={Annals of mathematics studies},
  volume={76},
  year={1975}
}

@article{bott1982,
  title={Differential Forms in Algebraic Topology},
  author={BOTT, R},
  journal={Grad. Texts in Math.},
  volume={82},
  year={1982},
  publisher={Springer-Verlag}
}

@article{abbott2016,
  title={Building functional networks of spiking model neurons},
  author={Abbott, Larry F and DePasquale, Brian and Memmesheimer, Raoul-Martin},
  journal={Nature neuroscience},
  volume={19},
  number={3},
  pages={350},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{arai2014a,
  title={The effects of theta precession on spatial learning and simplicial complex dynamics in a topological model of the hippocampal spatial map},
  author={Arai, Mamiko and Brandt, Vicky and Dabaghian, Yuri},
  journal={PLoS computational biology},
  volume={10},
  number={6},
  pages={e1003651},
  year={2014},
  publisher={Public Library of Science}
}

@incollection{arai2014b,
  title={Decomposition and Clustering for the Visualization of Dynamical Systems},
  author={Arai, Zin},
  booktitle={Mathematical Progress in Expressive Image Synthesis I},
  pages={13--20},
  year={2014},
  publisher={Springer}
}

@article{aurell2012,
  title={Inverse Ising inference using all the data},
  author={Aurell, Erik and Ekeberg, Magnus},
  journal={Physical review letters},
  volume={108},
  number={9},
  pages={090201},
  year={2012},
  publisher={APS}
}

@article{averbeck2006,
  title={Neural correlations, population coding and computation},
  author={Averbeck, Bruno B and Latham, Peter E and Pouget, Alexandre},
  journal={Nature reviews neuroscience},
  volume={7},
  number={5},
  pages={358},
  year={2006},
  publisher={Nature Publishing Group}
}

@article{bassett2018a,
  title={Self-organized attractor dynamics in the developing head direction circuit},
  author={Bassett, Joshua P and Wills, Thomas J and Cacucci, Francesca},
  journal={Current Biology},
  volume={28},
  number={4},
  pages={609--615},
  year={2018},
  publisher={Elsevier}
}

@article{bassett2018b,
  title={On the nature and use of models in network neuroscience},
  author={Bassett, Danielle S and Zurn, Perry and Gold, Joshua I},
  journal={Nature Reviews Neuroscience},
  volume={19},
  number={9},
  pages={566--578},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{salnikov2018,
  title={Simplicial complexes and complex systems},
  author={Salnikov, Vsevolod and Cassese, Daniele and Lambiotte, Renaud},
  journal={European Journal of Physics},
  volume={40},
  number={1},
  pages={014001},
  year={2018},
  publisher={IOP Publishing}
}

@article{basso2016,
  title={Gamma synchronization of the hippocampal spatial map---{T}opological model},
  author={Basso, Edward and Arai, Mamiko and Dabaghian, Yuri},
  journal={arXiv preprint arXiv:1603.06248},
  year={2016}
}

@article{battaglia1998,
  title={Attractor neural networks storing multiple space representations: {A} model for hippocampal place fields},
  author={Battaglia, Francesco P and Treves, Alessandro},
  journal={Physical Review E},
  volume={58},
  number={6},
  pages={7738},
  year={1998},
  publisher={APS}
}

@article{becker1997,
  title={The topology of multidimensional potential energy surfaces: Theory and application to peptide structure and kinetics},
  author={Becker, Oren M and Karplus, Martin},
  journal={The Journal of chemical physics},
  volume={106},
  number={4},
  pages={1495--1517},
  year={1997},
  publisher={AIP}
}

@inproceedings{bengio2004,
  title={Out-of-sample extensions for {LLE}, {ISOMAP}, {MDS}, eigenmaps, and spectral clustering},
  author={Bengio, Yoshua and Paiement, Jean-fran{\c{c}}cois and Vincent, Pascal and Delalleau, Olivier and Roux, Nicolas L and Ouimet, Marie},
  booktitle={Advances in neural information processing systems},
  pages={177--184},
  year={2004}
}

@article{bernacchia2007,
  title={Continuous or discrete attractors in neural circuits? {A} self-organized switch at maximal entropy},
  author={Bernacchia, Alberto},
  journal={arXiv preprint arXiv:0707.3511},
  year={2007}
}

@article{binchi2014,
  title={{jHoles}: A tool for understanding biological complex networks via clique weight rank persistent homology},
  author={Binchi, Jacopo and Merelli, Emanuela and Rucco, Matteo and Petri, Giovanni and Vaccarino, Francesco},
  journal={Electronic Notes in Theoretical Computer Science},
  volume={306},
  pages={5--18},
  year={2014},
  publisher={Elsevier}
}

@book{bishop2006,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M},
  year={2006},
  publisher={springer}
}

@book{hale1996,
  title={Dynamics and bifurcations},
  author={Hale, Jack K and Ko{\c{c}}ak, H{\"u}seyin},
  volume={3},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{chen2011b,
  title={Morse set classification and hierarchical refinement using {C}onley index},
  author={Chen, Guoning and Deng, Qingqing and Szymczak, Andrzej and Laramee, Robert S and Zhang, Eugene},
  journal={IEEE transactions on visualization and computer graphics},
  volume={18},
  number={5},
  pages={767--782},
  year={2011},
  publisher={IEEE}
}

@inproceedings{heine2016,
  title={A survey of topology-based methods in visualization},
  author={Heine, Christian and Leitte, Heike and Hlawitschka, Mario and Iuricich, Federico and De Floriani, Leila and Scheuermann, Gerik and Hagen, Hans and Garth, Christoph},
  booktitle={Computer Graphics Forum},
  volume={35},
  number={3},
  pages={643--667},
  year={2016},
  organization={Wiley Online Library}
}

@article{pilarczyk1999,
  title={Computer assisted method for proving existence of periodic orbits},
  author={Pilarczyk, Pawe{\l} and others},
  journal={Topological Methods in Nonlinear Analysis},
  volume={13},
  number={2},
  pages={365--377},
  year={1999},
  publisher={Juliusz P. Schauder Centre for Nonlinear Studies}
}

@article{boccaletti2014,
  title={The structure and dynamics of multilayer networks},
  author={Boccaletti, Stefano and Bianconi, Ginestra and Criado, Regino and Del Genio, Charo I and G{\'o}mez-Gardenes, Jes{\'u}s and Romance, Miguel and Sendina-Nadal, Irene and Wang, Zhen and Zanin, Massimiliano},
  journal={Physics Reports},
  volume={544},
  number={1},
  pages={1--122},
  year={2014},
  publisher={Elsevier}
}

@book{massey1991,
  title={A basic course in algebraic topology},
  author={Massey, William S},
  volume={127},
  year={2019},
  publisher={Springer}
}

@article{li2007,
  title={Morse decompositions for general dynamical systems and differential inclusions with applications to control systems},
  author={Li, Desheng},
  journal={SIAM Journal on Control and Optimization},
  volume={46},
  number={1},
  pages={35--60},
  year={2007},
  publisher={SIAM}
}

@article{spiegler2011,
  title={Modeling brain resonance phenomena using a neural mass model},
  author={Spiegler, Andreas and Kn{\"o}sche, Thomas R and Schwab, Karin and Haueisen, Jens and Atay, Fatihcan M},
  journal={PLoS computational biology},
  volume={7},
  number={12},
  year={2011},
  publisher={Public Library of Science}
}

@article{souza2012a,
  title={On {M}orse decompositions of control systems},
  author={Souza, Josiney A},
  journal={International journal of control},
  volume={85},
  number={7},
  pages={815--821},
  year={2012},
  publisher={Taylor \& Francis}
}

@article{souza2012b,
  title={Complete {L}yapunov functions of control systems},
  author={Souza, Josiney A},
  journal={Systems \& control letters},
  volume={61},
  number={2},
  pages={322--326},
  year={2012},
  publisher={Elsevier}
}

@incollection{bechtel2011,
  title={Complex biological mechanisms: {C}yclic, oscillatory, and autonomous},
  author={Bechtel, William and Abrahamsen, Adele},
  booktitle={Philosophy of complex systems},
  pages={257--285},
  year={2011},
  publisher={Elsevier}
}

@book{souza2020b,
  title={The Dynamic {M}orse Theory of Control Systems},
  author={Souza, Josiney A},
  year={2020},
  publisher={Cambridge Scholars Publishing}
}

@article{souza2020a,
  title={Poisson Stability and Periodicity of Control Affine Systems},
  author={Souza, Josiney A},
  journal={Journal of Optimization Theory and Applications},
  pages={1--8},
  year={2020},
  publisher={Springer}
}

@article{borg2003,
  title={Modern multidimensional scaling: Theory and applications},
  author={Borg, Ingwer and Groenen, Patrick},
  journal={Journal of Educational Measurement},
  volume={40},
  number={3},
  pages={277--280},
  year={2003},
  publisher={Wiley Online Library}
}

@article{boucheny2005continuous,
  title={A continuous attractor network model without recurrent excitation: {M}aintenance and integration in the head direction cell system},
  author={Boucheny, Christian and Brunel, Nicolas and Arleo, Angelo},
  journal={Journal of computational neuroscience},
  volume={18},
  number={2},
  pages={205--227},
  year={2005},
  publisher={Springer}
}

@article{dayan2001,
  title={Theoretical neuroscience: {C}omputational and mathematical modeling of neural systems},
  author={Dayan, Peter and Abbott, Laurence F},
  year={2001},
  publisher={MIT press}
}

@article{fasoli2016,
  title={The complexity of dynamics in small neural circuits},
  author={Fasoli, Diego and Cattani, Anna and Panzeri, Stefano},
  journal={PLoS computational biology},
  volume={12},
  number={8},
  year={2016},
  publisher={Public Library of Science}
}

@incollection{fasoli2017,
  title={Bifurcation Analysis of a Sparse Neural Network with Cubic Topology},
  author={Fasoli, Diego and Cattani, Anna and Panzeri, Stefano},
  booktitle={Mathematical and Theoretical Neuroscience},
  pages={87--98},
  year={2017},
  publisher={Springer}
}

@article{fasoli2018,
  title={Pattern storage, bifurcations, and groupwise correlation structure of an exactly solvable asymmetric neural network model},
  author={Fasoli, Diego and Cattani, Anna and Panzeri, Stefano},
  journal={Neural computation},
  volume={30},
  number={5},
  pages={1258--1295},
  year={2018},
  publisher={MIT Press}
}

@article{broderick2007,
  title={Faster solutions of the inverse pairwise {I}sing problem},
  author={Broderick, Tamara and Dudik, Miroslav and Tkacik, Ga{\v{s}}per and Schapire, Robert E and Bialek, William},
  journal={arXiv preprint arXiv:0712.2437},
  year={2007}
}

@article{burgess2002,
  title={The human hippocampus and spatial and episodic memory},
  author={Burgess, Neil and Maguire, Eleanor A and O'Keefe, John},
  journal={Neuron},
  volume={35},
  number={4},
  pages={625--641},
  year={2002},
  publisher={Elsevier}
}

@article{butts2006,
  title={Tuning curves, neuronal variability, and sensory coding},
  author={Butts, Daniel A and Goldman, Mark S},
  journal={PLoS biology},
  volume={4},
  number={4},
  pages={e92},
  year={2006},
  publisher={Public Library of Science}
}

@article{cang2018,
  title={Evolutionary homology on coupled dynamical systems},
  author={Cang, Zixuan and Munch, Elizabeth and Wei, Guo-Wei},
  journal={arXiv preprint arXiv:1802.04677},
  year={2018}
}

@article{carlsson2009,
  title={Topology and data},
  author={Carlsson, Gunnar},
  journal={Bulletin of the American Mathematical Society},
  volume={46},
  number={2},
  pages={255--308},
  year={2009}
}

@article{carpenter2015,
  title={Grid cells form a global representation of connected environments},
  author={Carpenter, Francis and Manson, Daniel and Jeffery, Kate and Burgess, Neil and Barry, Caswell},
  journal={Current Biology},
  volume={25},
  number={9},
  pages={1176--1182},
  year={2015},
  publisher={Elsevier}
}

@book{katok1997,
  title={Introduction to the modern theory of dynamical systems},
  author={Katok, Anatole and Hasselblatt, Boris},
  volume={54},
  year={1997},
  publisher={Cambridge university press}
}

@article{carstens2017,
  title={Topology of complex networks: {M}odels and analysis},
  author={Carstens, Corrie Jacobien},
  journal={Bulletin of the Australian Mathematical Society},
  volume={95},
  number={2},
  pages={347--349},
  year={2017},
  publisher={Cambridge University Press}
}

@inproceedings{chazal2009,
  title={Proximity of persistence modules and their diagrams},
  author={Chazal, Fr{\'e}d{\'e}ric and Cohen-Steiner, David and Glisse, Marc and Guibas, Leonidas J and Oudot, Steve Y},
  booktitle={Proceedings of the twenty-fifth annual symposium on Computational geometry},
  pages={237--246},
  year={2009},
  organization={ACM}
}

@article{chen2012,
  title={Uncovering spatial topology represented by rat hippocampal population neuronal codes},
  author={Chen, Zhe and Kloosterman, Fabian and Brown, Emery N and Wilson, Matthew A},
  journal={Journal of Computational Neuroscience},
  volume={33},
  number={2},
  pages={227--255},
  year={2012},
  publisher={Springer}
}

@article{chen2013,
  title={An overview of bayesian methods for neural spike train analysis},
  author={Chen, Zhe},
  journal={Computational intelligence and neuroscience},
  volume={2013},
  pages={1},
  year={2013},
  publisher={Hindawi Publishing Corp.}
}

@article{chen2014,
  title={Neural representation of spatial topology in the rodent hippocampus},
  author={Chen, Zhe and Gomperts, Stephen N and Yamamoto, Jun and Wilson, Matthew A},
  journal={Neural computation},
  volume={26},
  number={1},
  pages={1--39},
  year={2014},
  publisher={MIT Press}
}

@inproceedings{chen2017,
  title={Unfolding representations of trajectory coding in neuronal population spike activity},
  author={Chen, Zhe},
  booktitle={2017 51st Annual Conference on Information Sciences and Systems (CISS)},
  pages={1--6},
  year={2017},
  organization={IEEE}
}

@article{chen2019,
  title={Searching for collective behavior in a small brain},
  author={Chen, Xiaowen and Randi, Francesco and Leifer, Andrew M and Bialek, William},
  journal={Physical Review E},
  volume={99},
  number={5},
  pages={052418},
  year={2019},
  publisher={APS}
}

@article{cocco2018,
  title={Statistical physics and representations in real and artificial neural networks},
  author={Cocco, Simona and Monasson, R{\'e}mi and Posani, Lorenzo and Rosay, Sophie and Tubiana, J{\'e}r{\^o}me},
  journal={Physica A: Statistical Mechanics and its Applications},
  volume={504},
  pages={45--76},
  year={2018},
  publisher={Elsevier}
}

@article{kappos1995,
  title={The {C}onley index and global bifurcations {I}: Concepts and theory},
  author={Kappos, Efthimios},
  journal={International Journal of Bifurcation and Chaos},
  volume={5},
  number={04},
  pages={937--953},
  year={1995},
  publisher={World Scientific}
}

@article{eckmann1995,
  title={Recurrence plots of dynamical systems},
  author={Eckmann, JP and Kamphorst, S Oliffson and Ruelle, D and others},
  journal={World Scientific Series on Nonlinear Science Series A},
  volume={16},
  pages={441--446},
  year={1995},
  publisher={WORLD SCIENTIFIC PUBLISHING}
}

@software{scholzel2019,
  author       = {Schölzel, Christopher},
  title        = {Nonlinear measures for dynamical systems},
  month        = jun,
  year         = 2019,
  publisher    = {Zenodo},
  version      = {0.5.2},
  doi          = {10.5281/zenodo.3814723},
  url          = {https://doi.org/10.5281/zenodo.3814723}
}

@book{gevins1986,
  title={Handbook of Electroencephalography and Clinical Neurophysiology: Revised Series},
  author={Gevins, Alan S and R{\'e}mond, Antoine},
  year={1986},
  publisher={Elsevier}
}

@article{cohen2011,
  title={Measuring and interpreting neuronal correlations},
  author={Cohen, Marlene R and Kohn, Adam},
  journal={Nature neuroscience},
  volume={14},
  number={7},
  pages={811},
  year={2011},
  publisher={Nature Publishing Group}
}

@article{couey2013,
  title={Recurrent inhibitory circuitry as a mechanism for grid formation},
  author={Couey, Jonathan J and Witoelar, Aree and Zhang, Sheng-Jia and Zheng, Kang and Ye, Jing and Dunn, Benjamin and Czajkowski, Rafal and Moser, May-Britt and Moser, Edvard I and Roudi, Yasser and others},
  journal={Nature neuroscience},
  volume={16},
  number={3},
  pages={318},
  year={2013},
  publisher={Nature Publishing Group}
}

@book{cox2000,
  title={Multidimensional scaling},
  author={Cox, Trevor F and Cox, Michael AA},
  year={2000},
  publisher={Chapman and hall/CRC}
}

@article{chaudhuri2016,
  title={Computational principles of memory},
  author={Chaudhuri, Rishidev and Fiete, Ila},
  journal={Nature neuroscience},
  volume={19},
  number={3},
  pages={394},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{chaudhuri2019,
  title={The intrinsic attractor manifold and population dynamics of a canonical cognitive circuit across waking and sleep},
  author={Chaudhuri, Rishidev and Ger{\c{c}}ek, Berk and Pandey, Biraj and Peyrache, Adrien and Fiete, Ila},
  journal={Nature neuroscience},
  pages={1--9},
  year={2019},
  publisher={Nature Publishing Group}
}

@inproceedings{chowdhury2018,
  title={Persistent path homology of directed networks},
  author={Chowdhury, Samir and M{\'e}moli, Facundo},
  booktitle={Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms},
  pages={1152--1169},
  year={2018},
  organization={SIAM}
}

@article{collins2002,
  title={Logistic regression, {AdaB}oost and {B}regman distances},
  author={Collins, Michael and Schapire, Robert E and Singer, Yoram},
  journal={Machine Learning},
  volume={48},
  number={1-3},
  pages={253--285},
  year={2002},
  publisher={Springer}
}

@article{cunningham2014,
  title={Dimensionality reduction for large-scale neural recordings},
  author={Cunningham, John P and Byron, M Yu},
  journal={Nature neuroscience},
  volume={17},
  number={11},
  pages={1500},
  year={2014},
  publisher={Nature Publishing Group}
}

@article{curto2008,
  title={Cell groups reveal structure of stimulus space},
  author={Curto, Carina and Itskov, Vladimir},
  journal={PLoS computational biology},
  volume={4},
  number={10},
  pages={e1000205},
  year={2008},
  publisher={Public Library of Science}
}

@article{curto2013a,
  title={Combinatorial neural codes from a mathematical coding theory perspective},
  author={Curto, Carina and Itskov, Vladimir and Morrison, Katherine and Roth, Zachary and Walker, Judy L},
  journal={Neural computation},
  volume={25},
  number={7},
  pages={1891--1925},
  year={2013},
  publisher={MIT Press}
}

@article{curto2013b,
  title={The neural ring: {A}n algebraic tool for analyzing the intrinsic structure of neural codes},
  author={Curto, Carina and Itskov, Vladimir and Veliz-Cuba, Alan and Youngs, Nora},
  journal={Bulletin of mathematical biology},
  volume={75},
  number={9},
  pages={1571--1611},
  year={2013},
  publisher={Springer}
}

@article{curto2015,
  title={Clique topology of real symmetric matrices},
  author={Curto, Carina and Giusti, Chad and Itskov, Vladimir},
  journal={arXiv preprint arXiv:1502.06173},
  year={2015}
}


@article{curto2017a,
  title={What can topology tell us about the neural code?},
  author={Curto, Carina},
  journal={Bulletin of the American Mathematical Society},
  volume={54},
  number={1},
  pages={63--78},
  year={2017}
}

@article{curto2017b,
  title={What makes a neural code convex?},
  author={Curto, Carina and Gross, Elizabeth and Jeffries, Jack and Morrison, Katherine and Omar, Mohamed and Rosen, Zvi and Shiu, Anne and Youngs, Nora},
  journal={SIAM Journal on Applied Algebra and Geometry},
  volume={1},
  number={1},
  pages={222--238},
  year={2017},
  publisher={SIAM}
}

@article{dabaghian2012,
  title={A topological paradigm for hippocampal spatial map formation using persistent homology},
  author={Dabaghian, Yuri and M{\'e}moli, Facundo and Frank, Loren and Carlsson, Gunnar},
  journal={PLoS computational biology},
  volume={8},
  number={8},
  pages={e1002581},
  year={2012},
  publisher={Public Library of Science}
}

@article{dabaghian2019,
  title={Through synapses to spatial memory maps via a topological model},
  author={Dabaghian, Yuri},
  journal={Scientific reports},
  volume={9},
  number={1},
  pages={572},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{dama2015,
  title={Exploring valleys without climbing every peak: {M}ore efficient and forgiving metabasin metadynamics via robust on-the-fly bias domain restriction},
  author={Dama, James F and Hocky, Glen M and Sun, Rui and Voth, Gregory A},
  journal={Journal of chemical theory and computation},
  volume={11},
  number={12},
  pages={5638--5650},
  year={2015},
  publisher={ACS Publications}
}

@article{deshmukh2011,
  title={Representation of non-spatial and spatial information in the lateral entorhinal cortex},
  author={Deshmukh, Sachin S and Knierim, James J},
  journal={Frontiers in behavioral neuroscience},
  volume={5},
  pages={69},
  year={2011},
  publisher={Frontiers}
}

@article{dimitrov2003,
  title={Analysis of neural coding through quantization with an information-based distortion measure},
  author={Dimitrov, Alexander G and Miller, John P and Gedeon, Tomas and Aldworth, Zane and Parker, Albert E},
  journal={Network: Computation in Neural Systems},
  volume={14},
  number={1},
  pages={151--176},
  year={2003},
  publisher={Taylor \& Francis}
}

@article{kaczynski1995,
  title={Conley index for discrete multi-valued dynamical systems},
  author={Kaczynski, Tomasz and Mrozek, Marian},
  journal={Topology and its Applications},
  volume={65},
  number={1},
  pages={83--96},
  year={1995},
  publisher={Elsevier}
}

@article{gedeon1995,
  title={Structure of the global attractor of cyclic feedback systems},
  author={Gedeon, Tom{\'a}{\v{s}} and Mischaikow, Konstantin},
  journal={Journal of Dynamics and Differential Equations},
  volume={7},
  number={1},
  pages={141--190},
  year={1995},
  publisher={Springer}
}

@article{doiron2016,
  title={The mechanics of state-dependent neural correlations},
  author={Doiron, Brent and Litwin-Kumar, Ashok and Rosenbaum, Robert and Ocker, Gabriel K and Josi{\'c}, Kre{\v{s}}imir},
  journal={Nature neuroscience},
  volume={19},
  number={3},
  pages={383},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{dunn2015,
  title={Correlations and functional connections in a population of grid cells},
  author={Dunn, Benjamin and M{\o}rreaunet, Maria and Roudi, Yasser},
  journal={PLoS computational biology},
  volume={11},
  number={2},
  pages={e1004052},
  year={2015},
  publisher={Public Library of Science}
}

@article{durstewitz2006,
  title={Beyond bistability: {B}iophysics and temporal dynamics of working memory},
  author={Durstewitz, Daniel and Seamans, Jeremy K},
  journal={Neuroscience},
  volume={139},
  number={1},
  pages={119--133},
  year={2006},
  publisher={Elsevier}
}

@article{edelsbrunner,
  title={Persistent homology-{A} survey},
  author={Edelsbrunner, Herbert and Harer, John},
  journal={Contemporary mathematics},
  volume={453},
  pages={257--282},
  year={2008},
  publisher={Providence, RI: American Mathematical Society}
}

@article{eichenbaum1999,
  title={The hippocampus and mechanisms of declarative memory},
  author={Eichenbaum, Howard},
  journal={Behavioural brain research},
  volume={103},
  number={2},
  pages={123--133},
  year={1999},
  publisher={Elsevier}
}

@article{eichenbaum2000,
  title={Hippocampus: {M}apping or memory?},
  author={Eichenbaum, Howard},
  journal={Current biology},
  volume={10},
  number={21},
  pages={R785--R787},
  year={2000},
  publisher={Elsevier}
}

@article{eichenbaum2014,
  title={Can we reconcile the declarative memory and spatial navigation views on hippocampal function?},
  author={Eichenbaum, Howard and Cohen, Neal J},
  journal={Neuron},
  volume={83},
  number={4},
  pages={764--770},
  year={2014},
  publisher={Elsevier}
}

@article{eichenbaum2017,
  title={Memory: {O}rganization and control},
  author={Eichenbaum, Howard},
  journal={Annual review of psychology},
  volume={68},
  pages={19--45},
  year={2017},
  publisher={Annual Reviews}
}

@misc{expert2019,
  title={Topological Neuroscience},
  author={Expert, Paul and Lord, Louis-David and Kringelbach, Morten L and Petri, Giovanni},
  year={2019},
  publisher={MIT Press}
}

@article{eytan2006,
  title={Dynamics and effective topology underlying synchronization in networks of cortical neurons},
  author={Eytan, Danny and Marom, Shimon},
  journal={Journal of Neuroscience},
  volume={26},
  number={33},
  pages={8465--8476},
  year={2006},
  publisher={Soc Neuroscience}
}

@inproceedings{fabio2015,
  title={Comparing persistence diagrams through complex vectors},
  author={Di Fabio, Barbara and Ferri, Massimo},
  booktitle={International Conference on Image Analysis and Processing},
  pages={294--305},
  year={2015},
  organization={Springer}
}

@incollection{ferri2017,
  title={Persistent topology for natural data analysis—{A} survey},
  author={Ferri, Massimo},
  booktitle={Towards Integrative Machine Learning and Knowledge Extraction},
  pages={117--133},
  year={2017},
  publisher={Springer}
}

@article{fekete2010,
  title={Representational systems},
  author={Fekete, Tomer},
  journal={Minds and Machines},
  volume={20},
  number={1},
  pages={69--101},
  year={2010},
  publisher={Springer}
}

@article{franke2016,
  title={Structures of neural correlation and how they favor coding},
  author={Franke, Felix and Fiscella, Michele and Sevelev, Maksim and Roska, Botond and Hierlemann, Andreas and da Silveira, Rava Azeredo},
  journal={Neuron},
  volume={89},
  number={2},
  pages={409--422},
  year={2016},
  publisher={Elsevier}
}

@article{fung2010,
  title={A moving bump in a continuous manifold: {A} comprehensive study of the tracking dynamics of continuous attractor neural networks},
  author={Fung, CC Alan and Wong, KY Michael and Wu, Si},
  journal={Neural Computation},
  volume={22},
  number={3},
  pages={752--792},
  year={2010},
  publisher={MIT Press}
}

@article{fuster1971,
  title={Neuron activity related to short-term memory},
  author={Fuster, Joaquin M and Alexander, Garrett E},
  journal={Science},
  volume={173},
  number={3997},
  pages={652--654},
  year={1971},
  publisher={American Association for the Advancement of Science}
}

@article{fyhn2004,
  title={Spatial representation in the entorhinal cortex},
  author={Fyhn, Marianne and Molden, Sturla and Witter, Menno P and Moser, Edvard I and Moser, May-Britt},
  journal={Science},
  volume={305},
  number={5688},
  pages={1258--1264},
  year={2004},
  publisher={American Association for the Advancement of Science}
}

@article{fyhn2007,
  title={Hippocampal remapping and grid realignment in entorhinal cortex},
  author={Fyhn, Marianne and Hafting, Torkel and Treves, Alessandro and Moser, May-Britt and Moser, Edvard I},
  journal={Nature},
  volume={446},
  number={7132},
  pages={190},
  year={2007},
  publisher={Nature Publishing Group}
}

@article{ganmor2015,
  title={A thesaurus for a neural population code},
  author={Ganmor, Elad and Segev, Ronen and Schneidman, Elad},
  journal={Elife},
  volume={4},
  pages={e06134},
  year={2015},
  publisher={eLife Sciences Publications Limited}
}

@article{gardner2017,
  title={Correlation structure of grid cells is preserved during sleep},
  author={Gardner, Richard J and Lu, Li and Wernle, Tanja and Moser, May-Britt and Moser, Edvard I},
  journal={bioRxiv},
  pages={198499},
  year={2017},
  publisher={Cold Spring Harbor Laboratory}
}

@article{gerstner2012,
  title={Theory and simulation in neuroscience},
  author={Gerstner, Wulfram and Sprekeler, Henning and Deco, Gustavo},
  journal={science},
  volume={338},
  number={6103},
  pages={60--65},
  year={2012},
  publisher={American Association for the Advancement of Science}
}

@article{gilmore1998,
  title={Topological analysis of chaotic dynamical systems},
  author={Gilmore, Robert},
  journal={Reviews of Modern Physics},
  volume={70},
  number={4},
  pages={1455},
  year={1998},
  publisher={APS}
}

@article{giocomo2011,
  title={Computational models of grid cells},
  author={Giocomo, Lisa M and Moser, May-Britt and Moser, Edvard I},
  journal={Neuron},
  volume={71},
  number={4},
  pages={589--603},
  year={2011},
  publisher={Elsevier}
}

@article{giusti2015,
  title={Clique topology reveals intrinsic geometric structure in neural correlations},
  author={Giusti, Chad and Pastalkova, Eva and Curto, Carina and Itskov, Vladimir},
  journal={Proceedings of the National Academy of Sciences},
  volume={112},
  number={44},
  pages={13455--13460},
  year={2015},
  publisher={National Acad Sciences}
}

@article{giusti2016,
  title={Two’s company, three (or more) is a simplex},
  author={Giusti, Chad and Ghrist, Robert and Bassett, Danielle S},
  journal={Journal of computational neuroscience},
  volume={41},
  number={1},
  pages={1--14},
  year={2016},
  publisher={Springer}
}

@article{hafting2005,
  title={Microstructure of a spatial map in the entorhinal cortex},
  author={Hafting, Torkel and Fyhn, Marianne and Molden, Sturla and Moser, May-Britt and Moser, Edvard I},
  journal={Nature},
  volume={436},
  number={7052},
  pages={801},
  year={2005},
  publisher={Nature Publishing Group}
}

@article{haldeman2005,
  title={Critical branching captures activity in living neural networks and maximizes the number of metastable states},
  author={Haldeman, Clayton and Beggs, John M},
  journal={Physical review letters},
  volume={94},
  number={5},
  pages={058101},
  year={2005},
  publisher={APS}
}

@article{hargreaves2005,
  title={Major dissociation between medial and lateral entorhinal input to dorsal hippocampus},
  author={Hargreaves, Eric L and Rao, Geeta and Lee, Inah and Knierim, James J},
  journal={science},
  volume={308},
  number={5729},
  pages={1792--1794},
  year={2005},
  publisher={American Association for the Advancement of Science}
}

@book{hatcher2002,
  title={Algebraic topology},
  author={Hatcher, Allen},
  year={2005},
  publisher={Cambridge University Press,
Cambridge, England}
}

@article{heinze2017,
  title={Neural coding: Bumps on the move},
  author={Heinze, Stanley},
  journal={Current Biology},
  volume={27},
  number={11},
  pages={R409--R412},
  year={2017},
  publisher={Elsevier}
}

@article{hertz2011,
  title={Ising models for inferring network structure from spike data},
  author={Hertz, John and Roudi, Yasser and Tyrcha, Joanna},
  journal={arXiv preprint arXiv:1106.1752},
  year={2011}
}

@article{hoffman2016,
  title={A model of topological mapping of space in bat hippocampus},
  author={Hoffman, Kentaro and Babichev, Andrey and Dabaghian, Yuri},
  journal={Hippocampus},
  volume={26},
  number={10},
  pages={1345--1353},
  year={2016},
  publisher={Wiley Online Library}
}

@article{hopfield1982,
  title={Neural networks and physical systems with emergent collective computational abilities},
  author={Hopfield, John J},
  journal={Proceedings of the national academy of sciences},
  volume={79},
  number={8},
  pages={2554--2558},
  year={1982},
  publisher={National Acad Sciences}
}

@article{howard2014,
  title={A unified mathematical framework for coding time, space, and sequences in the hippocampal region},
  author={Howard, Marc W and MacDonald, Christopher J and Tiganj, Zoran and Shankar, Karthik H and Du, Qian and Hasselmo, Michael E and Eichenbaum, Howard},
  journal={Journal of Neuroscience},
  volume={34},
  number={13},
  pages={4692--4707},
  year={2014},
  publisher={Soc Neuroscience}
}

@article{huang2009,
  title={Cavity approach to the {S}ourlas code system},
  author={Huang, Haiping and Zhou, Haijun},
  journal={Physical Review E},
  volume={80},
  number={5},
  pages={056113},
  year={2009},
  publisher={APS}
}

@article{huang2013,
  title={Entropy landscape of solutions in the binary perceptron problem},
  author={Huang, Haiping and Wong, KY Michael and Kabashima, Yoshiyuki},
  journal={Journal of Physics A: Mathematical and Theoretical},
  volume={46},
  number={37},
  pages={375002},
  year={2013},
  publisher={IOP Publishing}
}

@article{huang2016,
  title={Clustering of neural code words revealed by a first-order phase transition},
  author={Huang, Haiping and Toyoizumi, Taro},
  journal={Physical Review E},
  volume={93},
  number={6},
  pages={062416},
  year={2016},
  publisher={APS}
}

@article{jaquette2017,
  title={On $\epsilon$ approximations of persistence diagrams},
  author={Jaquette, Jonathan and Kram{\'a}r, Miroslav},
  journal={Mathematics of Computation},
  volume={86},
  number={306},
  pages={1887--1912},
  year={2017}
}

@article{jaynes1957,
  title={Information theory and statistical mechanics},
  author={Jaynes, Edwin T},
  journal={Physical review},
  volume={106},
  number={4},
  pages={620},
  year={1957},
  publisher={APS}
}

@article{jeffery2011,
  title={Place cells, grid cells, attractors, and remapping},
  author={Jeffery, Kathryn J},
  journal={Neural plasticity},
  volume={2011},
  year={2011},
  publisher={Hindawi}
}


@article{kakaria2017,
  title={Ring attractor dynamics emerge from a spiking model of the entire protocerebral bridge},
  author={Kakaria, Kyobi S and de Bivort, Benjamin L},
  journal={Frontiers in behavioral neuroscience},
  volume={11},
  pages={8},
  year={2017},
  publisher={Frontiers}
}

@article{kass2005,
  title={Statistical issues in the analysis of neuronal data},
  author={Kass, Robert E and Ventura, Valerie and Brown, Emery N},
  journal={Journal of neurophysiology},
  volume={94},
  number={1},
  pages={8--25},
  year={2005},
  publisher={American Physiological Society}
}

@article{keene2016,
  title={Complementary functional organization of neuronal activity patterns in the perirhinal, lateral entorhinal, and medial entorhinal cortices},
  author={Keene, Christopher S and Bladon, John and McKenzie, Sam and Liu, Cindy D and O'Keefe, Joseph and Eichenbaum, Howard},
  journal={Journal of Neuroscience},
  volume={36},
  number={13},
  pages={3660--3675},
  year={2016},
  publisher={Soc Neuroscience}
}

@article{knierim1995,
  title={Place cells, head direction cells, and the learning of landmark stability},
  author={Knierim, James J and Kudrimoti, Hemant S and McNaughton, Bruce L},
  journal={Journal of Neuroscience},
  volume={15},
  number={3},
  pages={1648--1659},
  year={1995},
  publisher={Soc Neuroscience}
}

@article{knierim2006,
  title={Hippocampal place cells: Parallel input streams, subregional processing, and implications for episodic memory},
  author={Knierim, James J and Lee, Inah and Hargreaves, Eric L},
  journal={Hippocampus},
  volume={16},
  number={9},
  pages={755--764},
  year={2006},
  publisher={Wiley Online Library}
}

@article{knierim2012,
  title={Attractor dynamics of spatially correlated neural activity in the limbic system},
  author={Knierim, James J and Zhang, Kechen},
  journal={Annual review of neuroscience},
  volume={35},
  pages={267--285},
  year={2012},
  publisher={Annual Reviews}
}

@article{knierim2014,
  title={Functional correlates of the lateral and medial entorhinal cortex: {O}bjects, path integration and local--global reference frames},
  author={Knierim, James J and Neunuebel, Joshua P and Deshmukh, Sachin S},
  journal={Phil. Trans. R. Soc. B},
  volume={369},
  number={1635},
  pages={20130369},
  year={2014},
  publisher={The Royal Society}
}

@article{kornienko2018,
  title={Head-direction cells escaping attractor dynamics in the parahippocampal region},
  author={Kornienko, Olga and Latuske, Patrick and Kohler, Laura and Allen, Kevin},
  journal={BioRxiv},
  pages={268110},
  year={2018},
  publisher={Cold Spring Harbor Laboratory}
}

@misc{kraft2016,
  title={Illustrations of Data Analysis Using the {M}apper Algorithm and Persistent Homology},
  author={Kraft, Rami},
  year={2016}
}

@article{kraus2015,
  title={During running in place, grid cells integrate elapsed time and distance run},
  author={Kraus, Benjamin J and Brandon, Mark P and Robinson II, Robert J and Connerney, Michael A and Hasselmo, Michael E and Eichenbaum, Howard},
  journal={Neuron},
  volume={88},
  number={3},
  pages={578--589},
  year={2015},
  publisher={Elsevier}
}

@book{landau2014,
  title={A guide to {M}onte {C}arlo simulations in statistical physics},
  author={Landau, David P and Binder, Kurt},
  year={2014},
  publisher={Cambridge university press}
}

@article{liu2019,
  title={Generation of scale-invariant sequential activity in linear recurrent networks},
  author={Liu, Yue and Howard, Marc W},
  journal={bioRxiv},
  pages={580522},
  year={2019},
  publisher={Cold Spring Harbor Laboratory}
}

@article{lord2016,
  title={Insights into brain architectures from the homological scaffolds of functional connectivity networks},
  author={Lord, Louis-David and Expert, Paul and Fernandes, Henrique M and Petri, Giovanni and Van Hartevelt, Tim J and Vaccarino, Francesco and Deco, Gustavo and Turkheimer, Federico and Kringelbach, Morten L},
  journal={Frontiers in systems neuroscience},
  volume={10},
  pages={85},
  year={2016},
  publisher={Frontiers}
}

@article{lu2013,
  title={Impaired hippocampal rate coding after lesions of the lateral entorhinal cortex},
  author={Lu, Li and Leutgeb, Jill K and Tsao, Albert and Henriksen, Espen J and Leutgeb, Stefan and Barnes, Carol A and Witter, Menno P and Moser, May-Britt and Moser, Edvard I},
  journal={Nature neuroscience},
  volume={16},
  number={8},
  pages={1085},
  year={2013},
  publisher={Nature Publishing Group}
}

@article{macke2013,
  title={Estimation bias in maximum entropy models},
  author={Macke, Jakob and Murray, Iain and Latham, Peter},
  journal={Entropy},
  volume={15},
  number={8},
  pages={3109--3129},
  year={2013},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{mcnaughton1996,
  title={Deciphering the hippocampal polyglot: {T}he hippocampus as a path integration system.},
  author={McNaughton, Bruce L and Barnes, Carol A and Gerrard, Jason L and Gothard, Katalin and Jung, Min W and Knierim, James J and Kudrimoti, H and Qin, Y and Skaggs, WE and Suster, M and others},
  journal={Journal of Experimental Biology},
  volume={199},
  number={1},
  pages={173--185},
  year={1996},
  publisher={The Company of Biologists Ltd}
}

@article{mcnaughton2006,
  title={Path integration and the neural basis of the 'cognitive map'},
  author={McNaughton, Bruce L and Battaglia, Francesco P and Jensen, Ole and Moser, Edvard I and Moser, May-Britt},
  journal={Nature Reviews Neuroscience},
  volume={7},
  number={8},
  pages={663},
  year={2006},
  publisher={Nature Publishing Group}
}

@article{merchan2016,
  title={On the sufficiency of pairwise interactions in maximum entropy models of networks},
  author={Merchan, Lina and Nemenman, Ilya},
  journal={Journal of Statistical Physics},
  volume={162},
  number={5},
  pages={1294--1308},
  year={2016},
  publisher={Springer}
}

@article{merelli2015,
  title={Topological characterization of complex systems: Using persistent entropy},
  author={Merelli, Emanuela and Rucco, Matteo and Sloot, Peter and Tesei, Luca},
  journal={Entropy},
  volume={17},
  number={10},
  pages={6872--6892},
  year={2015},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{metropolis1953,
  title={Equation of state calculations by fast computing machines},
  author={Metropolis, Nicholas and Rosenbluth, Arianna W and Rosenbluth, Marshall N and Teller, Augusta H and Teller, Edward},
  journal={The journal of chemical physics},
  volume={21},
  number={6},
  pages={1087--1092},
  year={1953},
  publisher={AIP}
}

@article{mezard1988,
  title={Spin glass theory and beyond},
  author={Mezard, Marc and Parisi, Giorgio and Virasoro, Miguel Angel and Thouless, David J},
  journal={Physics Today},
  volume={41},
  pages={109},
  year={1988}
}

@article{mezard2001,
author = {M{\'{e}}zard, Marc and Parisi, Giorgio},
doi = {10.1007/PL00011099},
issn = {1434-6036},
journal = {The European Physical Journal B - Condensed Matter and Complex Systems},
number = {2},
pages = {217--233},
title = {{The {B}ethe lattice spin glass revisited}},
url = {https://doi.org/10.1007/PL00011099},
volume = {20},
year = {2001}
}

@article{mezard2005,
  title={Clustering of solutions in the random satisfiability problem},
  author={M{\'e}zard, Marc and Mora, Thierry and Zecchina, Riccardo},
  journal={Physical Review Letters},
  volume={94},
  number={19},
  pages={197205},
  year={2005},
  publisher={APS}
}

@book{mezard2009,
  title={Information, physics, and computation},
  author={M{\'e}zard, Marc and Montanari, Andrea},
  year={2009},
  publisher={Oxford University Press}
}

@book{britton1986,
  title={Reaction-diffusion equations and their applications to biology},
  author={Britton, Nicholas F and others},
  year={1986},
  publisher={Academic Press}
}

@article{rapp1993,
  title={Chaos in the neurosciences: {C}autionary tales from the frontier},
  author={Rapp, Paul E},
  journal={Biologist},
  volume={40},
  number={2},
  pages={89--94},
  year={1993}
}

@inproceedings{lesne2006,
  title={Chaos in biology},
  author={Lesne, Annick},
  booktitle={Rivista di Biologia Biology Forum},
  volume={99},
  number={3},
  pages={467},
  year={2006},
  organization={ANICIA SRL}
}

@book{tuckwell1988,
  title={Introduction to theoretical neurobiology: {V}olume 2, nonlinear and stochastic theories},
  author={Tuckwell, Henry C},
  volume={8},
  year={1988},
  publisher={Cambridge University Press}
}

@article{pence2017,
  title={Potential Controversies: Causation and the {H}odgkin and {H}uxley Equations},
  author={Pence, David Evan},
  journal={Philosophy of Science},
  volume={84},
  number={5},
  pages={1177--1188},
  year={2017},
  publisher={University of Chicago Press Chicago, IL}
}

@book{cronin1987,
  title={Mathematical aspects of {H}odgkin-{H}uxley neural theory},
  author={Cronin, Jane},
  volume={7},
  year={1987},
  publisher={Cambridge University Press}
}

@article{mrozek1996,
  title={Topological invariants, multivalued maps and computer assisted proofs in dynamics, Computers \& Mathematics, 32 (1996), 83-104},
  author={Mrozek, M},
  journal={MR 97h},
  volume={58144}
}

@incollection{mischaikow1995a,
  title={{C}onley index theory},
  author={Mischaikow, Konstantin},
  booktitle={Dynamical systems},
  pages={119--207},
  year={1995},
  publisher={Springer}
}

@article{mischaikow1995b,
  title={Isolating neighborhoods and chaos},
  author={Mischaikow, Konstantin and Mrozek, Marian},
  journal={Japan journal of industrial and applied mathematics},
  volume={12},
  number={2},
  pages={205--236},
  year={1995},
  publisher={Springer}
}

@article{mischaikow1995c,
  title={Chaos in the {L}orenz equations: A computer-assisted proof},
  author={Mischaikow, Konstantin and Mrozek, Marian},
  journal={Bulletin of the American Mathematical Society},
  volume={32},
  number={1},
  pages={66--72},
  year={1995}
}

@article{mischaikow1998,
  title={Chaos in the {L}orenz equations: A computer assisted proof. Part {II: D}etails},
  author={Mischaikow, Konstantin and Mrozek, Marian},
  journal={Mathematics of Computation},
  volume={67},
  number={223},
  pages={1023--1046},
  year={1998}
}

@article{wilson1973,
  title={Lyapunov functions and isolating blocks},
  author={Wilson Jr, F Wesley and Yorke, James A},
  journal={Journal of Differential Equations},
  volume={13},
  number={1},
  pages={106--123},
  year={1973},
  publisher={Academic Press}
}

@article{carpenter1977,
  title={A geometric approach to singular perturbation problems with applications to nerve impulse equations},
  author={Carpenter, Gail A},
  journal={Journal of Differential Equations},
  volume={23},
  number={3},
  pages={335--367},
  year={1977},
  publisher={Elsevier}
}

@article{mischaikow1999,
  title={The {C}onley index theory: A brief introduction},
  author={Mischaikow, Konstantin},
  journal={Banach Center Publications},
  volume={47},
  number={1},
  pages={9--19},
  year={1999}
}

@inbook{mischaikow2001,
title={Conley index},
booktitle={Handbook of Dynamical Systems},
year={2001},
volume={Vol. 2},
editor={Fiedler, Bernold},
publisher={Elsevier},
chapter={9}
}

@article{kaczynski2003,
  title={Computing homology},
  author={Kaczynski, Tomasz and Mischaikow, Konstantin and Mrozek, Marian and others},
  journal={Homology, Homotopy and Applications},
  volume={5},
  number={2},
  pages={233--256},
  year={2003},
  publisher={International Press of Boston}
}

@article{easton1989,
  title={Isolating blocks and epsilon chains for maps},
  author={Easton, Robert},
  journal={Physica D: Nonlinear Phenomena},
  volume={39},
  number={1},
  pages={95--110},
  year={1989},
  publisher={Elsevier}
}

@article{mischaikow2002,
  title={Topological techniques for efficient rigorous computation in dynamics},
  author={Mischaikow, Konstantin},
  journal={Acta Numerica},
  volume={11},
  pages={435--477},
  year={2002},
  publisher={Cambridge University Press}
}

@article{miller2006,
  title={Analysis of spike statistics in neuronal systems with continuous attractors or multiple, discrete attractor states},
  author={Miller, Paul},
  journal={Neural Computation},
  volume={18},
  number={6},
  pages={1268--1317},
  year={2006},
  publisher={MIT Press}
}

@article{miller2016,
  title={Dynamical systems, attractors, and neural circuits},
  author={Miller, Paul},
  journal={F1000Research},
  volume={5},
  year={2016},
  publisher={Faculty of 1000 Ltd}
}

@article{miller2017,
  title={Data structures for real multiparameter persistence modules},
  author={Miller, Ezra},
  journal={arXiv preprint arXiv:1709.08155},
  year={2017}
}

@article{monasson2015,
  title={Transitions between spatial attractors in place-cell models},
  author={Monasson, R{\'e}mi and Rosay, Sophie},
  journal={Physical review letters},
  volume={115},
  number={9},
  pages={098101},
  year={2015},
  publisher={APS}
}

@article{montanari2008,
  title={Clusters of solutions and replica symmetry breaking in random k-satisfiability},
  author={Montanari, Andrea and Ricci-Tersenghi, Federico and Semerjian, Guilhem},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2008},
  number={04},
  pages={P04004},
  year={2008},
  publisher={IOP Publishing}
}

@article{moser2008,
  title={Place cells, grid cells, and the brain's spatial representation system},
  author={Moser, Edvard I and Kropff, Emilio and Moser, May-Britt},
  journal={Annual review of neuroscience},
  volume={31},
  year={2008}
}

@article{moser2013,
  title={Grid cells and neural coding in high-end cortices},
  author={Moser, Edvard I and Moser, May-Britt},
  journal={Neuron},
  volume={80},
  number={3},
  pages={765--774},
  year={2013},
  publisher={Elsevier}
}

@article{moser2014,
  title={Network mechanisms of grid cells},
  author={Moser, Edvard I and Moser, May-Britt and Roudi, Yasser},
  journal={Phil. Trans. R. Soc. B},
  volume={369},
  number={1635},
  pages={20120511},
  year={2014},
  publisher={The Royal Society}
}

@article{moser2017,
  title={Spatial representation in the hippocampal formation: {A} history},
  author={Moser, Edvard I and Moser, May-Britt and McNaughton, Bruce L},
  journal={Nature neuroscience},
  volume={20},
  number={11},
  pages={1448},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{moyal2019,
  title={Dynamic Computation in Visual Thalamocortical Networks},
  author={Moyal, Roy and Edelman, Shimon},
  journal={Entropy},
  volume={21},
  number={5},
  pages={500},
  year={2019},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{mulder2017,
  title={Network Geometry and Complexity},
  author={Mulder, Daan and Bianconi, Ginestra},
  journal={arXiv preprint arXiv:1711.06290},
  year={2017}
}

@article{munch2017,
  title={A User's Guide to Topological Data Analysis.},
  author={Munch, Elizabeth},
  journal={Journal of Learning Analytics},
  volume={4},
  number={2},
  pages={47--61},
  year={2017},
  publisher={ERIC}
}

@article{narayanan2016,
  title={Ramping activity is a cortical mechanism of temporal control of action},
  author={Narayanan, Nandakumar S},
  journal={Current opinion in behavioral sciences},
  volume={8},
  pages={226--230},
  year={2016},
  publisher={Elsevier}
}

@phdthesis{neville2017,
  title={Topological techniques for characterization of patterns in differential equations},
  author={Neville, Rachel A},
  year={2017},
  school={Colorado State University. Libraries}
}

@article{nguyen2017,
  title={Inverse statistical problems: {F}rom the inverse {I}sing problem to data science},
  author={Nguyen, H Chau and Zecchina, Riccardo and Berg, Johannes},
  journal={Advances in Physics},
  volume={66},
  number={3},
  pages={197--261},
  year={2017},
  publisher={Taylor \& Francis}
}

@article{ocker2017,
  title={Linking structure and activity in nonlinear spiking networks},
  author={Ocker, Gabriel Koch and Josi{\'c}, Kre{\v{s}}imir and Shea-Brown, Eric and Buice, Michael A},
  journal={PLoS computational biology},
  volume={13},
  number={6},
  pages={e1005583},
  year={2017},
  publisher={Public Library of Science}
}

@article{okeefe1971,
  title={The hippocampus as a spatial map: Preliminary evidence from unit activity in the freely-moving rat},
  author={O'Keefe, John and Dostrovsky, Jonathan},
  journal={Brain research},
  year={1971},
  publisher={Elsevier Science}
}

@article{okeefe1979,
  title={A review of the hippocampal place cells},
  author={O'Keefe, John},
  journal={Progress in neurobiology},
  volume={13},
  number={4},
  pages={419--439},
  year={1979},
  publisher={Elsevier}
}

@article{patania2017,
  title={Topological analysis of data},
  author={Patania, Alice and Vaccarino, Francesco and Petri, Giovanni},
  journal={EPJ Data Science},
  volume={6},
  number={1},
  pages={7},
  year={2017},
  publisher={Springer}
}

@article{pernice2011,
  title={How structure determines correlations in neuronal networks},
  author={Pernice, Volker and Staude, Benjamin and Cardanobile, Stefano and Rotter, Stefan},
  journal={PLoS computational biology},
  volume={7},
  number={5},
  pages={e1002059},
  year={2011},
  publisher={Public Library of Science}
}

@article{petri2013,
  title={Topological strata of weighted complex networks},
  author={Petri, Giovanni and Scolamiero, Martina and Donato, Irene and Vaccarino, Francesco},
  journal={PloS one},
  volume={8},
  number={6},
  pages={e66506},
  year={2013},
  publisher={Public Library of Science}
}

@article{petri2014,
  title={Homological scaffolds of brain functional networks},
  author={Petri, Giovanni and Expert, Paul and Turkheimer, Federico and Carhart-Harris, Robin and Nutt, David and Hellyer, Peter J and Vaccarino, Francesco},
  journal={Journal of The Royal Society Interface},
  volume={11},
  number={101},
  pages={20140873},
  year={2014},
  publisher={The Royal Society}
}

@article{peyrache2015a,
  title={Extracellular recordings from multi-site silicon probes in the anterior thalamus and subicular formation of freely moving mice},
  author={Peyrache, A and Buzs{\'a}ki, G},
  journal={CRCNS. org},
  year={2015}
}

@article{peyrache2015b,
  title={Internally organized mechanisms of the head direction sense},
  author={Peyrache, Adrien and Lacroix, Marie M and Petersen, Peter C and Buzs{\'a}ki, Gy{\"o}rgy},
  journal={Nature neuroscience},
  volume={18},
  number={4},
  pages={569},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{pilkiw2017,
  title={Phasic and tonic neuron ensemble codes for stimulus-environment conjunctions in the lateral entorhinal cortex},
  author={Pilkiw, Maryna and Insel, Nathan and Cui, Younghua and Finney, Caitlin and Morrissey, Mark D and Takehara-Nishiuchi, Kaori},
  journal={Elife},
  volume={6},
  pages={e28611},
  year={2017},
  publisher={eLife Sciences Publications Limited}
}

@article{posani2017,
  title={Functional connectivity models for decoding of spatial representations from hippocampal {CA1} recordings},
  author={Posani, Lorenzo and Cocco, Simona and Je{\v{z}}ek, Karel and Monasson, R{\'e}mi},
  journal={Journal of Computational Neuroscience},
  volume={43},
  number={1},
  pages={17--33},
  year={2017},
  publisher={Springer}
}



@article{redish1996coupled,
  title={A coupled attractor model of the rodent head direction system},
  author={Redish, A David and Elga, Adam N and Touretzky, David S},
  journal={Network: Computation in Neural Systems},
  volume={7},
  number={4},
  pages={671--685},
  year={1996},
  publisher={Taylor \& Francis}
}

@article{reutimann2004,
  title={Climbing neuronal activity as an event-based cortical representation of time},
  author={Reutimann, Jan and Yakovlev, Volodya and Fusi, Stefano and Senn, Walter},
  journal={Journal of Neuroscience},
  volume={24},
  number={13},
  pages={3295--3303},
  year={2004},
  publisher={Soc Neuroscience}
}

@inproceedings{ritt2015,
  title={Neurocontrol: Methods, models and technologies for manipulating dynamics in the brain},
  author={Ritt, Jason T and Ching, ShiNung},
  booktitle={American Control Conference (ACC), 2015},
  pages={3765--3780},
  year={2015},
  organization={IEEE}
}

@article{rolls2007,
author = {Rolls, Edmund T},
doi = {10.1101/lm.631207.lesions.},
file = {:home/abel/Documents/Projects/BioMath/Rolls-2007-An attractor network in the hippocampus$\backslash$: Theory and neurophysiology.pdf:pdf},
isbn = {4418653104},
pages = {714--731},
title = {{An attractor network in the hippocampus : Theory and neurophysiology}},
year = {2007}
}

@article{rolls2010,
abstract = {An attractor network is a network of neurons with excitatory interconnections that can settle into a stable pattern of firing. This article shows how attractor networks in the cerebral cortex are important for long-term memory, short-term memory, attention, and decision making. The article then shows how the random firing of neurons can influence the stability of these networks by introducing stochastic noise, and how these effects are involved in probabilistic decision making, and implicated in some disorders of cortical function such as poor short-term memory and attention, schizophrenia, and obsessive-compulsive disorder.},
author = {Rolls, Edmund T.},
doi = {10.1002/wcs.1},
file = {:home/abel/Documents/Projects/BioMath/Rolls-2009- Attractor Networks.pdf:pdf},
issn = {19395078},
journal = {Wiley Interdisciplinary Reviews: Cognitive Science},
number = {1},
pages = {119--134},
title = {{Attractor networks}},
volume = {1},
year = {2010}
}

@article{rolls2011,
  title={The neuronal encoding of information in the brain},
  author={Rolls, Edmund T and Treves, Alessandro},
  journal={Progress in neurobiology},
  volume={95},
  number={3},
  pages={448--490},
  year={2011},
  publisher={Elsevier}
}

@article{rolls2013,
  title={The mechanisms for pattern completion and pattern separation in the hippocampus},
  author={Rolls, Edmund},
  journal={Frontiers in systems neuroscience},
  volume={7},
  pages={74},
  year={2013},
  publisher={Frontiers}
}

@article{Rolls2015,
author = {Rolls, Edmund T},
doi = {10.1016/j.nlm.2015.07.008},
file = {:home/abel/Documents/Projects/BioMath/LEC/Articles/Rolls-2015-Pattern separation, completion, and categorisation in the hippocampus and neocortex.pdf:pdf},
issn = {1074-7427},
journal = {Neurobiology of Learning and Memory},
mendeley-groups = {BioMath},
pages = {4--28},
publisher = {Elsevier Inc.},
title = {{Neurobiology of Learning and Memory Pattern separation , completion , and categorisation in the hippocampus and neocortex}},
url = {http://dx.doi.org/10.1016/j.nlm.2015.07.008},
volume = {129},
year = {2016}
}


@article{rosen2017,
  title={Convex neural codes in dimension 1},
  author={Rosen, Zvi and Zhang, Yan X},
  journal={arXiv preprint arXiv:1702.06907},
  year={2017}
}

@article{rostami2016,
  title={Pairwise maximum-entropy models and their Glauber dynamics: {B}imodality, bistability, non-ergodicity problems, and their elimination via inhibition},
  author={Rostami, Vahid and Mana, PierGianLuca Porta and Helias, Moritz},
  journal={arXiv preprint arXiv:1605.04740},
  year={2016}
}

@article{roudi2009,
  title={Pairwise maximum entropy models for studying large biological systems: {W}hen they can work and when they can't},
  author={Roudi, Yasser and Nirenberg, Sheila and Latham, Peter E},
  journal={PLoS computational biology},
  volume={5},
  number={5},
  pages={e1000380},
  year={2009},
  publisher={Public Library of Science}
}

@article{rubin2015,
  title={Hippocampal ensemble dynamics timestamp events in long-term memory},
  author={Rubin, Alon and Geva, Nitzan and Sheintuch, Liron and Ziv, Yaniv},
  journal={Elife},
  volume={4},
  pages={e12247},
  year={2015},
  publisher={eLife Sciences Publications Limited}
}

@article{rubin2016,
  title={Wild oscillations in a nonlinear neuron model with resets:({I}) Bursting, spike adding and chaos},
  author={Rubin, Jonathan E and Signerska-Rynkowska, Justyna and Touboul, Jonathan D and Vidal, Alexandre},
  journal={arXiv preprint arXiv:1611.02740},
  year={2016}
}

@article{rubin2015II,
  title={Wild oscillations in a nonlinear neuron model with resets:({II}) Mixed-mode oscillations},
  author={Rubin, Jonathan E and Signerska-Rynkowska, Justyna and Touboul, Jonathan and Vidal, Alexandre},
  journal={arXiv preprint arXiv:1509.08282},
  year={2015}
}

@article{rybakken2017,
  title={Decoding of neural data using cohomological learning},
  author={Rybakken, Erik and Baas, Nils and Dunn, Benjamin},
  journal={arXiv preprint arXiv:1711.07205},
  year={2017}
}

@article{durstewitz2008,
  title={Computational significance of transient dynamics in cortical networks},
  author={Durstewitz, Daniel and Deco, Gustavo},
  journal={European Journal of Neuroscience},
  volume={27},
  number={1},
  pages={217--227},
  year={2008},
  publisher={Wiley Online Library}
}

@article{kalies2018,
  title={An algorithmic approach to lattices and order in dynamics},
  author={Kalies, William D and Kasti, Dinesh and Vandervorst, Robert},
  journal={SIAM Journal on Applied Dynamical Systems},
  volume={17},
  number={2},
  pages={1617--1649},
  year={2018},
  publisher={SIAM}
}

@article{samsonovich1997path,
  title={Path integration and cognitive mapping in a continuous attractor neural network model},
  author={Samsonovich, Alexei and McNaughton, Bruce L},
  journal={Journal of Neuroscience},
  volume={17},
  number={15},
  pages={5900--5920},
  year={1997},
  publisher={Soc Neuroscience}
}

@article{sargolini2006,
  title={Conjunctive representation of position, direction, and velocity in entorhinal cortex},
  author={Sargolini, Francesca and Fyhn, Marianne and Hafting, Torkel and McNaughton, Bruce L and Witter, Menno P and Moser, May-Britt and Moser, Edvard I},
  journal={Science},
  volume={312},
  number={5774},
  pages={758--762},
  year={2006},
  publisher={American Association for the Advancement of Science}
}

@article{smith2008,
  title={Spatial and temporal scales of neuronal correlation in primary visual cortex},
  author={Smith, Matthew A and Kohn, Adam},
  journal={Journal of Neuroscience},
  volume={28},
  number={48},
  pages={12591--12603},
  year={2008},
  publisher={Soc Neuroscience}
}

@article{schneidman2006,
  title={Weak pairwise correlations imply strongly correlated network states in a neural population},
  author={Schneidman, Elad and Berry II, Michael J and Segev, Ronen and Bialek, William},
  journal={Nature},
  volume={440},
  number={7087},
  pages={1007},
  year={2006},
  publisher={Nature Publishing Group}
}

@article{silva2011,
  title={Persistent cohomology and circular coordinates},
  author={De Silva, Vin and Morozov, Dmitriy and Vejdemo-Johansson, Mikael},
  journal={Discrete \& Computational Geometry},
  volume={45},
  number={4},
  pages={737--759},
  year={2011},
  publisher={Springer}
}

@incollection{takens1981,
  title={Detecting strange attractors in turbulence},
  author={Takens, Floris},
  booktitle={Dynamical systems and turbulence, Warwick 1980},
  pages={366--381},
  year={1981},
  publisher={Springer}
}

@article{tang2008,
  title={A maximum entropy model applied to spatial and temporal correlations from cortical networks in vitro},
  author={Tang, Aonan and Jackson, David and Hobbs, Jon and Chen, Wei and Smith, Jodi L and Patel, Hema and Prieto, Anita and Petrusca, Dumitru and Grivich, Matthew I and Sher, Alexander and others},
  journal={Journal of Neuroscience},
  volume={28},
  number={2},
  pages={505--518},
  year={2008},
  publisher={Soc Neuroscience}
}

@article{tkacik2009,
  title={Spin glass models for a network of real neurons},
  author={Tka{\v{c}}ik, Ga{\v{s}}per and Schneidman, Elad and Berry, II and Michael, J and Bialek, William},
  journal={arXiv preprint arXiv:0912.5409},
  year={2009}
}

@article{tognoli2014,
  title={The metastable brain},
  author={Tognoli, Emmanuelle and Kelso, JA Scott},
  journal={Neuron},
  volume={81},
  number={1},
  pages={35--48},
  year={2014},
  publisher={Elsevier}
}

@article{tralie2018,
  doi = {10.21105/joss.00925},
  url = {https://doi.org/10.21105/joss.00925},
  year  = {2018},
  month = {Sep},
  publisher = {The Open Journal},
  volume = {3},
  number = {29},
  pages = {925},
  author = {Christopher Tralie and Nathaniel Saul and Rann Bar-On},
  title = {{Ripser.py}: A Lean Persistent Homology Library for Python},
  journal = {The Journal of Open Source Software}
}

@article{schneidman2011,
  title={Synergy from silence in a combinatorial neural code},
  author={Schneidman, Elad and Puchalla, Jason L and Segev, Ronen and Harris, Robert A and Bialek, William and Berry, Michael J},
  journal={Journal of Neuroscience},
  volume={31},
  number={44},
  pages={15732--15741},
  year={2011},
  publisher={Soc Neuroscience}
}

@article{series2004,
  title={Tuning curve sharpening for orientation selectivity: {C}oding efficiency and the impact of correlations},
  author={Seri{\`e}s, Peggy and Latham, Peter E and Pouget, Alexandre},
  journal={Nature neuroscience},
  volume={7},
  number={10},
  pages={1129},
  year={2004},
  publisher={Nature Publishing Group}
}

@article{sizemore2018a,
  title={Cliques and cavities in the human connectome},
  author={Sizemore, Ann E and Giusti, Chad and Kahn, Ari and Vettel, Jean M and Betzel, Richard F and Bassett, Danielle S},
  journal={Journal of computational neuroscience},
  volume={44},
  number={1},
  pages={115--145},
  year={2018},
  publisher={Springer}
}

@article{sizemore2018b,
  title={The importance of the whole: Topological data analysis for the network neuroscientist},
  author={Sizemore, Ann E and Phillips-Cremins, Jennifer and Ghrist, Robert and Bassett, Danielle S},
  journal={arXiv preprint arXiv:1806.05167},
  year={2018}
}

@article{solstad2008,
  title={Representation of geometric borders in the entorhinal cortex},
  author={Solstad, Trygve and Boccara, Charlotte N and Kropff, Emilio and Moser, May-Britt and Moser, Edvard I},
  journal={Science},
  volume={322},
  number={5909},
  pages={1865--1868},
  year={2008},
  publisher={American Association for the Advancement of Science}
}

@book{sporns2010,
  title={Networks of the Brain},
  author={Sporns, Olaf},
  year={2010},
  publisher={MIT press}
}

@article{spreemann2015,
  title={Using persistent homology to reveal hidden information in neural data},
  author={Spreemann, Gard and Dunn, Benjamin and Botnan, Magnus Bakke and Baas, Nils A},
  journal={arXiv preprint arXiv:1510.06629},
  year={2015}
}

@article{spreemann2018,
  title={Using persistent homology to reveal hidden covariates in systems governed by the kinetic {I}sing model},
  author={Spreemann, Gard and Dunn, Benjamin and Botnan, Magnus Bakke and Baas, Nils A},
  journal={Physical Review E},
  volume={97},
  number={3},
  pages={032313},
  year={2018},
  publisher={APS}
}

@article{stolz2014,
  title={Computational topology in neuroscience},
  author={Stolz, Bernadette},
  journal={Master's thesis (University of Oxford, 2014). Google Scholar},
  year={2014}
}

@article{stringer2002,
  title={Self-organizing continuous attractor networks and path integration: {O}ne-dimensional models of head direction cells},
  author={Stringer, SM and Trappenberg, TP and Rolls, ET and de Araujo, IET},
  journal={Network: Computation in Neural Systems},
  volume={13},
  number={2},
  pages={217--242},
  year={2002},
  publisher={Taylor \& Francis}
}

@inproceedings{sun2018,
  title={An analysis of a ring attractor model for cue integration},
  author={Sun, Xuelong and Mangan, Michael and Yue, Shigang},
  booktitle={Conference on Biomimetic and Biohybrid Systems},
  pages={459--470},
  year={2018},
  organization={Springer}
}

@article{tanaka2018,
  title={Non-Hermitian Quasi-Localization and Ring Attractor Neural Networks},
  author={Tanaka, Hidenori and Nelson, David R},
  journal={arXiv preprint arXiv:1811.07433},
  year={2018}
}

@article{tkacik2006,
  title={Ising models for networks of real neurons},
  author={Tka{\v}cik, Ga{\v{s}}per and Schneidman, Elad and Berry, II and Michael, J and Bialek, William},
  journal={arXiv preprint q-bio/0611072},
  year={2006}
}

@article{tkacik2014,
  title={Searching for collective behavior in a large network of sensory neurons},
  author={Tka{\v{c}}ik, Ga{\v{s}}per and Marre, Olivier and Amodei, Dario and Schneidman, Elad and Bialek, William and Berry II, Michael J},
  journal={PLoS computational biology},
  volume={10},
  number={1},
  pages={e1003408},
  year={2014},
  publisher={Public Library of Science}
}

@article{touretzky1996,
  title={Theory of rodent navigation based on interacting representations of space},
  author={Touretzky, David S and Redish, A David},
  journal={Hippocampus},
  volume={6},
  number={3},
  pages={247--270},
  year={1996},
  publisher={Wiley Online Library}
}

@article{tozzi2017,
  title={Towards Topological Mechanisms Underlying Experience Acquisition and Transmission in the Human Brain},
  author={Tozzi, Arturo and Peters, James F},
  journal={Integrative Psychological and Behavioral Science},
  volume={51},
  number={2},
  pages={303--323},
  year={2017},
  publisher={Springer}
}

@article{tsao2013,
  title={Traces of experience in the lateral entorhinal cortex},
  author={Tsao, Albert and Moser, May-Britt and Moser, Edvard I},
  journal={Current Biology},
  volume={23},
  number={5},
  pages={399--405},
  year={2013},
  publisher={Elsevier}
}

@article{tsao2018,
  title={Integrating time from experience in the lateral entorhinal cortex},
  author={Tsao, Albert and Sugar, J{\o}rgen and Lu, Li and Wang, Cheng and Knierim, James J and Moser, May-Britt and Moser, Edvard I},
  journal={Nature},
  pages={1},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{tsodyks1988,
  title={The enhanced storage capacity in neural networks with low activity level},
  author={Tsodyks, Mikhail V and Feigelman, Mikhail V},
  journal={EPL (Europhysics Letters)},
  volume={6},
  number={2},
  pages={101},
  year={1988},
  publisher={IOP Publishing}
}

@article{tsodyks1995,
  title={Associative memory and hippocampal place cells},
  author={Tsodyks, Misha and Sejnowski, Terrence},
  journal={International journal of neural systems},
  volume={6},
  pages={81--86},
  year={1995},
  publisher={World Scientific}
}

@article{urdapilleta2013,
  title={Self-organizing internal representations of space},
  author={Urdapilleta, Eugenio and Treves, Alessandro and others},
  journal={Theoretical Approaches to BioInformation Systems-proceedings of TABIS},
  pages={229--240},
  year={2013}
}

@article{velazquex2016,
  title={Studying brain networks via topological data analysis and hierarchical clustering},
  author={Almod{\'o}var Vel{\'a}zquez, Leyda Michelle},
  year={2016},
  publisher={University of Iowa}
}

@article{wang2001,
  title={Efficient, multiple-range random walk algorithm to calculate the density of states},
  author={Wang, Fugao and Landau, David P},
  journal={Physical review letters},
  volume={86},
  number={10},
  pages={2050},
  year={2001},
  publisher={APS}
}

@article{wasserman2016,
  title={Topological Data Analysis},
  author={Wasserman, Larry},
  journal={arXiv preprint arXiv:1609.08227},
  year={2016}
}

@incollection{widloski2014,
  title={How does the brain solve the computational problems of spatial navigation?},
  author={Widloski, John and Fiete, Ila},
  booktitle={Space, Time and Memory in the Hippocampal Formation},
  pages={373--407},
  year={2014},
  publisher={Springer}
}

@article{wills2005,
  title={Attractor dynamics in the hippocampal representation of the local environment},
  author={Wills, Tom J and Lever, Colin and Cacucci, Francesca and Burgess, Neil and O'Keefe, John},
  journal={Science},
  volume={308},
  number={5723},
  pages={873--876},
  year={2005},
  publisher={American Association for the Advancement of Science}
}

@incollection{winter2014,
  title={Head direction cells: {F}rom generation to integration},
  author={Winter, Shawn S and Taube, Jeffrey S},
  booktitle={Space, time and memory in the hippocampal formation},
  pages={83--106},
  year={2014},
  publisher={Springer}
}

@article{witter2006,
  title={Spatial representation and the architecture of the entorhinal cortex},
  author={Witter, Menno P and Moser, Edvard I},
  journal={Trends in neurosciences},
  volume={29},
  number={12},
  pages={671--678},
  year={2006},
  publisher={Elsevier}
}

@article{wright2015,
  title={Coordinate descent algorithms},
  author={Wright, Stephen J},
  journal={Mathematical Programming},
  volume={151},
  number={1},
  pages={3--34},
  year={2015},
  publisher={Springer}
}

@article{wu2008dynamics,
  title={Dynamics and computation of continuous attractors},
  author={Wu, Si and Hamaguchi, Kosuke and Amari, Shun-ichi},
  journal={Neural computation},
  volume={20},
  number={4},
  pages={994--1025},
  year={2008},
  publisher={MIT Press}
}

@article{wu2016,
  title={Continuous attractor neural networks: {C}andidate of a canonical model for neural information representation},
  author={Wu, Si and Wong, KY Michael and Fung, CC Alan and Mi, Yuanyuan and Zhang, Wenhao},
  journal={F1000Research},
  volume={5},
  year={2016},
  publisher={Faculty of 1000 Ltd}
}

@phdthesis{yoon2018,
  title={Cellular Sheaves and Cosheaves for Distributed Topological Data Analysis},
  author={Yoon, Hee Rhang},
  year={2018},
  school={University of Pennsylvania}
}

@article{yu2013a,
  title={Continuous attractors of discrete-time recurrent neural networks},
  author={Yu, Jiali and Tang, Huajin and Li, Haizhou},
  journal={Neural Computing and Applications},
  volume={23},
  number={1},
  pages={89--96},
  year={2013},
  publisher={Springer}
}

@article{zhang1996,
  title={Representation of spatial orientation by the intrinsic dynamics of the head-direction cell ensemble: {A} theory},
  author={Zhang, Kechen},
  journal={Journal of Neuroscience},
  volume={16},
  number={6},
  pages={2112--2126},
  year={1996},
  publisher={Soc Neuroscience}
}

@article{zhou2009,
  title={Communities of solutions in single solution clusters of a random K-satisfiability formula},
  author={Zhou, Haijun and Ma, Hui},
  journal={Physical Review E},
  volume={80},
  number={6},
  pages={066108},
  year={2009},
  publisher={APS}
}

@article{zhou2010,
  title={Ground-state configuration space heterogeneity of random finite-connectivity spin glasses and random constraint satisfaction problems},
  author={Zhou, Haijun and Wang, Chuang},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2010},
  number={10},
  pages={P10010},
  year={2010},
  publisher={IOP Publishing}
}

@article{zhou2016,
  title={Biologically inspired model of path integration based on head direction cells and grid cells},
  author={Zhou, Yang and Wu, De-wei},
  journal={Frontiers of Information Technology \& Electronic Engineering},
  volume={17},
  number={5},
  pages={435--448},
  year={2016},
  publisher={Springer}
}

@article{zylberberg2016,
  title={Direction-selective circuits shape noise to ensure a precise population code},
  author={Zylberberg, Joel and Cafaro, Jon and Turner, Maxwell H and Shea-Brown, Eric and Rieke, Fred},
  journal={Neuron},
  volume={89},
  number={2},
  pages={369--383},
  year={2016},
  publisher={Elsevier}
}

@article{Andersen2014,
abstract = {In this field guide, I distinguish five separate senses with which the term ‘mechanism' is used in contemporary philosophy of science. Many of these senses have overlapping areas of application but involve distinct philosophical claims and characterize the target mechanisms in relevantly differentways. This field guide will clarify the key features of each sense and introduce some main debates, distinguishing those that transpire within a given sense from those that are best understood as concerning two distinct senses. The ‘new mechanisms' sense is the primary sense from which other senses will be distinguished. In part II of this field guide, I consider three further senses of the term that are ontologically ‘flat' or at least not explicitly hierarchical in character: equations in structural equation models of causation, causal-physical processes, and information-theoretic constraints on states available to systems. After characterizing each sense, I clarify its ontological commitments, its methodological implications, how it figures in explanations, its implications for reduction, and the key manners in which it differs from other senses of mechanism. I conclude that there is no substantive core meaning shared by all senses, and that debates in contemporary philosophy of science can benefit from clarification regarding precisely which sense of mechanism is at stake},
author = {Andersen, Holly},
doi = {10.1111/phc3.12118},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Andersen-2014-A field guide to mechanisms$\backslash$: Part I.pdf:pdf},
isbn = {1747-9991},
issn = {17479991},
journal = {Philosophy Compass},
keywords = {causal,causation,explanation,information,interventions,mechanisms,methodology,processes,reduction},
number = {4},
pages = {284--293},
title = {A field guide to mechanisms: Part II},
volume = {9},
year = {2014}
}

@article{Andersen2016,
abstract = {Abstract. A finer-grained delineation of a given explanandum reveals a nexus of closely related causal and non-causal explanations, complementing one another i},
author = {Andersen, Holly},
doi = {10.1093/bjps/axw023},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Andersen-2014-Complements, Not Competitors$\backslash$: Causal and Mathematical Explanations.pdf:pdf},
issn = {14643537},
journal = {British Journal for the Philosophy of Science},
number = {2},
pages = {485--508},
title = {Complements, Not Competitors: Causal and Mathematical Explanations},
volume = {69},
year = {2016}
}

@article{Angius2017,
author = {Angius, Nicola and Tamburrini, Guglielmo},
doi = {10.1007/s13347-016-0235-1},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Angius-2017-Explaining Engineered Computing Systems' Behaviour$\backslash$: the Role of Abstraction and Idealization.pdf:pdf},
issn = {22105441},
journal = {Philosophy and Technology},
keywords = {Abstraction,Explanation,Idealization,Miscomputation,Philosophy of computer science},
number = {2},
pages = {239--258},
publisher = {Philosophy {\&} Technology},
title = {Explaining Engineered Computing Systems' Behaviour: the Role of Abstraction and Idealization},
volume = {30},
year = {2017}
}

@article{Atmanspacher2008,
abstract = {The dynamics of neuronal systems, briefly neurodynamics, has developed into an attractive and influential research branch within neuroscience. In this paper, we discuss a number of conceptual issues in neurodynamics that are important for an appropriate interpretation and evaluation of its results. We demonstrate their relevance for selected topics of theoretical and empirical work. In particular, we refer to the notions of determinacy and stochasticity in neurodynamics across levels of microscopic, mesoscopic and macroscopic descriptions. The issue of correlations between neural, mental and behavioral states is also addressed in some detail. We propose an informed discussion of conceptual foundations with respect to neurobiological results as a viable step to a fruitful future philosophy of neuroscience.},
author = {Atmanspacher, Harald and Rotter, Stefan},
doi = {10.1007/s11571-008-9067-8},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Atmanspacher-2008-Interpreting neurodynamics$\backslash$: concepts and facts.pdf:pdf},
isbn = {1871-4080 (Print)},
issn = {18714080},
journal = {Cognitive Neurodynamics},
keywords = {Causation,Determinism,Emergence,Mind-brain correlations,Neurodynamics,Ontic and epistemic descriptions,Stochasticity},
number = {4},
pages = {297--318},
pmid = {19003452},
title = {Interpreting neurodynamics: Concepts and facts},
volume = {2},
year = {2008}
}

@book{Ballard2015,
abstract = {The vast differences between the brain's neural circuitry and a computer's silicon circuitry might suggest that they have nothing in common. In fact, as Dana Ballard argues in this book, computational tools are essential for understanding brain function. Ballard shows that the hierarchical organization of the brain has many parallels with the hierarchical organization of computing; as in silicon computing, the complexities of brain computation can be dramatically simplified when its computation is factored into different levels of abstraction. Drawing on several decades of progress in computational neuroscience, together with recent results in Bayesian and reinforcement learning methodologies, Ballard factors the brain's principal computational issues in terms of their natural place in an overall hierarchy. Each of these factors leads to a fresh perspective. A neural level focuses on the basic forebrain functions and shows how processing demands dictate the extensive use of timing-based circuitry and an overall organization of tabular memories. An embodiment level organization works in reverse, making extensive use of multiplexing and on-demand processing to achieve fast parallel computation. An awareness level focuses on the brain's representations of emotion, attention and consciousness, showing that they can operate with great economy in the context of the neural and embodiment substrates.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Ballard, Dana.H},
booktitle = {MIT Press},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
pages = {435},
pmid = {25246403},
title = {Brain Computation as Hierarchical Abstraction},
year = {2015}
}

@article{Batterman2010,
abstract = {This paper examines contemporary attempts to explicate the explanatory role of math- ematics in the physical sciences. Most such approaches involve developing so-called mapping accounts of the relationships between the physical world and mathematical structures. The paper argues that the use of idealizations in physical theorizing poses serious difficulties for such mapping accounts. A new approach to the applicability of mathematics is proposed.},
author = {Batterman, Robert W.},
doi = {10.1093/bjps/axq025},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Batterman-2010-On the Explanatory Role of Mathematics in Empirical Science.pdf:pdf},
isbn = {1464-3537},
issn = {00070882},
journal = {British Journal for the Philosophy of Science},
number = {1},
pages = {1--25},
title = {On the Explanatory Role of Mathematics in Empirical Science Robert W. Batterman ABSTRACT},
volume = {62},
year = {2010}
}


@article{Batterman2014,
abstract = {This article discusses minimal model explanations, which we argue are distinct from various causal, mechanical, difference-making, and so on, strategies prominent in the philosophical literature. We contend that what accounts for the explanatory power of these models is not that they have certain features in common with real systems. Rather, the models are explanatory because of a story about why a class of systems will all display the same large-scale behavior because the details that distinguish them are irrelevant. This story explains patterns across extremely diverse systems and shows how minimal models can be used to understand real systems.},
author = {Batterman, Robert W. and Rice, Collin C.},
doi = {10.1086/676677},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Batterman-2013-Minimal Model Explanations.pdf:pdf},
issn = {0031-8248},
journal = {Philosophy of Science},
number = {3},
pages = {349--376},
title = {Minimal Model Explanations},
url = {http://www.journals.uchicago.edu/doi/10.1086/676677},
volume = {81},
year = {2014}
}
    
@article{Baxendale2018,
author = {Baxendale, Matthew},
doi = {10.1007/s11229-018-1683-1},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Baxendale-2018-Mapping the continuum of research strategies.pdf:pdf},
issn = {0039-7857},
journal = {Synthese},
keywords = {Research strategies,Reduction,Decomposition,Dynami,decomposition,dynamics,mutation theory,reduction,research strategies,somatic,tissue organisational field theory},
number = {January},
publisher = {Springer Netherlands},
title = {Mapping the continuum of research strategies},
url = {http://link.springer.com/10.1007/s11229-018-1683-1},
year = {2018},
volume={1}
}

@article{Bechtel2005,
abstract = {Explanations in the life sciences frequently involve presenting a model of the mechanism taken to be responsible for a given phenomenon. Such explanations depart in numerous ways from nomological explanations commonly presented in philosophy of science. This paper focuses on three sorts of differences. First, scientists who develop mechanistic explanations are not limited to linguistic representations and logical inference; they frequently employ diagrams to characterize mechanisms and simulations to reason about them. Thus, the epistemic resources for presenting mechanistic explanations are considerably richer than those suggested by a nomological framework. Second, the fact that mechanisms involve organized systems of component parts and operations provides direction to both the discovery and testing of mechanistic explanations. Finally, models of mechanisms are developed for specific exemplars and are not represented in terms of universally quantified statements. Generalization involves investigating both the similarity of new exemplars to those already studied and the variations between them. {\textcopyright} 2005 Elsevier Ltd. All rights reserved.},
author = {Bechtel, William and Abrahamsen, Adele},
doi = {10.1016/j.shpsc.2005.03.010},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Course/Bechtel, W. {\&} Abrahamsen, A. (2005) Explanation a Mechanistic Alternative.pdf:pdf},
isbn = {1369-8486},
issn = {13698486},
journal = {Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences},
keywords = {Diagrams,Discovery,Generalization, Mechanistic explanation,Simulation},
number = {2 SPEC. ISS.},
pages = {421--441},
pmid = {19260199},
title = {Explanation: A mechanist alternative},
volume = {36},
year = {2005}
}

@article{Bechtel2010,
abstract = {We consider computational modeling in two fields: chronobiology and cognitive science. In circadian rhythm models, variables generally correspond to properties of parts and operations of the responsible mechanism. A computational model of this complex mechanism is grounded in empirical discoveries and contributes a more refined understanding of the dynamics of its behavior. In cognitive science, on the other hand, computational modelers typically advance de novo proposals for mechanisms to account for behavior. They offer indirect evidence that a proposed mechanism is adequate to produce particular behavioral data, but typically there is no direct empirical evidence for the hypothesized parts and operations. Models in these two fields differ in the extent of their empirical grounding, but they share the goal of achieving dynamic mechanistic explanation. That is, they augment a proposed mechanistic explanation with a computational model that enables exploration of the mechanism's dynamics. Using exemplars from circadian rhythm research, we extract six specific contributions provided by computational models. We then examine cognitive science models to determine how well they make the same types of contributions. We suggest that the modeling approach used in circadian research may prove useful in cognitive science as researchers develop procedures for experimentally decomposing cognitive mechanisms into parts and operations and begin to understand their nonlinear interactions. {\textcopyright} 2010 Elsevier Ltd.},
author = {Bechtel, William and Abrahamsen, Adele},
doi = {10.1016/j.shpsa.2010.07.003},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Course/Bechtel-2010-Dynamic mechanistic explanation$\backslash$: computational modeling of circadian rhythms as an exemplar for cognitive science.pdf:pdf},
isbn = {00393681},
issn = {00393681},
journal = {Studies in History and Philosophy of Science Part A},
keywords = {Circadian rhythms,Cognitive science,Computational models,Dynamic models,Explanation,Mechanistic models,Oscillators},
number = {3},
pages = {321--333},
pmid = {21466124},
publisher = {Elsevier Ltd},
title = {Dynamic mechanistic explanation: Computational modeling of circadian rhythms as an exemplar for cognitive science},
url = {http://dx.doi.org/10.1016/j.shpsa.2010.07.003},
volume = {41},
year = {2010}
}

@article{Bechtel2013,
abstract = {Vitalism is understood as impacting the history of the life sciences, medicine and philosophy, representing an epistemological challenge to the dominance of mechanism over the last 200 years, and partly revived with organicism in early theoretical biology. The contributions in this volume portray the history of vitalism from the end of the Enlightenment to the modern day, suggesting some reassessment of what it means both historically and conceptually. As such it includes a wide range of material, employing both historical and philosophical methodologies, and it is divided fairly evenly between 19th and 20th century historical treatments and more contemporary analysis. This volume presents a significant contribution to the current literature in the history and philosophy of science and the history of medicine.},
author = {Bechtel, William},
doi = {10.1007/978-94-007-2445-7},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Bechtel-2013-Addressing the Vitalist's Challenge to Mechanistic Science$\backslash$: Dynamic Mechanistic Explanation.pdf:pdf},
isbn = {978-94-007-2444-0},
issn = {2211-1948},
keywords = {dynamics,mechanistic explanation,network motifs,non-sequential,organization,vitalism},
pages = {345--370},
title = {Vitalism and the Scientific Image in Post-Enlightenment Life Science, 1800-2010},
url = {http://link.springer.com/10.1007/978-94-007-2445-7},
volume = {2},
year = {2013}
}

@article{bechtel2016,
  title={Investigating neural representations: {T}he tale of place cells},
  author={Bechtel, William},
  journal={Synthese},
  volume={193},
  number={5},
  pages={1287--1321},
  year={2016},
  publisher={Springer}
}

@incollection{bilteanu2017,
  title={Symmetry and Noether Theorem for Brain Microcircuits},
  author={Bilteanu, Liviu and Casanova, Manuel F and Opris, Ioan},
  booktitle={The Physics of the Mind and Brain Disorders},
  pages={129--153},
  year={2017},
  publisher={Springer}
}

@article{bokulich2011,
  title={How scientific models can explain},
  author={Bokulich, Alisa},
  journal={Synthese},
  volume={180},
  number={1},
  pages={33--45},
  year={2011},
  publisher={Springer}
}

@incollection{bokulich2017,
  title={Models and explanation},
  author={Bokulich, Alisa},
  booktitle={Springer Handbook of Model-Based Science},
  pages={103--118},
  year={2017},
  publisher={Springer}
}

@article{bokulich2018,
  title={Searching for Noncausal Explanations in a Sea of Causes},
  author={Bokulich, Alisa},
  journal={Explanation Beyond Causation: Philosophical Perspectives on Non-Causal Explanations},
  pages={141},
  year={2018},
  publisher={Oxford University Press}
}

@article{Boone2016,
author={Boone, W. and Piccinini, G.},
doi = {10.1086/687855},
journal = {Philosophy of Science},
number = {5},
pages = {686--697},
title = {Mechanistic Abstraction},
url = {http://www.journals.uchicago.edu/doi/10.1086/687855},
volume = {83},
year = {2016}
}

@article{Bradley2018,
author = {Bradley, Darren},
doi = {10.1093/bjps/axy033/5047771},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Bradley-2016-Should Explanations Omit the Details?.pdf:pdf},
journal = {British Journal for the Philosophy of Science},
number = {August},
pages = {1--37},
title = {Should Explanations Omit the Details?},
year = {2018}
}

@article{Brigandt2013,
abstract = {The paper discusses how systems biology is working toward complex accounts that integrate explanation in terms of mechanisms and explanation by mathematical models-which some philosophers have viewed as rival models of explanation. Systems biology is an integrative approach, and it strongly relies on mathematical modeling. Philosophical accounts of mechanisms capture integrative in the sense of multilevel and multifield explanations, yet accounts of mechanistic explanation (as the analysis of a whole in terms of its structural parts and their qualitative interactions) have failed to address how a mathematical model could contribute to such explanations. I discuss how mathematical equations can be explanatorily relevant. Several cases from systems biology are discussed to illustrate the interplay between mechanistic research and mathematical modeling, and I point to questions about qualitative phenomena (rather than the explanation of quantitative details), where quantitative models are still indispensable to the explanation. Systems biology shows that a broader philosophical conception of mechanisms is needed, which takes into account functional-dynamical aspects, interaction in complex networks with feedback loops, system-wide functional properties such as distributed functionality and robustness, and a mechanism's ability to respond to perturbations (beyond its actual operation). I offer general conclusions for philosophical accounts of explanation.},
author = {Brigandt, Ingo},
doi = {10.1016/j.shpsc.2013.06.002},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Brigandt-2013-Systems biology and the integration of mechanistic explanation and mathematical explanation.pdf:pdf},
isbn = {1879-2499 (Electronic)$\backslash$r1369-8486 (Linking)},
issn = {13698486},
journal = {Studies in History and Philosophy of Science Part C :Studies in History and Philosophy of Biological and Biomedical Sciences},
keywords = {Integration,Mathematical models,Mechanisms,Mechanistic explanation,Systems biology},
number = {4},
pages = {477--492},
pmid = {23863399},
publisher = {Elsevier Ltd},
title = {Systems biology and the integration of mechanistic explanation and mathematical explanation},
url = {http://dx.doi.org/10.1016/j.shpsc.2013.06.002},
volume = {44},
year = {2013}
}

@incollection{Brigandt2017,
author = {Brigandt, Ingo and Green, Sara and O'Malley, M. A.},
booktitle = {The Routledge Handbook of Mechanisms and Mechanical Philosophy},
publisher={Routledge and Routledge},
doi = {10.4324/9781315731544},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Brigandt-2016-Systems Biology and Mechanistic Explanation.pdf:pdf},
isbn = {9781317552307},
pages = {1--474},
title = {Systems Biology and Mechanistic Explanation},
year = {2017}
}

@article{Burak2009,
abstract = {Grid cells in the rat entorhinal cortex display strikingly regular firing responses to the animal's position in 2-D space and have been hypothesized to form the neural substrate for dead-reckoning. However, errors accumulate rapidly when velocity inputs are integrated in existing models of grid cell activity. To produce grid-cell-like responses, these models would require frequent resets triggered by external sensory cues. Such inadequacies, shared by various models, cast doubt on the dead-reckoning potential of the grid cell system. Here we focus on the question of accurate path integration, specifically in continuous attractor models of grid cell activity. We show, in contrast to previous models, that continuous attractor models can generate regular triangular grid responses, based on inputs that encode only the rat's velocity and heading direction. We consider the role of the network boundary in the integration performance of the network and show that both periodic and aperiodic networks are capable of accurate path integration, despite important differences in their attractor manifolds. We quantify the rate at which errors in the velocity integration accumulate as a function of network size and intrinsic noise within the network. With a plausible range of parameters and the inclusion of spike variability, our model networks can accurately integrate velocity inputs over a maximum of approximately 10-100 meters and approximately 1-10 minutes. These findings form a proof-of-concept that continuous attractor dynamics may underlie velocity integration in the dorsolateral medial entorhinal cortex. The simulations also generate pertinent upper bounds on the accuracy of integration that may be achieved by continuous attractor dynamics in the grid cell network. We suggest experiments to test the continuous attractor model and differentiate it from models in which single cells establish their responses independently of each other.},
archivePrefix = {arXiv},
arxivId = {0811.1826},
author = {Burak, Yoram and Fiete, Ila R},
doi = {10.1371/journal.pcbi.1000291},
eprint = {0811.1826},
file = {:home/abel/Documents/Projects/ACT/MES/Neuros/Burak-2008-Accurate Path Integration in Continuous Attractor Network Models of Grid Cells.pdf:pdf},
isbn = {1553-7358 (Electronic)$\backslash$n1553-734X (Linking)},
issn = {1553734X},
journal = {PLoS Computational Biology},
number = {2},
pmid = {19229307},
title = {Accurate path integration in continuous attractor network models of grid cells},
volume = {5},
year = {2009}
}

@article{Chirimuuta2014,
abstract = {In a recent paper, Kaplan (Synthese 183:339--373, 2011) takes up the task of extending Cravers (Explaining the brain, 2007) mechanistic account of explanation in neuroscience to the new territory of computational neuroscience. He presents the model to mechanism mapping (3M) criterion as a condition for a models explanatory adequacy. This mechanistic approach is intended to replace earlier accounts which posited a level of computational analysis conceived as distinct and autonomous from underlying mechanistic details. In this paper I discuss work in computational neuroscience that creates difficulties for the mechanist project. Carandini and Heeger (Nat Rev Neurosci 13:51--62, 2012) propose that many neural response properties can be understood in terms of canonical neural computations. These are standard computational modules that apply the same fundamental operations in a variety of contexts.textquotedblrightImportantly, these computations can have numerous biophysical realisations, and so straightforward examination of the mechanisms underlying these computations carries little explanatory weight. Through a comparison between this modelling approach and minimal models in other branches of science, I argue that computational neuroscience frequently employs a distinct explanatory style, namely, efficient coding explanation. Such explanations cannot be assimilated into the mechanistic framework but do bear interesting similarities with evolutionary and optimality explanations elsewhere in biology},
author = {Chirimuuta, M.},
doi = {10.1007/s11229-013-0369-y},
isbn = {0039-7857},
issn = {00397857},
journal = {Synthese},
keywords = {Biology,Computation,Explanation,Mechanism,Neuroscience},
number = {2},
pages = {127--153},
title = {Minimal models and canonical neural computations: The distinctness of computational explanation in neuroscience},
volume = {191},
year = {2014}
}

@misc{Chirimuuta2017a,
  title={The development and application of efficient coding explanation in neuroscience},
  author={Chirimuuta, M},
  journal={Explanation beyond causation},
  year={2017},
  publisher={Oxford University Press Oxford}
}

@article{Chirimuuta2017b,
author = {Chirimuuta, M.},
doi = {10.1093/bjps/axw034},
isbn = {0007-0882},
issn = {0007-0882},
journal = {The British Journal for the Philosophy of Science},
pages = {849--880},
title = {Explanation in Computational Neuroscience: Causal and Non-causal},
url = {https://academic.oup.com/bjps/article/3069966/Explanation},
volume = {69},
year = {2017}
}

@article{chowdhury2016,
  title={A functorial Dowker theorem and persistent homology of asymmetric networks},
  author={Chowdhury, Samir and M{\'e}moli, Facundo},
  journal={arXiv preprint arXiv:1608.05432},
  year={2016}
}

@article{chowdhury2017,
  title={Distances and Isomorphism between Networks and the Stability of Network Invariants},
  author={Chowdhury, Samir and M{\'e}moli, Facundo},
  journal={arXiv preprint arXiv:1708.04727},
  year={2017}
}

@book{weber2004,
  title={Philosophy of experimental biology},
  author={Weber, Marcel},
  year={2004},
  publisher={Cambridge university press}
}

@article{bogen2008,
  title={The {H}odgkin-{H}uxley equations and the concrete model: Comments on {C}raver, {S}chaffner, and {W}eber},
  author={Bogen, Jim},
  journal={Philosophy of science},
  volume={75},
  number={5},
  pages={1034--1046},
  year={2008},
  publisher={The University of Chicago Press}
}

@article{Craver2006,
abstract = {Not all models are explanatory. Some models are data summaries. Some models sketch explanations but leave crucial details unspecified or hidden behind filler terms. Some models are used to conjecture a how-possibly explanation without regard to whether it is a how-actually explanation. I use the Hodgkin and Huxley model of the action potential to illustrate these ways that models can be useful without explaining. I then use the subsequent development of the explanation of the action potential to show what is required of an adequate mechanistic model. Mechanistic models are explanatory.},
author = {Craver, Carl F.},
doi = {10.1007/s11229-006-9097-x},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Craver-2006-When mechanistic models explain.pdf:pdf},
isbn = {0039-7857},
issn = {00397857},
journal = {Synthese},
keywords = {Action,Electrophysiology,Explanation,Functional Analysis,Hodgkin,Huxley,Mechanisms,Models,Potential},
number = {3},
pages = {355--376},
title = {When mechanistic models explain},
volume = {153},
year = {2006}
}

@book{craver2007a,
  title={Explaining the brain: Mechanisms and the mosaic unity of neuroscience},
  author={Craver, Carl F},
  year={2007},
  publisher={Oxford University Press}
}

@article{craver2007b,
abstract = {We argue that intelligible appeals to interlevel causes (top-down and bottom-up) can be understood, without remainder, as appeals to mechanistically mediated effects. Mechanistically mediated effects are hybrids of causal and constitutive relations, where the causal relations are exclusively intralevel. The idea of causation would have to stretch to the breaking point to accommodate interlevel causes. The notion of a mechanistically mediated effect is preferable because it can do all of the required work without appealing to mysterious interlevel causes. When interlevel causes can be translated into mechanistically mediated effects, the posited relationship is intelligible and should raise no special philosophical objections. When they cannot, they are suspect.},
author = {Craver, Carl F. and Bechtel, William},
doi = {10.1007/s10539-006-9028-8},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Course/Craver, C. {\&} Bechtel, W. (2007). Top-Down Causation without Top-Down Causes.pdf:pdf},
isbn = {0169-3867},
issn = {01693867},
journal = {Biology and Philosophy},
keywords = {Constitution,Emergence,Explanation,Interlevel causation,Levels,Mechanisms,Reduction,Top-down causation},
number = {4},
pages = {547--563},
pmid = {21892179},
title = {Top-down causation without top-down causes},
volume = {22},
year = {2007}
}

@article{craver2008,
  title={Physical law and mechanistic explanation in the {H}odgkin and {H}uxley model of the action potential},
  author={Craver, Carl F},
  journal={Philosophy of Science},
  volume={75},
  number={5},
  pages={1022--1033},
  year={2008},
  publisher={The University of Chicago Press}
}

@article{Craver2018,
author = {Craver, Carl and Kaplan, David M.},
doi = {10.1093/bjps/axy015},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Craver-2017-Are More Details Better? On the Norms of Completeness for Mechanistic Explanations.pdf:pdf},
isbn = {0007-0882},
issn = {0007-0882},
journal = {The British Journal for the Philosophy of Science},
title = {Are More Details Better? On the Norms of Completeness for Mechanistic Explanations},
url = {http://academic.oup.com/bjps/advance-article/doi/10.1093/bjps/axy015/4816342},
year = {2018}
}

@article{weber2008,
  title={Causes without mechanisms: Experimental regularities, physical laws, and neuroscientific explanation},
  author={Weber, Marcel},
  journal={Philosophy of Science},
  volume={75},
  number={5},
  pages={995--1007},
  year={2008},
  publisher={The University of Chicago Press}
}

@article{bickle2001,
  title={Philosophy of neuroscience},
  author={Bickle, John and Hardcastle, Valerie Gray},
  journal={e LS},
  year={2001},
  publisher={Wiley Online Library}
}

@article{morris1981,
  title={Voltage oscillations in the barnacle giant muscle fiber},
  author={Morris, Catherine and Lecar, Harold},
  journal={Biophysical journal},
  volume={35},
  number={1},
  pages={193--213},
  year={1981},
  publisher={Elsevier}
}

@article{curto2016,
  title={Pattern completion in symmetric threshold-linear networks},
  author={Curto, Carina and Morrison, Katherine},
  journal={Neural computation},
  volume={28},
  number={12},
  pages={2825--2852},
  year={2016},
  publisher={MIT Press}
}

@article{curto2017,
  title={What can topology tell us about the neural code?},
  author={Curto, Carina},
  journal={Bulletin of the American Mathematical Society},
  volume={54},
  number={1},
  pages={63--78},
  year={2017}
}

@article{dabaghian2012,
  title={A topological paradigm for hippocampal spatial map formation using persistent homology},
  author={Dabaghian, Yuri and M{\'e}moli, Facundo and Frank, Loren and Carlsson, Gunnar},
  journal={PLoS computational biology},
  volume={8},
  number={8},
  pages={e1002581},
  year={2012},
  publisher={Public Library of Science}
}

@article{Darrason2018,
abstract = {Medical explanations have often been thought on the model of biological ones and are frequently defined as mechanistic explanations of a biological dysfunc- tion. In this paper, I argue that topological explanations, which have been described in ecology or in cognitive sciences, can also be found in medicine and I discuss the relationships between mechanistic and topological explanations in medicine, through the example of network medicine and medical genetics. Network medicine is a recent discipline that relies on the analysis of various disease networks (including disease- gene networks) in order to find organizing principles in disease explanation. My aim is to show how topological explanations in network medicine can help solving the conceptual issues that pure mechanistic explanations of the genetics of disease are currently facing, namely the crisis of the concept of genetic disease, the progressive geneticization of diseases and the dissolution of the distinction between monogenic and polygenic diseases.However, I will also argue that topological explanations should not be considered as independent and radically different from mechanistic explana- tions for at least two reasons. First, in network medicine, topological explanations depend on and use mechanistic information. Second, they leave out some missing gaps in disease explanation that require, in turn, the development of new mechanistic explanations. Finally, I will insist on the specific contribution of topological expla- nations in medicine: they push us to develop an explanation of disease in general, instead of focusing on single explanations of individual diseases. This last point may have major consequences for biomedical research.},
author = {Darrason, Marie},
doi = {10.1007/s11229-015-0983-y},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Darrason-2018-Mechanistic and topological explanations in medicine$\backslash$: the case of medical genetics and network medicine.pdf:pdf},
issn = {15730964},
journal = {Synthese},
keywords = {Genetic disease,Geneticization,Mechanistic explanation,Network medicine,Philosophy of medicine,Philosophy of science,Topological explanation},
number = {1},
pages = {1--27},
publisher = {Springer Netherlands},
title = {Mechanistic and topological explanations in medicine: the case of medical genetics and network medicine},
volume = {195},
year = {2018}
}

@article{kalies2005,
  title={An algorithmic approach to chain recurrence},
  author={Kalies, William D and Mischaikow, Konstantin and Vandervorst, Robert CAM},
  journal={Foundations of Computational Mathematics},
  volume={5},
  number={4},
  pages={409--449},
  year={2005},
  publisher={Springer}
}

@book{cormen1990,
  title={Introduction to algorithms},
  author={Cormen, Thomas H and Leiserson, Charles E and Rivest, Ronald L and Stein, Clifford},
  publisher={The MIT Electrical Engineering and Computer Science Series, MIT Press},
  year={1990}
}

@book{spanier1989,
  title={Algebraic topology},
  author={Spanier, Edwin H},
  year={1989},
  publisher={Springer Science \& Business Media}
}

@article{smoller1984,
  title={Shock waves and reaction-diffusion equations},
  author={Smoller, Joel},
  journal={Bull. Amer. Math. Soc},
  volume={11},
  pages={204--214},
  year={1984}
}

@article{desmond2019,
  title={Shades of Grey: Granularity, Pragmatics, and Non-Causal Explanation},
  author={Desmond, Hugh},
  journal={Perspectives on Science},
  volume={27},
  number={1},
  pages={68--87},
  year={2019},
  publisher={MIT Press}
}

@article{deville2013,
  title={Modular dynamical systems on networks},
  author={DeVille, Lee and Lerman, Eugene},
  journal={arXiv preprint arXiv:1303.3907},
  year={2013}
}

@article{Elber-Dorozko2018,
author = {Elber-Dorozko, Lotem},
doi = {10.1007/s11229-018-01901-3},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Dorozko-2018-Manipulation is key$\backslash$: on why non-mechanistic explanations in the cognitive sciences also describe relations of manipulation and control.pdf:pdf},
isbn = {1122901801},
issn = {0039-7857},
journal = {Synthese},
keywords = {Explanation,Non-causal explanations,Manipulation a,cognitive sciences,counterfactual dependence,explanation,manipulation and control,non-causal explanations,the},
publisher = {Springer Netherlands},
title = {Manipulation is key: on why non-mechanistic explanations in the cognitive sciences also describe relations of manipulation and control},
url = {http://link.springer.com/10.1007/s11229-018-01901-3},
year = {2018}
}

@article{egan2017,
  title={Function-Theoretic Explanation},
  author={Egan, Frances},
  journal={Explanation and integration in mind and brain science},
  pages={145},
  year={2017},
  publisher={Oxford University Press}
}

@article{kaiser1977,
  title={Limit cycle model for brain waves},
  author={Kaiser, Fr},
  journal={Biological cybernetics},
  volume={27},
  number={3},
  pages={155--163},
  year={1977},
  publisher={Springer}
}

@article{schmidt2014,
  title={Dynamics on networks: The role of local dynamics and global networks on the emergence of hypersynchronous neural activity},
  author={Schmidt, Helmut and Petkov, George and Richardson, Mark P and Terry, John R},
  journal={PLoS computational biology},
  volume={10},
  number={11},
  year={2014},
  publisher={Public Library of Science}
}

@article{hopfield1994,
  title={Neurons, dynamics and computation},
  author={Hopfield, John J},
  journal={Physics Today},
  volume={47},
  number={2},
  pages={40--47},
  year={1994},
  publisher={[New York, American Institute of Physics]}
}

@article{heilman2004,
  title={Computational models of epileptiform activity in single neurons},
  author={Heilman, Avram D and Quattrochi, James},
  journal={Biosystems},
  volume={78},
  number={1-3},
  pages={1--21},
  year={2004},
  publisher={Elsevier}
}

@article{wendling2016,
  title={Computational models of epileptiform activity},
  author={Wendling, Fabrice and Benquet, Pascal and Bartolomei, Fabrice and Jirsa, Viktor},
  journal={Journal of neuroscience methods},
  volume={260},
  pages={233--251},
  year={2016},
  publisher={Elsevier}
}

@incollection{kaiser1987,
  title={The role of chaos in biological systems},
  author={Kaiser, F},
  booktitle={Energy transfer dynamics},
  pages={224--236},
  year={1987},
  publisher={Springer}
}

@article{kadji2007,
  title={Nonlinear dynamics and strange attractors in the biological system},
  author={Kadji, HG Enjieu and Orou, JB Chabi and Yamapi, R and Woafo, P},
  journal={Chaos, Solitons \& Fractals},
  volume={32},
  number={2},
  pages={862--882},
  year={2007},
  publisher={Elsevier}
}

@article{mccarley1986,
  title={A limit cycle mathematical model of the {REM} sleep oscillator system},
  author={McCARLEY, ROBERT W and Massaquoi, STEVE G},
  journal={American Journal of Physiology-Regulatory, Integrative and Comparative Physiology},
  volume={251},
  number={6},
  pages={R1011--R1029},
  year={1986}
}

@article{benjamin2012,
  title={A phenomenological model of seizure initiation suggests network structure may explain seizure frequency in idiopathic generalised epilepsy},
  author={Benjamin, Oscar and Fitzgerald, Thomas HB and Ashwin, Peter and Tsaneva-Atanasova, Krasimira and Chowdhury, Fahmida and Richardson, Mark P and Terry, John R},
  journal={The Journal of Mathematical Neuroscience},
  volume={2},
  number={1},
  pages={1},
  year={2012},
  publisher={SpringerOpen}
}

@article{bocchio2017,
  title={Synaptic plasticity, engrams, and network oscillations in amygdala circuits for storage and retrieval of emotional memories},
  author={Bocchio, Marco and Nabavi, Sadegh and Capogna, Marco},
  journal={Neuron},
  volume={94},
  number={4},
  pages={731--743},
  year={2017},
  publisher={Elsevier}
}

@article{hochstein2016,
  title={One mechanism, many models: A distributed theory of mechanistic explanation},
  author={Hochstein, Eric},
  journal={Synthese},
  volume={193},
  number={5},
  pages={1387--1407},
  year={2016},
  publisher={Springer}
}

@article{brette2005,
  title={Adaptive exponential integrate-and-fire model as an effective description of neuronal activity},
  author={Brette, Romain and Gerstner, Wulfram},
  journal={Journal of neurophysiology},
  volume={94},
  number={5},
  pages={3637--3642},
  year={2005},
  publisher={American Physiological Society}
}

@book{fraassen1980,
  title={The scientific image},
  author={Van Fraassen, Bas C},
  year={1980},
  publisher={Oxford University Press}
}

@article{giusti2015,
  title={Clique topology reveals intrinsic geometric structure in neural correlations},
  author={Giusti, Chad and Pastalkova, Eva and Curto, Carina and Itskov, Vladimir},
  journal={Proceedings of the National Academy of Sciences},
  volume={112},
  number={44},
  pages={13455--13460},
  year={2015},
  publisher={National Acad Sciences}
}

@article{Green2018,
abstract = {The increasing application of network models to interpret biological systems raises a number of important methodological and epistemological questions. What novel insights can network analysis provide in biology? Are network approaches an extension of or in conflict with mechanistic research strategies? When and how can network and mechanistic approaches interact in productive ways? In this paper we address these questions by focusing on how biological networks are represented and analyzed in a diverse class of case studies. Our examples span from the investigation of organizational properties of biological networks using tools from graph theory to the application of dynamical systems theory to understand the behavior of complex biological systems. We show how network approaches support and extend traditional mechanistic strategies but also offer novel strategies for dealing with biological complexity.},
author = {Green, Sara and Şerban, Maria and Scholl, Raphael and Jones, Nicholaos and Brigandt, Ingo and Bechtel, William},
doi = {10.1007/s11229-016-1307-6},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Green-2017-Network analyses in systems biology$\backslash$: new strategies for dealing with biological complexity.pdf:pdf},
issn = {15730964},
journal = {Synthese},
keywords = {Biological networks,Mechanistic research strategies,Network modeling,Representation,Systems biology},
number = {4},
pages = {1751--1777},
title = {Network analyses in systems biology: new strategies for dealing with biological complexity},
volume = {195},
year = {2018}
}

@article{Felline2015,
abstract = {This paper investigates the relationship between structural explanation (SE) and the New Mechanistic account of explanation. The aim of this paper is twofold: firstly, to argue that some phenomena in the domain of fundamental physics, although mechanically brute, are structurally explained; and secondly, by elaborating on the contrast between SE and mechanistic explanation (ME) to better clarify some features of SE. Finally, this paper will argue that, notwithstanding their apparently antithetical character, SE and ME can be reconciled within a unified account of general scientific explanation.},
author = {Felline, Laura},
doi = {10.1007/s11229-015-0746-9},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Felline-2018-Mechanisms meet structural explanation.pdf:pdf},
issn = {15730964},
journal = {Synthese},
keywords = {Epistemic conception of explanation,Mechanistic explanation,Structural explanation},
number = {1},
pages = {99--114},
publisher = {Springer Netherlands},
title = {Mechanisms meet structural explanation},
volume = {195},
year = {2015}
}

@book{fresco2014,
  title={Physical computation and cognitive science},
  author={Fresco, Nir},
  year={2014},
  publisher={Springer}
}

@article{heinze2017,
  title={Neural coding: Bumps on the move},
  author={Heinze, Stanley},
  journal={Current Biology},
  volume={27},
  number={11},
  pages={R409--R412},
  year={2017},
  publisher={Elsevier}
}

@article{huneman2010,
  title={Topological explanations and robustness in biological sciences},
  author={Huneman, Philippe},
  journal={Synthese},
  volume={177},
  number={2},
  pages={213--245},
  year={2010},
  publisher={Springer}
}

@article{Huneman2018a,
abstract = {Besides mechanistic explanations of phenomena, which have been seriously investigated in the last decade, biology and ecology also include explanations that pin-point specific mathematical properties as explanatory of the explanandum under focus. Among these structural explanations, one finds topological explanations, and recent science pervasively relies on them. This reliance is especially due to the necessity to model large sets of data with no practical possibility to track the proper activities of all the numerous entities. The paper first defines topological explanations and then explains why topological explanations and mechanisms are different in principle. Then it shows that they are pervasive both in the study of networks—whose importance has been increasingly acknowledged at each level of the biological hierarchy—and in contexts where the notion of selective neutrality is crucial; this allows me to capture the difference between mechanisms and topological explanations in terms of practical modelling practices. The rest of the paper investigates how in practice mechanisms and topologies are combined. They may be articulated in theoretical structures and explanatory strategies, first through a relation of constraint, second in interlevel the-ories (Sect. 3), or they may condition each other (Sect. 4). Finally, I explore how a particular model can integrate mechanistic informations, by focusing on the recent practice of merging networks in ecology and its consequences upon multiscale mod-elling (Sect. 5).},
author = {Huneman, Philippe},
doi = {10.1007/s11229-015-0808-z},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Pres/Huneman-2015-Diversifying the picture of explanations in biological sciences$\backslash$: ways of combining topology with mechanisms.pdf:pdf},
issn = {15730964},
journal = {Synthese},
keywords = {Community ecology,Evolution,Mechanism,Networks,Population genetics,Topological explanation},
number = {1},
pages = {115--146},
publisher = {Springer Netherlands},
title = {Diversifying the picture of explanations in biological sciences: ways of combining topology with mechanisms},
volume = {195},
year = {2018}
}

@article{huneman2018b,
  title={Outlines of a theory of structural explanations},
  author={Huneman, Philippe},
  journal={Philosophical Studies},
  volume={175},
  number={3},
  pages={665--702},
  year={2018},
  publisher={Springer}
}

@incollection{issad2015,
  title={Are Dynamic Mechanistic Explanations Still Mechanistic?},
  author={Issad, Tarik and Malaterre, Christophe},
  booktitle={Explanation in Biology},
  pages={265--292},
  year={2015},
  publisher={Springer}
}

@article{itskov2018,
  title={Hyperplane Neural Codes and the Polar Complex},
  author={Itskov, Vladimir and Kunin, Alex and Rosen, Zvi},
  journal={arXiv preprint arXiv:1801.02304},
  year={2018}
}

@article{Jones2014,
author = {Jones, Nicholaos},
doi = {10.1007/s10670-014-9598-9},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Jones-2014-Bowtie Structures, Pathway Diagrams, and Topological Explanation.pdf:pdf},
issn = {15728420},
journal = {Erkenntnis},
number = {5},
pages = {1135--1155},
title = {Bowtie structures, pathway diagrams, and topological explanation},
volume = {79},
year = {2014}
}

@article{Jansson2017,
author = {Jansson, Lina and Saatsi, Juha},
doi = {10.1093/bjps/axx016},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Jansson-2017-Explanatory Abstractions.pdf:pdf},
issn = {0007-0882},
journal = {The British Journal For The Philosophy Of Science},
pages = {1--28},
title = {Explanatory Abstractions},
url = {http://fdslive.oup.com/www.oup.com/pdf/production{\_}in{\_}progress.pdf},
volume = {0},
year = {2017}
}

@book{jensen1998,
  title={Self-organized criticality: {E}mergent complex behavior in physical and biological systems},
  author={Jensen, Henrik Jeldtoft},
  volume={10},
  year={1998},
  publisher={Cambridge university press}
}

@article{julien2019,
  title={Understanding does not depend on (causal) explanation},
  author={Verreault-Julien, Philippe},
  journal={European journal for philosophy of science},
  volume={9},
  number={2},
  pages={18},
  year={2019},
  publisher={Springer}
}

@article{kampen1961,
  title={A power series expansion of the master equation},
  author={Kampen, NG van},
  journal={Canadian Journal of Physics},
  volume={39},
  number={4},
  pages={551--567},
  year={1961},
  publisher={NRC Research Press}
}

@article{kaplan2011a,
  title={Explanation and description in computational neuroscience},
  author={Kaplan, David Michael},
  journal={Synthese},
  volume={183},
  number={3},
  pages={339},
  year={2011},
  publisher={Springer}
}

@article{Kaplan2011b,
abstract = {We argue that dynamical and mathematical models in systems and cognitive neuroscience explain (rather than redescribe) a phenomenon only if there is a plausible mapping between elements in the model and elements in the mechanism for the phenomenon. We demonstrate how this model-to-mechanism-mapping constraint, when satisfied, endows a model with explanatory force with respect to the phenomenon to be explained. Several paradigmatic models including the Haken-Kelso-Bunz model of bimanual coordination and the 'difference of Gaussians' model of visual receptive fields are explored.},
author = {Kaplan, David Michael and Craver, Carl F.},
doi = {10.1086/661755},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Kaplan-2011-The Explanatory Force of Dynamical and Mathematical Models in Neuroscience.pdf:pdf},
isbn = {0031-8248},
issn = {0031-8248},
journal = {Philosophy of Science},
number = {4},
pages = {601--627},
title = {The Explanatory Force of Dynamical and Mathematical Models in Neuroscience: A Mechanistic Perspective},
url = {http://www.journals.uchicago.edu/doi/10.1086/661755},
volume = {78},
year = {2011}
}

@article{Kaplan2011c,
abstract = {While agreeing that dynamical models play a major role in cognitive science, we reject Stepp, Chemero, and Turvey's contention that they constitute an alternative to mechanistic explanations. We review several problems dynamical models face as putative explanations when they are not grounded in mechanisms. Further, we argue that the opposition of dynamical models and mechanisms is a false one and that those dynamical models that characterize the operations of mechanisms overcome these problems. By briefly considering examples involving the generation of action potentials and circadian rhythms, we show how decomposing a mechanism and modeling its dynamics are complementary endeavors.},
author = {Kaplan, David M. and Bechtel, William},
doi = {10.1111/j.1756-8765.2011.01147.x},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Kaplan-2011-Dynamical Models$\backslash$: An Alternative or Complement to Mechanistic Explanations?.pdf:pdf},
isbn = {17568757},
issn = {17568757},
journal = {Topics in Cognitive Science},
keywords = {Dynamical models,Explanation,HKB model,Mechanistic explanation,Predictivism},
number = {2},
pages = {438--444},
pmid = {25164303},
title = {Dynamical models: An alternative or complement to mechanistic explanations?},
volume = {3},
year = {2011}
}

@article{Kaplan2015,
author = {Kaplan, David Michael},
doi = {10.1007/s10539-015-9499-6},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Kaplan-2015-Moving parts$\backslash$: the natural alliance between dynamical and mechanistic modeling approaches.pdf:pdf},
issn = {15728404},
journal = {Biology and Philosophy},
keywords = {Dynamics,Explanation,Mechanism,Models,Neuroscience},
number = {6},
pages = {757--786},
publisher = {Springer Netherlands},
title = {Moving parts: the natural alliance between dynamical and mechanistic modeling approaches},
volume = {30},
year = {2015}
}



@article{Kostic2014,
author = {Kosti{\'c}, Daniel},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Pres/Kostic-2014-Does network analysis provide a novel kind of topological explanations in life and brain sciences?.pdf:pdf},
pages = {1--4},
journal={Causality and Complexity in the Sciences},
title = {Does network analysis provide a novel kind of topological explanations in life and brain sciences ?},
year = {2014}
}

@article{Kostic2018a,
author = {Kosti{\'c}, Daniel},
doi = {10.1007/s11229-016-1257-z},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Kostic-2016-Mechanistic and topological explanations$\backslash$: an introduction.pdf:pdf},
issn = {15730964},
journal = {Synthese},
keywords = {Mechanistic explanation,Topological explanation},
number = {1},
pages = {1--10},
title = {Mechanistic and topological explanations: an introduction},
volume = {195},
year = {2018}
}

@article{Kostic2018b,
author = {Kosti{\'c}, Daniel},
doi = {10.1007/s11229-016-1248-0},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Kostic-2016-The topological realization.pdf:pdf},
issn = {15730964},
journal = {Synthese},
keywords = {Local and global levels,Micro and macro scales,Multiple realizability,Network analysis,Semantic realization,Topological explanation,Topological realization},
number = {1},
pages = {79--98},
title = {The topological realization},
volume = {195},
year = {2018}
}


@misc{kostic2018c,
month = {May},
title = {Minimal structure explanations, scientific understanding and explanatory depth},
author = {Daniel Kostic},
year = {2018},
keywords = {Minimal structure explanations, scientific understanding, explanatory depth, topological explanations.},
url = {http://philsci-archive.pitt.edu/14641/}
}


@article{Lamb2014,
author = {Lamb, Maurice and Chemero, Anthony},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Lamb-2014-Structure and Application of Dynamical Models in Cognitive Science.pdf:pdf},
journal = {Proceedings of the 36th Annual Conference of the Cognitive Science Society},
keywords = {dynamic systems,explanation,hkb,mechanism,neural field model,neuroscience,tripartite scheme},
pages = {809--814},
title = {Structure and Application of Dynamical Models in Cognitive Science},
year = {2014}
}

@article{legare1987,
  title={The use of general systems theory as metatheory for developing and evaluating theories in the neurosciences},
  author={Legare, Miriam},
  journal={Behavioral science},
  volume={32},
  number={2},
  pages={106--120},
  year={1987},
  publisher={Wiley Online Library}
}

@article{Levy2013,
abstract = {Proponents of mechanistic explanation all acknowledge the importance of organization. But they have also tended to emphasize specificity with respect to parts and operations in mechanisms. We argue that in understanding one important mode of organization--patterns of causal connectivity--a successful explanatory strategy abstracts from the specifics of the mechanism and invokes tools such as those of graph theory to explain how mechanisms with a particular mode of connectivity will behave. We discuss the connection between organization, abstraction, and mechanistic explanation and illustrate our claims by looking at an example from recent research on so-called network motifs. ABSTRACT FROM AUTHOR]; Copyright of Philosophy of Science is the property of Philosophy of Science Association and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)},
author = {Levy, Arnon and Bechtel, William},
doi = {10.1086/670300},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Levy-2013-Abstraction and the Organization of Mechanisms.pdf:pdf},
isbn = {00318248},
issn = {0031-8248},
journal = {Philosophy of Science},
number = {2},
pages = {241--261},
title = {Abstraction and the Organization of Mechanisms},
url = {http://www.journals.uchicago.edu/doi/10.1086/670300},
volume = {80},
year = {2013}
}

@article{levy2014,
  title={What was {H}odgkin and {H}uxley’s achievement?},
  author={Levy, Arnon},
  journal={The British Journal for the Philosophy of Science},
  volume={65},
  number={3},
  pages={469--492},
  year={2014},
  publisher={Oxford University Press}
}

@article{lynn2018,
  title={The physics of brain network structure, function, and control},
  author={Lynn, Christopher W and Bassett, Danielle S},
  journal={arXiv preprint arXiv:1809.06441},
  year={2018}
}

@article{lyon2013,
  title={Why are normal distributions normal?},
  author={Lyon, Aidan},
  journal={The British Journal for the Philosophy of Science},
  volume={65},
  number={3},
  pages={621--649},
  year={2013},
  publisher={Oxford University Press}
}

@article{Lyre2017,
abstract = {Proponents of mechanistic explanations have recently proclaimed that all explanations in the neurosciences appeal to mechanisms. The purpose of the paper is to critically assess this statement and to develop an integrative account that connects a large range of both mechanistic and dynamical explanations. I develop and defend four theses about the relationship between dynamical and mechanistic explanations: that dynamical explanations are structurally grounded, that they are multiply realiz-able, possess realizing mechanisms and provide a powerful top-down heuristic. Four examples shall support my points: the harmonic oscillator, the Haken–Kelso–Bunz model of bimanual coordination, the Watt governor and the Gierer–Meinhardt model of biological pattern formation. I also develop the picture of " horizontal " and " vertical " directions of explanations to illustrate the different perspectives of the dynamical and mechanistic approach as well as their potential integration by means of intersection points.},
author = {Lyre, Holger},
doi = {10.1007/s11229-017-1616-4},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Lyre-2017-Structures, dynamics and mechanisms in neuroscience$\backslash$: an integrative account.pdf:pdf},
issn = {15730964},
journal = {Synthese},
keywords = {Dynamical explanations,Generalizability,Gierer–Meinhardt model,HKB model,Harmonic oscillator,Horizontal versus vertical explanations,Mechanisms,Multi-realizability,Structures,Watt governor},
pages = {1--18},
publisher = {Springer Netherlands},
title = {Structures, dynamics and mechanisms in neuroscience: an integrative account},
year = {2017}
}

@article{Matthewson2018,
author = {Matthewson, John},
doi = {10.1016/j.shpsa.2018.06.001},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Matthewson-2018-Detail and generality in mechanistic explanation.pdf:pdf},
issn = {0039-3681},
journal = {Studies in History and Philosophy of Science Part A},
pages = {1--9},
publisher = {Elsevier Ltd},
title = {Detail and generality in mechanistic explanation},
url = {https://doi.org/10.1016/j.shpsa.2018.06.001},
year = {2018}
}

@article{masulli2016,
  title={The topology of the directed clique complex as a network invariant},
  author={Masulli, Paolo and Villa, Alessandro EP},
  journal={SpringerPlus},
  volume={5},
  number={1},
  pages={388},
  year={2016},
  publisher={Springer}
}

@article{Mebius2014,
abstract = {Much contemporary debate on the nature of mechanisms centers on the issue of modulating negative causes. One type of negative causability, which I refer to as "causation by absence," appears difficult to incorporate into modern accounts of mechanistic explanation. This paper argues that a recent attempt to resolve this problem, proposed by Benjamin Barros, requires improvement as it overlooks the fact that not all absences qualify as sources of mechanism failure. I suggest that there are a number of additional types of effects caused by absences that need to be incorporated to account for the diversity of causal connections in the biological sciences. Furthermore, it is argued that recognizing natural variability in mechanisms, such as attenuation, leads to some interesting line-drawing issues for contemporary philosophy of mechanisms. {\textcopyright} 2013 The Author.},
author = {Mebius, Alexander},
doi = {10.1016/j.shpsc.2013.11.001},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Mebius-2014-A weakened mechanism is still a mechanism$\backslash$: On the causal role of absences in mechanistic explanation.pdf:pdf},
issn = {13698486},
journal = {Studies in History and Philosophy of Science Part C :Studies in History and Philosophy of Biological and Biomedical Sciences},
keywords = {Attenuation,Benjamin Barros,Causal production,Causation by absence,Mechanism failure,Mechanisms},
number = {1},
pages = {43--48},
pmid = {24291470},
publisher = {Elsevier Ltd},
title = {A weakened mechanism is still a mechanism: On the causal role of absences in mechanistic explanation},
url = {http://dx.doi.org/10.1016/j.shpsc.2013.11.001},
volume = {45},
year = {2014}
}

@article{medaglia2017,
  title={Brain and cognitive reserve: {T}ranslation via network control theory},
  author={Medaglia, John Dominic and Pasqualetti, Fabio and Hamilton, Roy H and Thompson-Schill, Sharon L and Bassett, Danielle S},
  journal={Neuroscience \& Biobehavioral Reviews},
  volume={75},
  pages={53--64},
  year={2017},
  publisher={Elsevier}
}

@article{Perez2011,
abstract = {As important as the intrinsic properties of an individual nervous cell stands the network of neurons in which it is embedded and by virtue of which it acquires great part of its responsiveness and functionality. In this study we have explored how the topological properties and conduction delays of several classes of neural networks affect the capacity of their constituent cells to establish well-defined temporal relations among firing of their action potentials. This ability of a population of neurons to produce and maintain a millisecond-precise coordinated firing (either evoked by external stimuli or internally generated) is central to neural codes exploiting precise spike timing for the representation and communication of information. Our results, based on extensive simulations of conductance-based type of neurons in an oscillatory regime, indicate that only certain topologies of networks allow for a coordinated firing at a local and long-range scale simultaneously. Besides network architecture, axonal conduction delays are also observed to be another important factor in the generation of coherent spiking. We report that such communication latencies not only set the phase difference between the oscillatory activity of remote neural populations but determine whether the interconnected cells can set in any coherent firing at all. In this context, we have also investigated how the balance between the network synchronizing effects and the dispersive drift caused by inhomogeneities in natural firing frequencies across neurons is resolved. Finally, we show that the observed roles of conduction delays and frequency dispersion are not particular to canonical networks but experimentally measured anatomical networks such as the macaque cortical network can display the same type of behavior.},
author = {P{\'{e}}rez, Toni and Garcia, Guadalupe C. and Egu{\'{i}}luz, V{\'{i}}ctor M. and Vicente, Ra{\'{u}}l and Pipa, Gordon and Mirasso, Claudio},
doi = {10.1371/journal.pone.0019900},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Perez-2011-Effect of the Topology and Delayed Interactions in Neuronal Networks Synchronization.PDF:PDF},
isbn = {1932-6203 (Electronic)$\backslash$n1932-6203 (Linking)},
issn = {19326203},
journal = {PLoS ONE},
number = {5},
pages = {1--9},
pmid = {21637767},
title = {Effect of the topology and delayed interactions in neuronal networks synchronization},
volume = {6},
year = {2011}
}

@book{pincock2011,
  title={Mathematics and scientific representation},
  author={Pincock, Christopher},
  year={2011},
  publisher={Oxford University Press}
}

@article{Pincock2015,
abstract = {This article focuses on a case that expert practitioners count as an explanation: a mathematical account of Plateau's laws for soap films. I argue that this example falls into a class of explanations that I call abstract explanations. Abstract explanations involve an appeal to a more abstract entity than the state of affairs being explained. I show that the abstract entity need not be causally relevant to the explanandum for its features to be explanatorily relevant. However, it remains unclear how to unify abstract and causal explanations as instances of a single sort of thing. I conclude by examining the implications of the claim that explanations require objective dependence relations. If this claim is accepted, then there are several kinds of objective dependence relations.},
author = {Pincock, Christopher},
doi = {10.1093/bjps/axu016},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Pincock-2015-Abstract Explanations in Science.pdf:pdf},
issn = {14643537},
journal = {British Journal for the Philosophy of Science},
number = {4},
pages = {857--882},
title = {Abstract explanations in science},
volume = {66},
year = {2015}
}

@article{polya1920,
  title={{\"U}ber den zentralen Grenzwertsatz der Wahrscheinlichkeitsrechnung und das Momentenproblem},
  author={P{\'o}lya, Georg},
  journal={Mathematische Zeitschrift},
  volume={8},
  number={3},
  pages={171--181},
  year={1920},
  publisher={Springer}
}

@article{povich2018,
  title={Mechanistic Levels, Reduction, and Emergence},
  author={Povich, Mark and Craver, Carl F},
  journal={The Routledge Handbook of Mechanisms and Mechanical Philosophy},
  pages={185--197},
  year={2018},
  publisher={Routledge New York}
}

@article{abbondandolo2018,
  title={Chain recurrence, chain transitivity, {L}yapunov functions and rigidity of {L}agrangian submanifolds of optical hypersurfaces},
  author={Abbondandolo, Alberto and Bernardi, Olga and Cardin, Franco},
  journal={Journal of Dynamics and Differential Equations},
  volume={30},
  number={1},
  pages={287--308},
  year={2018},
  publisher={Springer}
}

@article{osowski2007,
  title={Epileptic seizure characterization by {L}yapunov exponent of {EEG} signal},
  author={Osowski, Stanislaw and Swiderski, Bartosz and Cichocki, Andrzej and Rysz, Andrzej},
  journal={COMPEL-The international journal for computation and mathematics in electrical and electronic engineering},
  year={2007},
  publisher={Emerald Group Publishing Limited}
}

@book{brin2002,
  title={Introduction to dynamical systems},
  author={Brin, Michael and Stuck, Garrett},
  year={2002},
  publisher={Cambridge university press}
}

@book{robinson1995,
  title={Dynamical systems: {S}tability, symbolic dynamics, and chaos},
  author={Robinson, Clark},
  year={1995},
  publisher={CRC press}
}

@article{chen2009,
  title={Random chain recurrent sets for random dynamical systems},
  author={Chen, Xiaopeng and Duan, Jinqiao},
  journal={Dynamical Systems},
  volume={24},
  number={4},
  pages={537--546},
  year={2009},
  publisher={Taylor \& Francis}
}

@article{chen2011a,
  title={State space decomposition for non-autonomous dynamical systems},
  author={Chen, Xiaopeng and Duan, Jinqiao},
  journal={Proceedings of the Royal Society of Edinburgh Section A: Mathematics},
  volume={141},
  number={5},
  pages={957--974},
  year={2011},
  publisher={Royal Society of Edinburgh Scotland Foundation}
}

@article{caraballo2013,
  title={Morse decomposition of attractors for non-autonomous dynamical systems},
  author={Caraballo, Tom{\'a}s and Jara, Juan C and Langa, Jos{\'e} A and Liu, Zhenxin},
  journal={Advanced Nonlinear Studies},
  volume={13},
  number={2},
  pages={309--329},
  year={2013},
  publisher={Advanced Nonlinear Studies, Inc.}
}

@article{ayala2006,
  title={Morse decomposition, attractors and chain recurrence},
  author={Ayala, Jose and Corbin, Patrick and Mc Conville, Kelly and Colonius, Fritz and Kliemann, Wolfgang and Peters, Justin},
  journal={Proyecciones (Antofagasta)},
  volume={25},
  number={1},
  pages={79--109},
  year={2006},
  publisher={Universidad Cat{\'o}lica del Norte, Departamento de Matem{\'a}ticas}
}

@article{Rathkopf2018,
abstract = {In this article, network science is discussed from a methodological perspec-tive, and two central theses are defended. The first is that network science exploits the very properties that make a system complex. Rather than using idealization techniques to strip those properties away, as is standard practice in other areas of science, network science brings them to the fore, and uses them to furnish new forms of explanation. The second thesis is that network representations are particularly helpful in explaining the properties of non-decomposable systems. Where part-whole decomposition is not possible, network science provides a much-needed alternative method of compressing information about the behavior of complex systems, and does so without succumb-ing to problems associated with combinatorial explosion. The article concludes with a comparison between the uses of network representation analyzed in the main dis-cussion, and an entirely distinct use of network representation that has recently been discussed in connection with mechanistic modeling.},
author = {Rathkopf, Charles},
doi = {10.1007/s11229-015-0726-0},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Rathkopf-2015-Network representation and complex systems.pdf:pdf},
issn = {0039-7857},
journal = {Synthese},
keywords = {Network,Representation,Explanation,Mechanism,Decom,decomposition,explanation,mechanism,network,representation},
number = {1},
pages = {55--78},
publisher = {Springer Netherlands},
title = {Network representation and complex systems},
url = {http://link.springer.com/10.1007/s11229-015-0726-0},
volume = {195},
year = {2018}
}

@article{Reutlinger2016,
author = {Reutlinger, Alexander and Andersen, Holly},
doi = {10.1080/02698595.2016.1265867},
issn = {14699281},
journal = {International Studies in the Philosophy of Science},
number = {2},
pages = {129--146},
publisher = {Taylor {\&} Francis},
title = {Abstract versus Causal Explanations?},
volume = {30},
year = {2016}
}

@article{reutlinger2017,
  title={Explanation beyond causation? New directions in the philosophy of scientific explanation},
  author={Reutlinger, Alexander},
  journal={Philosophy Compass},
  volume={12},
  number={2},
  pages={e12395},
  year={2017},
  publisher={Wiley Online Library}
}

@book{reutlinger2018,
  title={Explanation beyond causation: philosophical perspectives on non-causal explanations},
  author={Reutlinger, Alexander and Saatsi, Juha},
  year={2018},
  publisher={Oxford University Press}
}

@misc{rivelli2015,
  title={Modularity, Antimodularity and Explanation in Complex Systems},
  author={Rivelli, Luca},
  year={2015},
  publisher={Universit{\`a} degli Studi di Padova e Universit{\'e} Paris 1 Panth{\'e}on-Sorbonne}
}

@article{Ross2015,
abstract = {Kaplan and Craver claim that all explanations in neuroscience appeal to mechanisms. They extend this view to the use of mathematical models in neuroscience and propose a constraint such models must meet in order to be explanatory. I analyze a mathematical model used to provide explanations in dynamical systems neuroscience and indicate how this explanation cannot be accommodated by the mechanist framework. I argue that this explanation is well characterized by Batterman's account of minimal model explanations and that it demonstrates how relationships between explanatory models in neuroscience and the systems they represent is more complex than has been appreciated.},
author = {Ross, Lauren N.},
doi = {10.1086/679038},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Ross-2015-Dynamical Models and Explanation in Neuroscience.pdf:pdf},
issn = {00318248},
journal = {Philosophy of Science},
number = {1},
pages = {32--54},
title = {Dynamical Models and Explanation in Neuroscience},
url = {http://www.jstor.org/stable/info/10.1086/679038},
volume = {82},
year = {2015}
}

@article{Ross2018,
abstract = {In the last two decades few topics in philosophy of science have received as much attention as mechanistic explanation. A significant motivation for these accounts is that scientists frequently use the term “mechanism” in their explanations of biological phenomena. While scientists appeal to a variety of causal concepts in their explanations, many philosophers argue or assume that all of these concepts are well understood with the single notion of mechanism (Robins and Craver 2009; Craver 2007). This reveals a significant problem with mainstream mechanistic accounts– although philosophers use the term “mechanism” interchangeably with other causal concepts, this is not something that scientists always do. This paper analyses two causal concepts in biology–the notions of “mechanism” and “pathway”–and how they figure in biological explanation. I argue that these concepts have unique features, that they are associated with distinct strategies of causal investigation, and that they figure in importantly different types of explanation.},
author = {Ross, Lauren N},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Ross-2016-Causal concepts in biology$\backslash$: How pathways differ from mechanisms and why it matters.pdf:pdf},
pages = {1--22},
title = {Causal concepts in biology: How pathways differ from mechanisms and why it matters},
year = {2018}
}

@article{Rusanen2016,
abstract = {Computational explanations focus on information processing required in specific cognitive capacities, such as perception, reasoning or decision-making. These explanations specify the nature of the information processing task, what information needs to be represented, and why it should be operated on in a particular manner. In this article, the focus is on three questions concerning the nature of computational explanations: (1) What type of explanations they are, (2) in what sense computational explanations are explanatory and (3) to what extent they involve a special, “independent” or “autonomous” level of explanation. In this paper, we defend the view computational explanations are genuine explanations, which track non-causal/formal dependencies. Specifically, we argue that they do not provide mere sketches for explanation, in contrast to what for example Piccinini and Craver (Synthese 183(3):283–311, 2011) suggest. This view of computational explanations implies some degree of “autonomy” for the computational level. However, as we will demonstrate that does not make this view “computationally chauvinistic” in a way that Piccinini (Synthese 153:343–353, 2006b) or Kaplan (Synthese 183(3):339–373, 2011) have charged it to be.},
author = {Rusanen, Anna Mari and Lappi, Otto},
doi = {10.1007/s11229-016-1101-5},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Rusanen-2016-On computational explanations.pdf:pdf},
issn = {15730964},
journal = {Synthese},
keywords = {Causality,Computational chauvinism,Computational explanation,Marr,Mechanistic explanation,Philosophy of neuroscience},
number = {12},
pages = {3931--3949},
publisher = {Springer Netherlands},
title = {On computational explanations},
volume = {193},
year = {2016}
}

@inproceedings{salmon1977,
  title={A third dogma of empiricism},
  author={Salmon, Wesley C},
  booktitle={Basic problems in methodology and linguistics},
  pages={149--166},
  year={1977},
  organization={Springer}
}

@book{salmon1984,
  title={Scientific explanation and the causal structure of the world},
  author={Salmon, Wesley C},
  year={1984},
  publisher={Princeton University Press}
}

@article{schroeder2013,
  title={Crisis in science: In search for new theoretical foundations},
  author={Schroeder, Marcin J},
  journal={Progress in biophysics and molecular biology},
  volume={113},
  number={1},
  pages={25--32},
  year={2013},
  publisher={Elsevier}
}

@article{Serban2015,
abstract = {An increasing number of philosophers have promoted the idea that mech- anism provides a fruitful framework for thinking about the explanatory contributions of computational approaches in cognitive neuroscience. For instance, Piccinini and Bahar (Cogn Sci 37(3):453–488, 2013) have recently argued that neural computation constitutes a sui generis category of physical computation which can play a genuine explanatory role in the context of investigating neural and cognitive processes. The core of their proposal is to conceive of computational explanations in cognitive neu- roscience as a subspecies of mechanistic explanations. This paper identifies several challenges facing their mechanistic account and sketches an alternative way of think- ing about the epistemic roles of computational approaches used in the study of brain and cognition. Drawing on examples from both low-level and systems-level compu- tational neuroscience, I argue that at least some computational explanations of neural and cognitive processes are partially independent from mechanistic constraints.},
author = {Serban, Maria},
doi = {10.1007/s11229-015-0709-1},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Serban-2015-The scope and limits of a mechanistic view of computational explanation.pdf:pdf},
issn = {15730964},
journal = {Synthese},
keywords = {Computational models,Explanation,Mechanism,Neural computation},
number = {10},
pages = {3371--3396},
publisher = {Springer Netherlands},
title = {The scope and limits of a mechanistic view of computational explanation},
url = {http://dx.doi.org/10.1007/s11229-015-0709-1},
volume = {192},
year = {2015}
}

@article{Shagrir2018,
abstract = {An underlying assumption in computational approaches in cognitive and brain sciences is that the nervous system is an input--output model of the world: Its input--output functions mirror certain relations in the target domains. I argue that the input--output modelling assumption plays distinct methodological and explanatory roles. Methodologically, input--output modelling serves to discover the computed function from environmental cues. Explanatorily, input--output modelling serves to account for the appropriateness of the computed function to the explanandum information-processing task. I compare very briefly the modelling explanation to mechanistic and optimality explanations, noting that in both cases the explanations can be seen as complementary rather than contrastive or competing.},
author = {Shagrir, Oron},
doi = {10.1007/s11023-017-9443-4},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Shagrir-2017-The Brain as an Input–Output Model of the World.pdf:pdf},
issn = {15728641},
journal = {Minds and Machines},
keywords = {Cognitive neuroscience,Computational models,Mechanistic explanations,Modelling,Optimality,Representation},
number = {1},
pages = {53--75},
publisher = {Springer Netherlands},
title = {The Brain as an Input–Output Model of the World},
volume = {28},
year = {2018}
}

@article{Spivak2013,
author = {Spivak, David I and Kent, Robert E},
file = {:home/abel/Documents/Projects/ACT/Spivak/Spivak-2012-OLOGS$\backslash$: A CATEGORICAL FRAMEWORK FOR KNOWLEDGE REPRESENTATION.pdf:pdf},
pages = {1--37},
title = {Ologs : A Categorical Framework for Knowledge Representation},
year = {2013}
}

@article{sporns2005,
  title={The human connectome: A structural description of the human brain},
  author={Sporns, Olaf and Tononi, Giulio and K{\"o}tter, Rolf},
  journal={PLoS computational biology},
  volume={1},
  number={4},
  pages={e42},
  year={2005},
  publisher={Public Library of Science}
}

@article{thompson2012,
  title={Limiting the Scope of Mechanistic Explanation},
  author={Thompson, Morgan},
  year={2012}
}

@article{tononi2003,
  title={Measuring information integration},
  author={Tononi, Giulio and Sporns, Olaf},
  journal={BMC neuroscience},
  volume={4},
  number={1},
  pages={31},
  year={2003},
  publisher={BioMed Central}
}

@misc{Walsh2013,
author = {Walsh, Daniel},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Walsh-2013-A Physicist's Introduction to Topology$\backslash$: Connections to Bosons, Fermions, and Anyons.pdf:pdf},
pages = {1--4},
title = {A Physicist's Introduction to Topology : Connections to Bosons , Fermions , and Anyons},
year = {2013}
}

@article{Wang2015,
abstract = {Basic studies in system science explore the theories, principles, and properties of abstract and concrete systems as well as their applications in system engineering. Systems are the most complicated entities and phenomena in abstract, physical, information, cognitive, brain, and social worlds across a wide range of science and engineering dis-ciplines. The mathematical model of a general system is embodied as a hyperstructure of the abstract system. The the-oretical framework of system science is formally described by a set of algebraic operations on abstract systems known as system algebra. A set of abstract structures, properties, behaviors, and principles is rigorously formalized in contem-porary system theories. Applications of the formal theories of system science in system engineering, intelligent engi-neering, cognitive informatics, cognitive robotics, software engineering, cognitive linguistics, and cognitive computing are demonstrated, which reveals how system structural and behavioral complexities may be efficiently reduced in system representation, modeling, analysis, synthesis, inference, and implementation.},
author = {Wang, Yingxu},
doi = {10.1007/s40747-016-0007-7},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Wang-2015-Towards the abstract system theory of system science for cognitive.pdf:pdf},
issn = {2199-4536},
journal = {Complex {\&} Intelligent Systems},
keywords = {Abstract systems,Cognitive informatics,Cognitive robotics,Denotational mathematics,Engineering applications,Formal system theories,Formal systems,Mathematical models,System algebra,System engineering,System philosophy,System science,System theories,abstract,cognitive informatics,cognitive robotics,denotational,formal system theories,formal systems,mathematical models,mathematics,system algebra,system science,system theories,systems},
number = {1-4},
pages = {23--23},
publisher = {Springer Berlin Heidelberg},
title = {Towards the abstract system theory of system science for cognitive and intelligent systems},
url = {http://link.springer.com/10.1007/s40747-016-0007-7},
volume = {1},
year = {2015}
}

@article{Wang2017,
abstract = {It is recognized that systems are the most complicated entities in abstract, physical, information, cognitive, brain, and social worlds across a wide range of science and engineering disciplines. This paper presents a theory of distributed system topologies and adaptive fusion mechanisms. It reveals that structural and functional relations play a centric role in system behavior modeling. A special phenomenon of system incremental fusion is discovered and formally described in order to explain the adaptive mechanisms of system gains. The hierarchical topology theory of systems provides a methodology for efficiently handling distributed structural and behavioral complexities in system representation, modeling, analysis, synthesis, inference, and implementation. Applications of the formal theories of distributed system topology and adaptive hierarchical relations are explored in distributed, adaptive and cognitive system engineering.},
author = {Wang, Yingxu},
doi = {10.1109/SMC.2017.8123120},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Wang-2017-A hierarchical theory of system topology and distributed functional fusions.pdf:pdf},
isbn = {9781538616451},
journal = {2017 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2017},
keywords = {Adaptive systems,Distributed system composition,Hierarchical relations,Incremental fusion mechanism,Relation theory,System coordination,System fusion,System theory,System topology},
pages = {3195--3200},
title = {A hierarchical theory of system topology and distributed functional fusions},
volume = {2017},
year = {2017}
}

@article{Weisberg2008,
abstract = {Explanations of psychological phenomena seem to generate more public interest when they contain neuroscientific information. Even irrelevant neuroscience information in an explanation of a psychological phenomenon may interfere with people's abilities to critically consider the underlying logic of this explanation. We tested this hypothesis by giving na{\"{i}}ve adults, students in a neuroscience course, and neuroscience experts brief descriptions of psychological phenomena followed by one of four types of explanation, according to a 2 (good explanation vs. bad explanation) x 2 (without neuroscience vs. with neuroscience) design. Crucially, the neuroscience information was irrelevant to the logic of the explanation, as confirmed by the expert subjects. Subjects in all three groups judged good explanations as more satisfying than bad ones. But subjects in the two nonexpert groups additionally judged that explanations with logically irrelevant neuroscience information were more satisfying than explanations without. The neuroscience information had a particularly striking effect on nonexperts' judgments of bad explanations, masking otherwise salient problems in these explanations.},
archivePrefix = {arXiv},
arxivId = {10.1177/0013164408323233 doi},
author = {Weisberg, Deena Skolnick and Keil, Frank C. and Goodstein, Joshua and Rawson, Elizabeth and Gray, Jeremy R.},
doi = {10.1162/jocn.2008.20040},
eprint = {0013164408323233 doi},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Weisberg-2008-The Seductive Allure of Neuroscience Explanations.pdf:pdf},
isbn = {0898-929X},
issn = {15308898},
journal = {Journal of Cognitive Neuroscience},
number = {3},
pages = {470--477},
pmid = {18004955},
primaryClass = {10.1177},
title = {The seductive allure of neuroscience explanations},
volume = {20},
year = {2008}
}

@book{weisberg2012,
  title={Simulation and similarity: Using models to understand the world},
  author={Weisberg, Michael},
  year={2012},
  publisher={Oxford University Press}
}

@article{Weiskopf2011,
abstract = {Mechanistic explanation has an impressive track record of advancing our understanding of complex, hierarchically organized physical systems, particularly biological and neural systems. But not every complex system can be understood mechanistically. Psychological capacities are often understood by providing cognitive models of the systems that underlie them. I argue that these models, while superficially similar to mechanistic models, in fact have a substantially more complex relation to the real underlying system. They are typically constructed using a range of techniques for abstracting the functional properties of the system, which may not coincide with its mechanistic organization. I describe these techniques and show that despite being non-mechanistic, these cognitive models can satisfy the normative constraints on good explanations.},
author = {Weiskopf, Daniel A.},
doi = {10.1007/s11229-011-9958-9},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Pres/Weiskopf-2011-Models and Mechanisms of Psychological Explanation.pdf:pdf},
isbn = {0039-7857},
issn = {00397857},
journal = {Synthese},
keywords = {Cognition,Mechanisms,Models,Psychological explanation,Realization},
number = {3},
pages = {313--338},
title = {Models and mechanisms in psychological explanation},
volume = {183},
year = {2011}
}

@article{Woodward2010,
abstract = {A number of writers, myself included, have recently argued that an “interventionist” treatment of causation of the sort defended in Woodward, 2003 can be used to cast light on so-called “causal exclusion” arguments. This interventionist treatment of causal exclusion has in turn been criticized by other philosophers. This paper responds to these criticisms. It describes an interventionist framework for thinking about causal relationships when supervenience relations are present. I contend that this framework helps us to see that standard arguments for causal exclusion involve mistaken assumptions about what it is appropriate to control for or hold fixed in assessing causal claims. The framework also provides a natural way of capturing the idea that properties that supervene on but that are not identical with realizing properties can be causally efficacious},
author = {Woodward, James},
doi = {10.1111/phpr.12095},
file = {:home/abel/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Woodward - 2010 - No Title.pdf:pdf},
issn = {19331592},
journal = {Philosophy and Phenomenological Research},
number = {2},
pages = {303--347},
title = {Interventionism and Causal Exclusion},
volume = {91},
year = {2015}
}

@article{woodward2000,
  title={Explanation and invariance in the special sciences},
  author={Woodward, James},
  journal={The British Journal for the Philosophy of Science},
  volume={51},
  number={2},
  pages={197--254},
  year={2000},
  publisher={Oxford University Press}
}

@book{woodward2003,
  title={Making things happen: A theory of causal explanation},
  author={Woodward, James},
  year={2003},
  publisher={Oxford university press}
}

@article{Woodward2013,
abstract = {This paper explores the question of whether all or most explanations in biology are, or ideally should be, ‘mechanistic'. I begin by providing an account of mechanistic explanation, making use of the interventionist ideas about causation I have developed elsewhere. This account emphasizes the way in which mechanistic explanations, at least in the biological sciences, integrate difference-making and spatio-temporal information, and exhibit what I call fine-tunedness of organization. I also emphasize the role played by modularity conditions in mechanistic explanation. I will then argue, in agreement with John Dupr{\'{e}}, that, given this account, it is plausible that many biological systems require explanations that are relatively non- mechanical or depart from expectations one associates with the behaviour of machines.},
author = {Woodward, James},
doi = {10.1111/j.1467-8349.2013.00219.x},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Woodward-2013-II—Mechanistic Explanation$\backslash$: Its Scope and Limits
.pdf:pdf},
issn = {0309-7013},
journal = {Aristotelian Society Supplementary Volume},
number = {1},
pages = {39--65},
title = {II—James Woodward: Mechanistic Explanation: Its Scope and Limits},
url = {https://academic.oup.com/aristoteliansupp/article-lookup/doi/10.1111/j.1467-8349.2013.00219.x},
volume = {87},
year = {2013}
}

@article{woodward2015,
  title={Interventionism and causal exclusion},
  author={Woodward, James},
  journal={Philosophy and Phenomenological Research},
  volume={91},
  number={2},
  pages={303--347},
  year={2015},
  publisher={Wiley Online Library}
}

@article{Zednik2011,
abstract = {The received view of dynamical explanation is that dynamical cognitive science seeks to provide covering-law explanations of cognitive phenomena. By analyzing three prominent examples of dynamicist research, I show that the received view is misleading: some dynamical explanations are mechanistic explanations and in this way resemble computational and connectionist explanations. Interestingly, these dynamical explanations invoke the mathematical framework of dynamical systems theory to describe mechanisms far more complex and distributed than the ones typically considered by philosophers. Therefore, contemporary dynamicist research reveals the need for a more sophisticated account of mechanistic explanation.},
author = {Zednik, Carlos},
doi = {10.1086/659221},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Course/Zednik, C. (2010) The Nature of Dynamical Explanation: (2010) The Nature of Dynamical Explanation},
issn = {0031-8248},
journal = {Philosophy of Science},
number = {2},
pages = {238--263},
title = {The Nature of Dynamical Explanation},
url = {https://www.journals.uchicago.edu/doi/10.1086/659221},
volume = {78},
year = {2011}
}

@inproceedings{Zednik2014,
author = {Zednik, Carlos},
booktitle = {Philosophy of Science Association 24th Biennial Meeting},
file = {:home/abel/Documents/UvA/Master/Courses/3/Phil Appr/Zednik-2014-Are Systems Neuroscience Explanations Mechanistic?.pdf:pdf},
pages = {1--22},
title = {Are Systems Neuroscience Explanations Mechanistic?},
year = {2014}
}

@article{zednik2018,
  title={Models and Mechanisms in Network Neuroscience},
  journal={Philosophical Psychology},
  author={Zednik, Carlos},
  year={2018}
}

@article{parker1998,
  title={Sense and the single neuron: probing the physiology of perception},
  author={Parker, Andrew J and Newsome, William T},
  journal={Annual review of neuroscience},
  volume={21},
  number={1},
  pages={227--277},
  year={1998},
  publisher={Annual Reviews 4139 El Camino Way, PO Box 10139, Palo Alto, CA 94303-0139, USA}
}

@inproceedings{schafer2006recurrent,
  title={Recurrent neural networks are universal approximators},
  author={Sch{\"a}fer, Anton Maximilian and Zimmermann, Hans Georg},
  booktitle={Artificial Neural Networks--ICANN 2006: 16th International Conference, Athens, Greece, September 10-14, 2006. Proceedings, Part I 16},
  pages={632--640},
  year={2006},
  organization={Springer}
}

@article{doya1993uni,
  title={Universality of fully connected recurrent neural networks},
  author={Doya, Kenji},
  journal={Dept. of Biology, UCSD, Tech. Rep},
  volume={1},
  pages={1--6},
  year={1993},
  publisher={Citeseer}
}

@article{saxe2019,
  title={A mathematical theory of semantic development in deep neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={23},
  pages={11537--11546},
  year={2019},
  publisher={National Acad Sciences}
}

@article{barak2013,
  title={From fixed points to chaos: Three models of delayed discrimination},
  author={Barak, Omri and Sussillo, David and Romo, Ranulfo and Tsodyks, Misha and Abbott, LF},
  journal={Progress in neurobiology},
  volume={103},
  pages={214--222},
  year={2013},
  publisher={Elsevier}
}

@article{barak2017,
  title={Recurrent neural networks as versatile tools of neuroscience research},
  author={Barak, Omri},
  journal={Current opinion in neurobiology},
  volume={46},
  pages={1--6},
  year={2017},
  publisher={Elsevier}
}

@article{ranzato2007,
  title={Sparse feature learning for deep belief networks},
  author={Ranzato, Marc'Aurelio and Boureau, Y-Lan and Cun, Yann and others},
  journal={Advances in neural information processing systems},
  volume={20},
  year={2007}
}

@article{valente2022,
  title={Probing the Relationship Between Latent Linear Dynamical Systems and Low-Rank Recurrent Neural Network Models},
  author={Valente, Adrian and Ostojic, Srdjan and Pillow, Jonathan W},
  journal={Neural Computation},
  volume={34},
  number={9},
  pages={1871--1892},
  year={2022},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{gao2015,
  title={On simplicity and complexity in the brave new world of large-scale neuroscience},
  author={Gao, Peiran and Ganguli, Surya},
  journal={Current opinion in neurobiology},
  volume={32},
  pages={148--155},
  year={2015},
  publisher={Elsevier}
}

@article{piet2018,
  title={Rats adopt the optimal timescale for evidence integration in a dynamic environment},
  author={Piet, Alex T and El Hady, Ahmed and Brody, Carlos D},
  journal={Nature communications},
  volume={9},
  number={1},
  pages={1--12},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{brunton2013,
  title={Rats and humans can optimally accumulate evidence for decision-making},
  author={Brunton, Bingni W and Botvinick, Matthew M and Brody, Carlos D},
  journal={Science},
  volume={340},
  number={6128},
  pages={95--98},
  year={2013},
  publisher={American Association for the Advancement of Science}
}

@article{mcintyre2003,
  title={Patterns of brain acetylcholine release predict individual differences in preferred learning strategies in rats},
  author={McIntyre, Christa K and Marriott, Lisa K and Gold, Paul E},
  journal={Neurobiology of learning and memory},
  volume={79},
  number={2},
  pages={177--183},
  year={2003},
  publisher={Elsevier}
}

@article{musall2019,
  title={Harnessing behavioral diversity to understand neural computations for cognition},
  author={Musall, Simon and Urai, Anne E and Sussillo, David and Churchland, Anne K},
  journal={Current opinion in neurobiology},
  volume={58},
  pages={229--238},
  year={2019},
  publisher={Elsevier}
}

@article{berger2020,
  title={Compact task representations as a normative model for higher-order brain activity},
  author={Berger, Severin and Machens, Christian K},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3209--3219},
  year={2020}
}

@article{kozachkov2020,
  title={Achieving stable dynamics in neural circuits},
  author={Kozachkov, Leo and Lundqvist, Mikael and Slotine, Jean-Jacques and Miller, Earl K},
  journal={PLoS computational biology},
  volume={16},
  number={8},
  pages={e1007659},
  year={2020},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{ceni2020,
  title={Interpreting recurrent neural networks behaviour via excitable network attractors},
  author={Ceni, Andrea and Ashwin, Peter and Livi, Lorenzo},
  journal={Cognitive Computation},
  volume={12},
  number={2},
  pages={330--356},
  year={2020},
  publisher={Springer}
}

@article{wang2022,
  title={State-Regularized Recurrent Neural Networks to Extract Automata and Explain Predictions},
  author={Wang, Cheng and Lawrence, Carolin and Niepert, Mathias},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2022},
  publisher={IEEE}
}

@article{schaeffer2020,
  title={Reverse-engineering recurrent neural network solutions to a hierarchical inference task for mice},
  author={Schaeffer, Rylan and Khona, Mikail and Meshulam, Leenoy and Fiete, Ila Rani and others},
  journal={bioRxiv},
  year={2020},
  publisher={Cold Spring Harbor Laboratory}
}

@article{turner2021,
  title={Charting and navigating the space of solutions for recurrent neural networks},
  author={Turner, Elia and Dabholkar, Kabir V and Barak, Omri},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={25320--25333},
  year={2021}
}

@article{liebe2022,
  title={Phase of firing does not reflect temporal order in sequence memory of humans and recurrent neural networks},
  author={Liebe, Stefanie and Niediek, Johannes and Pals, Matthijs and Reber, Thomas P and Faber, Jennifer and Bostroem, Jan and Elger, Christian E and Macke, Jakob H and Mormann, Florian},
  journal={bioRxiv},
  year={2022},
  publisher={Cold Spring Harbor Laboratory}
}

@article{wang2022,
  title={State-Regularized Recurrent Neural Networks to Extract Automata and Explain Predictions},
  author={Wang, Cheng and Lawrence, Carolin and Niepert, Mathias},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2022},
  publisher={IEEE}
}

@article{chaisangmongkon2017,
  title={Computing by robust transience: {H}ow the fronto-parietal network performs sequential, category-based decisions},
  author={Chaisangmongkon, Warasinee and Swaminathan, Sruthi K and Freedman, David J and Wang, Xiao-Jing},
  journal={Neuron},
  volume={93},
  number={6},
  pages={1504--1517},
  year={2017},
  publisher={Elsevier}
}

@article{piccinini2013,
  title={Neural computation and the computational theory of cognition},
  author={Piccinini, Gualtiero and Bahar, Sonya},
  journal={Cognitive science},
  volume={37},
  number={3},
  pages={453--488},
  year={2013},
  publisher={Wiley Online Library}
}

@article{sussillo2013blackbox,
  title={Opening the black box: Low-dimensional dynamics in high-dimensional recurrent neural networks},
  author={Sussillo, David and Barak, Omri},
  journal={Neural computation},
  volume={25},
  number={3},
  pages={626--649},
  year={2013},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals}
}

@article{sussillo2014,
  title={Neural circuits as computational dynamical systems},
  author={Sussillo, David},
  journal={Current opinion in neurobiology},
  volume={25},
  pages={156--163},
  year={2014},
  publisher={Elsevier}
}

@article{golub2018fixedpointfinder,
  title={FixedPointFinder: A {Tensorflow} toolbox for identifying and characterizing fixed points in recurrent neural networks},
  author={Golub, Matthew D and Sussillo, David},
  journal={Journal of Open Source Software},
  volume={3},
  number={31},
  pages={1003},
  year={2018}
}

@article{levenstein2020,
  title={On the role of theory and modeling in neuroscience},
  author={Levenstein, Daniel and Alvarez, Veronica A and Amarasingham, Asohan and Azab, Habiba and Chen, Zhe Sage and Gerkin, Richard C and Hasenstaub, Andrea and Iyer, Ramakrishnan and Jolivet, Renaud B and Marzen, Sarah and others},
  journal={arXiv preprint arXiv:2003.13825},
  year={2020}
}

@article{rotstein2022,
  title={Development of theoretical frameworks in neuroscience: A pressing need in a sea of data},
  author={Rotstein, Horacio G and Santamaria, Fidel},
  journal={arXiv preprint arXiv:2209.09953},
  year={2022}
}

@article{ganguli2008,
  title={One-dimensional dynamics of attention and decision making in {LIP}},
  author={Ganguli, Surya and Bisley, James W and Roitman, Jamie D and Shadlen, Michael N and Goldberg, Michael E and Miller, Kenneth D},
  journal={Neuron},
  volume={58},
  number={1},
  pages={15--25},
  year={2008},
  publisher={Elsevier}
}

@inproceedings{zoltowski2020,
  title={A general recurrent state space framework for modeling neural dynamics during decision-making},
  author={Zoltowski, David and Pillow, Jonathan and Linderman, Scott},
  booktitle={International Conference on Machine Learning},
  pages={11680--11691},
  year={2020},
  organization={PMLR}
}

@article{pollock2020,
  title={Engineering recurrent neural networks from task-relevant manifolds and dynamics},
  author={Pollock, Eli and Jazayeri, Mehrdad},
  journal={PLoS computational biology},
  volume={16},
  number={8},
  pages={e1008128},
  year={2020},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{shadlen2013,
  title={Decision making as a window on cognition},
  author={Shadlen, Michael N and Kiani, Roozbeh},
  journal={Neuron},
  volume={80},
  number={3},
  pages={791--806},
  year={2013},
  publisher={Elsevier}
}

@article{wang2008,
  title={Decision making in recurrent neuronal circuits},
  author={Wang, Xiao-Jing},
  journal={Neuron},
  volume={60},
  number={2},
  pages={215--234},
  year={2008},
  publisher={Elsevier}
}

@article{huk2005,
  title={Neural activity in macaque parietal cortex reflects temporal integration of visual motion signals during perceptual decision making},
  author={Huk, Alexander C and Shadlen, Michael N},
  journal={Journal of Neuroscience},
  volume={25},
  number={45},
  pages={10420--10436},
  year={2005},
  publisher={Soc Neuroscience}
}

@article{duncker2020,
  title={Organizing recurrent network dynamics by task-computation to enable continual learning},
  author={Duncker, Lea and Driscoll, Laura and Shenoy, Krishna V and Sahani, Maneesh and Sussillo, David},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={14387--14397},
  year={2020}
}

@article{damour2020,
  title={Underspecification presents challenges for credibility in modern machine learning},
  author={D’Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D and others},
  journal={Journal of Machine Learning Research},
  year={2020}
}

@article{beer2006,
  title={Parameter space structure of continuous-time recurrent neural networks},
  author={Beer, Randall D},
  journal={Neural computation},
  volume={18},
  number={12},
  pages={3009--3051},
  year={2006},
  publisher={MIT Press}
}

@article{jazayeri2015,
  title={A neural mechanism for sensing and reproducing a time interval},
  author={Jazayeri, Mehrdad and Shadlen, Michael N},
  journal={Current Biology},
  volume={25},
  number={20},
  pages={2599--2609},
  year={2015},
  publisher={Elsevier}
}

@article{kingma2014,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{krueger2015,
  title={Regularizing {RNNs} by stabilizing activations},
  author={Krueger, David and Memisevic, Roland},
  journal={arXiv preprint arXiv:1511.08400},
  year={2015}
}

@article{wen2015,
  title={Semantically conditioned {LSTM}-based natural language generation for spoken dialogue systems},
  author={Wen, Tsung-Hsien and Gasic, Milica and Mrksic, Nikola and Su, Pei-Hao and Vandyke, David and Young, Steve},
  journal={arXiv preprint arXiv:1508.01745},
  year={2015}
}

@article{jonschkowski2015,
  title={Learning state representations with robotic priors},
  author={Jonschkowski, Rico and Brock, Oliver},
  journal={Autonomous Robots},
  volume={39},
  number={3},
  pages={407--428},
  year={2015},
  publisher={Springer}
}

@article{robinson1976,
  title={Structural stability of {C^1} diffeomorphisms},
  author={Robinson, Clark},
  journal={Journal of differential equations},
  volume={22},
  number={1},
  pages={28--73},
  year={1976},
  publisher={Elsevier}
}


@article{hart2020,
  title={Recurrent circuit dynamics underlie persistent activity in the macaque frontoparietal network},
  author={Hart, Eric and Huk, Alexander C},
  journal={Elife},
  volume={9},
  pages={e52460},
  year={2020},
  publisher={eLife Sciences Publications Limited}
}

@article{doya1993bif,
  title={Bifurcations of recurrent neural networks in gradient descent learning},
  author={Doya, Kenji},
  journal={IEEE Transactions on neural networks},
  volume={1},
  number={75},
  pages={218},
  year={1993},
  publisher={Citeseer}
}

@article{myre1993,
  title={Bistability, switches and working memory in a two-neuron inhibitory-feedback model},
  author={Myre, CD and Woodward, DJ},
  journal={Biological cybernetics},
  volume={68},
  number={5},
  pages={441--449},
  year={1993},
  publisher={Springer}
}

@article{machens2005,
  title={Flexible control of mutual inhibition: A neural model of two-interval discrimination},
  author={Machens, Christian K and Romo, Ranulfo and Brody, Carlos D},
  journal={Science},
  volume={307},
  number={5712},
  pages={1121--1124},
  year={2005},
  publisher={American Association for the Advancement of Science}
}

@book{amit1989,
  title={Modeling brain function: The world of attractor neural networks},
  author={Amit, Daniel J and Amit, Daniel J},
  year={1989},
  publisher={Cambridge university press}
}

@article{nair2022,
  title={An approximate line attractor in the hypothalamus that encodes an aggressive internal state},
  author={Nair, Aditya and Karigo, Tomomi and Yang, Bin and Linderman, Scott W and Anderson, David J and Kennedy, Ann},
  journal={bioRxiv},
  pages={2022--04},
  year={2022},
  publisher={Cold Spring Harbor Laboratory}
}

@article{kanai2017,
  title={Preventing gradient explosions in gated recurrent units},
  author={Kanai, Sekitoshi and Fujiwara, Yasuhiro and Iwamura, Sotetsu},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{darshan2022,
  title={Learning to represent continuous variables in heterogeneous neural networks},
  author={Darshan, Ran and Rivkind, Alexander},
  journal={Cell Reports},
  volume={39},
  number={1},
  pages={110612},
  year={2022},
  publisher={Elsevier}
}

@phdthesis{pollock2022,
  title={Understanding computation through low-dimensional dynamics with recurrent neural networks},
  author={Pollock, Eli Barton},
  year={2022},
  school={Massachusetts Institute of Technology}
}

@article{inagaki2019,
  title={Discrete attractor dynamics underlies persistent activity in the frontal cortex},
  author={Inagaki, Hidehiko K and Fontolan, Lorenzo and Romani, Sandro and Svoboda, Karel},
  journal={Nature},
  volume={566},
  number={7743},
  pages={212--217},
  year={2019},
  publisher={Nature Publishing Group UK London}
}

@article{chadwick2023,
  title={Learning shapes cortical dynamics to enhance integration of relevant sensory input},
  author={Chadwick, Angus and Khan, Adil G and Poort, Jasper and Blot, Antonin and Hofer, Sonja B and Mrsic-Flogel, Thomas D and Sahani, Maneesh},
  journal={Neuron},
  volume={111},
  number={1},
  pages={106--120},
  year={2023},
  publisher={Elsevier}
}

@article{machens2008,
  title={Design of continuous attractor networks with monotonic tuning using a symmetry principle},
  author={Machens, Christian K and Brody, Carlos D},
  journal={Neural computation},
  volume={20},
  number={2},
  pages={452--485},
  year={2008},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{eliasmith2005,
  title={A unified approach to building and controlling spiking attractor networks},
  author={Eliasmith, Chris},
  journal={Neural computation},
  volume={17},
  number={6},
  pages={1276--1314},
  year={2005},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@book{eliasmith2003,
  title={Neural engineering: Computation, representation, and dynamics in neurobiological systems},
  author={Eliasmith, Chris and Anderson, Charles H},
  year={2003},
  publisher={MIT press}
}

@article{schaeffer2022,
  title={No free lunch from deep learning in neuroscience: A case study through models of the entorhinal-hippocampal circuit},
  author={Schaeffer, Rylan and Khona, Mikail and Fiete, Ila},
  journal={bioRxiv},
  pages={2022--08},
  year={2022},
  publisher={Cold Spring Harbor Laboratory}
}

@book{samek2019,
  title={Explainable {AI}: Interpreting, explaining and visualizing deep learning},
  author={Samek, Wojciech and Montavon, Gr{\'e}goire and Vedaldi, Andrea and Hansen, Lars Kai and M{\"u}ller, Klaus-Robert},
  volume={11700},
  year={2019},
  publisher={Springer Nature}
}

@article{vicente2023,
  title={Emergence of universal computations through neural manifold dynamics},
  author={Gort Vicente, Joan},
  journal={bioRxiv},
  pages={2023--02},
  year={2023},
  publisher={Cold Spring Harbor Laboratory}
}

@article{zeeman1962,
  title={The topology of the brain and visual perception},
  author={Zeeman, E Christopher},
  journal={Topology of},
  volume={3},
  pages={240--256},
  year={1962}
}

@article{bel2021,
  title={Periodic solutions in threshold-linear networks and their entrainment},
  author={Bel, Andrea and Cobiaga, Romina and Reartes, Walter and Rotstein, Horacio G},
  journal={SIAM Journal on Applied Dynamical Systems},
  volume={20},
  number={3},
  pages={1177--1208},
  year={2021},
  publisher={SIAM}
}


@article{fenichel1971,
  title={Persistence and smoothness of invariant manifolds for flows},
  author={Fenichel, Neil and Moser, JK},
  journal={Indiana University Mathematics Journal},
  volume={21},
  number={3},
  pages={193--226},
  year={1971},
  publisher={JSTOR}
}

@article{fenichel1979geometric,
  title={Geometric singular perturbation theory for ordinary differential equations},
  author={Fenichel, Neil},
  journal={Journal of differential equations},
  volume={31},
  number={1},
  pages={53--98},
  year={1979},
  publisher={Academic Press}
}

@INPROCEEDINGS{Santoro2016,
  title     = "{Meta-Learning} with {Memory-Augmented} Neural Networks",
  booktitle = "Proceedings of The 33rd International Conference on Machine
               Learning",
  author    = "Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and
               Wierstra, Daan and Lillicrap, Timothy",
  editor    = "Balcan, Maria Florina and Weinberger, Kilian Q",
  abstract  = "Despite recent breakthroughs in the applications of deep neural
               networks, one setting that presents a persistent challenge is
               that of ``one-shot learning.'' Traditional gradient-based
               networks require a lot of data to learn, often through extensive
               iterative training. When new data is encountered, the models
               must inefficiently relearn their parameters to adequately
               incorporate the new information without catastrophic
               interference. Architectures with augmented memory capacities,
               such as Neural Turing Machines (NTMs), offer the ability to
               quickly encode and retrieve new information, and hence can
               potentially obviate the downsides of conventional models. Here,
               we demonstrate the ability of a memory-augmented neural network
               to rapidly assimilate new data, and leverage this data to make
               accurate predictions after only a few samples. We also introduce
               a new method for accessing an external memory that focuses on
               memory content, unlike previous methods that additionally use
               memory location-based focusing mechanisms.",
  publisher = "PMLR",
  volume    =  48,
  pages     = "1842--1850",
  series    = "Proceedings of Machine Learning Research",
  year      =  2016,
  url       = "https://proceedings.mlr.press/v48/santoro16.html",
  address   = "New York, New York, USA"
}

@article{Graves2014,
  author       = {Alex Graves and
                  Greg Wayne and
                  Ivo Danihelka},
  title        = {Neural Turing Machines},
  journal      = {CoRR},
  volume       = {abs/1410.5401},
  year         = {2014},
  url          = {http://arxiv.org/abs/1410.5401},
  eprinttype    = {arXiv},
  eprint       = {1410.5401},
  timestamp    = {Mon, 13 Aug 2018 16:46:28 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/GravesWD14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{Greff2017,
  title         = "{LSTM}: A Search Space Odyssey",
  author        = "Greff, Klaus and Srivastava, Rupesh K and Koutnik, Jan and
                   Steunebrink, Bas R and Schmidhuber, Jurgen",
  abstract      = "Several variants of the long short-term memory (LSTM)
                   architecture for recurrent neural networks have been
                   proposed since its inception in 1995. In recent years, these
                   networks have become the state-of-the-art models for a
                   variety of machine learning problems. This has led to a
                   renewed interest in understanding the role and utility of
                   various computational components of typical LSTM variants.
                   In this paper, we present the first large-scale analysis of
                   eight LSTM variants on three representative tasks: speech
                   recognition, handwriting recognition, and polyphonic music
                   modeling. The hyperparameters of all LSTM variants for each
                   task were optimized separately using random search, and
                   their importance was assessed using the powerful functional
                   ANalysis Of VAriance framework. In total, we summarize the
                   results of 5400 experimental runs ( $\approx$ 15 years of
                   CPU time), which makes our study the largest of its kind on
                   LSTM networks. Our results show that none of the variants
                   can improve upon the standard LSTM architecture
                   significantly, and demonstrate the forget gate and the
                   output activation function to be its most critical
                   components. We further observe that the studied
                   hyperparameters are virtually independent and derive
                   guidelines for their efficient adjustment.",
  journal       = "IEEE transactions on neural networks and learning systems",
  volume        =  28,
  number        =  10,
  pages         = "2222--2232",
  month         =  oct,
  year          =  2017,
  url           = "http://dx.doi.org/10.1109/TNNLS.2016.2582924",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  language      = "en",
  archivePrefix = "arXiv",
  eprint        = "1503.04069",
  primaryClass  = "cs.NE",
  issn          = "2162-2388, 2162-237X",
  pmid          = "27411231",
  arxivid       = "1503.04069",
  doi           = "10.1109/TNNLS.2016.2582924"
}

@ARTICLE{Khona2022,
  title    = "Attractor and integrator networks in the brain",
  author   = "Khona, Mikail and Fiete, Ila R",
  abstract = "In this Review, we describe the singular success of attractor
              neural network models in describing how the brain maintains
              persistent activity states for working memory, corrects errors
              and integrates noisy cues. We consider the mechanisms by which
              simple and forgetful units can organize to collectively generate
              dynamics on the long timescales required for such computations.
              We discuss the myriad potential uses of attractor dynamics for
              computation in the brain, and showcase notable examples of brain
              systems in which inherently low-dimensional continuous-attractor
              dynamics have been concretely and rigorously identified. Thus, it
              is now possible to conclusively state that the brain constructs
              and uses such systems for computation. Finally, we highlight
              recent theoretical advances in understanding how the fundamental
              trade-offs between robustness and capacity and between structure
              and flexibility can be overcome by reusing and recombining the
              same set of modular attractors for multiple functions, so they
              together produce representations that are structurally
              constrained and robust but exhibit high capacity and are
              flexible.",
  journal  = "Nature reviews. Neuroscience",
  volume   =  23,
  number   =  12,
  pages    = "744--766",
  month    =  dec,
  year     =  2022,
  url      = "http://dx.doi.org/10.1038/s41583-022-00642-0",
  language = "en",
  issn     = "1471-003X, 1471-0048",
  pmid     = "36329249",
  doi      = "10.1038/s41583-022-00642-0"
}


@INCOLLECTION{Jones1995,
  title     = "Geometric singular perturbation theory",
  booktitle = "Dynamical Systems: Lectures Given at the 2nd Session of the
               Centro Internazionale Matematico Estivo ({C.I.M.E}.) held in
               Montecatini Terme, Italy, June 13--22, 1994",
  author    = "Jones, Christopher K R T",
  editor    = "Arnold, Ludwig and Jones, Christopher K R T and Mischaikow,
               Konstantin and Raugel, Genevi{\`e}ve and Johnson, Russell",
  publisher = "Springer Berlin Heidelberg",
  pages     = "44--118",
  year      =  1995,
  url       = "https://doi.org/10.1007/BFb0095239",
  address   = "Berlin, Heidelberg",
  isbn      = "9783540494157",
  doi       = "10.1007/BFb0095239"
}

@ARTICLE{Egger2019,
  title    = "Internal models of sensorimotor integration regulate cortical
              dynamics",
  author   = "Egger, Seth W and Remington, Evan D and Chang, Chia-Jung and
              Jazayeri, Mehrdad",
  abstract = "Sensorimotor control during overt movements is characterized in
              terms of three building blocks: a controller, a simulator and a
              state estimator. We asked whether the same framework could
              explain the control of internal states in the absence of
              movements. Recently, it was shown that the brain controls the
              timing of future movements by adjusting an internal speed
              command. We trained monkeys in a novel task in which the speed
              command had to be dynamically controlled based on the timing of a
              sequence of flashes. Recordings from the frontal cortex provided
              evidence that the brain updates the internal speed command after
              each flash based on the error between the timing of the flash and
              the anticipated timing of the flash derived from a simulated
              motor plan. These findings suggest that cognitive control of
              internal states may be understood in terms of the same
              computational principles as motor control.",
  journal  = "Nature neuroscience",
  volume   =  22,
  number   =  11,
  pages    = "1871--1882",
  month    =  nov,
  year     =  2019,
  url      = "http://dx.doi.org/10.1038/s41593-019-0500-6",
  language = "en",
  issn     = "1097-6256, 1546-1726",
  pmid     = "31591558",
  doi      = "10.1038/s41593-019-0500-6"
}

@ARTICLE{Goldman2003-cz,
  title     = "Robust Persistent Neural Activity in a Model Integrator with
               Multiple Hysteretic Dendrites per Neuron",
  author    = "Goldman, Mark S and Levine, Joseph H and Major, Guy and Tank,
               David W and Seung, H S",
  journal   = "Cerebral cortex",
  publisher = "Oxford University Press",
  volume    =  13,
  number    =  11,
  pages     = "1185--1195",
  abstract  = "Short-term memory is often correlated with persistent changes in
               neuronal firing rates in response to transient inputs. We model
               the persistent maintenance of an analog eye position signal by an
               oculomotor neural integrator receiving transient eye movement
               commands. Previous models of this network rely on precisely tuned
               positive feedback with <1\% tolerance to mistuning, or use
               neurons that exhibit large discontinuities in firing rate with
               small changes in eye position. We show analytically how using
               neurons with multiple bistable dendritic compartments can enhance
               the robustness of eye fixations to mistuning while reproducing
               the approximately linear and continuous relationship between
               neuronal firing rates and eye position, and the dependence of
               neuron pair firing rate relationships on the direction of the
               previous saccade. The response of the model to continuously
               varying inputs makes testable predictions for the performance of
               the vestibuloocular reflex. Our results suggest that dendritic
               bistability could stabilize the persistent neural activity
               observed in working memory systems.",
  month     =  nov,
  year      =  2003,
  url       = "https://academic.oup.com/cercor/article-abstract/13/11/1185/274065",
  keywords  = "dendrites; persistence; feedback; eye fixation",
  doi       = "10.1093/cercor/bhg095",
  issn      = "1047-3211"
}

@ARTICLE{Goldman2009,
  title     = "Memory without feedback in a neural network",
  author    = "Goldman, Mark S",
  abstract  = "Memory storage on short timescales is thought to be maintained
               by neuronal activity that persists after the remembered stimulus
               is removed. Although previous work suggested that positive
               feedback is necessary to maintain persistent activity, here it
               is demonstrated how neuronal responses can instead be maintained
               by a purely feedforward mechanism in which activity is passed
               sequentially through a chain of network states. This feedforward
               form of memory storage is shown to occur both in architecturally
               feedforward networks and in recurrent networks that nevertheless
               function in a feedforward manner. The networks can be tuned to
               be perfect integrators of their inputs or to reproduce the
               time-varying firing patterns observed during some working memory
               tasks but not easily reproduced by feedback-based attractor
               models. This work illustrates a mechanism for maintaining
               short-term memory in which both feedforward and feedback
               processes interact to govern network behavior.",
  journal   = "Neuron",
  publisher = "Cell Press,",
  volume    =  61,
  number    =  4,
  pages     = "621--634",
  month     =  feb,
  year      =  2009,
  url       = "http://dx.doi.org/10.1016/j.neuron.2008.12.012",
  keywords  = "dynamical-system, feed-forward, memory",
  language  = "en",
  issn      = "0896-6273, 1097-4199",
  pmid      = "19249281",
  doi       = "10.1016/j.neuron.2008.12.012",
  pmc       = "PMC2674525"
}

@ARTICLE{Jaeger2007,
  title    = "Special issue on echo state networks and liquid state machines",
  author   = "Jaeger, Herbert and Maass, Wolfgang and Principe, Jose",
  journal  = "Neural networks: the official journal of the International Neural
              Network Society",
  volume   =  20,
  number   =  3,
  pages    = "287--289",
  month    =  apr,
  year     =  2007,
  issn     = "0893-6080",
  doi      = "10.1016/j.neunet.2007.04.001"
}

@ARTICLE{Maass2002,
  title   = "{Real-Time} Computing Without Stable States: A New Framework for
             Neural Computation Based on Perturbations",
  author  = "Maass, Wolfgang and Natschl{\"a}ger, Thomas and Markram, Henry",
  journal = "Neural computation",
  volume  =  14,
  pages   = "2531--2560",
  year    =  2002,
  issn    = "0899-7667"
}

@ARTICLE{deVries1992,
  title     = "The gamma model---A new neural model for temporal processing",
  author    = "{de Vries}, Bert and Principe, Jose C",
  abstract  = "In this paper we develop the gamma neural model, a new neural
               net architecture for processing of temporal patterns. Time
               varying patterns are normally segmented into a sequence of
               static patterns that are successively presented to a neural net.
               In the approach presented here segmentation is avoided. Only
               current signal values are presented to the neural net, which
               adapts its own internal memory to store the past. Thus, in the
               gamma neural net, an adaptive short term mechanism obviates a
               priori signal segmentation. We evaluate the relation between the
               gamma net and competing dynamic neural models. Interestingly,
               the gamma model brings many popular dynamic net architectures,
               such as the time-delay-neural-net and the
               concentration-in-time-neural-net, into a unifying framework. In
               fact, the gamma memory structure appears as general as a
               temporal convolution memory structure with arbitrary time
               varying weight kernel w(t). Yet, the gamma model remains
               mathematically equivalent to the additive (Grossberg) model with
               constant weights. We present a back propagation procedure to
               adapt the weights in a particular feedforward structure, the
               focused gamma net.",
  journal   = "Neural networks: the official journal of the International
               Neural Network Society",
  publisher = "Elsevier",
  volume    =  5,
  number    =  4,
  pages     = "565--576",
  month     =  jul,
  year      =  1992,
  url       = "http://www.sciencedirect.com/science/article/pii/S0893608005800358",
  keywords  = "Temporal processing; Dynamic neural nets; Short term memory;
               ARMA model; Gamma model; Focused back propagation",
  issn      = "0893-6080",
  doi       = "10.1016/S0893-6080(05)80035-8"
}


@ARTICLE{Waibel1989,
  title     = "Phoneme recognition using time-delay neural networks",
  author    = "Waibel, A and Hanazawa, T and Hinton, G and Shikano, K and Lang,
               K J",
  abstract  = "The authors present a time-delay neural network (TDNN) approach
               to phoneme recognition which is characterized by two important
               properties: (1) using a three-layer arrangement of simple
               computing units, a hierarchy can be constructed that allows for
               the formation of arbitrary nonlinear decision surfaces, which
               the TDNN learns automatically using error backpropagation; and
               (2) the time-delay arrangement enables the network to discover
               acoustic-phonetic features and the temporal relationships
               between them independently of position in time and therefore not
               blurred by temporal shifts in the input. As a recognition task,
               the speaker-dependent recognition of the phonemes B, D, and G in
               varying phonetic contexts was chosen. For comparison, several
               discrete hidden Markov models (HMM) were trained to perform the
               same task. Performance evaluation over 1946 testing tokens from
               three speakers showed that the TDNN achieves a recognition rate
               of 98.5\% correct while the rate obtained by the best of the
               HMMs was only 93.7\%.",
  journal   = "IEEE transactions on acoustics, speech, and signal processing",
  publisher = "ieeexplore.ieee.org",
  volume    =  37,
  number    =  3,
  pages     = "328--339",
  month     =  mar,
  year      =  1989,
  url       = "http://dx.doi.org/10.1109/29.21701",
  keywords  = "Neural networks;Speech recognition;Hidden Markov models;Computer
               science;Psychology;Character recognition;Computer
               networks;Backpropagation;Acoustic testing;Loudspeakers",
  issn      = "0096-3518",
  doi       = "10.1109/29.21701"
}

@INCOLLECTION{Vaswani2017,
  title     = "Attention is All you Need",
  booktitle = "Advances in Neural Information Processing Systems 30",
  author    = "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and
               Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser,
               {\L} Ukasz and Polosukhin, Illia",
  editor    = "Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and
               Fergus, R and Vishwanathan, S and Garnett, R",
  publisher = "Curran Associates, Inc.",
  pages     = "5998--6008",
  year      =  2017,
  url       = "http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf"
}

@ARTICLE{Wang2002,
  title    = "Probabilistic Decision Making by Slow Reverberation in Cortical
              Circuits",
  author   = "Wang, Xiao-Jing",
  abstract = "Recent physiological studies of alert primates have revealed
              cortical neural correlates of key steps in a perceptual
              decision-making process. To elucidate synaptic mechanisms of
              decision making, I investigated a biophysically realistic
              cortical network model for a visual discrimination experiment. In
              the model, slow recurrent excitation and feedback inhibition
              produce attractor dynamics that amplify the difference between
              conflicting inputs and generates a binary choice. The model is
              shown to account for salient characteristics of the observed
              decision-correlated neural activity, as well as the animal's
              psychometric function and reaction times. These results suggest
              that recurrent excitation mediated by NMDA receptors provides a
              candidate cellular mechanism for the slow time integration of
              sensory stimuli and the formation of categorical choices in a
              decision-making neocortical network.",
  journal  = "Neuron",
  volume   =  36,
  number   =  5,
  pages    = "955--968",
  month    =  dec,
  year     =  2002,
  url      = "http://dx.doi.org/10.1016/s0896-6273(02)01092-9",
  keywords = "computational-neuroscience, decision-making,
              drift-diffusion-model, dynamical-system, neural-dynamics",
  issn     = "0896-6273",
  doi      = "10.1016/s0896-6273(02)01092-9"
}

@ARTICLE{Machens2005,
  title     = "Flexible Control of Mutual Inhibition: A Neural Model of
               {Two-Interval} Discrimination",
  author    = "Machens, Christian K and Romo, Ranulfo and Brody, Carlos D",
  abstract  = "Networks adapt to environmental demands by switching between
               distinct dynamical behaviors. The activity of frontal-lobe
               neurons during two-interval discrimination tasks is an example
               of these adaptable dynamics. Subjects first perceive a stimulus,
               then hold it in working memory, and finally make a decision by
               comparing it with a second stimulus. We present a simple
               mutual-inhibition network model that captures all three task
               phases within a single framework. The model integrates both
               working memory and decision making because its dynamical
               properties are easily controlled without changing its
               connectivity. Mutual inhibition between nonlinear units is a
               useful design motif for networks that must display multiple
               behaviors.",
  journal   = "Science",
  publisher = "American Association for the Advancement of Science",
  volume    =  307,
  number    =  5712,
  pages     = "1121--1124",
  month     =  feb,
  year      =  2005,
  url       = "http://dx.doi.org/10.1126/science.1104171",
  keywords  = "interval-timing, neural-code, neural-network",
  issn      = "0036-8075",
  pmid      = "15718474",
  doi       = "10.1126/science.1104171"
}

@ARTICLE{Lim2012,
  title    = "Noise tolerance of attractor and feedforward memory models",
  author   = "Lim, Sukbin and Goldman, Mark S",
  abstract = "In short-term memory networks, transient stimuli are represented
              by patterns of neural activity that persist long after stimulus
              offset. Here, we compare the performance of two prominent classes
              of memory networks, feedback-based attractor networks and
              feedforward networks, in conveying information about the
              amplitude of a briefly presented stimulus in the presence of
              gaussian noise. Using Fisher information as a metric of memory
              performance, we find that the optimal form of network
              architecture depends strongly on assumptions about the forms of
              nonlinearities in the network. For purely linear networks, we
              find that feedforward networks outperform attractor networks
              because noise is continually removed from feedforward networks
              when signals exit the network; as a result, feedforward networks
              can amplify signals they receive faster than noise accumulates
              over time. By contrast, attractor networks must operate in a
              signal-attenuating regime to avoid the buildup of noise. However,
              if the amplification of signals is limited by a finite dynamic
              range of neuronal responses or if noise is reset at the time of
              signal arrival, as suggested by recent experiments, we find that
              attractor networks can outperform feedforward ones. Under a
              simple model in which neurons have a finite dynamic range, we
              find that the optimal attractor networks are forgetful if there
              is no mechanism for noise reduction with signal arrival but
              nonforgetful (perfect integrators) in the presence of a strong
              reset mechanism. Furthermore, we find that the maximal Fisher
              information for the feedforward and attractor networks exhibits
              power law decay as a function of time and scales linearly with
              the number of neurons. These results highlight prominent factors
              that lead to trade-offs in the memory performance of networks
              with different architectures and constraints, and suggest
              conditions under which attractor or feedforward networks may be
              best suited to storing information about previous stimuli.",
  journal  = "Neural computation",
  volume   =  24,
  number   =  2,
  pages    = "332--390",
  month    =  feb,
  year     =  2012,
  url      = "http://dx.doi.org/10.1162/NECO_a_00234",
  language = "en",
  issn     = "0899-7667, 1530-888X",
  pmid     = "22091664",
  doi      = "10.1162/NECO\_a\_00234",
  pmc      = "PMC5529185"
}


@ARTICLE{Lim2013,
  title    = "Balanced cortical microcircuitry for maintaining information in
              working memory",
  author   = "Lim, Sukbin and Goldman, Mark S",
  abstract = "Persistent neural activity in the absence of a stimulus has been
              identified as a neural correlate of working memory, but how such
              activity is maintained by neocortical circuits remains unknown.
              We used a computational approach to show that the inhibitory and
              excitatory microcircuitry of neocortical memory-storing regions
              is sufficient to implement a corrective feedback mechanism that
              enables persistent activity to be maintained stably for prolonged
              durations. When recurrent excitatory and inhibitory inputs to
              memory neurons were balanced in strength and offset in time,
              drifts in activity triggered a corrective signal that
              counteracted memory decay. Circuits containing this mechanism
              temporally integrated their inputs, generated the irregular
              neural firing observed during persistent activity and were robust
              against common perturbations that severely disrupted previous
              models of short-term memory storage. These results reveal a
              mechanism for the accumulation and storage of memories in
              neocortical circuits based on principles of corrective negative
              feedback that are widely used in engineering applications.",
  journal  = "Nature neuroscience",
  volume   =  16,
  number   =  9,
  pages    = "1306--1314",
  month    =  sep,
  year     =  2013,
  url      = "http://dx.doi.org/10.1038/nn.3492",
  language = "en",
  issn     = "1097-6256, 1546-1726",
  pmid     = "23955560",
  doi      = "10.1038/nn.3492",
  pmc      = "PMC3772089"
}

@UNPUBLISHED{Champion2021,
  title    = "An Oscillatory Mechanism for Multi-level Storage in Short-term
              Memory",
  author   = "Champion, Kathleen P and Gozel, Olivia and Lankow, Benjamin S and
              Bard Ermentrout, G and Goldman, Mark S",
  abstract = "Oscillatory activity is commonly observed during the maintenance
              of information in short-term memory, but its role remains
              unclear. Non-oscillatory models of short-term memory storage are
              able to encode stimulus identity through their spatial patterns
              of activity, but are typically limited to either an all-or-none
              representation of stimulus amplitude or exhibit a biologically
              implausible exact-tuning condition. Here, we demonstrate a simple
              phase-locking mechanism by which oscillatory input enables a
              circuit to generate persistent or sequential activity patterns
              that encode information not only in their location but also in a
              discrete set of amplitudes. \#\#\# Competing Interest Statement
              The authors have declared no competing interest.",
  journal  = "bioRxiv",
  pages    = "2021.07.29.454329",
  month    =  jul,
  year     =  2021,
  url      = "https://www.biorxiv.org/content/10.1101/2021.07.29.454329v1.full",
  language = "en",
  doi      = "10.1101/2021.07.29.454329"
}

@ARTICLE{Widrow1990,
  title     = "30 years of adaptive neural networks: Perceptron, {M}adaline, and
               backpropagation",
  author    = "Widrow, Bernard and Lehr, Michael A",
  journal   = "Proceedings of the IEEE. Institute of Electrical and Electronics
               Engineers",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  78,
  number    =  9,
  pages     = "1415--1442",
  year      =  1990,
  url       = "https://www-isl.stanford.edu/~widrow/papers/j199030years.pdf",
  copyright = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html",
  issn      = "0018-9219, 1558-2256",
  doi       = "10.1109/5.58323"
}


@MISC{Hopfield1984,
  title   = "Neurons with graded response have collective computational
             properties like those of two-state neurons",
  author  = "Hopfield, J J",
  journal = "Proceedings of the National Academy of Sciences",
  volume  =  81,
  number  =  10,
  pages   = "3088--3092",
  year    =  1984,
  url     = "http://dx.doi.org/10.1073/pnas.81.10.3088",
  doi     = "10.1073/pnas.81.10.3088"
}

@ARTICLE{Bellec2020,
  title    = "A solution to the learning dilemma for recurrent networks of
              spiking neurons",
  author   = "Bellec, Guillaume and Scherr, Franz and Subramoney, Anand and
              Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass,
              Wolfgang",
  abstract = "Recurrently connected networks of spiking neurons underlie the
              astounding information processing capabilities of the brain. Yet
              in spite of extensive research, how they can learn through
              synaptic plasticity to carry out complex network computations
              remains unclear. We argue that two pieces of this puzzle were
              provided by experimental data from neuroscience. A mathematical
              result tells us how these pieces need to be combined to enable
              biologically plausible online network learning through gradient
              descent, in particular deep reinforcement learning. This learning
              method-called e-prop-approaches the performance of
              backpropagation through time (BPTT), the best-known method for
              training recurrent neural networks in machine learning. In
              addition, it suggests a method for powerful on-chip learning in
              energy-efficient spike-based hardware for artificial
              intelligence.",
  journal  = "Nature communications",
  volume   =  11,
  number   =  1,
  pages    = "3625",
  month    =  jul,
  year     =  2020,
  url      = "http://dx.doi.org/10.1038/s41467-020-17236-y",
  language = "en",
  issn     = "2041-1723",
  pmid     = "32681001",
  doi      = "10.1038/s41467-020-17236-y",
  pmc      = "PMC7367848"
}

@ARTICLE{Richards2019,
  title     = "A deep learning framework for neuroscience",
  author    = "Richards, Blake A and Lillicrap, Timothy P and Beaudoin,
               Philippe and Bengio, Yoshua and Bogacz, Rafal and Christensen,
               Amelia and Clopath, Claudia and Costa, Rui Ponte and de Berker,
               Archy and Ganguli, Surya and Gillon, Colleen J and Hafner,
               Danijar and Kepecs, Adam and Kriegeskorte, Nikolaus and Latham,
               Peter and Lindsay, Grace W and Miller, Kenneth D and Naud,
               Richard and Pack, Christopher C and Poirazi, Panayiota and
               Roelfsema, Pieter and Sacramento, Jo{\~a}o and Saxe, Andrew and
               Scellier, Benjamin and Schapiro, Anna C and Senn, Walter and
               Wayne, Greg and Yamins, Daniel and Zenke, Friedemann and
               Zylberberg, Joel and Therien, Denis and Kording, Konrad P",
  abstract  = "Systems neuroscience seeks explanations for how the brain
               implements a wide variety of perceptual, cognitive and motor
               tasks. Conversely, artificial intelligence attempts to design
               computational systems based on the tasks they will have to
               solve. In artificial neural networks, the three components
               specified by design are the objective functions, the learning
               rules and the architectures. With the growing success of deep
               learning, which utilizes brain-inspired architectures, these
               three designed components have increasingly become central to
               how we model, engineer and optimize complex artificial learning
               systems. Here we argue that a greater focus on these components
               would also benefit systems neuroscience. We give examples of how
               this optimization-based framework can drive theoretical and
               experimental progress in neuroscience. We contend that this
               principled perspective on systems neuroscience will help to
               generate more rapid progress.",
  journal   = "Nature neuroscience",
  publisher = "nature.com",
  volume    =  22,
  number    =  11,
  pages     = "1761--1770",
  month     =  nov,
  year      =  2019,
  url       = "http://dx.doi.org/10.1038/s41593-019-0520-2",
  language  = "en",
  issn      = "1097-6256, 1546-1726",
  pmid      = "31659335",
  doi       = "10.1038/s41593-019-0520-2"
}

@INCOLLECTION{Hochreiter2001,
  title     = "Gradient flow in recurrent nets: The difficulty of learning
               long-term dependencies",
  booktitle = "A Field Guide to Dynamical Recurrent Neural Networks",
  author    = "Hochreiter, Sepp and Bengio, Yoshua and Frasconi, Paolo and
               Schmidhuber, J{\"u}rgen",
  editor    = "{S. C. Kremer and J. F. Kolen}",
  publisher = "IEEE",
  year      =  2001,
  url       = "https://www.bioinf.jku.at/publications/older/ch7.pdf"
}

@ARTICLE{Bengio1994,
  title    = "Learning long-term dependencies with gradient descent is
              difficult",
  author   = "Bengio, Y and Simard, P and Frasconi, P",
  abstract = "Recurrent neural networks can be used to map input sequences to
              output sequences, such as for recognition, production or
              prediction problems. However, practical difficulties have been
              reported in training recurrent neural networks to perform tasks
              in which the temporal contingencies present in the input/output
              sequences span long intervals. We show why gradient based
              learning algorithms face an increasingly difficult problem as the
              duration of the dependencies to be captured increases. These
              results expose a trade-off between efficient learning by gradient
              descent and latching on information for long periods. Based on an
              understanding of this problem, alternatives to standard gradient
              descent are considered.",
  journal  = "IEEE transactions on neural networks / a publication of the IEEE
              Neural Networks Council",
  volume   =  5,
  number   =  2,
  pages    = "157--166",
  year     =  1994,
  url      = "http://dx.doi.org/10.1109/72.279181",
  language = "en",
  issn     = "1045-9227",
  pmid     = "18267787",
  doi      = "10.1109/72.279181"
}

@INPROCEEDINGS{Pascanu2013,
  title      = "On the difficulty of training recurrent neural networks",
  booktitle  = "International Conference on Machine Learning",
  author     = "Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua",
  abstract   = "There are two widely known issues with properly training
                recurrent neural networks, the vanishing and the exploding
                gradient problems detailed in Bengio et al. (1994). In this
                paper we attempt to improve the understanding of the underlying
                issues by exploring these problems from an analytical, a
                geometric and a dynamical systems perspective. Our analysis is
                used to justify a simple yet effective solution. We propose a
                gradient norm clipping strategy to deal with exploding
                gradients and a soft constraint for the vanishing gradients
                problem. We validate empirically our hypothesis and proposed
                solutions in the experimental section.",
  publisher  = "jmlr.org",
  pages      = "1310--1318",
  month      =  feb,
  year       =  2013,
  url        = "http://proceedings.mlr.press/v28/pascanu13.html",
  language   = "en",
  conference = "International Conference on Machine Learning"
}

@INPROCEEDINGS{Glorot2010,
  title     = "Understanding the difficulty of training deep feedforward neural
               networks",
  booktitle = "Proceedings of the thirteenth international conference on
               artificial intelligence and statistics",
  author    = "Glorot, Xavier and Bengio, Yoshua",
  publisher = "jmlr.org",
  pages     = "249--256",
  year      =  2010,
}

@ARTICLE{Lillicrap2020,
  title    = "Backpropagation and the brain",
  author   = "Lillicrap, Timothy P and Santoro, Adam and Marris, Luke and
              Akerman, Colin J and Hinton, Geoffrey",
  abstract = "During learning, the brain modifies synapses to improve
              behaviour. In the cortex, synapses are embedded within
              multilayered networks, making it difficult to determine the
              effect of an individual synaptic modification on the behaviour of
              the system. The backpropagation algorithm solves this problem in
              deep artificial neural networks, but historically it has been
              viewed as biologically problematic. Nonetheless, recent
              developments in neuroscience and the successes of artificial
              neural networks have reinvigorated interest in whether
              backpropagation offers insights for understanding learning in the
              cortex. The backpropagation algorithm learns quickly by computing
              synaptic updates using feedback connections to deliver error
              signals. Although feedback connections are ubiquitous in the
              cortex, it is difficult to see how they could deliver the error
              signals required by strict formulations of backpropagation. Here
              we build on past and recent developments to argue that feedback
              connections may instead induce neural activities whose
              differences can be used to locally approximate these signals and
              hence drive effective learning in deep networks in the brain.",
  journal  = "Nature reviews. Neuroscience",
  month    =  apr,
  year     =  2020,
  url      = "http://dx.doi.org/10.1038/s41583-020-0277-3",
  language = "en",
  issn     = "1471-003X, 1471-0048",
  pmid     = "32303713",
  doi      = "10.1038/s41583-020-0277-3"
}

@ARTICLE{Hochreiter1997,
  title     = "Long short-term memory",
  author    = "Hochreiter, S and Schmidhuber, J",
  abstract  = "Learning to store information over extended time intervals by
               recurrent backpropagation takes a very long time, mostly because
               of insufficient, decaying error backflow. We briefly review
               Hochreiter's (1991) analysis of this problem, then address it by
               introducing a novel, efficient, gradient-based method called
               long short-term memory (LSTM). Truncating the gradient where
               this does not do harm, LSTM can learn to bridge minimal time
               lags in excess of 1000 discrete-time steps by enforcing constant
               error flow through constant error carousels within special
               units. Multiplicative gate units learn to open and close access
               to the constant error flow. LSTM is local in space and time; its
               computational complexity per time step and weight is O(1). Our
               experiments with artificial data involve local, distributed,
               real-valued, and noisy pattern representations. In comparisons
               with real-time recurrent learning, back propagation through
               time, recurrent cascade correlation, Elman nets, and neural
               sequence chunking, LSTM leads to many more successful runs, and
               learns much faster. LSTM also solves complex, artificial
               long-time-lag tasks that have never been solved by previous
               recurrent network algorithms.",
  journal   = "Neural computation",
  publisher = "people.idsia.ch",
  volume    =  9,
  number    =  8,
  pages     = "1735--1780",
  month     =  nov,
  year      =  1997,
  url       = "https://www.ncbi.nlm.nih.gov/pubmed/9377276",
  language  = "en",
  issn      = "0899-7667",
  pmid      = "9377276"
}

@ARTICLE{Beer1995,
  title     = "On the Dynamics of Small {Continuous-Time} Recurrent Neural
               Networks",
  author    = "Beer, Randall D",
  abstract  = "Dynamical neural networks are being increasingly employed in a
               variety of contexts, including as simple model nervous systems
               for autonomous agents. For this reason, there is a growing need
               for a comprehensive understanding of their dynamical properties.
               Using a combination of elementary analysis and numerical
               studies, this article begins a systematic examination of the
               dynamics of continuous-time recurrent neural networks.
               Specifically, a fairly complete description of the possible
               dynamical behavior and bifurcations of one- and two-neuron
               circuits is given, along with a few specific results for larger
               networks. This analysis provides both qualitative insight and,
               in many cases, quantitative formulas for predicting the
               dynamical behavior of particular circuits and how that behavior
               changes as network parameters are varied. These results
               demonstrate that even small circuits are capable of a rich
               variety of dynamical behavior (including chaotic dynamics). An
               approach to understanding the dynamics of circuits with
               time-varying inputs is also presented. Finally, based on this
               analysis, several strategies for focusing evolutionary searches
               into fruitful regions of network parameter space are suggested.",
  journal   = "Adaptive behavior",
  publisher = "SAGE Publications Ltd STM",
  volume    =  3,
  number    =  4,
  pages     = "469--509",
  month     =  mar,
  year      =  1995,
  url       = "https://doi.org/10.1177/105971239500300405",
  annote    = "doi: 10.1177/105971239500300405",
  issn      = "1059-7123",
  doi       = "10.1177/105971239500300405"
}

@INCOLLECTION{Cowan1972,
  title     = "Stochastic models of neuroelectric activity",
  booktitle = "Towards a Theoretical Biology",
  author    = "Cowan, J D",
  editor    = "Waddington, C H",
  publisher = "Edinburgh University Press",
  volume    =  4,
  pages     = "169--188",
  year      =  1972
}

@ARTICLE{Hinton2006,
  title    = "Reducing the dimensionality of data with neural networks",
  author   = "Hinton, G E and Salakhutdinov, R R",
  abstract = "High-dimensional data can be converted to low-dimensional codes
              by training a multilayer neural network with a small central
              layer to reconstruct high-dimensional input vectors. Gradient
              descent can be used for fine-tuning the weights in such
              ``autoencoder'' networks, but this works well only if the initial
              weights are close to a good solution. We describe an effective
              way of initializing the weights that allows deep autoencoder
              networks to learn low-dimensional codes that work much better
              than principal components analysis as a tool to reduce the
              dimensionality of data.",
  journal  = "Science",
  volume   =  313,
  number   =  5786,
  pages    = "504--507",
  month    =  jul,
  year     =  2006,
  url      = "http://dx.doi.org/10.1126/science.1127647",
  language = "en",
  issn     = "0036-8075, 1095-9203",
  pmid     = "16873662",
  doi      = "10.1126/science.1127647"
}

@ARTICLE{Ozturk2007,
  title    = "Analysis and design of echo state networks",
  author   = "Ozturk, Mustafa C and Xu, Dongming and Pr{\'\i}ncipe, Jos{\'e} C",
  abstract = "The design of echo state network (ESN) parameters relies on the
              selection of the maximum eigenvalue of the linearized system
              around zero (spectral radius). However, this procedure does not
              quantify in a systematic manner the performance of the ESN in
              terms of approximation error. This article presents a functional
              space approximation framework to better understand the operation
              of ESNs and proposes an information-theoretic metric, the average
              entropy of echo states, to assess the richness of the ESN
              dynamics. Furthermore, it provides an interpretation of the ESN
              dynamics rooted in system theory as families of coupled
              linearized systems whose poles move according to the input signal
              dynamics. With this interpretation, a design methodology for
              functional approximation is put forward where ESNs are designed
              with uniform pole distributions covering the frequency spectrum
              to abide by the richness metric, irrespective of the spectral
              radius. A single bias parameter at the ESN input, adapted with
              the modeling error, configures the ESN spectral radius to the
              input-output joint space. Function approximation examples compare
              the proposed design methodology versus the conventional design.",
  journal  = "Neural computation",
  volume   =  19,
  number   =  1,
  pages    = "111--138",
  month    =  jan,
  year     =  2007,
  url      = "http://dx.doi.org/10.1162/neco.2007.19.1.111",
  language = "en",
  issn     = "0899-7667",
  pmid     = "17134319",
  doi      = "10.1162/neco.2007.19.1.111"
}

@INPROCEEDINGS{Srivastava2015,
  title     = "Training Very Deep Networks",
  booktitle = "Advances in Neural Information Processing Systems",
  author    = "Srivastava, Rupesh K and Greff, Klaus and Schmidhuber,
               J{\"u}rgen",
  editor    = "Cortes, C and Lawrence, N and Lee, D and Sugiyama, M and
               Garnett, R",
  publisher = "Curran Associates, Inc.",
  volume    =  28,
  year      =  2015,
  url       = "https://proceedings.neurips.cc/paper/2015/file/215a71a12769b056c3c32e7299f1c5ed-Paper.pdf"
}

@ARTICLE{Baldi1995,
  title     = "Gradient descent learning algorithm overview: a general
               dynamical systems perspective",
  author    = "Baldi, Pierre",
  abstract  = "Gives a unified treatment of gradient descent learning
               algorithms for neural networks using a general framework of
               dynamical systems. This general approach organizes and
               simplifies all the known algorithms and results which have been
               originally derived for different problems (fixed
               point/trajectory learning), for different models
               (discrete/continuous), for different architectures
               (forward/recurrent), and using different techniques
               (backpropagation, variational calculus, adjoint methods, etc.).
               The general approach can also be applied to derive new
               algorithms. The author then briefly examines some of the
               complexity issues and limitations intrinsic to gradient descent
               learning. Throughout the paper, the author focuses on the
               problem of trajectory learning.",
  journal   = "IEEE transactions on neural networks",
  publisher = "ieeexplore.ieee.org",
  volume    =  6,
  number    =  1,
  pages     = "182--195",
  year      =  1995,
  url       = "http://dx.doi.org/10.1109/72.363438",
  language  = "en",
  issn      = "1045-9227",
  pmid      = "18263297",
  doi       = "10.1109/72.363438"
}

@BOOK{Pontryagin1964,
  title     = "Mathematical theory of optimal processes",
  author    = "Pontryagin, Lev Semenovich and Boltyanskii, Vladimir Grigorevich
               and Gamkrelidze, Revaz Valerianovich and Mishchenko, Evgenii
               Frolovich",
  publisher = "Macmillan company",
  year      =  1964,
  url       = "https://www.taylorfrancis.com/books/9781351433075"
}

@BOOK{Brockett1970,
  title     = "Finite Dimensional Linear Systems",
  author    = "Brockett, Roger W",
  publisher = "Wiley",
  year      =  1970,
  language  = "en"
}

@INPROCEEDINGS{He2016,
  title     = "Deep residual learning for image recognition",
  booktitle = "Proceedings of the {IEEE} conference on computer vision and
               pattern recognition",
  author    = "He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian",
  pages     = "770--778",
  year      =  2016,
}

@INCOLLECTION{Pennington2017a,
  title     = "Resurrecting the sigmoid in deep learning through dynamical
               isometry: theory and practice",
  booktitle = "Advances in Neural Information Processing Systems 30",
  author    = "Pennington, Jeffrey and Schoenholz, Samuel and Ganguli, Surya",
  editor    = "Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and
               Fergus, R and Vishwanathan, S and Garnett, R",
  publisher = "Curran Associates, Inc.",
  pages     = "4785--4795",
  year      =  2017,
  url       = "http://papers.nips.cc/paper/7064-resurrecting-the-sigmoid-in-deep-learning-through-dynamical-isometry-theory-and-practice.pdf"
}

@INPROCEEDINGS{Saxe2014,
  title     = "Exact solutions to the nonlinear dynamics of learning in deep
               linear neural networks",
  booktitle = "International Conference on Learning Represenatations",
  author    = "Saxe, Andrew M and McClelland, James L and Ganguli, Surya",
  abstract  = "Despite the widespread practical success of deep learning
               methods, our theoretical understanding of the dynamics of
               learning in deep neural networks remains quite sparse. We
               attempt to bridge the gap between the theory and practice of
               deep learning by systematically analyzing learning dynamics for
               the restricted case of deep linear neural networks. Despite the
               linearity of their input-output map, such networks have
               nonlinear gradient descent dynamics on weights that change with
               the addition of each new hidden layer. We show that deep linear
               networks exhibit nonlinear learning phenomena similar to those
               seen in simulations of nonlinear networks, including long
               plateaus followed by rapid transitions to lower error solutions,
               and faster convergence from greedy unsupervised pretraining
               initial conditions than from random initial conditions. We
               provide an analytical description of these phenomena by finding
               new exact solutions to the nonlinear dynamics of deep learning.
               Our theoretical analysis also reveals the surprising finding
               that as the depth of a network approaches infinity, learning
               speed can nevertheless remain finite: for a special class of
               initial conditions on the weights, very deep networks incur only
               a finite, depth independent, delay in learning speed relative to
               shallow networks. We show that, under certain conditions on the
               training data, unsupervised pretraining can find this special
               class of initial conditions, while scaled random Gaussian
               initializations cannot. We further exhibit a new class of random
               orthogonal initial conditions on weights that, like unsupervised
               pre-training, enjoys depth independent learning times. We
               further show that these initial conditions also lead to faithful
               propagation of gradients even in deep nonlinear networks, as
               long as they operate in a special regime known as the edge of
               chaos.",
  year      =  2014,
  url       = "http://arxiv.org/abs/1312.6120"
}

@INPROCEEDINGS{Ioffe2015,
  title     = "Batch Normalization: Accelerating Deep Network Training by
               Reducing Internal Covariate Shift",
  booktitle = "International Conference on Machine Learning",
  author    = "Ioffe, Sergey and Szegedy, Christian",
  abstract  = "Training Deep Neural Networks is complicated by the fact that
               the distribution of each layer's inputs changes during training,
               as the parameters of the previous layers change. This slows down
               the training by requiring lower learning rates and careful
               parameter initialization, and makes it notoriously hard to train
               models with saturating nonlinearities. We refer to this
               phenomenon as internal covariate shift, and address the problem
               by normalizing layer inputs. Our method draws its strength from
               making normalization a part of the model architecture and
               performing the normalization for each training mini-batch. Batch
               Normalization allows us to use much higher learning rates and be
               less careful about initialization. It also acts as a
               regularizer, in some cases eliminating the need for Dropout.
               Applied to a state-of-the-art image classification model, Batch
               Normalization achieves the same accuracy with 14 times fewer
               training steps, and beats the original model by a significant
               margin. Using an ensemble of batch-normalized networks, we
               improve upon the best published result on ImageNet
               classification: reaching 4.9\% top-5 validation error (and 4.8\%
               test error), exceeding the accuracy of human raters.",
  year      =  2015,
  url       = "http://arxiv.org/abs/1502.03167"
}

@INPROCEEDINGS{Mhammedi2017,
  title = 	 {Efficient Orthogonal Parametrisation of Recurrent Neural Networks Using Householder Reflections},
  author =       {Zakaria Mhammedi and Andrew Hellicar and Ashfaqur Rahman and James Bailey},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {2401--2409},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/mhammedi17a/mhammedi17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/mhammedi17a.html},
  abstract = 	 {The problem of learning long-term dependencies in sequences using Recurrent Neural Networks (RNNs) is still a major challenge. Recent methods have been suggested to solve this problem by constraining the transition matrix to be unitary during training which ensures that its norm is equal to one and prevents exploding gradients. These methods either have limited expressiveness or scale poorly with the size of the network when compared with the simple RNN case, especially when using stochastic gradient descent with a small mini-batch size. Our contributions are as follows; we first show that constraining the transition matrix to be unitary is a special case of an orthogonal constraint. Then we present a new parametrisation of the transition matrix which allows efficient training of an RNN while ensuring that the matrix is always orthogonal. Our results show that the orthogonal constraint on the transition matrix applied through our parametrisation gives similar benefits to the unitary constraint, without the time complexity limitations.}
}

@BOOK{Chicone2006,
  title     = "Ordinary Differential Equations with Applications",
  author    = "Chicone, Carmen",
  abstract  = "Mathematics is playing an ever more important role in the
               physical and biological sciences, provoking a blurring of
               boundaries between scienti?c disciplines and a resurgence of
               interest in the modern as well as the cl- sical techniques of
               applied mathematics. This renewal of interest, both in research
               and teaching, has led to the establishment of the series Texts
               in Applied Mathematics (TAM).
               Thedevelopmentofnewcoursesisanaturalconsequenceofahighlevelof
               excitement on the research frontier as newer techniques, such as
               numerical and symbolic computer systems, dynamical systems, and
               chaos, mix with and reinforce the traditional methods of applied
               mathematics. Thus, the purpose of this textbook series is to
               meet the current and future needs of these advances and to
               encourage the teaching of new courses. TAM will publish
               textbooks suitable for use in advanced undergraduate and
               beginning graduate courses, and will complement the Applied Ma-
               ematical Sciences (AMS) series, which will focus on advanced
               textbooks and research-level monographs. Pasadena, California
               J.E. Marsden New York, New York L. Sirovich College Park,
               Maryland S.S. Antman Preface This book is based on a
               two-semester course in ordinary di?erential eq- tions that I
               have taught to graduate students for two decades at the U-
               versity of Missouri. The scope of the narrative evolved over
               time from an embryonic collection of supplementary notes,
               through many classroom tested revisions, to a treatment of the
               subject that is suitable for a year (or more) of graduate study.",
  publisher = "Springer Science \& Business Media",
  month     =  sep,
  year      =  2006,
  url       = "https://market.android.com/details?id=book-Vn9EQ5onbvUC",
  language  = "en",
  isbn      = "9780387357942"
}

@ARTICLE{Sanz-Serna2016,
  title     = "Symplectic {Runge--Kutta} Schemes for Adjoint Equations,
               Automatic Differentiation, Optimal Control, and More",
  author    = "Sanz-Serna, J",
  abstract  = "The study of the sensitivity of the solution of a system of
               differential equations with respect to changes in the initial
               conditions leads to the introduction of an adjoint system, whose
               discretization is related to reverse accumulation in automatic
               differentiation. Similar adjoint systems arise in optimal
               control and other areas, including classical mechanics. Adjoint
               systems are introduced in such a way that they exactly preserve
               a relevant quadratic invariant (more precisely, an inner
               product). Symplectic Runge--Kutta and partitioned Runge--Kutta
               methods are defined through the exact conservation of a
               differential geometric structure, but may be characterized by
               the fact that they preserve exactly quadratic invariants of the
               system being integrated. Therefore, the symplecticness (or lack
               of symplecticness) of a Runge--Kutta or partitioned Runge--Kutta
               integrator should be relevant to understanding its performance
               when applied to the computation of sensitivities, to optimal
               control problems, and in other applications requiring the use of
               adjoint systems. This paper examines the links between
               symplectic integration and those applications and presents in a
               new, unified way a number of results currently scattered among
               or implicit in the literature. In particular, we show how some
               common procedures, such as the direct method in optimal control
               theory and the computation of sensitivities via reverse
               accumulation, imply, probably unbeknownst to the user, ?hidden?
               integrations with symplectic partitioned Runge--Kutta schemes.",
  journal   = "SIAM Review",
  publisher = "Society for Industrial and Applied Mathematics",
  volume    =  58,
  number    =  1,
  pages     = "3--33",
  month     =  jan,
  year      =  2016,
  url       = "https://doi.org/10.1137/151002769",
  annote    = "doi: 10.1137/151002769",
  issn      = "0036-1445",
  doi       = "10.1137/151002769"
}

@ARTICLE{Benny_Toomarian1992,
  title     = "Learning a trajectory using adjoint functions and teacher
               forcing",
  author    = "Benny Toomarian, Nikzad and Barhen, Jacob",
  abstract  = "A new methodology for faster supervised temporal learning in
               nonlinear neural networks is presented. It builds upon the
               concept of adjoint operators, to enable a fast computation of
               the gradients of an error functional with respect to all
               parameters of the neural architecture, and exploits the concept
               of teacher forcing to incorporate information regarding the
               desired output into the activation dynamics. The importance of
               the initial or final time conditions for the adjoint equations
               (i.e., the error propagation equations) is discussed. A new
               algorithm is presented, in which the adjoint equations are
               solved simultaneously (i.e., forward in time) with the
               activation dynamics of the neural network. We also indicate how
               teacher forcing can be modulated in time as learning proceeds.
               The algorithm is illustrated by examples. The results show that
               the learning time is reduced by one to two orders of magnitude
               with respect to previously published results, while trajectory
               tracking is significantly improved. The proposed methodology
               makes hardware implementation of temporal learning attractive
               for real-time applications.",
  journal   = "Neural networks: the official journal of the International
               Neural Network Society",
  publisher = "Elsevier",
  volume    =  5,
  number    =  3,
  pages     = "473--484",
  month     =  jan,
  year      =  1992,
  url       = "http://www.sciencedirect.com/science/article/pii/0893608092900098",
  keywords  = "Temporal learning; Trajectory learning; Adjoint operators;
               Teacher forcing; Recurrent networks",
  issn      = "0893-6080",
  doi       = "10.1016/0893-6080(92)90009-8"
}

@INPROCEEDINGS{Huh2018,
  title     = "Gradient Descent for Spiking Neural Networks",
  booktitle = "Advances in Neural Information Processing Systems 31",
  author    = "Huh, Dongsung and Sejnowski, Terrence J",
  editor    = "Bengio, S and Wallach, H and Larochelle, H and Grauman, K and
               Cesa-Bianchi, N and Garnett, R",
  publisher = "Curran Associates, Inc.",
  pages     = "1440--1450",
  year      =  2018,
  url       = "http://papers.nips.cc/paper/7417-gradient-descent-for-spiking-neural-networks.pdf"
}

@INPROCEEDINGS{Gholami2019,
  title     = "{ANODE}: Unconditionally Accurate {Memory-Efficient} Gradients
               for Neural {ODEs}",
  booktitle = "Proceedings of the {Twenty-Eighth} International Joint
               Conference on Artificial Intelligence ({IJCAI-19})",
  author    = "Gholami, Amir and Keutzer, Kurt and Biros, George",
  abstract  = "Residual neural networks can be viewed as the forward Euler
               discretization of an Ordinary Differential Equation (ODE) with a
               unit time step. This has recently motivated researchers to
               explore other discretization approaches and train ODE based
               networks. However, an important challenge of neural ODEs is
               their prohibitive memory cost during gradient backpropogation.
               Recently a method proposed in [8], claimed that this memory
               overhead can be reduced from O(LN\_t), where N\_t is the number
               of time steps, down to O(L) by solving forward ODE backwards in
               time, where L is the depth of the network. However, we will show
               that this approach may lead to several problems: (i) it may be
               numerically unstable for ReLU/non-ReLU activations and general
               convolution operators, and (ii) the proposed
               optimize-then-discretize approach may lead to divergent training
               due to inconsistent gradients for small time step sizes. We
               discuss the underlying problems, and to address them we propose
               ANODE, an Adjoint based Neural ODE framework which avoids the
               numerical instability related problems noted above, and provides
               unconditionally accurate gradients. ANODE has a memory footprint
               of O(L) + O(N\_t), with the same computational cost as reversing
               ODE solve. We furthermore, discuss a memory efficient algorithm
               which can further reduce this footprint with a trade-off of
               additional computational cost. We show results on Cifar-10/100
               datasets using ResNet and SqueezeNext neural networks.",
  month     =  feb,
  year      =  2019,
  url       = "http://arxiv.org/abs/1902.10298"
}


@INPROCEEDINGS{Arjovsky2017,
  title       = "Wasserstein generative adversarial networks",
  booktitle   = "International conference on machine learning",
  author      = "Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on",
  pages       = "214--223",
  institution = "PMLR",
  year        =  2017,
  url         = "https://proceedings.mlr.press/v70/arjovsky17a.html"
}

@INPROCEEDINGS{Rusch2021,
  title     = "{UnICORNN}: A recurrent model for learning very long time
               dependencies",
  booktitle = "Proceedings of the 38th International Conference on Machine
               Learning",
  author    = "Rusch, T Konstantin and Mishra, Siddhartha",
  editor    = "Meila, Marina and Zhang, Tong",
  abstract  = "The design of recurrent neural networks (RNNs) to accurately
               process sequential inputs with long-time dependencies is very
               challenging on account of the exploding and vanishing gradient
               problem. To overcome this, we propose a novel RNN architecture
               which is based on a structure preserving discretization of a
               Hamiltonian system of second-order ordinary differential
               equations that models networks of oscillators. The resulting RNN
               is fast, invertible (in time), memory efficient and we derive
               rigorous bounds on the hidden state gradients to prove the
               mitigation of the exploding and vanishing gradient problem. A
               suite of experiments are presented to demonstrate that the
               proposed RNN provides state of the art performance on a variety
               of learning tasks with (very) long-time dependencies.",
  publisher = "PMLR",
  volume    =  139,
  pages     = "9168--9178",
  series    = "Proceedings of Machine Learning Research",
  year      =  2021,
  url       = "https://proceedings.mlr.press/v139/rusch21a.html"
}

@INCOLLECTION{Toomarian1991,
  title     = "Adjoint-Functions and Temporal Learning Algorithms in Neural Networks",
  booktitle = "Advances in Neural Information Processing Systems 3",
  author    = "Toomarian, N and Barhen, J",
  editor    = "Lippmann, R P and Moody, J E and Touretzky, D S",
  publisher = "Morgan-Kaufmann",
  pages     = "113--120",
  year      =  1991,
  url       = "http://papers.nips.cc/paper/425-adjoint-functions-and-temporal-learning-algorithms-in-neural-networks.pdf"
}

@ARTICLE{Josic2008,
  title     = "Unstable Solutions of Nonautonomous Linear Differential
               Equations",
  author    = "Josi{\'c}, K and Rosenbaum, R",
  abstract  = "The fact that the eigenvalues of the family of matrices $A(t)$
               do not determine the stability of nonautonomous differential
               equations $x'=A(t)x$ is well known. This point is often
               illustrated using examples in which the matrices $A(t)$ have
               constant eigenvalues with negative real part, but the solutions
               of the corresponding differential equation grow in time. Here we
               provide an intuitive, geometric explanation of the idea that
               underlies these examples. The discussion is accompanied by a
               number of animations and easily modifiable Mathematica programs.
               We conclude with a discussion of possible extensions of the
               ideas that may provide suitable topics for undergraduate
               research.",
  journal   = "SIAM Review",
  publisher = "Society for Industrial and Applied Mathematics",
  volume    =  50,
  number    =  3,
  pages     = "570--584",
  month     =  jan,
  year      =  2008,
  url       = "https://doi.org/10.1137/060677057",
  annote    = "doi: 10.1137/060677057",
  issn      = "0036-1445",
  doi       = "10.1137/060677057"
}

@INPROCEEDINGS{Mikhaeil2022,
  title     = "On the difficulty of learning chaotic dynamics with {RNNs}",
  booktitle = "Advances in Neural Information Processing",
  author    = "Mikhaeil, Jonas M and Monfared, Zahra and Durstewitz, Daniel",
  abstract  = "Recurrent neural networks (RNNs) are wide-spread machine
               learning tools for modeling sequential and time series data.
               They are notoriously hard to train because their loss gradients
               backpropagated in time tend to saturate or diverge during
               training. This is known as the exploding and vanishing gradient
               problem. Previous solutions to this issue either built on rather
               complicated, purpose-engineered architectures with gated memory
               buffers, or - more recently - imposed constraints that ensure
               convergence to a fixed point or restrict (the eigenspectrum of)
               the recurrence matrix. Such constraints, however, convey severe
               limitations on the expressivity of the RNN. Essential intrinsic
               dynamics such as multistability or chaos are disabled. This is
               inherently at disaccord with the chaotic nature of many, if not
               most, time series encountered in nature and society. It is
               particularly problematic in scientific applications where one
               aims to reconstruct the underlying dynamical system. Here we
               offer a comprehensive theoretical treatment of this problem by
               relating the loss gradients during RNN training to the Lyapunov
               spectrum of RNN-generated orbits. We mathematically prove that
               RNNs producing stable equilibrium or cyclic behavior have
               bounded gradients, whereas the gradients of RNNs with chaotic
               dynamics always diverge. Based on these analyses and insights we
               suggest ways of how to optimize the training process on chaotic
               data according to the system's Lyapunov spectrum, regardless of
               the employed RNN architecture.",
  year      =  2022,
  url       = "http://arxiv.org/abs/2110.07238"
}

@ARTICLE{Haber2017,
  title     = "Stable architectures for deep neural networks",
  author    = "Haber, Eldad and Ruthotto, Lars",
  journal   = "Inverse problems",
  publisher = "IOP Publishing",
  volume    =  34,
  number    =  1,
  pages     = "014004",
  year      =  2017,
  url       = "https://iopscience.iop.org/article/10.1088/1361-6420/aa9a90/pdf",
  issn      = "0266-5611"
}

@INPROCEEDINGS{Chang2019,
  title     = "{AntisymmetricRNN}: A dynamical system view on recurrent neural
               networks",
  booktitle = "International Conference for Learning Representations",
  author    = "Chang, Bo and Chen, Minmin and Haber, Eldad and Chi, Ed H",
  abstract  = "Recurrent neural networks have gained widespread use in modeling
               sequential data. Learning long-term dependencies using these
               models remains difficult though, due to exploding or vanishing
               gradients. In this paper, we draw connections between recurrent
               networks and ordinary differential equations. A special form of
               recurrent networks called the AntisymmetricRNN is proposed under
               this theoretical framework, which is able to capture long-term
               dependencies thanks to the stability property of its underlying
               differential equation. Existing approaches to improving RNN
               trainability often incur significant computation overhead. In
               comparison, AntisymmetricRNN achieves the same goal by design.
               We showcase the advantage of this new architecture through
               extensive simulations and experiments. AntisymmetricRNN exhibits
               much more predictable dynamics. It outperforms regular LSTM
               models on tasks requiring long-term memory and matches the
               performance on tasks where short-term dependencies dominate
               despite being much simpler.",
  month     =  feb,
  year      =  2019,
  url       = "http://arxiv.org/abs/1902.09689",
  copyright = "http://creativecommons.org/licenses/by-nc-sa/4.0/"
}

@INPROCEEDINGS{Hanin2018,
  title     = "How to start training: The effect of initialization and
               architecture",
  booktitle = "Neural Information Processing Systems",
  author    = "Hanin, Boris and Rolnick, David",
  abstract  = "We identify and study two common failure modes for early
               training in deep ReLU nets. For each we give a rigorous proof of
               when it occurs and how to avoid it, for fully connected and
               residual architectures. The first failure mode,
               exploding/vanishing mean activation length, can be avoided by
               initializing weights from a symmetric distribution with variance
               2/fan-in and, for ResNets, by correctly weighting the residual
               modules. We prove that the second failure mode, exponentially
               large variance of activation length, never occurs in residual
               nets once the first failure mode is avoided. In contrast, for
               fully connected nets, we prove that this failure mode can happen
               and is avoided by keeping constant the sum of the reciprocals of
               layer widths. We demonstrate empirically the effectiveness of
               our theoretical results in predicting when networks are able to
               start training. In particular, we note that many popular
               initializations fail our criteria, whereas correct
               initialization and architecture allows much deeper networks to
               be trained.",
  publisher = "proceedings.neurips.cc",
  volume    =  31,
  month     =  mar,
  year      =  2018,
  url       = "https://proceedings.neurips.cc/paper/2018/hash/d81f9c1be2e08964bf9f24b15f0e4900-Abstract.html",
  copyright = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/"
}

@ARTICLE{Romo1999,
  title    = "Neuronal correlates of parametric working memory in the
              prefrontal cortex",
  author   = "Romo, R and Brody, C D and Hern{\'a}ndez, A and Lemus, L",
  abstract = "Humans and monkeys have similar abilities to discriminate the
              difference in frequency between two mechanical vibrations applied
              sequentially to the fingertips. A key component of this sensory
              task is that the second stimulus is compared with the trace left
              by the first (base) stimulus, which must involve working memory.
              Where and how is this trace held in the brain? This question was
              investigated by recording from single neurons in the prefrontal
              cortex of monkeys while they performed the somatosensory
              discrimination task. Here we describe neurons in the inferior
              convexity of the prefrontal cortex whose discharge rates varied,
              during the delay period between the two stimuli, as a monotonic
              function of the base stimulus frequency. We describe this as
              'monotonic stimulus encoding', and we suggest that the result may
              generalize: monotonic stimulus encoding may be the basic
              representation of one-dimensional sensory stimulus quantities in
              working memory. Thus we predict that other behavioural tasks that
              require ordinal comparisons between scalar analogue stimuli would
              give rise to monotonic responses similar to those reported here.",
  journal  = "Nature",
  volume   =  399,
  number   =  6735,
  pages    = "470--473",
  month    =  jun,
  year     =  1999,
  url      = "http://dx.doi.org/10.1038/20939",
  language = "en",
  issn     = "0028-0836",
  pmid     = "10365959",
  doi      = "10.1038/20939"
}

@ARTICLE{Averbeck2006,
  title    = "Neural correlations, population coding and computation",
  author   = "Averbeck, Bruno B and Latham, Peter E and Pouget, Alexandre",
  abstract = "How the brain encodes information in population activity, and how
              it combines and manipulates that activity as it carries out
              computations, are questions that lie at the heart of systems
              neuroscience. During the past decade, with the advent of
              multi-electrode recording and improved theoretical models, these
              questions have begun to yield answers. However, a complete
              understanding of neuronal variability, and, in particular, how it
              affects population codes, is missing. This is because variability
              in the brain is typically correlated, and although the exact
              effects of these correlations are not known, it is known that
              they can be large. Here, we review studies that address the
              interaction between neuronal noise and population codes, and
              discuss their implications for population coding in general.",
  journal  = "Nature reviews. Neuroscience",
  volume   =  7,
  number   =  5,
  pages    = "358--366",
  month    =  may,
  year     =  2006,
  url      = "http://dx.doi.org/10.1038/nrn1888",
  language = "en",
  issn     = "1471-003X",
  pmid     = "16760916",
  doi      = "10.1038/nrn1888"
}

@ARTICLE{Renart2003,
  title    = "Robust spatial working memory through homeostatic synaptic
              scaling in heterogeneous cortical networks",
  author   = "Renart, Alfonso and Song, Pengcheng and Wang, Xiao-Jing",
  abstract = "The concept of bell-shaped persistent neural activity represents
              a cornerstone of the theory for the internal representation of
              analog quantities, such as spatial location or head direction.
              Previous models, however, relied on the unrealistic assumption of
              network homogeneity. We investigate this issue in a network model
              where fine tuning of parameters is destroyed by heterogeneities
              in cellular and synaptic properties. Heterogeneities result in
              the loss of stored spatial information in a few seconds. Accurate
              encoding is recovered when a homeostatic mechanism scales the
              excitatory synapses to each cell to compensate for the
              heterogeneity in cellular excitability and synaptic inputs.
              Moreover, the more realistic model produces a wide diversity of
              tuning curves, as commonly observed in recordings from prefrontal
              neurons. We conclude that recurrent attractor networks in
              conjunction with appropriate homeostatic mechanisms provide a
              robust, biologically plausible theoretical framework for
              understanding the neural circuit basis of spatial working memory.",
  journal  = "Neuron",
  volume   =  38,
  number   =  3,
  pages    = "473--485",
  month    =  may,
  year     =  2003,
  url      = "http://dx.doi.org/10.1016/s0896-6273(03)00255-1",
  language = "en",
  issn     = "0896-6273",
  pmid     = "12741993",
  doi      = "10.1016/s0896-6273(03)00255-1"
}

@ARTICLE{Koulakov2002,
  title     = "Model for a robust neural integrator",
  author    = "Koulakov, Alexei A and Raghavachari, Sridhar and Kepecs, Adam
               and Lisman, John E",
  abstract  = "Integrator circuits in the brain show persistent firing that
               reflects the sum of previous excitatory and inhibitory inputs
               from external sources. Integrator circuits have been implicated
               in parametric working memory, decision making and motor control.
               Previous work has shown that stable integrator function can be
               achieved by an excitatory recurrent neural circuit, provided
               synaptic strengths are tuned with extreme precision (better than
               1\% accuracy). Here we show that integrator circuits can
               function without fine tuning if the neuronal units have bistable
               properties. Two specific mechanisms of bistability are analyzed,
               one based on local recurrent excitation, and the other on the
               voltage-dependence of the NMDA (N-methyl-D-aspartate) channel.
               Neither circuit requires fine tuning to perform robust
               integration, and the latter actually exploits the variability of
               neuronal conductances.",
  journal   = "Nature neuroscience",
  publisher = "Nature Publishing Group",
  volume    =  5,
  number    =  8,
  pages     = "775--782",
  month     =  aug,
  year      =  2002,
  url       = "http://dx.doi.org/10.1038/nn893",
  address   = "Salk Institute for Biological Studies, 10010 North Torrey Pines
               Road, La Jolla, California 92037, USA. akula@physics.utah.edu",
  keywords  = "fixed-point, neural-dynamics, neural-integration, nmda,
               temporal-integration, theoretical-neuroscience",
  issn      = "1097-6256",
  pmid      = "12134153",
  doi       = "10.1038/nn893"
}


@BOOK{Amit1989,
  title     = "Modeling Brain Function: The World of Attractor Neural Networks",
  author    = "Amit, Daniel J",
  abstract  = "One of the most exciting and potentially rewarding areas of
               scientific research is the study of the principles and
               mechanisms underlying brain function. It is also of great
               promise to future generations of computers. A growing group of
               researchers, adapting knowledge and techniques from a wide range
               of scientific disciplines, have made substantial progress
               understanding memory, the learning process, and self
               organization by studying the properties of models of neural
               networks - idealized systems containing very large numbers of
               connected neurons, whose interactions give rise to the special
               qualities of the brain. This book introduces and explains the
               techniques brought from physics to the study of neural networks
               and the insights they have stimulated. It is written at a level
               accessible to the wide range of researchers working on these
               problems - statistical physicists, biologists, computer
               scientists, computer technologists and cognitive psychologists.
               The author presents a coherent and clear nonmechanical
               presentation of all the basic ideas and results. More technical
               aspects are restricted, wherever possible, to special sections
               and appendices in each chapter. The book is suitable as a text
               for graduate courses in physics, electrical engineering,
               computer science and biology.",
  publisher = "Cambridge University Press",
  year      =  1989,
  language  = "en",
  isbn      = "9780521421249"
}

@INCOLLECTION{Barreira2006,
  title     = "Smooth ergodic theory and nonuniformly hyperbolic dynamics",
  booktitle = "Handbook of Dynamical Systems",
  author    = "Barreira, Luis and Pesin, Yakov",
  editor    = "Katok, B Hasselblatt And",
  publisher = "Elsevier",
  volume    = "1B",
  pages     = "57--263",
  year      =  2006,
}

@ARTICLE{Haken1983,
  title    = "At least one {L}yapunov exponent vanishes if the trajectory of an
              attractor does not contain a fixed point",
  author   = "Haken, H",
  abstract = "We treat a set of coupled ordinary nonlinear differential
              equations and show that for each trajectory which belongs to an
              attractor (or to its basin) and which does not contain a fixed
              point, at least one Lyapunov exponent vanishes.",
  journal  = "Physics letters. A",
  volume   =  94,
  number   =  2,
  pages    = "71--72",
  month    =  feb,
  year     =  1983,
  url      = "http://www.sciencedirect.com/science/article/pii/0375960183902098",
  issn     = "0375-9601",
  doi      = "10.1016/0375-9601(83)90209-8"
}

@BOOK{Nayfeh1995,
  title     = "Applied Nonlinear Dynamics: Analytical, Computational, and
               Experimental Methods",
  author    = "Nayfeh, Ali H and Balachandran, Balakumar",
  publisher = "John Wiley \& Sons",
  year      =  1995,
  language  = "en",
  isbn      = "9783527617555"
}

@INPROCEEDINGS{Doya1992,
  title     = "Bifurcations in the learning of recurrent neural networks",
  booktitle = "{IEEE} International Symposium on Circuits and Systems",
  author    = "Doya, K",
  abstract  = "Gradient descent algorithms in recurrent neural networks can
               have problems when the network dynamics experience bifurcations
               in the course of learning. The possible hazards caused by the
               bifurcations of the network dynamics and the learning equations
               are investigated. The roles of teacher forcing, preprogramming
               of network structures, and the approximate learning algorithms
               are discussed.",
  publisher = "IEEE",
  volume    =  6,
  pages     = "2777--2780",
  month     =  may,
  year      =  1992,
  url       = "http://dx.doi.org/10.1109/ISCAS.1992.230622",
  keywords  = "bifurcation;learning (artificial intelligence);recurrent neural
               nets;learning;recurrent neural networks;bifurcations;network
               dynamics;learning equations;teacher
               forcing;preprogramming;network structures;learning
               algorithms;Bifurcation;Intelligent networks;Recurrent neural
               networks;Biological system modeling;Equations;Feedforward
               systems;Biological neural networks;Hazards;Supervised
               learning;Speech recognition",
  doi       = "10.1109/ISCAS.1992.230622"
}

@INPROCEEDINGS{Skaggs1995,
  title     = "A model of the neural basis of the rat's sense of direction",
  booktitle = "Advances in neural information processing systems",
  author    = "Skaggs, W E and Knierim, J J and Kudrimoti, H S and McNaughton,
               B L",
  abstract  = "In the last decade the outlines of the neural structures
               subserving the sense of direction have begun to emerge. Several
               investigations have shed light on the effects of vestibular
               input and visual input on the head direction representation. In
               this paper, a model is formulated of the neural mechanisms
               underlying the head direction system. The model is built out of
               simple ingredients, depending on nothing more complicated than
               connectional specificity, attractor dynamics, Hebbian learning,
               and sigmoidal nonlinearities, but it behaves in a sophisticated
               way and is consistent with most of the observed properties of
               real head direction cells. In addition it makes a number of
               predictions that ought to be testable by reasonably
               straightforward experiments.",
  volume    =  7,
  pages     = "173--180",
  year      =  1995,
  url       = "https://www.ncbi.nlm.nih.gov/pubmed/11539168",
  language  = "en",
  issn      = "1049-5258",
  pmid      = "11539168"
}

@ARTICLE{Camperi1998,
  title    = "A model of visuospatial working memory in prefrontal cortex:
              {R}ecurrent network and cellular bistability",
  author   = "Camperi, M and Wang, X J",
  abstract = "We report a computer simulation of the visuospatial
              delayed-response experiments of Funahashi et al. (1989), using a
              firing-rate model that combines intrinsic cellular bistability
              with the recurrent local network architecture of the neocortex.
              In our model, the visuospatial working memory is stored in the
              form of a continuum of network activity profiles that coexist
              with a spontaneous activity state. These neuronal firing patterns
              provide a population code for the cue position in a graded
              manner. We show that neuronal persistent activity and tuning
              curves of delay-period activity (memory fields) can be generated
              by an excitatory feedback circuit and recurrent synaptic
              inhibition. However, if the memory fields are constructed solely
              by network mechanisms, noise may induce a random drift over time
              in the encoded cue position, so that the working memory storage
              becomes unreliable. Furthermore, a ``distraction'' stimulus
              presented during the delay period produces a systematic shift in
              the encoded cue position. We found that the working memory
              performance can be rendered robust against noise and distraction
              stimuli if single neurons are endowed with cellular bistability
              (presumably due to intrinsic ion channel mechanisms) that is
              conditional and realized only with sustained synaptic inputs from
              the recurrent network. We discuss how cellular bistability at the
              single cell level may be detected by analysis of spike trains
              recorded during delay-period activity and how local modulation of
              intrinsic cell properties and/or synaptic transmission can alter
              the memory fields of individual neurons in the prefrontal cortex.",
  journal  = "Journal of computational neuroscience",
  volume   =  5,
  number   =  4,
  pages    = "383--405",
  month    =  dec,
  year     =  1998,
  url      = "http://dx.doi.org/10.1023/a:1008837311948",
  language = "en",
  issn     = "0929-5313",
  pmid     = "9877021",
  doi      = "10.1023/a:1008837311948"
}

@INPROCEEDINGS{Smith2021,
  title     = "On the Origin of Implicit Regularization in Stochastic Gradient
               Descent",
  booktitle = "International Conference on Learning Representations",
  author    = "Smith, Samuel L and Dherin, Benoit and Barrett, David G T and
               De, Soham",
  abstract  = "For infinitesimal learning rates, stochastic gradient descent
               (SGD) follows the path of gradient flow on the full batch loss
               function. However moderately large learning rates can achieve
               higher test accuracies, and this generalization benefit is not
               explained by convergence bounds, since the learning rate which
               maximizes test accuracy is often larger than the learning rate
               which minimizes training loss. To interpret this phenomenon we
               prove that for SGD with random shuffling, the mean SGD iterate
               also stays close to the path of gradient flow if the learning
               rate is small and finite, but on a modified loss. This modified
               loss is composed of the original loss function and an implicit
               regularizer, which penalizes the norms of the minibatch
               gradients. Under mild assumptions, when the batch size is small
               the scale of the implicit regularization term is proportional to
               the ratio of the learning rate to the batch size. We verify
               empirically that explicitly including the implicit regularizer
               in the loss can enhance the test accuracy when the learning rate
               is small.",
  month     =  jan,
  year      =  2021,
  url       = "https://openreview.net/forum?id=rq_Qr0c1Hyo"
}

@BOOK{Kuznetsov1995,
  title     = "Elements of Applied Bifurcation Theory",
  author    = "Kuznetsov, Yuri A",
  publisher = "Springer",
  year      =  1995,
  url       = "https://link.springer.com/book/10.1007/978-1-4757-3978-7",
  doi       = "10.1007/978-1-4757-3978-7"
}

@ARTICLE{Boerlin2013,
  title    = "Predictive coding of dynamical variables in balanced spiking
              networks",
  author   = "Boerlin, Martin and Machens, Christian K and Den{\`e}ve, Sophie",
  abstract = "Two observations about the cortex have puzzled neuroscientists
              for a long time. First, neural responses are highly variable.
              Second, the level of excitation and inhibition received by each
              neuron is tightly balanced at all times. Here, we demonstrate
              that both properties are necessary consequences of neural
              networks that represent information efficiently in their spikes.
              We illustrate this insight with spiking networks that represent
              dynamical variables. Our approach is based on two assumptions: We
              assume that information about dynamical variables can be read out
              linearly from neural spike trains, and we assume that neurons
              only fire a spike if that improves the representation of the
              dynamical variables. Based on these assumptions, we derive a
              network of leaky integrate-and-fire neurons that is able to
              implement arbitrary linear dynamical systems. We show that the
              membrane voltage of the neurons is equivalent to a prediction
              error about a common population-level signal. Among other things,
              our approach allows us to construct an integrator network of
              spiking neurons that is robust against many perturbations. Most
              importantly, neural variability in our networks cannot be equated
              to noise. Despite exhibiting the same single unit properties as
              widely used population code models (e.g. tuning curves, Poisson
              distributed spike trains), balanced networks are orders of
              magnitudes more reliable. Our approach suggests that spikes do
              matter when considering how the brain computes, and that the
              reliability of cortical representations could have been strongly
              underestimated.",
  journal  = "PLoS computational biology",
  volume   =  9,
  number   =  11,
  pages    = "e1003258",
  month    =  nov,
  year     =  2013,
  url      = "http://dx.doi.org/10.1371/journal.pcbi.1003258",
  language = "en",
  issn     = "1553-734X, 1553-7358",
  pmid     = "24244113",
  doi      = "10.1371/journal.pcbi.1003258",
  pmc      = "PMC3828152"
}

@BOOK{Strogatz2000,
  title     = "Nonlinear dynamics and chaos : with applications to physics,
               biology, chemistry, and engineering",
  author    = "Strogatz, Steven H",
  abstract  = "An introductory text in nonlinear dynamics and chaos,
               emphasizing applications in several areas of science, which
               include vibrations, biological rhythms, insect outbreaks, and
               genetic control systems. Contains a rich selection of
               illustrations, with many exercises and examples. Softcover.",
  publisher = "Westview Press",
  series    = "Studies in nonlinearity",
  edition   =  1,
  month     =  jan,
  year      =  2000,
  keywords  = "dynamical-system, mathematics",
  isbn      = "9780738204536"
}

@BOOK{Guckenheimer1983,
  title     = "Nonlinear Oscillations, Dynamical Systems, and Bifurcations of
               Vector Fields",
  author    = "Guckenheimer, John and Holmes, Philip",
  publisher = "Springer, New York, NY",
  year      =  1983,
  url       = "https://link.springer.com/book/10.1007/978-1-4612-1140-2",
  isbn      = "9780387908199, 9781461211402",
  doi       = "10.1007/978-1-4612-1140-2"
}

@ARTICLE{Vogt2022,
  title    = "On {L}yapunov Exponents for {RNNs}: Understanding Information
              Propagation Using Dynamical Systems Tools",
  author   = "Vogt, Ryan and Puelma Touzel, Maximilian and Shlizerman, Eli and
              Lajoie, Guillaume",
  abstract = "Recurrent neural networks (RNNs) have been successfully applied
              to a variety of problems involving sequential data, but their
              optimization is sensitive to parameter initialization,
              architecture, and optimizer hyperparameters. Considering RNNs as
              dynamical systems, a natural way to capture stability, i.e., the
              growth and decay over long iterates, are the Lyapunov Exponents
              (LEs), which form the Lyapunov spectrum. The LEs have a bearing
              on stability of RNN training dynamics since forward propagation
              of information is related to the backward propagation of error
              gradients. LEs measure the asymptotic rates of expansion and
              contraction of non-linear system trajectories, and generalize
              stability analysis to the time-varying attractors structuring the
              non-autonomous dynamics of data-driven RNNs. As a tool to
              understand and exploit stability of training dynamics, the
              Lyapunov spectrum fills an existing gap between prescriptive
              mathematical approaches of limited scope and
              computationally-expensive empirical approaches. To leverage this
              tool, we implement an efficient way to compute LEs for RNNs
              during training, discuss the aspects specific to standard RNN
              architectures driven by typical sequential datasets, and show
              that the Lyapunov spectrum can serve as a robust readout of
              training stability across hyperparameters. With this
              exposition-oriented contribution, we hope to draw attention to
              this under-studied, but theoretically grounded tool for
              understanding training stability in RNNs.",
  journal  = "Frontiers in Applied Mathematics and Statistics",
  volume   =  8,
  year     =  2022,
  url      = "https://www.frontiersin.org/articles/10.3389/fams.2022.818799",
  issn     = "2297-4687",
  doi      = "10.3389/fams.2022.818799"
}

@ARTICLE{Buzsaki2023,
  title    = "Brain rhythms have come of age",
  author   = "Buzs{\'a}ki, Gy{\"o}rgy and V{\"o}r{\"o}slakos, Mih{\'a}ly",
  abstract = "Neuronal oscillations offer access to neuronal operations,
              bringing microscopic and macroscopic mechanisms, experimental
              methods, and explanations to a common platform. The field of
              brain rhythms has become the agora of discussions from temporal
              coordination of neuronal populations within and across brain
              regions to cognitive phenomena, including language and brain
              diseases.",
  journal  = "Neuron",
  volume   =  111,
  number   =  7,
  pages    = "922--926",
  month    =  apr,
  year     =  2023,
  url      = "http://dx.doi.org/10.1016/j.neuron.2023.03.018",
  language = "en",
  issn     = "0896-6273, 1097-4199",
  pmid     = "37023714",
  doi      = "10.1016/j.neuron.2023.03.018"
}

@ARTICLE{Marschall2019,
  title         = "A unified framework of online learning algorithms for
                   training recurrent neural networks",
  author        = "Marschall, Owen and Cho, Kyunghyun and Savin, Cristina",
  abstract      = "We present a framework for compactly summarizing many recent
                   results in efficient and/or biologically plausible online
                   training of recurrent neural networks (RNN). The framework
                   organizes algorithms according to several criteria: (a) past
                   vs. future facing, (b) tensor structure, (c) stochastic vs.
                   deterministic, and (d) closed form vs. numerical. These axes
                   reveal latent conceptual connections among several recent
                   advances in online learning. Furthermore, we provide novel
                   mathematical intuitions for their degree of success. Testing
                   various algorithms on two synthetic tasks shows that
                   performances cluster according to our criteria. Although a
                   similar clustering is also observed for gradient alignment,
                   alignment with exact methods does not alone explain ultimate
                   performance, especially for stochastic algorithms. This
                   suggests the need for better comparison metrics.",
  journal       = "Journal of Machine Learning Research",
  month         =  jul,
  year          =  2019,
  url           = "https://www.jmlr.org/papers/volume21/19-562/19-562.pdf",
  archivePrefix = "arXiv",
  eprint        = "1907.02649",
  primaryClass  = "cs.LG",
  arxivid       = "1907.02649"
}

@ARTICLE{Williams1989,
  title     = "A Learning Algorithm for Continually Running Fully Recurrent
               Neural Networks",
  author    = "Williams, Ronald J and Zipser, David",
  abstract  = "The exact form of a gradient-following learning algorithm for
               completely recurrent networks running in continually sampled
               time is derived and used as the basis for practical algorithms
               for temporal supervised learning tasks. These algorithms have
               (1) the advantage that they do not require a precisely defined
               training interval, operating while the network runs; and (2) the
               disadvantage that they require nonlocal communication in the
               network being trained and are computationally expensive. These
               algorithms allow networks having recurrent connections to learn
               complex tasks that require the retention of information over
               time periods having either fixed or indefinite length.",
  journal   = "Neural computation",
  publisher = "MIT Press",
  volume    =  1,
  number    =  2,
  pages     = "270--280",
  month     =  jun,
  year      =  1989,
  url       = "https://doi.org/10.1162/neco.1989.1.2.270",
  annote    = "doi: 10.1162/neco.1989.1.2.270",
  issn      = "0899-7667",
  doi       = "10.1162/neco.1989.1.2.270"
}

@ARTICLE{Engelken2020,
  title         = "{L}yapunov spectra of chaotic recurrent neural networks",
  author        = "Engelken, Rainer and Wolf, Fred and Abbott, L F",
  abstract      = "Brains process information through the collective dynamics
                   of large neural networks. Collective chaos was suggested to
                   underlie the complex ongoing dynamics observed in cerebral
                   cortical circuits and determine the impact and processing of
                   incoming information streams. In dissipative systems,
                   chaotic dynamics takes place on a subset of phase space of
                   reduced dimensionality and is organized by a complex tangle
                   of stable, neutral and unstable manifolds. Key topological
                   invariants of this phase space structure such as attractor
                   dimension, and Kolmogorov-Sinai entropy so far remained
                   elusive. Here we calculate the complete Lyapunov spectrum of
                   recurrent neural networks. We show that chaos in these
                   networks is extensive with a size-invariant Lyapunov
                   spectrum and characterized by attractor dimensions much
                   smaller than the number of phase space dimensions. We find
                   that near the onset of chaos, for very intense chaos, and
                   discrete-time dynamics, random matrix theory provides
                   analytical approximations to the full Lyapunov spectrum. We
                   show that a generalized time-reversal symmetry of the
                   network dynamics induces a point-symmetry of the Lyapunov
                   spectrum reminiscent of the symplectic structure of chaotic
                   Hamiltonian systems. Fluctuating input reduces both the
                   entropy rate and the attractor dimension. For trained
                   recurrent networks, we find that Lyapunov spectrum analysis
                   provides a quantification of error propagation and stability
                   achieved. Our methods apply to systems of arbitrary
                   connectivity, and we describe a comprehensive set of
                   controls for the accuracy and convergence of Lyapunov
                   exponents. Our results open a novel avenue for
                   characterizing the complex dynamics of recurrent neural
                   networks and the geometry of the corresponding chaotic
                   attractors. They also highlight the potential of Lyapunov
                   spectrum analysis as a diagnostic for machine learning
                   applications of recurrent networks.",
  month         =  jan,
  year          =  2023,
  journal	= "Physical Review Research",
  url           = "http://arxiv.org/abs/2006.02427",
  archivePrefix = "arXiv",
  eprint        = "2006.02427",
  primaryClass  = "nlin.CD",
  arxivid       = "2006.02427"
}

@INPROCEEDINGS{Chen2018,
  title     = "Dynamical isometry and a mean field theory of {RNNs}: Gating
               enables signal propagation in recurrent neural networks",
  booktitle = "Proceedings of the 35th International Conference on Machine
               Learning",
  author    = "Chen, Minmin and Pennington, Jeffrey and Schoenholz, Samuel S",
  editor    = "Dy, Jennifer and Krause, Andreas",
  abstract  = "Recurrent neural networks have gained widespread use in modeling
               sequence data across various domains. While many successful
               recurrent architectures employ a notion of gating, the exact
               mechanism that enables such remarkable performance is not well
               understood. We develop a theory for signal propagation in
               recurrent networks after random initialization using a
               combination of mean field theory and random matrix theory. To
               simplify our discussion, we introduce a new RNN cell with a
               simple gating mechanism that we call the minimalRNN and compare
               it with vanilla RNNs. Our theory allows us to define a maximum
               timescale over which RNNs can remember an input. We show that
               this theory predicts trainability for both recurrent
               architectures. We show that gated recurrent networks feature a
               much broader, more robust, trainable region than vanilla RNNs,
               which corroborates recent experimental findings. Finally, we
               develop a closed-form critical initialization scheme that
               achieves dynamical isometry in both vanilla RNNs and
               minimalRNNs. We show that this results in significantly
               improvement in training dynamics. Finally, we demonstrate that
               the minimalRNN achieves comparable performance to its more
               complex counterparts, such as LSTMs or GRUs, on a language
               modeling task.",
  publisher = "PMLR",
  volume    =  80,
  pages     = "873--882",
  series    = "Proceedings of Machine Learning Research",
  month     =  jun,
  year      =  2018,
  url       = "https://proceedings.mlr.press/v80/chen18i.html",
  copyright = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/"
}

@ARTICLE{Murray2019,
  title     = "Local online learning in recurrent networks with random feedback",
  author    = "Murray, James M",
  abstract  = "Recurrent neural networks (RNNs) enable the production and
               processing of time-dependent signals such as those involved in
               movement or working memory. Classic gradient-based algorithms
               for training RNNs have been available for decades, but are
               inconsistent with biological features of the brain, such as
               causality and locality. We derive an approximation to
               gradient-based learning that comports with these constraints by
               requiring synaptic weight updates to depend only on local
               information about pre- and postsynaptic activities, in addition
               to a random feedback projection of the RNN output error. In
               addition to providing mathematical arguments for the
               effectiveness of the new learning rule, we show through
               simulations that it can be used to train an RNN to perform a
               variety of tasks. Finally, to overcome the difficulty of
               training over very large numbers of timesteps, we propose an
               augmented circuit architecture that allows the RNN to
               concatenate short-duration patterns into longer sequences.",
  journal   = "eLife",
  publisher = "elifesciences.org",
  volume    =  8,
  month     =  may,
  year      =  2019,
  url       = "http://dx.doi.org/10.7554/eLife.43299",
  keywords  = "machine learning; motor control; neuroscience; none; recurrent
               neural networks; supervised learning; working memory",
  language  = "en",
  issn      = "2050-084X",
  pmid      = "31124785",
  doi       = "10.7554/eLife.43299",
  pmc       = "PMC6561704"
}

@ARTICLE{Deutsch2020,
  title    = "The neural basis for a persistent internal state in Drosophila
              females",
  author   = "Deutsch, David and Pacheco, Diego and Encarnacion-Rivera, Lucas
              and Pereira, Talmo and Fathy, Ramie and Clemens, Jan and
              Girardin, Cyrille and Calhoun, Adam and Ireland, Elise and Burke,
              Austin and Dorkenwald, Sven and McKellar, Claire and Macrina,
              Thomas and Lu, Ran and Lee, Kisuk and Kemnitz, Nico and Ih, Dodam
              and Castro, Manuel and Halageri, Akhilesh and Jordan, Chris and
              Silversmith, William and Wu, Jingpeng and Seung, H Sebastian and
              Murthy, Mala",
  abstract = "Sustained changes in mood or action require persistent changes in
              neural activity, but it has been difficult to identify the neural
              circuit mechanisms that underlie persistent activity and
              contribute to long-lasting changes in behavior. Here, we show
              that a subset of Doublesex+ pC1 neurons in the Drosophila female
              brain, called pC1d/e, can drive minutes-long changes in female
              behavior in the presence of males. Using automated reconstruction
              of a volume electron microscopic (EM) image of the female brain,
              we map all inputs and outputs to both pC1d and pC1e. This reveals
              strong recurrent connectivity between, in particular, pC1d/e
              neurons and a specific subset of Fruitless+ neurons called aIPg.
              We additionally find that pC1d/e activation drives long-lasting
              persistent neural activity in brain areas and cells overlapping
              with the pC1d/e neural network, including both Doublesex+ and
              Fruitless+ neurons. Our work thus links minutes-long persistent
              changes in behavior with persistent neural activity and recurrent
              circuit architecture in the female brain.",
  journal  = "eLife",
  volume   =  9,
  month    =  nov,
  year     =  2020,
  url      = "http://dx.doi.org/10.7554/eLife.59502",
  keywords = "D. melanogaster; connectomics; courtship; internal state; neural
              circuits; neural imaging; neuroscience; social interaction",
  language = "en",
  issn     = "2050-084X",
  pmid     = "33225998",
  doi      = "10.7554/eLife.59502",
  pmc      = "PMC7787663"
}

@INCOLLECTION{Legenstein2007,
  title     = "What makes a dynamical system computationally powerful?",
  booktitle = "New Directions in Statistical Signal Processing: From Systems to
               Brain",
  author    = "Legenstein, Robert and Maass, Wolfgang",
  editor    = "Haykin, Simon S and Pr\'incipe, Jos\'e
               C and Sejnowski, Terrence J and McWhirter, John",
  abstract  = "We review methods for estimating the computational capability of
               a complex dynamical system. The main examples that we discuss
               are models for cortical neural microcircuits with varying
               degrees of biological accuracy, in the context of online
               computations on complex input streams. We address in particular
               the question to what extent earlier results about the
               relationship between the edge of chaos and the computational
               power of dynamical systems in discrete time for off-line
               computing also apply to this case.",
  publisher = "MIT Press",
  pages     = "127--154",
  year      =  2007,
  url       = "http://www.igi.tu-graz.ac.at/maass/psfiles/165.pdf",
  isbn      = "9780262083485"
}

@ARTICLE{Engelken2023,
  title         = "{L}yapunov spectra of chaotic recurrent neural networks",
  author        = "Engelken, Rainer and Wolf, Fred and Abbott, L F",
  abstract      = "Brains process information through the collective dynamics
                   of large neural networks. Collective chaos was suggested to
                   underlie the complex ongoing dynamics observed in cerebral
                   cortical circuits and determine the impact and processing of
                   incoming information streams. In dissipative systems,
                   chaotic dynamics takes place on a subset of phase space of
                   reduced dimensionality and is organized by a complex tangle
                   of stable, neutral and unstable manifolds. Key topological
                   invariants of this phase space structure such as attractor
                   dimension, and Kolmogorov-Sinai entropy so far remained
                   elusive. Here we calculate the complete Lyapunov spectrum of
                   recurrent neural networks. We show that chaos in these
                   networks is extensive with a size-invariant Lyapunov
                   spectrum and characterized by attractor dimensions much
                   smaller than the number of phase space dimensions. We find
                   that near the onset of chaos, for very intense chaos, and
                   discrete-time dynamics, random matrix theory provides
                   analytical approximations to the full Lyapunov spectrum. We
                   show that a generalized time-reversal symmetry of the
                   network dynamics induces a point-symmetry of the Lyapunov
                   spectrum reminiscent of the symplectic structure of chaotic
                   Hamiltonian systems. Fluctuating input reduces both the
                   entropy rate and the attractor dimension. For trained
                   recurrent networks, we find that Lyapunov spectrum analysis
                   provides a quantification of error propagation and stability
                   achieved. Our methods apply to systems of arbitrary
                   connectivity, and we describe a comprehensive set of
                   controls for the accuracy and convergence of Lyapunov
                   exponents. Our results open a novel avenue for
                   characterizing the complex dynamics of recurrent neural
                   networks and the geometry of the corresponding chaotic
                   attractors. They also highlight the potential of Lyapunov
                   spectrum analysis as a diagnostic for machine learning
                   applications of recurrent networks.",
  journal       = "Physical Review Research",
  year          =  2023,
  url           = "http://arxiv.org/abs/2006.02427",
  archivePrefix = "arXiv",
  eprint        = "2006.02427",
  primaryClass  = "nlin.CD",
  arxivid       = "2006.02427"
}

@INCOLLECTION{Kerg2019,
  title     = "Non-normal Recurrent Neural Network ({nnRNN)}: learning long
               time dependencies while improving expressivity with transient
               dynamics",
  booktitle = "Advances in Neural Information Processing Systems 32",
  author    = "Kerg, Giancarlo and Goyette, Kyle and Puelma Touzel, Maximilian
               and Gidel, Gauthier and Vorontsov, Eugene and Bengio, Yoshua and
               Lajoie, Guillaume",
  pages     = "13613--13623",
  year      =  2019,
  url       = "http://papers.nips.cc/paper/9513-non-normal-recurrent-neural-network-nnrnn-learning-long-time-dependencies-while-improving-expressivity-with-transient-dynamics.pdf"
}

@ARTICLE{Toyoizumi2011,
  title    = "Beyond the edge of chaos: amplification and temporal integration
              by recurrent networks in the chaotic regime",
  author   = "Toyoizumi, T and Abbott, L F",
  abstract = "Randomly connected networks of neurons exhibit a transition from
              fixed-point to chaotic activity as the variance of their synaptic
              connection strengths is increased. In this study, we analytically
              evaluate how well a small external input can be reconstructed
              from a sparse linear readout of network activity. At the
              transition point, known as the edge of chaos, networks display a
              number of desirable features, including large gains and
              integration times. Away from this edge, in the nonchaotic regime
              that has been the focus of most models and studies, gains and
              integration times fall off dramatically, which implies that
              parameters must be fine tuned with considerable precision if high
              performance is required. Here we show that, near the edge,
              decoding performance is characterized by a critical exponent that
              takes a different value on the two sides. As a result, when the
              network units have an odd saturating nonlinear response function,
              the falloff in gains and integration times is much slower on the
              chaotic side of the transition. This means that, under
              appropriate conditions, good performance can be achieved with
              less fine tuning beyond the edge, within the chaotic regime.",
  journal  = "Physical review. E, Statistical, nonlinear, and soft matter
              physics",
  volume   =  84,
  number   = "5 Pt 1",
  pages    = "051908",
  month    =  nov,
  year     =  2011,
  url      = "http://dx.doi.org/10.1103/PhysRevE.84.051908",
  language = "en",
  issn     = "1539-3755, 1550-2376",
  pmid     = "22181445",
  doi      = "10.1103/PhysRevE.84.051908",
  pmc      = "PMC5558624"
}

@ARTICLE{Spalla2021,
  title    = "Continuous attractors for dynamic memories",
  author   = "Spalla, Davide and Cornacchia, Isabel Maria and Treves,
              Alessandro",
  abstract = "Episodic memory has a dynamic nature: when we recall past
              episodes, we retrieve not only their content, but also their
              temporal structure. The phenomenon of replay, in the hippocampus
              of mammals, offers a remarkable example of this temporal
              dynamics. However, most quantitative models of memory treat
              memories as static configurations, neglecting the temporal
              unfolding of the retrieval process. Here, we introduce a
              continuous attractor network model with a memory-dependent
              asymmetric component in the synaptic connectivity, which
              spontaneously breaks the equilibrium of the memory configurations
              and produces dynamic retrieval. The detailed analysis of the
              model with analytical calculations and numerical simulations
              shows that it can robustly retrieve multiple dynamical memories,
              and that this feature is largely independent of the details of
              its implementation. By calculating the storage capacity, we show
              that the dynamic component does not impair memory capacity, and
              can even enhance it in certain regimes.",
  journal  = "eLife",
  volume   =  10,
  month    =  sep,
  year     =  2021,
  url      = "http://dx.doi.org/10.7554/eLife.69499",
  keywords = "computational biology; continuous attractors; memory; neural
              dynamics; neuroscience; none; systems biology",
  language = "en",
  issn     = "2050-084X",
  pmid     = "34520345",
  doi      = "10.7554/eLife.69499",
  pmc      = "PMC8439658"
}

@ARTICLE{Nair2023,
  title    = "An approximate line attractor in the hypothalamus encodes an
              aggressive state",
  author   = "Nair, Aditya and Karigo, Tomomi and Yang, Bin and Ganguli, Surya
              and Schnitzer, Mark J and Linderman, Scott W and Anderson, David
              J and Kennedy, Ann",
  abstract = "Summary The hypothalamus regulates innate social behaviors,
              including mating and aggression. These behaviors can be evoked by
              optogenetic stimulation of specific neuronal subpopulations
              within MPOA and VMHvl, respectively. Here, we perform dynamical
              systems modeling of population neuronal activity in these nuclei
              during social behaviors. In VMHvl, unsupervised analysis
              identified a dominant dimension of neural activity with a large
              time constant (>50 s), generating an approximate line attractor
              in neural state space. Progression of the neural trajectory along
              this attractor was correlated with an escalation of agonistic
              behavior, suggesting that it may encode a scalable state of
              aggressiveness. Consistent with this, individual differences in
              the magnitude of the integration dimension time constant were
              strongly correlated with differences in aggressiveness. In
              contrast, approximate line attractors were not observed in MPOA
              during mating; instead, neurons with fast dynamics were tuned to
              specific actions. Thus, different hypothalamic nuclei employ
              distinct neural population codes to represent similar social
              behaviors.",
  journal  = "Cell",
  volume   =  186,
  number   =  1,
  pages    = "178--193.e15",
  month    =  jan,
  year     =  2023,
  url      = "https://www.sciencedirect.com/science/article/pii/S0092867422014714",
  keywords = "aggression; courtship; hypothalamus; VMH; MPOA; line attractor;
              dynamical systems; innate behavior; calcium imaging; rSLDS",
  issn     = "0092-8674",
  doi      = "10.1016/j.cell.2022.11.027"
}

@ARTICLE{Seung2000,
  title    = "Stability of the memory of eye position in a recurrent network of
              conductance-based model neurons",
  author   = "Seung, H S and Lee, D D and Reis, B Y and Tank, D W",
  abstract = "Studies of the neural correlates of short-term memory in a wide
              variety of brain areas have found that transient inputs can cause
              persistent changes in rates of action potential firing, through a
              mechanism that remains unknown. In a premotor area that is
              responsible for holding the eyes still during fixation,
              persistent neural firing encodes the angular position of the eyes
              in a characteristic manner: below a threshold position the neuron
              is silent, and above it the firing rate is linearly related to
              position. Both the threshold and linear slope vary from neuron to
              neuron. We have reproduced this behavior in a biophysically
              plausible network model. Persistence depends on precise tuning of
              the strength of synaptic feedback, and a relatively long synaptic
              time constant improves the robustness to mistuning.",
  journal  = "Neuron",
  volume   =  26,
  number   =  1,
  pages    = "259--271",
  month    =  apr,
  year     =  2000,
  url      = "http://dx.doi.org/10.1016/s0896-6273(00)81155-1",
  language = "en",
  issn     = "0896-6273",
  pmid     = "10798409",
  doi      = "10.1016/s0896-6273(00)81155-1"
}

@INBOOK{kolen2001,
  author={Kolen, John F. and Kremer, Stefan C.},
  booktitle={A Field Guide to Dynamical Recurrent Networks}, 
  title={Gradient Flow in Recurrent Nets: The Difficulty of Learning LongTerm Dependencies}, 
  year={2001},
  volume={},
  number={},
  pages={237-243},
  doi={10.1109/9780470544037.ch14}}

@article{seung1996,
  title = {How the Brain Keeps the Eyes Still},
  author = {Seung, H. S.},
  date = {1996-11-12},
  year={1996},
  journal = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {93},
  number = {23},
  pages = {13339--13344},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.93.23.13339},
  url = {https://pnas.org/doi/full/10.1073/pnas.93.23.13339},
  urldate = {2023-03-06},
  abstract = {The brain can hold the eyes still because it stores a memory of eye position. The brain’s memory of horizontal eye position appears to be represented by persistent neural activity in a network known as the neural integrator, which is localized in the brainstem and cerebellum. Existing experimental data are reinterpreted as evidence for an ‘‘attractor hypothesis’’ that the persistent patterns of activity observed in this network form an attractive line of fixed points in its state space. Line attractor dynamics can be produced in linear or nonlinear neural networks by learning mechanisms that precisely tune positive feedback.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\YAQ8MLQ7\\Seung - 1996 - How the brain keeps the eyes still.pdf}
}

@article{seung1997learning,
  title={Learning continuous attractors in recurrent networks},
  author={Seung, H Sebastian},
  journal={Advances in neural information processing systems},
  volume={10},
  year={1997}
}

@article{seung1998,
  title = {Continuous Attractors and Oculomotor Control},
  author = {Seung, S. H.},
  date = {1998-10},
  year={1998},
  journal = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {11},
  number = {7-8},
  pages = {1253--1258},
  issn = {08936080},
  doi = {10.1016/S0893-6080(98)00064-1},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608098000641},
  urldate = {2023-03-06},
  abstract = {A recurrent neural network can possess multiple stable states, a property that many brain theories have implicated in learning and memory. There is good evidence for such multistability in the brainstem neural network that controls eye position. Because the stable states are arranged in a continuous dynamical attractor, the network can store a memory of eye position with analog neural encoding. Continuous attractors in model networks depend on precisely tuned positive feedback, and their robust maintenance requires mechanisms of synaptic plasticity. These ideas may have wider scope than just the oculomotor system. More generally, the internal models postulated by theories of biological motor control may be recurrent networks with continuous attractors. ᭧ 1998 Published by Elsevier Science Ltd. All rights reserved.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\JBLU5X5N\\Sebastian Seung - 1998 - Continuous attractors and oculomotor control.pdf}
}

@inproceedings{arjovsky2016,
  title={Unitary evolution recurrent neural networks},
  author={Arjovsky, Martin and Shah, Amar and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={1120--1128},
  year={2016},
  organization={PMLR}
}

@InProceedings{Henaff2016,
  title = 	 {Recurrent Orthogonal Networks and Long-Memory Tasks},
  author = 	 {Henaff, Mikael and Szlam, Arthur and LeCun, Yann},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2034--2042},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/henaff16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/henaff16.html},
  abstract = 	 {Although RNNs have been shown to be power- ful tools for processing sequential data, finding architectures or optimization strategies that al- low them to model very long term dependencies is still an active area of research. In this work, we carefully analyze two synthetic datasets orig- inally outlined in (Hochreiter &amp; Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store information over many time steps. We explicitly construct RNN solutions to these problems, and using these constructions, illumi- nate both the problems themselves and the way in which RNNs store different types of information in their hidden states. These constructions fur- thermore explain the success of recent methods that specify unitary initializations or constraints on the transition matrices.}
}

@article{doya1993,
  title={Bifurcations of recurrent neural networks in gradient descent learning},
  author={Doya, Kenji},
  journal={IEEE Transactions on neural networks},
  volume={1},
  number={75},
  pages={218},
  year={1993},
  publisher={Citeseer}
}

@article {Noorman2022,
	author = {Marcella Noorman and Brad K Hulse and Vivek Jayaraman and Sandro Romani and Ann M Hermundstad},
	title = {Accurate angular integration with only a handful of neurons},
	elocation-id = {2022.05.23.493052},
	year = {2022},
	doi = {10.1101/2022.05.23.493052},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {To flexibly navigate, many animals rely on internal spatial representations that persist when the animal is standing still in darkness, and update accurately by integrating the animal{\textquoteright}s movements in the absence of localizing sensory cues. Theories of mammalian head direction cells have proposed that these dynamics can be realized in a special class of networks that maintain a localized bump of activity via structured recurrent connectivity, and that shift this bump of activity via angular velocity input. Although there are many different variants of these so-called ring attractor networks, they all rely on large numbers of neurons to generate representations that persist in the absence of input and accurately integrate angular velocity input. Surprisingly, in the fly, Drosophila melanogaster, a head direction representation is maintained by a much smaller number of neurons whose dynamics and connectivity resemble those of a ring attractor network. These findings challenge our understanding of ring attractors and their putative implementation in neural circuits. Here, we analyzed failures of angular velocity integration that emerge in small attractor networks with only a few computational units. Motivated by the peak performance of the fly head direction system in darkness, we mathematically derived conditions under which small networks, even with as few as 4 neurons, achieve the performance of much larger networks. The resulting description reveals that by appropriately tuning the network connectivity, the network can maintain persistent representations over the continuum of head directions, and it can accurately integrate angular velocity inputs. We then analytically determined how performance degrades as the connectivity deviates from this optimally-tuned setting, and we find a trade-off between network size and the tuning precision needed to achieve persistence and accurate integration. This work shows how even small networks can accurately track an animal{\textquoteright}s movements to guide navigation, and it informs our understanding of the functional capabilities of discrete systems more broadly.Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2022/05/25/2022.05.23.493052},
	eprint = {https://www.biorxiv.org/content/early/2022/05/25/2022.05.23.493052.full.pdf},
	journal = {bioRxiv}
}


@article{shimizu2021,
  title = {Computational Roles of Intrinsic Synaptic Dynamics},
  author = {Shimizu, Genki and Yoshida, Kensuke and Kasai, Haruo and Toyoizumi, Taro},
  year={2021},
  date = {2021-10},
  journaltitle = {Current Opinion in Neurobiology},
  shortjournal = {Current Opinion in Neurobiology},
  volume = {70},
  pages = {34--42},
  issn = {09594388},
  doi = {10.1016/j.conb.2021.06.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0959438821000672},
  urldate = {2023-05-08},
  abstract = {Conventional theories assume that long-term information storage in the brain is implemented by modifying synaptic efficacy. Recent experimental findings challenge this view by demonstrating that dendritic spine sizes, or their corresponding synaptic weights, are highly volatile even in the absence of neural activity. Here, we review previous computational works on the roles of these intrinsic synaptic dynamics. We first present the possibility for neuronal networks to sustain stable performance in their presence, and we then hypothesize that intrinsic dynamics could be more than mere noise to withstand, but they may improve information processing in the brain.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Shimizu-2021-Computational roles of intrinsic synaptic dynamics.pdf}
}

@report{terada2023,
  type = {preprint},
  title = {Chaotic Neural Dynamics Facilitate Probabilistic Computations through Sampling},
  author = {Terada, Yu and Toyoizumi, Taro},
  date = {2023-05-05},
  institution = {{Neuroscience}},
  doi = {10.1101/2023.05.04.539470},
  url = {http://biorxiv.org/lookup/doi/10.1101/2023.05.04.539470},
  urldate = {2023-05-08},
  abstract = {Cortical neurons exhibit highly variable responses over trials and time. Theoretical works posit that this variability arises potentially from chaotic network dynamics of recurrently connected neurons. Here we demonstrate that chaotic neural dynamics, formed through synaptic learning, allow networks to perform sensory cue integration in a sampling-based implementation. We show that the emergent chaotic dynamics provide neural substrates for generating samples not only of a static variable but also of a dynamical trajectory, where generic recurrent networks acquire these abilities with a biologically-plausible learning rule through trial and error. Furthermore, the networks generalize their experience in the stimulus-evoked samples to the inference without partial or all sensory information, which suggests a computational role of spontaneous activity as a representation of the priors as well as a tractable biological computation for marginal distributions. These \hspace{0.6em}ndings suggest that chaotic neural dynamics may serve for the brain function as a Bayesian generative model.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\YuTerada-2023-Chaotic neural dynamics facilitate probabilistic computations through sampling.pdf}
}

@article{ajabi2023,
  title={Population dynamics of head-direction neurons during drift and reorientation},
  author={Ajabi, Zaki and Keinath, Alexandra T and Wei, Xue-Xin and Brandon, Mark P},
  journal={Nature},
  volume={615},
  number={7954},
  pages={892--899},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{zhang2014,
  title = {A {{Comprehensive Review}} of {{Stability Analysis}} of {{Continuous-Time Recurrent Neural Networks}}},
  author = {Zhang, Huaguang and Wang, Zhanshan and Liu, Derong},
  date = {2014-07},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {25},
  number = {7},
  pages = {1229--1262},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2014.2317880},
  abstract = {Stability problems of continuous-time recurrent neural networks have been extensively studied, and many papers have been published in the literature. The purpose of this paper is to provide a comprehensive review of the research on stability of continuous-time recurrent neural networks, including Hopfield neural networks, Cohen-Grossberg neural networks, and related models. Since time delay is inevitable in practice, stability results of recurrent neural networks with different classes of time delays are reviewed in detail. For the case of delay-dependent stability, the results on how to deal with the constant/variable delay in recurrent neural networks are summarized. The relationship among stability results in different forms, such as algebraic inequality forms, M-matrix forms, linear matrix inequality forms, and Lyapunov diagonal stability forms, is discussed and compared. Some necessary and sufficient stability conditions for recurrent neural networks without time delays are also discussed. Concluding remarks and future directions of stability analysis of recurrent neural networks are given.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}} and {{Learning Systems}}},
  keywords = {\textbackslash (M\textbackslash ) -matrix,Biological neural networks,Cohen-Grossberg neural networks,Cohen–Grossberg neural networks,Delays,discrete delay,distributed delays,Hopfield neural networks,linear matrix inequality (LMI),Lyapunov diagonal stability (LDS),M-matrix,Neurons,recurrent neural networks,Recurrent neural networks,robust stability,stability,Stability criteria,stability.},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\LWMFYGAB\\Zhang et al. - 2014 - A Comprehensive Review of Stability Analysis of Co.pdf}
}

@online{clevert2015,
  title = {Fast and {{Accurate Deep Network Learning}} by {{Exponential Linear Units}} ({{ELUs}})},
  author={Clevert, Djork-Arn{\'e} and Unterthiner, Thomas and Hochreiter, Sepp},
  journal={arXiv preprint arXiv:1511.07289},
  year={2015}
}

@article{barron2017,
  title={Continuously differentiable exponential linear units},
  author={Barron, Jonathan T},
  journal={arXiv preprint arXiv:1704.07483},
  year={2017}
}

@article{jagtap2023,
  title={How important are activation functions in regression and classification? A survey, performance comparison, and future directions},
  author={Jagtap, Ameya D and Karniadakis, George Em},
  journal={Journal of Machine Learning for Modeling and Computing},
  volume={4},
  number={1},
  year={2023},
  publisher={Begel House Inc.}
}

@article{ramachandran2017,
  title={Searching for activation functions},
  author={Ramachandran, Prajit and Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1710.05941},
  year={2017}
}

@inproceedings{hayou2019,
  title={On the impact of the activation function on deep neural networks training},
  author={Hayou, Soufiane and Doucet, Arnaud and Rousseau, Judith},
  booktitle={International conference on machine learning},
  pages={2672--2680},
  year={2019},
  organization={PMLR}
}

@article{le2015,
  title={A simple way to initialize recurrent networks of rectified linear units},
  author={Le, Quoc V and Jaitly, Navdeep and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1504.00941},
  year={2015}
}

@article{simpson2018,
  title={Dimension reduction for slow-fast, piecewise-smooth, continuous systems of {ODEs}},
  author={Simpson, David JW},
  journal={arXiv preprint arXiv:1801.04653},
  year={2018}
}

@article{yu2009,
  title={Representations of continuous attractors of recurrent neural networks},
  author={Yu, Jiali and Yi, Zhang and Zhang, Lei},
  journal={IEEE transactions on neural networks},
  volume={20},
  number={2},
  pages={368--372},
  year={2009},
  publisher={IEEE}
}

@article{gu2022,
  title={Unsupervised learning for robust working memory},
  author={Gu, Jintao and Lim, Sukbin},
  journal={PLoS Computational Biology},
  volume={18},
  number={5},
  pages={e1009083},
  year={2022},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{goodridge2000,
  title={Modeling attractor deformation in the rodent head-direction system},
  author={Goodridge, Jeremy P and Touretzky, David S},
  journal={Journal of neurophysiology},
  volume={83},
  number={6},
  pages={3402--3410},
  year={2000},
  publisher={American Physiological Society Bethesda, MD}
}

@article{seeholzer2019,
  title={Stability of working memory in continuous attractor networks under the control of short-term plasticity},
  author={Seeholzer, Alexander and Deger, Moritz and Gerstner, Wulfram},
  journal={PLoS computational biology},
  volume={15},
  number={4},
  pages={e1006928},
  year={2019},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{beiran2021,
  title={Shaping dynamics with multiple populations in low-rank recurrent networks},
  author={Beiran, Manuel and Dubreuil, Alexis and Valente, Adrian and Mastrogiuseppe, Francesca and Ostojic, Srdjan},
  journal={Neural Computation},
  volume={33},
  number={6},
  pages={1572--1615},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209}
}

@article{beiran2023parametric,
  title={Parametric control of flexible timing through low-dimensional neural manifolds},
  author={Beiran, Manuel and Meirhaeghe, Nicolas and Sohn, Hansem and Jazayeri, Mehrdad and Ostojic, Srdjan},
  journal={Neuron},
  volume={111},
  number={5},
  pages={739--753},
  year={2023},
  publisher={Elsevier}
}

@article{morrison2024diversity,
  title={Diversity of emergent dynamics in competitive threshold-linear networks},
  author={Morrison, Katherine and Degeratu, Anda and Itskov, Vladimir and Curto, Carina},
  journal={SIAM Journal on Applied Dynamical Systems},
  volume={23},
  number={1},
  pages={855--884},
  year={2024},
  publisher={SIAM}
}

@article{seeholzer2017efficient,
  title={Efficient low-dimensional approximation of continuous attractor networks},
  author={Seeholzer, Alexander and Deger, Moritz and Gerstner, Wulfram},
  journal={arXiv preprint arXiv:1711.08032},
  year={2017}
}

@article{compte2000synaptic,
  title={Synaptic mechanisms and network dynamics underlying spatial working memory in a cortical network model},
  author={Compte, Albert and Brunel, Nicolas and Goldman-Rakic, Patricia S and Wang, Xiao-Jing},
  journal={Cerebral cortex},
  volume={10},
  number={9},
  pages={910--923},
  year={2000},
  publisher={Oxford University Press}
}

@article{mante2013context,
  title={Context-dependent computation by recurrent dynamics in prefrontal cortex},
  author={Mante, Valerio and Sussillo, David and Shenoy, Krishna V and Newsome, William T},
  journal={nature},
  volume={503},
  number={7474},
  pages={78--84},
  year={2013},
  publisher={Nature Publishing Group UK London}
}

@article{ghazizadeh2021slow,
  title={Slow manifolds within network dynamics encode working memory efficiently and robustly},
  author={Ghazizadeh, Elham and Ching, ShiNung},
  journal={PLoS computational biology},
  volume={17},
  number={9},
  pages={e1009366},
  year={2021},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{remington2018flexible,
  title={Flexible sensorimotor computations through rapid reconfiguration of cortical dynamics},
  author={Remington, Evan D and Narain, Devika and Hosseini, Eghbal A and Jazayeri, Mehrdad},
  journal={Neuron},
  volume={98},
  number={5},
  pages={1005--1019},
  year={2018},
  publisher={Elsevier}
}

@article{maheswaranathan2019universality,
  title={Universality and individuality in neural dynamics across large populations of recurrent networks},
  author={Maheswaranathan, Niru and Williams, Alex and Golub, Matthew and Ganguli, Surya and Sussillo, David},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{driscoll2022,
  title={Flexible multitask computation in recurrent networks utilizes shared dynamical motifs},
  author={Driscoll, Laura and Shenoy, Krishna and Sussillo, David},
  journal={bioRxiv},
  pages={2022--08},
  year={2022},
  publisher={Cold Spring Harbor Laboratory}
}

@article{yang2019task,
  title={Task representations in neural networks trained to perform many cognitive tasks},
  author={Yang, Guangyu Robert and Joglekar, Medha R and Song, Hongjie F and Newsome, William T and Wang, Xiao-Jing},
  journal={Nature Neuroscience},
  volume={22},
  number={2},
  pages={297--306},
  year={2019},
  publisher={Nature Publishing Group},
  doi={10.1038/s41593-018-0310-2}
}

@article{cueva2019headdirection,
  author = {Cueva, Carlos J. and Wang, Pei Y. and Chin, Mable and Wei, Xinxin X.},
  title = {Emergence of Functional and Structural Properties of the Head Direction System by Optimization of Recurrent Neural Networks},
  journal = {arXiv preprint},
  eprint = {arXiv:1912.10189},
  year = {2019},
}

@article{cueva2021continuous,
  title={Recurrent neural network models for working memory of continuous variables: Activity manifolds, connectivity patterns, and dynamic codes},
  author={Cueva, Christopher J and Ardalan, Adel and Tsodyks, Misha and Qian, Ning},
  journal={arXiv preprint arXiv:2111.01275},
  year={2021}
}

@article{orhan2019diverse,
  title={A diverse range of factors affect the nature of neural representations underlying short-term memory},
  author={Orhan, A Emin and Ma, Wei Ji},
  journal={Nature neuroscience},
  volume={22},
  number={2},
  pages={275--283},
  year={2019},
  publisher={Nature Publishing Group US New York}
}

@article{esnaola2022flexible,
  title={Flexible integration of continuous sensory evidence in perceptual estimation tasks},
  author={Esnaola-Acebes, Jose M and Roxin, Alex and Wimmer, Klaus},
  journal={Proceedings of the National Academy of Sciences},
  volume={119},
  number={45},
  pages={e2214441119},
  year={2022},
  publisher={National Acad Sciences}
}

@article{panichello2019,
  title={Error-correcting dynamics in visual working memory},
  author={Panichello, Matthew F and DePasquale, Brian and Pillow, Jonathan W and Buschman, Timothy J},
  journal={Nature communications},
  volume={10},
  number={1},
  pages={3366},
  year={2019},
  publisher={Nature Publishing Group UK London}
}

@article{vafidis2022,
  title={Learning accurate path integration in ring attractor models of the head direction system},
  author={Vafidis, Pantelis and Owald, David and D'Albis, Tiziano and Kempter, Richard},
  journal={Elife},
  volume={11},
  pages={e69841},
  year={2022},
  publisher={eLife Sciences Publications Limited}
}

@article{kim2017ring,
  title={Ring attractor dynamics in the {Drosophila} central brain},
  author={Kim, Sung Soo and Rouault, Herv{\'e} and Druckmann, Shaul and Jayaraman, Vivek},
  journal={Science},
  volume={356},
  number={6340},
  pages={849--853},
  year={2017},
  publisher={American Association for the Advancement of Science}
}

@article{kim2019generation,
  title={Generation of stable heading representations in diverse visual scenes},
  author={Kim, Sung Soo and Hermundstad, Ann M and Romani, Sandro and Abbott, LF and Jayaraman, Vivek},
  journal={Nature},
  volume={576},
  number={7785},
  pages={126--131},
  year={2019},
  publisher={Nature Publishing Group UK London}
}

@article{turner2020neuroanatomical,
  title={The neuroanatomical ultrastructure and function of a biological ring attractor},
  author={Turner-Evans, Daniel B and Jensen, Kristopher T and Ali, Saba and Paterson, Tyler and Sheridan, Arlo and Ray, Robert P and Wolff, Tanya and Lauritzen, J Scott and Rubin, Gerald M and Bock, Davi D and others},
  journal={Neuron},
  volume={108},
  number={1},
  pages={145--163},
  year={2020},
  publisher={Elsevier}
}

@article{turner2017angular,
  title={Angular velocity integration in a fly heading circuit},
  author={Turner-Evans, Daniel and Wegener, Stephanie and Rouault, Herve and Franconville, Romain and Wolff, Tanya and Seelig, Johannes D and Druckmann, Shaul and Jayaraman, Vivek},
  journal={Elife},
  volume={6},
  pages={e23496},
  year={2017},
  publisher={eLife Sciences Publications, Ltd}
}

@article{hulse2020mechanisms,
  author = {Hulse, Benjamin K. and Jayaraman, Vivek},
  title = {Mechanisms Underlying the Neural Computation of Head Direction},
  journal = {Annual Review of Neuroscience},
  volume = {43},
  pages = {31-54},
  year = {2020},
}

@article{wimmer2014,
  title={Bump attractor dynamics in prefrontal cortex explains behavioral precision in spatial working memory},
  author={Wimmer, Klaus and Nykamp, Duane Q and Constantinidis, Christos and Compte, Albert},
  journal={Nature neuroscience},
  volume={17},
  number={3},
  pages={431--439},
  year={2014},
  publisher={Nature Publishing Group US New York}
}

@article{barak2021mapping,
  title={Mapping low-dimensional dynamics to high-dimensional neural activity: A derivation of the ring model from the neural engineering framework},
  author={Barak, Omri and Romani, Sandro},
  journal={Neural Computation},
  volume={33},
  number={3},
  pages={827--852},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@book{frisch1993dance,
  title={The dance language and orientation of bees},
  author={Frisch, Karl von},
  year={1993},
  publisher={Harvard University Press}
}

@article{druckmann2012neuronal,
  title={Neuronal circuits underlying persistent representations despite time varying activity},
  author={Druckmann, Shaul and Chklovskii, Dmitri B},
  journal={Current Biology},
  volume={22},
  number={22},
  pages={2095--2103},
  year={2012},
  publisher={Elsevier}
}

@article{curtis2010beyond,
  title={Beyond working memory: The role of persistent activity in decision making},
  author={Curtis, Clayton E and Lee, Daeyeol},
  journal={Trends in cognitive sciences},
  volume={14},
  number={5},
  pages={216--222},
  year={2010},
  publisher={Elsevier}
}

@article{hirsch1995computing,
  title={Computing with dynamic attractors in neural networks},
  author={Hirsch, Morris W and Baird, Bill},
  journal={Biosystems},
  volume={34},
  number={1-3},
  pages={173--195},
  year={1995},
  publisher={Elsevier}
}

@article{amsaleg2018extreme,
  title={Extreme-value-theoretic estimation of local intrinsic dimensionality},
  author={Amsaleg, Laurent and Chelly, Oussama and Furon, Teddy and Girard, St{\'e}phane and Houle, Michael E and Kawarabayashi, Ken-ichi and Nett, Michael},
  journal={Data Mining and Knowledge Discovery},
  volume={32},
  number={6},
  pages={1768--1805},
  year={2018},
  publisher={Springer}
}

@article{altan2021estimating,
  title={Estimating the dimensionality of the manifold underlying multi-electrode neural recordings},
  author={Altan, Ege and Solla, Sara A and Miller, Lee E and Perreault, Eric J},
  journal={PLoS computational biology},
  volume={17},
  number={11},
  pages={e1008591},
  year={2021},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{taube1998head,
  title={Head direction cells and the neurophysiological basis for a sense of direction},
  author={Taube, Jeffrey S},
  journal={Progress in neurobiology},
  volume={55},
  number={3},
  pages={225--256},
  year={1998},
  publisher={Elsevier}
}

@article{taube1990head,
  title={Head-direction cells recorded from the postsubiculum in freely moving rats. {I. Description and quantitative analysis}},
  author={Taube, Jeffrey S and Muller, Robert U and Ranck, James B},
  journal={Journal of Neuroscience},
  volume={10},
  number={2},
  pages={420--435},
  year={1990},
  publisher={Soc Neuroscience}
}

@article{taube2007head,
  title={The head direction signal: Origins and sensory-motor integration},
  author={Taube, Jeffrey S},
  journal={Annu. Rev. Neurosci.},
  volume={30},
  pages={181--207},
  year={2007},
  publisher={Annual Reviews}
}

@inproceedings{paszke2017automatic,
  title={Automatic differentiation in {PyTorch}},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  booktitle={NIPS-W},
  year={2017}
}

@article{tsodyks1995associative,
  title={Associative memory and hippocampal place cells},
  author={Tsodyks, Misha and Sejnowski, Terrence},
  journal={International journal of neural systems},
  volume={6},
  pages={81--86},
  year={1995},
  publisher={World Scientific}
}

@article{prohens2013canard,
  title={Canard trajectories in {3D} piecewise linear systems},
  author={Prohens, Rafel and Teruel, Antonio E},
  journal={Discrete Contin. Dyn. Syst},
  volume={33},
  number={3},
  pages={4595--4611},
  year={2013}
}

@article{prohens2016slow,
  title={Slow--fast n-dimensional piecewise linear differential systems},
  author={Prohens, R and Teruel, AE and Vich, C},
  journal={Journal of Differential Equations},
  volume={260},
  number={2},
  pages={1865--1892},
  year={2016},
  publisher={Elsevier}
}

@article{mane1978persistent,
  title={Persistent manifolds are normally hyperbolic},
  author={Ma\~{n}{\'e}, Ricardo},
  journal={Transactions of the American Mathematical Society},
  volume={246},
  pages={261--283},
  year={1978}
}

@article{mane1987proof,
  title={A proof of the $ C^{1}$ stability conjecture},
  author={Ma{\~n}{\'e}, Ricardo},
  journal={Publications Math{\'e}matiques de l'IH{\'E}S},
  volume={66},
  pages={161--210},
  year={1987}
}

@article{li2005persistence,
  title={Persistence of hyperbolic tori in {Hamiltonian} systems},
  author={Li, Yong and Yi, Yingfei},
  journal={Journal of Differential Equations},
  volume={208},
  number={2},
  pages={344--387},
  year={2005},
  publisher={Elsevier}
}

@article{broer2018persistence,
  title={Persistence properties of normally hyperbolic tori},
  author={Broer, Henk and Han{\ss}mann, Heinz and Wagener, Florian},
  journal={Regular and Chaotic Dynamics},
  volume={23},
  pages={212--225},
  year={2018},
  publisher={Springer}
}

@book{folland1999real,
  title={Real analysis: Modern techniques and their applications},
  author={Folland, Gerald B},
  volume={40},
  year={1999},
  publisher={John Wiley \& Sons}
}

@article{biswas2022geometric,
  title={Geometric framework to predict structure from function in neural networks},
  author={Biswas, Tirthabir and Fitzgerald, James E},
  journal={Physical review research},
  volume={4},
  number={2},
  pages={023255},
  year={2022},
  publisher={APS}
}

@article{yang2022brainstem,
  title={A brainstem integrator for self-location memory and positional homeostasis in zebrafish},
  author={Yang, En and Zwart, Maarten F and James, Ben and Rubinov, Mikail and Wei, Ziqiang and Narayan, Sujatha and Vladimirov, Nikita and Mensh, Brett D and Fitzgerald, James E and Ahrens, Misha B},
  journal={Cell},
  volume={185},
  number={26},
  pages={5011--5027},
  year={2022},
  publisher={Elsevier}
}

@article{petrucco2023neural,
  title={Neural dynamics and architecture of the heading direction circuit in zebrafish},
  author={Petrucco, Luigi and Lavian, Hagar and Wu, You Kure and Svara, Fabian and {\v{S}}tih, Vilim and Portugues, Ruben},
  journal={Nature neuroscience},
  volume={26},
  number={5},
  pages={765--773},
  year={2023},
  publisher={Nature Publishing Group US New York}
}

@book{hirsch2013differential,
  title={Differential equations, dynamical systems, and an introduction to chaos},
  author={Hirsch, Morris W and Smale, Stephen and Devaney, Robert L},
  year={2013},
  publisher={Academic press}
}

@article{langdon2023unifying,
  title={A unifying perspective on neural manifolds and circuits for cognition},
  author={Langdon, Christopher and Genkin, Mikhail and Engel, Tatiana A},
  journal={Nature Reviews Neuroscience},
  volume={24},
  number={6},
  pages={363--377},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{altan2023low,
  title={Low-dimensional neural manifolds for the control of constrained and unconstrained movements},
  author={Altan, Ege and Ma, Xuan and Miller, Lee E and Perreault, Eric J and Solla, Sara A},
  journal={bioRxiv},
  pages={2023--05},
  year={2023},
  publisher={Cold Spring Harbor Laboratory}
}

@article{warnberg2019perturbing,
  title={Perturbing low dimensional activity manifolds in spiking neuronal networks},
  author={W{\"a}rnberg, Emil and Kumar, Arvind},
  journal={PLOS computational biology},
  volume={15},
  number={5},
  pages={e1007074},
  year={2019},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{fanthomme2021low,
  title={Low-dimensional manifolds support multiplexed integrations in recurrent neural networks},
  author={Fanthomme, Arnaud and Monasson, R{\'e}mi},
  journal={Neural Computation},
  volume={33},
  number={4},
  pages={1063--1112},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{recanatesi2018signatures,
  title={Signatures and mechanisms of low-dimensional neural predictive manifolds},
  author={Recanatesi, Stefano and Farrell, Matthew and Lajoie, Guillaume and Deneve, Sophie and Rigotti, Mattia and Shea-Brown, Eric},
  journal={bioRxiv},
  pages={471987},
  year={2018},
  publisher={Cold Spring Harbor Laboratory}
}

@inproceedings{ehresmann1950connexions,
  title={Les connexions infinit{\'e}simales dans un espace fibr{\'e} diff{\'e}rentiable},
  author={Ehresmann, Charles},
  booktitle={Colloque de topologie, Bruxelles},
  volume={29},
  pages={55--75},
  year={1950}
}

@book{guillemin2010differential,
  title={Differential topology},
  author={Guillemin, Victor and Pollack, Alan},
  volume={370},
  year={2010},
  publisher={American Mathematical Soc.}
}


@book{golubitsky1988singularities,
  title = {Singularities and {{Groups}} in {{Bifurcation Theory II}}},
  author = {Golubitsky, Martin and Stewart, Ian and Schaeffer, David G.},
  editor = {Marsden, J. E. and Sirovich, L.},
  editortype = {redactor},
  date = {1988},
  series = {Applied {{Mathematical Sciences}}},
  volume = {69},
  publisher = {Springer New York},
  location = {New York, NY},
  doi = {10.1007/978-1-4612-4574-2},
  isbn = {978-1-4612-8929-6 978-1-4612-4574-2}
}

@book{golubitsky2002symmetry,
  title = {The Symmetry Perspective: From Equilibrium to Chaos in Phase Space and Physical Space},
  shorttitle = {The Symmetry Perspective},
  author = {Golubitsky, Martin and Stewart, Ian},
  date = {2002},
  series = {Progress in Mathematics},
  number = {200},
  publisher = {Birkh\"auser},
  location = {Basel Boston Berlin},
  isbn = {978-3-7643-6609-4},
  pagetotal = {325}
}

@article{benyishai1995theory,
  title={Theory of orientation tuning in visual cortex.},
  author={Ben-Yishai, Rani and Bar-Or, R Lev and Sompolinsky, Haim},
  journal={Proceedings of the National Academy of Sciences},
  volume={92},
  number={9},
  pages={3844--3848},
  year={1995},
  publisher={National Acad Sciences}
}

@article{battista2020capacity,
  title={Capacity-resolution trade-off in the optimal learning of multiple low-dimensional manifolds by attractor neural networks},
  author={Battista, Aldo and Monasson, R{\'e}mi},
  journal={Physical review letters},
  volume={124},
  number={4},
  pages={048302},
  year={2020},
  publisher={APS}
}

@article{kuhn2023information,
  title={Information content in continuous attractor neural networks is preserved in the presence of moderate disordered background connectivity},
  author={K{\"u}hn, Tobias and Monasson, R{\'e}mi},
  journal={Physical Review E},
  volume={108},
  number={6},
  pages={064301},
  year={2023},
  publisher={APS}
}

@book{eldering2013normally,
  title={Normally hyperbolic invariant manifolds: The noncompact case},
  author={Eldering, Jaap and others},
  volume={2},
  year={2013},
  publisher={Springer}
}

@inproceedings{monfared2020transformation,
  title={Transformation of {ReLU}-based recurrent neural networks from discrete-time to continuous-time},
  author={Monfared, Zahra and Durstewitz, Daniel},
  booktitle={International Conference on Machine Learning},
  pages={6999--7009},
  year={2020},
  organization={PMLR}
}

@article{schmidt2019identifying,
  title={Identifying nonlinear dynamical systems with multiple time scales and long-range dependencies},
  author={Schmidt, Dominik and Koppe, Georgia and Monfared, Zahra and Beutelspacher, Max and Durstewitz, Daniel},
  journal={arXiv preprint arXiv:1910.03471},
  year={2019}
}

@article{durstewitz2023reconstructing,
  title={Reconstructing computational system dynamics from neural data with recurrent neural networks},
  author={Durstewitz, Daniel and Koppe, Georgia and Thurm, Max Ingo},
  journal={Nature Reviews Neuroscience},
  volume={24},
  number={11},
  pages={693--710},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{wojtak2023robust,
  title={Robust working memory in a two-dimensional continuous attractor network},
  author={Wojtak, Weronika and Coombes, Stephen and Avitabile, Daniele and Bicho, Estela and Erlhagen, Wolfram},
  journal={Cognitive Neurodynamics},
  pages={1--17},
  year={2023},
  publisher={Springer}
}

@article{hermansen2024uncovering,
  title={Uncovering 2-{D} toroidal representations in grid cell ensemble activity during 1-{D} behavior},
  author={Hermansen, Erik and Klindt, David A and Dunn, Benjamin A},
  journal={Nature Communications},
  volume={15},
  number={1},
  pages={5429},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{cho2014learning,
  title={Learning phrase representations using {RNN} encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1406.1078},
  year={2014}
}

@article{ostrow2024beyond,
  title={Beyond geometry: Comparing the temporal structure of computation in neural circuits with dynamical similarity analysis},
  author={Ostrow, Mitchell and Eisen, Adam and Kozachkov, Leo and Fiete, Ila},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@INCOLLECTION{Kopell1996,
  title     = "Global center manifolds and singularly perturbed equations: A
               brief (and biased) guide to (some of) the literature",
  author    = "Kopell, Nancy",
  editor    = "Deift, Percy and David Levermore, C and Eugene Wayne, C",
  booktitle = "Dynamical Systems and Probabilistic Methods in Partial
               Differential Equations: 1994 Summer Seminar on Dynamical Systems
               and Probabilistic Methods for Nonlinear Waves, June 20-July 1,
               1994, MSRI, Berkeley, CA",
  publisher = "American Mathematical Soceity",
  volume    =  31,
  pages     = "47--50",
  year      =  1996
}

@book{kirchgraber1990geometry,
  title     = {Geometry in the Neighborhood of Invariant Manifolds of Maps and Flows and Linearization},
  author    = {Kirchgraber, Urs and Palmer, Kenneth J.},
  year      = {1990},
  series    = {Pitman Research Notes in Mathematics Series},
  publisher = {Longman Scientific \& Technical},
  address   = {New York},
  note      = {Published in the United States with John Wiley \& Sons, Inc.}
}

@inproceedings{rinzel1985excitation,
  title={Excitation dynamics: Insights from simplified membrane models.},
  author={Rinzel, John},
  booktitle={Federation proceedings},
  volume={44},
  number={15},
  pages={2944--2946},
  year={1985}
}

@article{ermentrout1986parabolic,
  title={Parabolic bursting in an excitable system coupled with a slow oscillation},
  author={Ermentrout, G Bard and Kopell, Nancy},
  journal={SIAM journal on applied mathematics},
  volume={46},
  number={2},
  pages={233--253},
  year={1986},
  publisher={SIAM}
}

@article{hodgkin1952quantitative,
  title={A quantitative description of membrane current and its application to conduction and excitation in nerve},
  author={Hodgkin, Alan L and Huxley, Andrew F},
  journal={The Journal of physiology},
  volume={117},
  number={4},
  pages={500},
  year={1952},
  publisher={Wiley}
}

@book{ozaki2012time,
  title={Time series modeling of neuroscience data},
  author={Ozaki, Tohru},
  year={2012},
  publisher={CRC press}
}

@article{tzen2019neural,
  title={Neural stochastic differential equations: Deep latent gaussian models in the diffusion limit},
  author={Tzen, Belinda and Raginsky, Maxim},
  journal={arXiv preprint arXiv:1905.09883},
  year={2019}
}

@article{peixoto1959structural,
  title={Structural stability in the plane with enlarged boundary conditions},
  author={Peixoto, Marilia C and Peixoto, Maur{\'\i}cio M},
  journal={An. Acad. Brasil. Ci},
  volume={31},
  number={2},
  pages={135--160},
  year={1959}
}

@incollection{palis2000structural,
  title={Structural stability theorems},
  author={Palis, Jacob and Smale, Stephen},
  booktitle={The Collected Papers of Stephen Smale: Volume 2},
  pages={739--747},
  year={2000},
  publisher={World Scientific}
}

@article{robbin1971structural,
  title={A structural stability theorem},
  author={Robbin, Joel W},
  journal={Annals of Mathematics},
  volume={94},
  number={3},
  pages={447--493},
  year={1971},
  publisher={JSTOR}
}

@incollection{robinson1974structural,
  title={Structural stability of {$C^{1}$} flows},
  author={Robinson, R Clark},
  booktitle={Dynamical Systems—Warwick 1974},
  year={1974}
}

@article{can2021emergence,
  title={Emergence of memory manifolds},
  author={Can, Tankut and Krishnamurthy, Kamesh},
  journal={arXiv preprint arXiv:2109.03879},
  year={2021}
}

@article{spalla2021continuous,
  title={Continuous attractors for dynamic memories},
  author={Spalla, Davide and Cornacchia, Isabel Maria and Treves, Alessandro},
  journal={Elife},
  volume={10},
  pages={e69499},
  year={2021},
  publisher={eLife Sciences Publications Limited}
}

@article{itskov2011short,
  title={Short-term facilitation may stabilize parametric working memory trace},
  author={Itskov, Vladimir and Hansel, David and Tsodyks, Misha},
  journal={Frontiers in computational neuroscience},
  volume={5},
  pages={40},
  year={2011},
  publisher={Frontiers Research Foundation}
}

@article{durstewitz2000neurocomputational,
  title={Neurocomputational models of working memory},
  author={Durstewitz, Daniel and Seamans, Jeremy K and Sejnowski, Terrence J},
  journal={Nature neuroscience},
  volume={3},
  number={11},
  pages={1184--1191},
  year={2000},
  publisher={Nature Publishing Group}
}

@article{hansel2013short,
  title={Short-term plasticity explains irregular persistent activity in working memory tasks},
  author={Hansel, David and Mato, German},
  journal={Journal of Neuroscience},
  volume={33},
  number={1},
  pages={133--149},
  year={2013},
  publisher={Soc Neuroscience}
}

@article{yates2020simple,
  title={A simple linear readout of {MT} supports motion direction-discrimination performance},
  author={Yates, Jacob L and Katz, Leor N and Levi, Aaron J and Pillow, Jonathan W and Huk, Alexander C},
  journal={Journal of neurophysiology},
  year={2020},
  publisher={American Physiological Society Bethesda, MD}
}

@article{knierim2016tracking,
  title={Tracking the flow of hippocampal computation: Pattern separation, pattern completion, and attractor dynamics},
  author={Knierim, James J and Neunuebel, Joshua P},
  journal={Neurobiology of learning and memory},
  volume={129},
  pages={38--49},
  year={2016},
  publisher={Elsevier}
}

@article{xie2022neural,
  title={Neural mechanisms of working memory accuracy revealed by recurrent neural networks},
  author={Xie, Yuanqi and Liu, Yichen Henry and Constantinidis, Christos and Zhou, Xin},
  journal={Frontiers in Systems Neuroscience},
  volume={16},
  pages={760864},
  year={2022},
  publisher={Frontiers Media SA}
}

@article{chen2024synaptic,
  title={Synaptic ring attractor: a unified framework for attractor dynamics and multiple cues integration},
  author={Chen, Yani and Zhang, Lin and Chen, Hao and Sun, Xuelong and Peng, Jigen},
  journal={Heliyon},
  year={2024},
  publisher={Elsevier}
}

@article{angelaki2020head,
  title={The head direction cell network: Attractor dynamics, integration within the navigation system, and three-dimensional properties},
  author={Angelaki, Dora E and Laurens, Jean},
  journal={Current opinion in neurobiology},
  volume={60},
  pages={136--144},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{sun2018analysis,
  title={An analysis of a ring attractor model for cue integration},
  author={Sun, Xuelong and Mangan, Michael and Yue, Shigang},
  booktitle={Biomimetic and Biohybrid Systems: 7th International Conference, Living Machines 2018, Paris, France, July 17--20, 2018, Proceedings 7},
  pages={459--470},
  year={2018},
  organization={Springer}
}

@article{kutschireiter2023bayesian,
  title={Bayesian inference in ring attractor networks},
  author={Kutschireiter, Anna and Basnak, Melanie A and Wilson, Rachel I and Drugowitsch, Jan},
  journal={Proceedings of the National Academy of Sciences},
  volume={120},
  number={9},
  pages={e2210622120},
  year={2023},
  publisher={National Acad Sciences}
}

@article{fisher2022flexible,
  title={Flexible navigational computations in the {Drosophila} central complex},
  author={Fisher, Yvette E},
  journal={Current opinion in neurobiology},
  volume={73},
  pages={102514},
  year={2022},
  publisher={Elsevier}
}

@article{xie2002double,
  title={Double-ring network model of the head-direction system},
  author={Xie, Xiaohui and Hahnloser, Richard HR and Seung, H Sebastian},
  journal={Physical Review E},
  volume={66},
  number={4},
  pages={041902},
  year={2002},
  publisher={APS}
}

@article{taube2003persistent,
  title={Persistent neural activity in head direction cells},
  author={Taube, Jeffrey S and Bassett, Joshua P},
  journal={Cerebral Cortex},
  volume={13},
  number={11},
  pages={1162--1172},
  year={2003},
  publisher={Oxford University Press}
}

@article{song2005angular,
  title={Angular path integration by moving “hill of activity”: A spiking neuron model without recurrent excitation of the head-direction system},
  author={Song, Pengcheng and Wang, Xiao-Jing},
  journal={Journal of Neuroscience},
  volume={25},
  number={4},
  pages={1002--1014},
  year={2005},
  publisher={Soc Neuroscience}
}



# vim: paste
