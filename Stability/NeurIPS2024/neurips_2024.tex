\documentclass{article} % For LaTeX2e
\usepackage[preprint]{neurips_2024}
\pdfminorversion=6
%\usepackage[utf8]{inputenc}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{thmtools,mathtools,mathrsfs}
\usepackage{amsfonts}       		% blackboard math symbols
\usepackage{forloop}
\usepackage{xspace}
%\usepackage{natbib}
\setcitestyle{open={},close={},comma,numbers,sort&compress,super}
\usepackage{graphicx}
\usepackage{tikz-cd}
\PassOptionsToPackage{dvipsnames}{xcolor}
\usepackage{xcolor}
\definecolor{ForestGreen}{rgb}{0.13, 0.55, 0.13}
\definecolor{MidnightBlue}{rgb}{0.1, 0.1, 0.44}
\definecolor{BurntOrange}{rgb}{0.8, 0.33, 0.0}
\definecolor{Plum}{rgb}{0.56, 0.27, 0.52}
\usepackage[colorlinks=true,linkcolor=MidnightBlue,citecolor=ForestGreen,filecolor=TealBlue,urlcolor=Plum]{hyperref}       % hyperlinks
\hypersetup{breaklinks=true}
\usepackage{pifont} 			%xmark
\usepackage{caption,subcaption} 			%\usepackage{subcaption}
\usepackage{float}
\usepackage{bm}				%bold
\usepackage[T1]{fontenc}    		% use 8-bit T1 fonts
\usepackage{url}            			% simple URL typesetting
\usepackage{booktabs}      		% professional-quality tables
\usepackage{nicefrac}       		% compact symbols for 1/2, etc.
\usepackage{microtype}      		% microtypography
\usepackage[inline]{enumitem}		%inline enumerate

 %% space saving tricks
\usepackage{sidecap} % caption on the side SCfigure
\sidecaptionvpos{figure}{c} % vertically align scfigure in sidecap
\usepackage[compact]{titlesec}
\setlength{\abovedisplayskip}{2pt}
\setlength{\belowdisplayskip}{2pt}
\AtBeginDocument{ \setlength{\textfloatsep}{1pt }}
\AtBeginDocument{\setlength\abovedisplayskip{1pt}}
\AtBeginDocument{\setlength\belowdisplayskip{1pt}}
\titlespacing*{\section}{0pt}{0pt}{0pt}
\titlespacing*{\subsection}{0pt}{0pt}{0pt}

\newcommand{\ptitle}[1]{\textbf{#1:}\xspace}
\definecolor{mpcolor}{rgb}{1, 0.1, 0.59}
\definecolor{ascolor}{rgb}{1, 0.5, 0.}
\newcommand{\mpcomment}[1]{\textcolor{mpcolor}{(#1)}}
\newcommand{\ascomment}[1]{\textcolor{ascolor}{(#1)}}
\newcommand{\pscomment}[1]{\textcolor{BurntOrange}{(#1)}}

\DeclareGraphicsExtensions{.pdf,.png,.jpg,.mps,.eps,.ps}
\graphicspath{{../figures/}{../figures/inv_man/}}

\newcommand{\defvec}[1]{\expandafter\newcommand\csname v#1\endcsname{{\mathbf{#1}}}}
\newcounter{ct}
\forLoop{1}{26}{ct}{
    \edef\letter{\alph{ct}}
    \expandafter\defvec\letter%
}

% captial \vA
\forLoop{1}{26}{ct}{
    \edef\letter{\Alph{ct}}
    \expandafter\defvec\letter%
}

\newcommand{\dm}[1]{\ensuremath{\mathrm{d}{#1}}} % dx dy dz dmu
\newcommand{\RN}[2]{\frac{\dm{#1}}{\dm{#2}}} % (Radon-Nikodym) derivative
\newcommand{\PD}[2]{\frac{\partial#1}{\partial#2}} % partial derivative
\newcommand{\overbar}[1]{\mkern1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern1.5mu}
\newcommand{\win}{\vW_{\text{in}}}
\newcommand{\wout}{\vW_{\text{out}}}
\newcommand{\bout}{\vb_{\text{out}}}
\newcommand{\reals}{\mathbb{R}}

\newcommand{\manifold}{\mathcal{M}}
\newcommand{\uniformNorm}[1]{\left\|#1\right\|_\infty} % uniform norm
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}

\DeclareMathOperator{\relu}{ReLU}
\DeclareMathOperator{\basin}{Basin}
\newcommand{\iidsample}{\stackrel{iid}{\sim}}
\newcommand{\identity}{\ensuremath{\mathbb{I}}}

\newcommand{\probP}{\text{I\kern-0.15em P}}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\theoremstyle{remark}
\newtheorem{remark}{Remark}

%\title{RNNs with gracefully degrading continuous attractors}
%\title{The universal approximation of continuous attractors}

\title{Back to the Continuous Attractor}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\renewcommand{\cite}{\citep}

\author{
    \'Abel S\'agodi, Guillermo Mart\'in-S\'anchez, Piotr Sok\'o\l, Il Memming Park \\
    %\thanks{Use footnote for providing further information about author (webpage, alternative address).} \\
    \\
    Champalimaud Centre for the Unknown\\
    Champalimaud Foundation, Lisbon, Portugal\\
    \texttt{\{abel.sagodi,guillermo.martin,memming.park\}@research.fchampalimaud.org}\\
    \texttt{piotr.sokol@protonmail.com}
}

\begin{document}

%keywords:
%neural computation, robustness, bifurcation analysis, exploding gradient problem, continuous attractors, persistence of invariant manifolds

%TLDR

\maketitle

\begin{abstract}
Continuous attractors offer a unique class of solutions for storing continuous-valued variables in recurrent system states for indefinitely long time intervals.
Unfortunately, continuous attractors suffer from severe structural instability in general---they are destroyed by most infinitesimal changes of the dynamical law that defines them.
This fragility limits their utility especially in biological systems as their recurrent dynamics are subject to constant perturbations.
We observe that the bifurcations from continuous attractors in theoretical neuroscience models display various structurally stable forms.
Although their asymptotic behaviors to maintain memory are categorically distinct, their finite-time behaviors are similar.
We build on the persistent manifold theory to explain the commonalities between bifurcations from and approximations of continuous attractors.
Fast-slow decomposition analysis uncovers the existence of a persistent slow manifold that survives the seemingly destructive bifurcation, relating the flow within the manifold to the size of the perturbation. Moreover, this allows the bounding of the memory error of these approximations of continuous attractors.
Finally, we train recurrent neural networks on analog memory tasks to support the appearance of these systems as solutions and their generalization capabilities.
Therefore, we conclude that continuous attractors are \emph{functionally robust} and remain useful as a universal analogy for understanding analog memory.
% to the problem of maintaining a memory of continuous-valued variables for arbitrarily long intervals.
% poses a problem in biological neural networks, as they are constantly subjected to perturbations caused by noise.
% Ultimately, our theory is not about solving the fine-tuning problem. It's about realizing that it is not a problem.
% To address the issue of how animals robustly store continuous variables, we use Fenichel's persistence theorem from dynamical systems theory.
% This theory shows that normally hyperbolic continuous attractors are robust to perturbations.
% We derive that attractive slow manifolds are universal approximations of continuous attractors.
% In simple line and ring attractors, we verify that all perturbations preserve the invariant manifold and demonstrate the principle numerically in various ring attractor systems.
% Finally, we show in trained RNNs that they all approximate continuous attractors as slow manifolds and characterize the dynamics through a fast-slow decomposition.
\end{abstract}


\section{Introduction}
Biological systems exhibit robust behaviors that require neural information processing of analog variables such as intensity, direction, and distance.
Virtually all neural models of working memory for continuous-valued information rely on persistent internal representations through recurrent dynamics.
The continuous attractor structure in their recurrent dynamics has been a pivotal theoretical tool due to their ability to maintain activity patterns indefinitely through neural population states~\cite{vyas2020,dayan2001,Khona2022,durstewitz2023reconstructing}.
They are hypothesized to be the neural mechanism for the maintenance of eye positions, heading direction, self-location, target location, sensory evidence, working memory, and decision variables, to name a few~\cite{seung1996,Seung2000,Romo1999}.
Observations of persistent neural activity across many brain areas, organisms, and tasks have corroborated the existence of continuous attractors~\cite{Romo1999,Noorman2022,petrucco2023neural,yang2022brainstem,constantinidis2018persistent,nair2023approximate,mountoufaris2024line,vinograd2024causal}.
%and anatomy?

%par1: animals can represent CVs
%The natural behavior of animals, including their ability to navigate without sensory cues, implies the presence of a robust internal representation of key variables such as location and head direction.
%This robustness is achieved despite the constantly changing synaptic connections \citep{shimizu2021}. %\citep{gallego2020, kim2019generation, flesch2023continual}.

%Recurrent neural networks (RNNs) can process sequential observations and model temporal dependencies of arbitrary length.
%At the same time, they are fundamentally limited by their finite-sized hidden states which form the only channel between the past and the future.
%Continuous attractors are the ideal solution to the problem of how to hold continuous valued variables.
%This made them prevalent in theoretical neuroscience and they have been extensively used as tools to model neural representation and computation ranging from internal representations of head direction \citep{Skaggs1995, goodridge2000} and eye positions \citep{seung1996} to perceptual decision-making and working memory dependent on continuous variables~\cite{Khona2022}.

% Animal behavior showcases remarkable abilities to navigate and interact with the environment, even without sensory cues, as seen in rodents' spatial memory skills.
% The Computation through Dynamics (CTD) framework assumes that the brain processes sensory information and generates behavior through neural dynamics, including representations of continuous-valued variables like space and direction~\cite{vyas2020}.
% Continuous attractors are the ideal solution to the problem of how to hold continuous valued variables~\cite{Dayan2001,Burak2009,Khona2022}.
% Continuous attractors are biologically plausible, theoretically elegant, consistent with neural recordings, and avoids the asymptotic exploding and vanishing gradient problem~\cite{Park2023a}.
% When used to accumulate stimuli, continuous attractors are also called neural integrators that are hypothesized to be the underlying computation for the maintenance of eye positions, heading direction, self-location, target location, sensory evidence, working memory, decision variables, to name a few~\cite{seung1996,Seung2000,Romo1999}.
% Neural representation of continuous values have been observed as persistent activity in the prefrontal cortex of primates, ellipsoid body of the fly, and hypothalamus~\cite{Romo1999,Noorman2022,Nair2023}.
% A typical computational implementation of a continuous attractor is a bump attractor network model which requires a mean-field limit~\cite{Skaggs1995,Camperi1998,Renart2003} or finite sized networks with threshold linear units \cite{Noorman2022,Spalla2021}, see also Sec.~\ref{sec:ras}.


%par3: problems with CAs
Despite their widespread adoption as models of analog memory, continuous attractors are brittle mathematical objects, casting significant doubts on their ontological value and hence suitability in accurately representing biological functions.
Even the smallest arbitrary change in recurrent dynamics can be problematic, destroying the continuum of fixed points essential for continuous-valued working memory.
In neuroscience, this vulnerability is well-known and often referred to as the ``fine-tuning problem''~\cite{seung1996,Renart2003,chaudhuri2016,machens2008,Park2023a,itskov2011short}.
There are two primary sources of perturbations in the recurrent network dynamics:
(1) the stochastic nature of online learning signals that act via synaptic plasticity, and
(2) spontaneous fluctuations in synaptic weights~\cite{Fusi2007-yg,shimizu2021}.
Thus, additional mechanisms are necessary to compensate for the degradation in particular implementations, by bringing the short-term behavior closer to that of a continuous attractor~\cite{Lim2012,Lim2013,Boerlin2013,Koulakov2002,Renart2003,gu2022,hansel2013short}.
However, we lack the theoretical basis to understand how much this matters in practice, i.e. what are the effects of different levels of degradation on memory. This is fundamental to justify relying on the brittle concept of continuous attractors for understanding biological analog working memory.

%\pscomment{Some papers list their contributions here. Maybe it would be a convenient place to re-write it in such a way, and then tack on\ref{sec:implications}?\\
%That way the implications stand out.}
In this study, we explore perturbations and approximations of continuous attractors in the space of dynamical models.
We first report on the differences and similarities between the various structurally stable dynamics in the vicinity of continuous attractors in the space of dynamical systems models.
Our analysis reveals the presence of a ``ghost'' continuous attractor (a.k.a. slow manifold) in all of them (Sec.~\ref{sec:critique}).
By assuming normal hyperbolicity we separate the time scales to obtain a decomposition of the dynamics by separating out the fast flow normal to and the slow flow within the slow manifold.
We derive theoretical results that ensure the existence of a slow manifold and determine its closeness to a continuous attractor (Sec.~\ref{sec:theory}).
We explore task-trained recurrent neural networks (RNNs) to show that these systems appear naturally as solutions to the task (Sec.~\ref{sec:experiments}) and that their generalization capabilities can easily be studied as the distance to the continuous attractor (Sec.~\ref{sec:generalization}).
The proposed decomposition applied to theoretical models and task-trained RNNs reveals a ``universal motif'' of analog memory mechanism with various potential topologies.
This leads to the connection of different systems with different topologies as \textbf{approximate continuous attractors}\footnote{This correspond a type of ``ideal pattern'' in the vocabulary of \citet{chirimuuta2024brain}.} (Sec.~\ref{sec:converse}).
Our theory guarantees that systems close to a continuous attractor (in the space of vector fields) will have similar behavior to it, implying that the concept of continuous attractors remains a crucial framework for understanding the neural computation underlying analog memory (Sec.~\ref{sec:implications}).




\section{A critique of pure continuous attractors}\label{sec:critique}
We will first lay out a number of observations about the dynamics of bifurcations and approximations of continuous attractors used in theoretical neuroscience.
Ordinary differential equations (ODEs) are commonly used to describe the dynamical laws governing the temporal evolution of firing rates or latent population states\cite{vyas2020}.
In this framework, neural systems are viewed as implementing the continuous time evolution of neural states to perform computations.
We will consider a continuous attractor as a mechanism that implements analog memory computation: carrying a particular memory representation over time.
To define it formally, let \(\vx(t) \in \reals^{d}\) denote the neural state, and \(\dot{\vx} = \vf(\vx)\) represent its dynamics.
Let \(\manifold \subset \reals^{d}\) be a manifold.
We say \(\manifold\) is a continuous attractor, if (1) every state on the manifold is a fixed point, \(\forall \vx \in \manifold, \vf(\vx) = 0\), and (2) the fixed points are marginally stable tangent to the manifold and stable normal to the manifold.
In other words, the continuous attractor is a continuum of equilibrium points such that the neural state near the manifold is attracted to it, and on the manifold, the state does not move.
Marginal stability implies that continuous systems are structurally unstable, meaning that small perturbations or variations in the system's parameters lead to significant changes in the system's behavior or stability \citep{peixoto1959structural, palis2000structural, robbin1971structural, robinson1974structural}.
We will now study some examples of continuous attractors and how perturbations change their dynamics.

% noise-instability is a problem for all CAN types in the literature

\subsection{Motivating example: bounded line attractor}\label{sec:motivating:line}
\begin{figure}[tbhp]
  \centering
  \includegraphics[width=\textwidth]{lara_bifurcations_s}
  \caption{The critical weakness of continuous attractors is their inherent brittleness as they are rare in the parameter space, i.e., infinitesimal changes in parameters destroy the continuous attractor implemented in RNNs~\cite{seung1996,Renart2003}.
  Some of the structure seems to remain; there is an invariant manifold that is topologically equivalent to the original continuous attractor.
    \textbf{(A)} Phase portraits for the bounded line attractor (Eq.~\eqref{eq:TLN}).
    Under perturbation of parameters, it bifurcates to systems without the continuous attractor.
    \textbf{(B)} The low-rank ring attractor approximation (Sec.~\eqref{sec:supp:lowrank}).
    Different topologies exist for different realizations of a low-rank attractor: different numbers of fixed points (4, 8, 12), or a limit cycle (right bottom).
    Yet, they all share the existence of a ring invariant set.
}\label{fig:lara_bifurcations}
\end{figure}


As an example, we can construct a line attractor (a continuous attractor with a line manifold) as follows:
%We will describe the bifurcation space of some continuous attractors.
%Neurons exhibit complex dynamics that can be described by differential equations.
\begin{align}\label{eq:TLN}
    \dot{\vx} = -\vx + \left[ \vW \vx + \vb \right]_{+}
\end{align}where \(\vW = [0, -1; -1, 0]\) and \(\vb = [1; 1]\), and \([\cdot]_{+} = \max(0,\cdot)\) is the threshold nonlinearity per unit.
%where \(\vx \in \reals^{2}\), \(\vb \in \reals^{2}\) is the bias, \(\vW \in \reals^{2 \times 2}\) is the recurrent weight, and \([\cdot]_{+} = \max(0,\cdot)\) is the threshold nonlinearity per unit.
%In discrete time, this corresponds to a ReLU RNN (see Sec.~\ref{sec:rnn:integration}).
%The non-trivial activity of this network is limited to the (non-negative) first quadrant.
%The second kind of continuous attractor is created through negative feedback.
We get \(\dot{\vx} = 0\) on the \(x_{1} = -x_{2} + 1\) line segment in the first quadrant as the manifold (Fig.~\ref{fig:lara_bifurcations}A, left; black line).
%We refer to it as the bounded line attractor (BLA).
Linearization of the fixed points on the manifold exhibits two eigenvalues, \(0\) and \(-2\);
the \(0\) eigenvalue allows the continuum of fixed points, while \(-2\) makes the flow normal to the manifold attractive (Fig.~\ref{fig:lara_bifurcations}A, left; flow field).

In general, continuous attractors are not only structurally unstable \citep{mane1987proof}, they bifurcate almost certainly for an arbitrary perturbation.
Small changes to the parameters \((\vb,\vW)\) perturb the eigenvalues and any perturbation to the \(0\) eigenvalue destroys the continuous attractor: it bifurcates to either
a single stable fixed point (Fig.~\ref{fig:lara_bifurcations}A, top) or two stable fixed points separated with a saddle-node in between (Fig.~\ref{fig:lara_bifurcations}A, bottom).
However, interestingly, after bifurcation, continuous attractors seemingly tend to leave a ``ghost'' manifold topologically equivalent to the original continuous attractor (note the slow speed).
Furthermore, the flow after the bifurcation is contained in the ghost manifold, i.e., it is an invariant manifold.
This phenomenon, wherein a continuous attractor is approximated by a manifold within the neural space along which the drift occurs at a very slow pace, has previously been commented on~\citep{seung1997learning,mante2013context,schmidt2019identifying}.
%Similar to the BLA in \citep{schmidt2019identifying} a line attractor approximation was described as a slightly detuned line attractor with arbitrarily slow time constants.



%The membrane potential of a neuron changes over time according to the input it receives and its intrinsic properties, which can be mathematically modeled using ODEs.
%Consider an RNN (without input or output for now) expressed in continuous time as an ordinary differential equation:
%A common form of the ODE for the firing rate \( r(t) \) of a neuron is:
%\begin{align}\label{eq:TLN2}
%\tau \frac{d r(t)}{d t} = -r(t) + F(I(t))
%\end{align}%where:
%\(\tau\) is the time constant of the neuron,
%\(-r(t)\) represents the decay of the firing rate over time,
%\(I(t)\) is the total input to the neuron at time \( t \) and
%\(F(I)\) is a non-linear function that converts the input \( I(t) \) into an output firing rate.

% The Computation through Dynamics (CTD) framework assumes that the brain processes sensory information and generates behavior through neural dynamics, including representations of continuous-valued variables like space and direction~\cite{vyas2020}.

%Examples abound.

%When \(d = 2\) and \(F(\cdot)= \max(0,\cdot)\) is the threshold nonlinearity, we can build a line continuous attractor as follows.
%First, through positive feedback, \(\vW = [0, 1; 1, 0]\) and no bias \(\vb = \mathbf{0}\), we can create a continuous attractor, i.e., \(\dot{\vx} = 0\) on the \(x_{1} = x_{2} \geq 0\) half-line, and surrounding attractive flow (Fig.~\ref{fig:ublabla}A left).
%We refer to it as an \textbf{unbounded line attractor (UBLA)}.
%For any point on the line attractor, linearization results in eigenvalues \(0\) and \(-2\), corresponding to the zero flow and attractive flow respectively.
%When \(\vW\) is perturbed, the null eigenvalue can easily become non-zero and the continuous line attractor disappears.
%If it becomes negative, the system bifurcates to a stable fixed point at the origin (Fig.~\ref{fig:ublabla}A bottom).
%However, if it becomes positive (Fig.~\ref{fig:ublabla}A top), \emph{the resulting flow diverges to infinity along the diagonal}.
%Corresponding to the divergent flow, the backpropagating gradient over time exponentially grows in magnitude, thus rendering gradient descent impractical without truncation in time.
%
%any slight imperfections cause slow drift along the line This sort of approximate continuous attract or is what is found in real networks, including those trained by the learning
%``a manifold of stable fixed points'' \citep{Seung2000}.

\subsection{Theoretical models of ring attractors}\label{sec:ras}
% Furthermore, an invariant ring manifold with fixed points on it has been observed in finite dimensional low-rank attractors which converge to a ring attractor in the infinite unit limit~\citep{mastrogiuseppe2018,beiran2021}.
% We show some examples of such continuous attractor approximations in Fig.~\ref{fig:lara_bifurcations}B.
% Similarly, this has been observed in ring attractors with a finite number of units \citep{Noorman2022}, see also Fig.~\ref{fig:bio_rings}A.

%\subsubsection{Angular integration networks}\label{sec:hd}
For circular variables such as the goal-direction (e.g. for home navigation and working memory for communication	 in bees \citep{frisch1993dance}) %home navigation + dance in bees: the angle of the waggle phase relative to gravity reflects the angle of the food source relative to the sun
%duration of the waggle phse reflects the distance to the food source from the hive
or head-direction, the temporal integration, and working memory functions are naturally solved by a ring attractor (continuous attractor with a ring topology) \citep{kim2017ring,kim2019generation,turner2017angular,turner2020neuroanatomical,hulse2020mechanisms,taube2003persistent,taube2007head,angelaki2020head,fisher2022flexible}.
Other examples include
integration of evidence for continuous perceptual judgments, e.g. a hunting predator that needs to compute the net direction of motion of a large group of prey \citep{esnaola2022flexible}.
In this section, we investigate the bifurcations of various implementations of continuous attractors.
Continuous attractor network models of the head direction are based on the interactions of neurons that (anatomically) form a ring-like overlapping neighbor connectivity~\citep{zhang1996,Noorman2022,ajabi2023,vafidis2022,boucheny2005continuous,knierim2012,song2005angular,xie2002double}.
%Since continuous attractors are susceptible to noise and perturbations the precise representation of the head direction can in principle be disrupted easily.
Similarly to the line attractor, the ring attractor bifurcates with almost any perturbation. However, the resulting dynamics continue to follow a familiar pattern: they remain confined to a ghost manifold that closely approximates the original continuous attractor.


\ptitle{Piecewise-linear ring attractor model of the central complex}
Firstly, we discuss perturbations of a continuous ring attractor proposed as a model for the head direction representation in fruit flies~\citep{Noorman2022}.
This model is composed of \(N\) heading-tuned neurons with preferred headings \(\theta_{j} \in \{\nicefrac{2\pi i}{N}\}_{i = 1\dots N}\) radians (Sec.~\ref{sec:supp:headdirection}).
For sufficiently strong local excitation (given by the parameter \(J_{E}\)) and broad inhibition (\(J_{I}\)), this network will generate a stable bump of activity corresponding to the head direction.
This continuum of fixed points forms a \(N\)-sided polygon. %: a one-dimensional manifold homeomorphic to the circle.

\begin{figure}[tbhp]
     \centering
  \includegraphics[width=\textwidth]{bio_rings_ss}
       \caption{ Perturbations of different implementations and approximations of ring attractors lead to bifurcations that all leave the ring invariant manifold intact. For each model, the network dynamics is constrained to a ring which in turn is populated by stable fixed points (green) and saddle nodes (red).
       (A) 	Perturbations to the ring attractor \citep{Noorman2022}. The ring attractor can be perturbed in systems with an even number of fixed points (FPs) up to \(2N\) (stable and saddle points are paired).
       (B) 	Perturbations to a tanh approximation of a ring attractor \citep{seeholzer2017efficient}.
       (C) 	Different Embedding Manifolds with Population-level Jacobians (EMPJ) approximations of a ring attractor \citep{pollock2020}.
       }\label{fig:bio_rings}
\end{figure}

We evaluate the effect of parametric perturbations of the form \( \vW \leftarrow \vW + \vV\) with \(\vV_{i, j}\iidsample\mathcal{N}(0,\nicefrac{1}{100})\) on a network of size \(N = 6\) (forming an hexagon, see also Sec.~\ref{sec:supp:ring_perturbations}). %with \(J_{E} = 4\) and \(J_{I}=-2.4\) by identifying all bifurcations.
We found that the ring (consisting of infinite fixed points) can be perturbed into systems with between 2 and 12 fixed points (Fig.~\ref{fig:bio_rings}A).
As far as we know, this bifurcation from a ring of equilibria to a saddle and node has not been described previously in the literature.
The probability of each type of bifurcation was numerically estimated (Sec.~\ref{sec:supp:headdirection}).
%There are several additional co-dim 1 bifurcations with measure zero (Fig.~\ref{fig:meaure_zero_perturbations}).
Surprisingly, the number of fixed points is maintained throughout a range of perturbation sizes and hence depends only on the direction of the perturbation (Fig.~\ref{fig:noorman_ring_allfxdpnts_allnorm}).
%The space of possible perturbations in Sec. ~\ref{sec:imt} is very large. To be able to form an image of what happens in the theorem we will work out the effect of some examples of perturbations.


%In modeling neural dynamics, ensuring biological plausibility is paramount.
\ptitle{Bump attractor model}
A well-established approach, consistent with the principles observed in the fly head direction network \citep{kakaria2017}, involves utilizing a circulant matrix as the connectivity matrix\citep{benyishai1995theory,samsonovich1997path}.
This type of ring attractor network can support a stable ``activity bump'' that can move around the ring in correspondence with changes in head direction.
This can be accomplished with a connection matrix \(W\) with entries that follow a circular Gaussian function of \(i - j\) \citep{seeholzer2017efficient,redish1996coupled,goodridge2000,compte2000synaptic} (see for more details Sec.~\ref{sec:supp:goodridge}).
For finite-sized networks, the dynamics are constrained to an attractive invariant ring, covered with \(N\) stable fixed points for a network of size \(N\) (Fig.~\ref{fig:bio_rings}B).
For such networks the number of fixed points does change with the size of the perturbation (Fig.~\ref{fig:noorman_ring_allfxdpnts_allnorm}).
%Furthermore, our theory indicates that the reason why networks of generic spiking neurons with connectivity inferred from light-resolution microscopy gives rise to approximate ring attractors\citep{kakaria2017}, is because they are nearby ring attractors as dynamical systems.

\ptitle{Low-rank ring attractor model} %mastrogiuseppe, beiran, ostojic
Low-rank networks can be used to approximate ring attractors~\citep{mastrogiuseppe2018, beiran2021}.
In the limit of infinite-size networks, one can construct a ring attractor through a rank 2 network by constraining the overlap of the right- and left-connectivity vectors (see for more details  Sec.~\ref{sec:supp:lowrank}).
However, in simulations of finite-size networks with this constraint, the dynamics instead always converge to a small number of stable fixed points arranged on a ring (Fig.~\ref{fig:lara_bifurcations}B).

\ptitle{Embedding Manifolds with Population-level Jacobians} %pollock, jazayeri
Approximate ring attractors can be constructed by constraining the connectivity so that the networks Jacobian satisfies certain requirements for a ring attractor to exist~\citep{pollock2020} (see for more details Sec.~\ref{sec:supp:empj}).
The models fitted with this method also can contain an invariant ring manifold on which the dynamics contain stable and saddle fixed points (Fig.~\ref{fig:bio_rings}C).
It has been observed that approximate continuous attractor emerge in networks trained on sampled points with other methods as well \citep{darshan2022}.

\ptitle{Similarity between all bifurcations and approximations of continuous attractors}
In all discussed models of ring attractors, we verify that they suffer from the fine-tuning problem.
However, importantly, we also observe in all the systems  the existence of ghosts of the continuous attractor (either through bifurcation or from finite-size effects) in the form of an \emph{attractive invariant manifold}.
Therefore, while they are not strictly a continuous attractor in the mathematical sense, they are \textit{approximate} ring attractors in the sense that the fixed points and connecting orbits still form a circle.

Is this lawful degradation a universal phenomenon?
 And if so, how does it relate to the size of the perturbation? And what are the implications for the memory performance of these approximations? (Sec.~\ref{sec:theory}).
Do these approximations appear as natural solutions to the memory storage problem? (Sec.~\ref{sec:experiments}).
 And if so, how well do they generalize to longer time requirements? (Sec.~\ref{sec:generalization}).
%In conclusion, are approximate continuous attractors ubiquitous and well-behaved?
 Finally, are continuous attractors in practice still useful as an idealized model of how animals represent continuous variables? (Sec.~\ref{sec:converse}).




%In this paper, we lay out a new theory of general continuous-valued memory to answer the following questions:
%\begin{enumerate}
%%    \item Can we avoid exploding gradients under parameter perturbation?
%    \item Do we need to worry about the brittleness of the continuous attractor solutions in practice?
%    \item Are continuous attractors useful as a model of how animals might represent continuous variables?
%    %How can biological networks approximate the representation of continuous variables?
%\end{enumerate}
%\mpcomment{These are good questions to be embedded in the text. Probably not as bullet points? Not sure.}
%Our theory provides answers to both questions under mild assumptions in an architecture agnostic manner.
%Using Fenichel's invariant manifold theorem, we derive a sufficient condition for RNNs implementing continuous attractors to remain free of exploding gradients.
%Moreover, even after a bifurcation, these RNNs still approximately behave like the original continuous attractor (for a while).

%Using Fenichel's invariant manifold theorem, we derive a universality statement for the bifurcation landscape for one-dimensional continuous attractors:
% the behavior of one-dimensional continuous attractors can be effectively described by slow ring manifolds.
% These manifolds can either take the form of collections of stable fixed points and saddle points with connecting orbits or that of a limit cycle.
% This characterization provides a fundamental understanding of a continuous attractors bifurcation landscape.
%%what is uni statement?
%We verify our theoretical insights with numerical experiments.
%We show that all approximations, whether it is trained RNNs or hand designed contintuous attractors, are all to be described as slow manifolds with the same topology as the original continuous attractor,
%%punchlines
%even though they differ from each other in terms of the topology of the dynamics (e.g. number of fixed points).


%\section{Theory of gracefully degrading continuous attractors}\label{sec:theory}
\section{Theory of Approximate Continuous Attractors}\label{sec:theory}
In this section, we theoretically answer in an implementation-agnostic manner the degradation questions posed from the exploration.
To do so, we apply invariant manifold theory to continuous attractor models and translate the results for the neuroscience audience (see also Sec.~\ref{sec:supp:simplifications}).
%Our emphasis in this paper centers on investigating the distinctive properties of continuous attractors that prove essential for specific tasks, with a deliberate exclusion of considerations related to learning chaotic dynamics.
%\subsection{Invariant Continuous Attractor Manifold Theory}\label{sec:imt}
\subsection{Persistent Manifold Theorem}\label{sec:imt}
First, we argue that the lawful degradation into a system with a slow manifold is universally guaranteed (as long as the perturbation is small, and the continuous attractor was normally hyperbolic).
%We start by formulating an ODE implementing a continuous attractor in continuous time: \(\dot{\vx} = \vf(\vx)\).
Let \(l\) be the intrinsic dimension of the manifold of equilibria that defines the continuous attractor.
Given a perturbation \(\vp(\vx)\) to the ODE that induces a bifurcation,
\begin{align}\label{eq:perturbation:nonparam}
	\dot{\vx} &= \vf(\vx) + \epsilon\,\vp(\vx)
\end{align}where \(\uniformNorm{\vp(\cdot)} = 1\) and \(\epsilon > 0\) is the bifurcation parameter,
we can reparameterize the dynamics around the manifold with coordinates \(\vy \in \reals^{l}\) and the remaining ambient space with \(\vz \in \reals^{d - l}\).
To describe an arbitrary bifurcation of interest, we introduce a sufficiently smooth function \(\vg\) and \(\vh\), such that the following system is equivalent to the original ODE:
\begin{align}
    \dot{\vy} &=           \epsilon  \vg(\vy, \vz, \epsilon) \qquad \text{(tangent)}\label{eq:fenichel:flowtangent}\\
    \dot{\vz} &= \hphantom{\epsilon} \vh(\vy, \vz, \epsilon) \qquad \text{(normal)}\label{eq:fenichel:flownormal}
\end{align}where \(\epsilon = 0\) gives the condition for the continuous attractor \(\dot{\vy} = \mathbf{0}\).
We denote the corresponding manifold of \(l\) dimensions \(\manifold_{0} = \{(\vy,\vz) \mid \vh(\vy,\vz, 0) = \mathbf{0} \}\).

We say the flow around the manifold is \emph{normally hyperbolic}, if the flow normal to the manifold is hyperbolic: the Jacobians \(\nabla_{\vz} \vh\) evaluated at any point on the \(\manifold_{0}\) has \(d - l\) eigenvalues with their real part uniformly bounded away from zero, and \(\nabla_{\vy} \vg\) has \(l\) eigenvalues with zero real part.
%Roughly speaking, \(f\) is normally hyperbolic at \(N\) if the tangent map \(Tf\), restricted to the normal direction to \(N\), is hyperbolic (it expands and contracts complementary directions) and if it dominates the restriction of \(Tf\) to the tangent direction \(TN\) (that is, expansion and contraction in the tangent direction, if any, are weaker than those in the normal direction).
More specifically, for continuous attractors, the real part of the eigenvalues of \(\nabla_{\vz} \vh\) will be negative, representing sufficiently strong attractive flow toward the manifold.
Equivalently, for the ODE, \(\dot{\vx} = \vf(\vx)\), the variational system is of constant rank and has exactly \((d - l)\) eigenvalues with negative real parts uniformly away from zero and \(l\) eigenvalues with zero real parts everywhere along the continuous attractor.



\setlength\belowcaptionskip{-5ex}
\begin{SCfigure}[10][bthp]
  \centering
  \includegraphics[width=0.5\textwidth]{FenichelThm}
  \caption{
    Persistent manifold theorem applied to compact continuous attractor guarantees the flow on the slow manifold \(\manifold_{\epsilon}\) is invariant and continues to be attractive.
    The dashed line is a trajectory ``trapped'' in the slow manifold (which has the same topology as the continuous attractor). %change to forward invariant?
    %mention topology?
  }\label{fig:fenichel}
\end{SCfigure}

%When \(\epsilon > 0\), the continuous attractor bifurcates.
For any parameterization \(\vg\), \(\epsilon > 0\) induces a bifurcation of the continuous attractor.
What can we say about the fate of the perturbed system?
The continuous dependence theorem~\citep{Chicone2006} says that the trajectories will change continuously as a function of \(\epsilon\) without a guarantee on how quickly they change.
However, the topological structure and the asymptotic behavior of trajectories change discontinuously due to the bifurcation.
Yet, surprisingly, there is a strong connection in the geometry due to Fenichel's theorem~\cite{fenichel1971}.\footnote{The Persistent Manifold Theorem has been successfully applied previously in neuroscience\citep{Kopell1996,ermentrout2010}, for example to reduce the dimensionality of the Hodgkin-Huxley model\citep{rinzel1985excitation,ermentrout1986parabolic}.}
We informally present a special case due to~\citet{Jones1995}:

\setlength\belowcaptionskip{2ex}

\begin{theorem}[Persistent Manifold]\label{theorem:persistent}
Let \(\manifold_{0}\) be a connected, compact\footnote{See \citep{eldering2013normally} for results of persistence of noncompact invariant manifolds.}, normally hyperbolic manifold of equilibria originating from a sufficiently smooth ODE.
For a sufficiently small perturbation \(\epsilon > 0\), there exists a manifold \(\manifold_{\epsilon}\) diffeomorphic to \(\manifold_{0}\) and  invariant under the flow of Eq.~\eqref{eq:fenichel:flowtangent}-\eqref{eq:fenichel:flownormal}. %locally
%Moreover, \(\manifold_{\epsilon}\) has \(\mathcal{O}(\epsilon)\) Hausdroff distance to \(\manifold_{0}\) and has the same smoothness as \(g\) and \(h\).
\end{theorem}

The manifold \(\manifold_{\epsilon}\) is called the \emph{slow manifold} which is no longer necessarily a continuum of equilibria.
However, the  invariance implies that trajectories remain within the manifold except potentially at the boundary. %local
Furthermore, the non-zero flow on the slow manifold is slow and given in the \(\epsilon \to 0\) limit as \(\RN{\vy}{\tau} = \vg(c^{\epsilon}(\vy), \vy, 0)\) where \(\tau = \epsilon t\) is a rescaled time and \(c^{\epsilon}(\cdot)\) parameterizes the \(l\) dimensional slow manifold.
In addition, the stable manifold of \(\manifold_{0}\) is similarly persistent~\cite{Jones1995}, implying that the manifold \(\manifold_{\epsilon}\) remains attractive.
Finally, the persisting invariant manifold is very close in space to the original continuous attractor (see also Theorem ~\ref{theorem:originalpersistent}).
%\pscomment{\textbf{Suggestion}:implying the stability of the manifold \(\manifold_{\epsilon}\) is preserved.}

%Informally, normal hyperbolicity guarantees that the perturbations change the structure of the manifold before they can destroy the attractiveness of the manifold.

These conditions are met for the examples in Fig.~\ref{fig:lara_bifurcations} (see Sec.~\ref{sec:supp:fast_slow_form} for the fast-slow rerparametrization of the BLA
\footnote{As a technical note, for the theory to apply to a continuous piecewise-linear system, it is required that the invariant manifold is globally attracting~\cite{simpson2018}, which is also the case for the BLA (see also \citep{prohens2013canard,prohens2016slow} for a discussion of geometric singular perturbation theory for piecewise linear dynamical systems). So we consider systems that are at least continuous, but some extra conditions apply if a system is not differentiable (discontinuous systems are not considered in our theory).}).
As the theory predicts, BLA bifurcates into a 1-dimensional slow manifold (Fig.~\ref{fig:lara_bifurcations}, dark-colored regions) that contains fixed points and connecting orbits and is overall still attractive.
Furthermore, Fenichel's Persistent Manifold theorem explains the bifurcation structure of the theoretical models discussed in Sec.~\ref{sec:ras}.
Because continuous ring attractors are bounded, they persist as invariant manifold and remain attractive under small perturbations~\citep{wiggins1994}.


% region
%\subsection{Implications on Machine Learning}\label{sec:imp:ML}
%Extending the memory time constant of RNNs have long been an important area of research with much focus on random weights~\cite{Legenstein2007,Goldman2009,Toyoizumi2011,Kerg2019,Chen2018,Henaff2016,Rusch2021,arjovsky2016}.
%Various initializations for the recurrent weights have been proposed to help learning: initialization with the identity matrix \citep{le2015}, with a random orthogonal matrix \citep{saxe2014,Henaff2016}, with a unitary matrix \citep{arjovsky2016} and with a block diagonal weight matrix that creates a quasi-periodic system with limit cycles \citep{Sokol2019a}.
%However, despite the capacity to maintain representation of continuous quantities for arbitrary duration of time, continuous attractor mechanism has not been pursued in machine learning research because of its brittleness.
%The stochasticity in gradients inherited from the training data, regularization strategy, and multi-task learning objectives act as a perturbation on the recurrent dynamics, and continuous attractors break down even if it could be learned.
%Remedies emerged in machine learning to hard-code continuous-valued memory structures within the RNNs---e.g., the cell state in vanilla LSTM.
%However, our theory shows that the geometric structure of the manifold and the flow around the manifold play a critical role in enabling gradient descent learning of continuous attractors using standard methods such as backpropagation through time (BPTT)~\cite{Toomarian1991}.
%
%It is well known that asymptotic exploding gradients comes from positive Lyapunov exponents~\cite{Mikhaeil2022,Vogt2022,Engelken2023}.
%It has also been pointed out that bifurcations can cause arbitrarily large gradients~\cite{doya1993} as well as discontinuity in the Lyapunov spectrum~\cite{Park2023a}.
%These gradient propagation theories suggested that bifurcations should be avoided, including the continuous attractors.
%
%As far as we know, there is no architecture agnostic theory describing the loss landscape around RNN solutions.
%We remark that due to the singular nature of the center manifold that supports the continuous attractor, the usual analysis approach of linearization fails.
%\emph{Our theory successfully connects the invariant manifold theory and the gradient signal propagation theory in RNNs to describe two types of loss landscape around continuous attractor solutions.}
%In one case, when the theorem holds, the landscape is shallow in all directions due to (asymptotically) vanishing gradients induced by the attractor structure---we have the gracefully degrading continuous attractor.
%In the other case, we can find examples where the theorem does not hold, and the continuous attractor solution is at the boundary of network configurations with exploding gradients, meaning the loss landscape is very steep in some directions.
%While exploding gradients would prevent gradient descent to correct for deviations from the optima,
%for gracefully degrading ones, one can apply restorative forces via gradient descent to be in the vicinity of the brittle continuous attractor solution (see Sec.~\ref{sec:exp:maintaining}).
% endregion

\subsection{Fast-slow decomposition and the revival of continuous attractors}\label{sec:revival}
Second, we relate the flow tangent to the slow manifold to the size of the perturbation needed to bring it back to a continuous attractor.
Consider a behaviorally relevant timescale for working memory, for example, roughly up to a few tens of seconds.
If the dynamical system is orders of magnitude slower, for example, 1000 sec or longer, its effect is too slow to have a practical impact on the behavior that relies on the working memory mechanism.
This clear gap in the fast and slow time scales can be recast as \emph{normal hyperbolicity} of the slow manifold by relaxing the zero real part to a separation of time scales (reciprocal of eigenvalues or Lyapunov exponents).
In other words, the attractive flow normal to the manifold needs to be uniformly faster than the flow on the slow manifold.
%
By taking the limit of the slow flow on the manifold to arbitrarily long time constant (i.e., to zero flow), we achieve the reversal of the persistent manifold theorem.
\begin{prop}[Revival of continuous attractor]\label{prop:revival}
Let \(\manifold_{\epsilon}\) be a connected, compact, normally hyperbolic slow manifold  (as parametrized by Eq.~\eqref{eq:fenichel:flowtangent}-\eqref{eq:fenichel:flownormal}).
Further, assume that the real part of the eigenvalues of \(\nabla_{\vz} \vh\) are negative. % in the neighborhood of a continuous attractor.
Let the uniform norm of the flow tangent to the manifold be \(\|\dot \vy\|_{\infty}=\eta\).
There exists a perturbation with uniform norm at most \(\eta\) that induces a bifurcation to a continuous attractor manifold. % \(\manifold_{0}\) diffeomorphic to \(\manifold_{\epsilon}\).
\end{prop}

An explicit perturbation is derived in Sec.~\ref{sec:supp:proofprop1}.
This makes the uniform norm of the vector field on a (slow) manifold a useful measure to express the distance of an approximation to a continuous attractor.
Prop.~\ref{prop:revival}  can be extended to the case where the invariant manifold has additional dynamics to which the output mapping is invariant (see Theorem~\ref{thrm:near_ca}).
These systems can be perturbed onto a decomposable system where one of the subsystems has a slow flow.

\subsection{Relevance of dynamics on the memory performance of the slow manifold}\label{sec:attractor_bif}

Third, we relate the flow of the manifold (and, through Prop.~\ref{prop:revival}, the \emph{size} of the perturbation) to the memory error of the approximation in short-time scale.
We also discuss the implications of the theoretical insights on the memory error in the asymptotic time scale.

In the short-time scale the memory performance is bounded by the uniform norm of the flow tangent to the manifold.
Let \(\vx_{0} \in \manifold\), and \(\varphi = \vp(\cdot)\vert_{\manifold}\) be the vector field restricted to the manifold (following the notation in Eq.\ref{eq:perturbation:nonparam}).
The average deviation from initial memory \(\vx_{0}\) over time is bounded linearly (for a derivation, see Sec.~\ref{sec:supp:ub}):
\begin{align}\label{eq:distance:ub}
\frac{1}{\operatorname{vol}\manifold}\int_{\manifold}
%\frac{1}{T}\int_0^T
\abs{\vx(t, \vx_{0}) - \vx_{0}}\,
%\dm{t}
\dm{\vx_{0}}
\leq t\uniformNorm{\varphi}
\qquad
\text{(error bound)}
\end{align}
Note that this bound is the worst case and tighter for sufficiently small \(t \geq 0\).
Furthermore, for compact invariant manifolds the error is bounded by the diameter of the manifold and hence this bound becomes irrelevant for \(t\) large.

While the uniform norm gives insight on the short-time scale behavior of the perturbed ODE, we expect that working memory tasks generalize to longer durations~\citep{Park2023a}.
The long-time scale behavior on the slow manifold is dominated by the stability structure, i.e., the topology of the dynamics.
Although we have seen numerous topologies in Sec.~\ref{sec:critique}, dynamical systems theory says that they are fundamentally limited, especially in low dimensions (see for more details Sec.~\ref{sec:supp:persistence_extra}).
This is especially relevant as previous works have identified a low-dimensional organization of neural activity to explain the brain's ability to adapt behavioral responses to changing stimuli and environments \citep{beiran2023parametric,altan2023low,fanthomme2021low}.
%This resilience is fundamental for cognitive functions, learning, and memory.
%Thanks to Fenichel's Persistent Manifold Theorem, we can understand how these persistent pathways, or manifolds, survive even when the system undergoes  bifurcations.
%Essentially, this theorem assures us that the underlying structure of continuous attractors remains intact, maintaining a consistent topology despite the changes.
%The theory implies that in the space of dynamical systems, all systems in the neighbourhood of a continuous attractor all share the same topology for the attractive persistent invariant manifold and that the flow on the manifold is slow.
For a ring attractor, this implies that the stability structure of the invariant manifold is either
(1) composed of an equal number of stable fixed points and saddle nodes, placed alternatingly and with connecting orbits, or (2) a limit cycle.
These different strability structures have different generalization properties (see Sec.~\ref{sec:generalization}).
%saddle node with homoclinic orbit: with noise equivalent to a limit cycle
%For 2 dimensional attractors, stable fixed points and saddle nodes can coexist with limit cycles.
In more complex scenarios, such as a two-dimensional attractor, stable fixed points and saddle nodes can coexist with limit cycles, creating a rich tapestry of possible neural states.
%The persistence of hyperbolic tori in Hamiltonian systems was described in \citep{li2005persistence}.
%\pscomment{Technically, only parallel translations, and KAM gives a type of structural stability result.}
%For higher dimensional attractors, there is the possibility of chaos emerging, but we conjuncture that the bifurcations leading to such behavior are a measure zero set.
%\mpcomment{refer to later section on generalization performance?}

%No matter the dimension, the essential takeaway is that there will always be an attractive invariant manifold that mirrors the topology and dimension of the original continuous attractor.
%This means that, even with perturbations, neural states aren't drastically altered from where they would have been, maintaining a semblance of the original flow.
% the compact case: the invariant manifolds are diffeomorphic to the original one. So neural states arent too far off from where they would have been before the perturbation.
%\mpcomment{This subsection is very dry. Make it sound cool and also explain why these are important considerations in the context of neuroscience.}

\subsection{Implications on experimental neuroscience}\label{sec:implications}
Animal behavior exhibits strong resilience to changes in their neural dynamics, such as the continuous fluctuations in the synapses or slight variations in neuromodulator levels or temperature.
Hence, any theoretical model of neural or cognitive function that requires fine-tuning, such as the continuous attractor model for analog working memory, raises concerns, as they are seemingly biologically irrelevant.
Moreover, unbiased data-driven models of time series data and task-trained recurrent network models cannot recover such continuous attractor theories precisely.
Our theory shows that this apparent fragility is not as devastating as previously thought: despite the ``qualitative differences'' in the phase portrait, the ``effective behavior'' of the system can be arbitrarily close, especially in the behaviorally relevant time scales.
We show that as long as the attractive flow to the memory representation manifold is fast and the flow on the manifold is sufficiently slow, it represents an approximate continuous attractor.
Furthermore, our theory bounds the error in working memory incurred over time for such approximate continuous attractors.
Therefore, the concept of continuous attractors remains a crucial framework for understanding the neural computation underlying analog memory, even if the ideal continuous attractor is never observed in practice.
Experimental observations that indicate the slowly changing population representations during the ``delay periods'' where working memory is presumably required, do not necessarily contradict the continuous attractor hypothesis. 
Perturbative experiments can further measure the attractive nature of the manifold and their causal role through manipulating the memory content.



\section{Numerical Experiments on Task-optimized Recurrent Networks}\label{sec:experiments}

While our theory describes the abundance of approximate continuous attractors in the vicinity of a continuous attractor, it does not imply that there are no approximate solutions away from continuous attractors.
In this section, we use task-optimized RNNs as a means to search for plausible solutions for analog memory for a circular variable.
We train a diverse set of RNNs, and then identify the solution type of trained RNNs to gain insights into its performance, error patterns, generalization capabilities, and, ultimately, proximity to a continuous attractor.

Understanding the implemented computation in neural systems in terms of dynamical systems is a well-established practice~\citep{seung1996,sompolinsky1988,vyas2020}.
Researchers have analyzed task-optimized RNNs through nonlinear dynamical systems analysis~\citep{sussillo2013blackbox,sussillo2014,barak2013,driscoll2022,maheswaranathan2019universality,cueva2019headdirection,cueva2021continuous} and to compare those artificial networks to biological circuits~\citep{mante2013context,remington2018flexible,ghazizadeh2021slow}.
Previously, systematic analysis of the variability in network dynamics has been surveyed in vanilla RNNs, and variations in dynamical solutions over architecture and nonlinearity have been quantified~\citep{sussillo2013blackbox,mante2013context,yang2019task,maheswaranathan2019universality,driscoll2022}.
Furthermore, working memory mechanisms in RNNs had tendencies to find sequential or persistent representations through training depending on the task specification~\citep{orhan2019diverse}.
We therefore investigated to what extent training RNNs on a task uniquely determines the low-dimensional dynamics, independent of neural architectures.
We see that all the solutions have a slow invariant manifold, making all of them an instatiation of approximate continuous attractors.

\subsection{Model Architectures and Training Procedure}
Building upon prior work, which has shown their capabilities on such tasks, we trained RNNs\footnote{ See Sec.\ref{sec:supp:discretization} for how this relates to Eq.~\ref{eq:perturbation:nonparam}.} to either
(1) estimate head direction through integration of angular velocity \citep{cueva2019headdirection,cueva2021continuous}
or (2) perform a memory-guided saccade task for a ring variable \citep{wimmer2014,xie2022neural} (details in Sec.~\ref{sec:supp:tasks}).
We numerically minimized the loss \(L\) which was the mean squared error (MSE) between the network output \(\mathbf{y}(t)\) and the target output \(\hat{\mathbf{y}(t)}\):
\(L_{MSE} \coloneqq y_{i, t}-\hat y_{i, t})^{2}, \).
%We implemented a mask, \(m_{i, t}\), for modulating the loss with respect to certain time intervals.
We trained vanilla RNNs with ReLU, tanh, and rectified tanh activation functions. For each activation function and each network architecture (vanilla RNN, LSTM, and GRU), we trained 10 networks per hidden size: 64, 128, and 256.



 %method
\subsection{Numerical Fast-Slow Decomposition}\label{sec:fastslowmethod}
For each trained network, we find the slow manifold by integrating the autonomous dynamics, then selecting the parts of the trajectories that have speed slower than a threshold (Sec.~\ref{sec:supp:fastslowmethod}).
%We assess the intrinsic dimensionality of the manifold with the Method Of Moments algorithm \citep{amsaleg2018extreme, altan2021estimating}.
%https://scikit-dimension.readthedocs.io/en/latest/skdim.id.MOM.html#skdim.id.MOM
We identify the points on the invariant manifold from the simulated trajectories that are projected closest to a set of points in the output space relevant to the task after convergence, i.e. on the target ring.
We parametrize the one-dimensional invariant manifold by fitting a cubic spline with periodic boundary constraints to these points (black line in Fig.~\ref{fig:fastslow_decomposition}A and B).
Normal hyperbolicity is measured by a gap in the timescales of the system (measured from the eigenvalue spectrum of the linearization along points on the invariant manifold, Fig.~\ref{fig:fastslow_decomposition}E and F).
%Leveraging a fast-slow decomposition methodology, premised on the assumption of normal hyperbolicity within the system, we first identify the slow manifold through a velocity bound criterion, and then identify the slow flow by its asymptotic behavior.


We find the fixed points on the invariant ring by identifying regions where the direction of the flow flips (Sec.~\ref{sec:supp:fpf}).
%We assess the direction through projection onto the output mapping and calculate the angular flow.
Stable fixed points are identified where the flow directions are both pointing towards this flip point,
while saddle nodes are identified where they are pointing away (Fig.~\ref{fig:fastslow_decomposition}A,B.) % (filled and hollow circles Fig.~\ref{fig:fastslow_decomposition}A and B for stable and saddle fixed points respectively).



\subsection{Variations in the Topologies of the Networks}\label{sec:topologies}
To understand what solutions the RNNs found to solve the task, we investigate their memory mechanism.
For this, we dissect the dynamics of RNNs by segregating time scales to delineate the rapid flow normal to the slow manifold, and the flow on the manifold (Sec.~\ref{sec:fsdecmethod}).
All solutions involve a slow manifold with the same topology as the relevant variable in the task.
The different solutions are different in their asymptotic dynamics (Fig.~\ref{fig:fastslow_decomposition}).
The most often found solution is of the type \emph{fixed point ring manifold} (Fig.~\ref{fig:fastslow_decomposition}A and B).
These solutions are consistent with observations that persistent activity relies on discrete attractors \citep{brody2003, inagaki2019}.
Less commonly found topologies includes the slow torus around a repulsive ring invariant manifold (Fig.~\ref{fig:fastslow_decomposition}D).
This solution in turn is consistent with both observations of the possibility of using non-constant dynamics for memory storage~\citep{hirsch1995computing, Park2023a} and neuronal circuits underlying persistent representations despite time-varying activity~\citep{druckmann2012neuronal}.
All stability structures (fixed points and limit cycles) are mapped close to the target output circle (Figs.~\ref{fig:fastslow_decomposition_otuput},~\ref{fig:im_rep},~\ref{fig:im_all}).

We verify that this gap exists in the trained networks shown in Fig.~\ref{fig:fastslow_decomposition}B and C.
The largest  eigenvalue of the Jacobian is non-zero (the invariant manifold is not a continuous attractor), but it is removed from the second largest, see Fig.~\ref{fig:fastslow_decomposition}E and F.
This implies that normal hyperbolicity for these trained networks holds.

%Fig 4
\begin{figure}[tbhp]
  \centering
  \includegraphics[width=.95\textwidth]{fastslow_decomposition_normhyp_s}
  \caption{Slow manifold approximation of different trained networks on the memory-guided saccade and angular velocity integration tasks.
 (A1) Output of an example trajectory on the angular velocity integration task.
 (A2) Output of a example trajectories on the memory-guided saccade task.
 (B) An example fixed-point type solution to the memory-guided saccade task. Circles indicate fixed points of the system (filled for stable, empty for saddle) and the decoded angular value on the output ring is indicated with the color according to A1.
 (C) An example of a found solution to the angular velocity integration task.
 (D) An example slow-torus type solution to the memory-guided saccade task. The colored curves indicate stable limit cycles of the system.
 (E+F) The eigenvalue spectrum for the trained networks in B and C show a gap between the first two largest eigenvalues.
}\label{fig:fastslow_decomposition}
\end{figure}

\subsection{Universality amongst Good Solutions}
The fixed point topologies show a lot of variation across networks (Fig.~\ref{fig:fastslow_decomposition}A and B),
 much like the systems next to continuous attractors (Fig.~\ref{fig:lara_bifurcations} and Fig.~\ref{fig:bio_rings}).
Previously, it has been observed that fixed point analysis has a major limitation, namely, that the number of fixed points must be equal across compared networks  \citep{maheswaranathan2019universality}.
Our methodology effectively addresses and overcomes this limitation.
The universal structure of continuous attractor approximations as slow invariant manifolds allows us to connect different topologies as \textbf{approximate continuous attractors} (Sec.~\ref{sec:attractor_bif}).
For results on LSTMs and GRUs and a higher dimensional task, see Sec.~\ref{sec:supp:lstmgru} and  Sec.~\ref{sec:supp:davit}, respectively.







\section{Generalization Analysis}\label{sec:generalization}

Finally, in this section, we use the task-trained RNNs to study the relationship between the dynamics and the generalization capabilities.
When neuroscientists study neural computations in animals, the animals are trained on tasks with finite durations. As a result, it is unclear whether the animals learn the intended computation or merely a finite-time approximation of it. The same issue arises with trained neural networks.
We will investigate to what extent the networks have the intended memory necessary for perfect memory or whether they implement the task only on the timescale on which they have been trained.


The two possible approximations of a ring attractor discussed in Sec.~\ref{sec:critique} exhibit markedly distinct generalization characteristics.
Approximating the system as a limit cycle results in a memory trace that gradually diminishes over time.
Conversely, the alternative approximation's memory states are contingent upon the quantity and positioning of stable fixed points within the system.
We describe in detail the generalization properties of the trained networks\footnote{We tested all networks with a validation set and took a cutoff for the normalized MSE for the networks we consider for the analysis at -20 dB (Fig.~\ref{fig:angular_loss}B).} 
on the angular velocity integration task at two different time scales:  asymptotic and finite time.
%We tested all networks with a validation set and took a cutoff for the normalized MSE for the networks we consider for the analysis at -20 dB (Fig.~\ref{fig:angular_loss}B).

%\setlength\belowcaptionskip{-3ex}
\begin{figure}[tbhp]
  \centering
  \includegraphics[width=\textwidth]{angular_losses_s}
  \caption{The different measures for memory capacity reflect the generalization properties implied by the topology of the found solution.
  (A) The average accumulated angular error versus the uniform norm on the vector field (left and right hand side of Eq.~\ref{eq:distance:ub}, respectively), shown for finite time (time of trial length on which networks were trained, \(T_{1}\)) indicated with filled markers and at asymptotic time (with hollow markers).
  (B) The memory capacity versus the average accumulated angular error.
  (C) The number of fixed points versus average accumulated angular error, with the average distance between neighboring fixed points indicated in magenta.
  (D) The average accumulated angular error over time for two selected networks, indicated with the blue and orange arrows in (C).
  (E) Distribution of network performance measured on a validation dataset measured as normalized MSE (NMSE).
  }\label{fig:angular_loss}
\end{figure}
%\setlength\belowcaptionskip{2ex}


\ptitle{Finite time}
Aside from the angular velocity integration component of the task, the trained networks learn to store a memory of an angular variable.
We assess the performance of the network to store the memory of the angle over time.
The networks typically perform well on the timescale on which they have been trained \(T_{1} = 256\) time steps (Fig.~\ref{fig:angular_loss}C).
This loss is, as theoretically predicted (Prop.~\ref{prop:ub}), bounded by the uniform norm of the vector field on the invariant manifold, and therefore by the distance to a continuous attractor (Prop.~\ref{prop:revival}, Fig.~\ref{fig:angular_loss}A,  see Sec.~\ref{sec:supp:vf} and~\ref{sec:supp:ub}).


\ptitle{Asymptotic time}
Looking beyond the finite timescale provides valuable insights into the network's ability to store information.
For the asymptotic time scale, we capture the asymptotic behavior of the system by identifying to what part of the system evolves to in the limit \(t\rightarrow\infty\) (see also Sec.\ref{sec:supp:asymbehav}).
For a one-dimensional system, this will either be fixed points or a limit cycle.
For the fixed-point type solution, the maximal error is given by the maximal distance to the next fixed point, while for a limit cycle, this will always be \(\pi\).
We calculate the \emph{average fixed point distance} by takeing the average of the inter-fixed-point interval for each neighboring pair of fixed points.
We identify the location of the fixed points as described in Sec.~\ref{sec:fastslowmethod}.

Besides the maximal error that the network will make in the asymptotic limit, we can also characterize how many different angles the network would confuse in the asymptotic limit.
We construct a probability distribution of what part of state space we end up in an infinite time through the calculation of the size of the basins of attraction of stable fixed points as a proportion of the ring.
Finally, we characterize the \emph{memory capacity} of the network by calculating the entropy of this probability distribution (see Sec.~\ref{sec:supp:asymbehav}).


\ptitle{Error Accumulation in Neural Networks}
The mean accumulated error at the time at which the task was trained has an exponential relationship with the number of fixed points (Fig.~\ref{fig:angular_loss}A).
Furthermore, this error is bounded by the mean distance between stable and unstable fixed points (red dots in Fig.~\ref{fig:angular_loss}D).
This is another indication that the networks rely on a ring invariant manifold to implement the task.
Networks with different numbers of fixed points might have the same performance on the finite time scale (bounded by \(T_{1}\uniformNorm{\varphi}\)) %for the trial time \(T_{1}\) and the uniform norm of the vector field on the invariant slow manifold \(\uniformNorm{\varphi}\))
 but have vastly different generalization properties because they differ in the number of fixed points (Fig.~\ref{fig:angular_loss}C).


\section{Approximate Slow Manifolds are near Continuous Attractors}\label{sec:converse}
In Sec.~\ref{sec:ras}, we presented a theory of approximate solutions in the neighborhood of continuous attractors.
When are approximate solutions to the analog working memory problem near a continuous attractor?
We posit that there are four conditions (see for more detail Sec.~\ref{sec:condition_clarifications}):
\begin{enumerate*}[label=\textbf{(C\arabic*)}]
\item sufficiently smooth approximate bijection between neural activity and memory content,\label{converse:bijection}
\item the speed of drift of memory content is bounded,\label{converse:persistent}
\item robustness against state (S-type) noise, and\label{converse:Stype}
\item robustness against dynamical (D-type) noise.\label{converse:Dtype}
\end{enumerate*}
The correspondence implied by~\ref{converse:bijection} translates to the existence of a manifold in the neural activity space with the same topology as the memory content.\footnote{Note that effectively feedforward solutions~\citep{Goldman2009} do not satisfy~\ref{converse:bijection}.}
Persistence~\ref{converse:persistent} requires that the flow on the manifold is slow and bounded.
S-type robustness~\ref{converse:Stype} implies non-expansive flow, i.e., non-positive Lyapunov exponents.
Along with D-type robustness~\ref{converse:Dtype}, it implies the manifold is ``attractive'', and normally hyperbolic (see also Sec.~\ref{sec:persitencempliesnh}).

If these four conditions hold, for example for task-trained RNNs, there exists a smooth function with a uniform norm matching the slowness on the manifold such that when added, the slow manifold becomes a continuous attractor (Prop.~\ref{prop:revival} and Theorem~\ref{thrm:near_ca}, see also Sec.~\ref{sec:near_ca}).
For the RNN experiments, we added state-noise while training using stochastic gradient descent, satisfying~\ref{converse:Stype} and~\ref{converse:Dtype}.
We have also verified that~\ref{converse:persistent} holds (Fig.~\ref{fig:angular_loss}A).
Although the stochastic optimization cannot lead to \emph{the} continuous attractor solution, it gets to the neighborhood where all approximate solutions share the same main feature: having a subsystem that has a slow flow.




\section{Discussion}
%2 paragraph conclusion
Continuous attractors are highly prone to bifurcation under arbitrary perturbations unless they exist in special parametric forms.
This sensitivity to perturbations has traditionally made them seem unsuitable for modeling neural computation in noisy biological systems, according to conventional views on robustness.
Nevertheless, we demonstrate that continuous attractors can exhibit functional robustness, making them a crucial concept in explaining the neural computation underlying analog memory. 
We show that approximations of analog memory must possess slow manifold dynamics, placing them near continuous attractors within the space of dynamical systems. This implies that both biological systems and artificial neural networks only need to be near a continuous attractor to effectively solve problems in a manner similar to the ideal theoretical model, particularly on behaviorally relevant timescales.
Theoretical models that satisfy conditions~\ref{converse:bijection}-\ref{converse:Dtype} will be near continuous attractors (in parameter space).
%Nevertheless, the topological structure of the found solutions still determines how well a network will perform generalization for longer time scales.

%We investigated the neighborhood of the continuous attractor networks and analyzed diverse bifurcations in various example systems.
%Although the configurations corresponding to a continuous attractor are measure-zero in parameter space, we showed that when the persistent manifold theorem holds, the finite time behavior of the trajectories only slowly breaks down under perturbations.
%%For all perturbations and approximations there a diverse set of topologies.	
%However, all systems share an essential element: an attractive invariant slow manifold.
%%This empirical evidence strongly corroborates our proposed theory of analog memory.
%
%Training RNNs on tasks involving continuous variables similarly yields a wide range of solutions.
%%However, even within this variety, with our theory we can identify common structures that are different from the invariances highlighted previously in fixed point analyses.
%As the theory predicted, trained networks on a circular analog memory task all have an attractive ring invariant manifold, which allows for a more useful comparison between trained networks than existing fixed point analyses.
%This supports our theory that theoretical models that satisfy conditions~\ref{converse:bijection}-\ref{converse:Dtype} will be near continuous attractors (in parameter space).
%Nevertheless, the topological structure of the found solutions still determines how well a network will perform generalization for longer time scales.

%limitations
\paragraph{Limitations}
Although, we only explicitly describe the topology and dimensionality of the identified invariant manifolds for a representative set, the results indicate that most solutions have a ring invariant manifold with a slow flow on it.
Our numerical analysis relies on identifying a time scale separation from simulated trajectories. If the separation of time scales is too small, it may inadvertently identify parts of the state space that are only forward invariant (i.e., transient). However, this did not pose a problem in our analysis of the trained RNNs, which is unsurprising, as the separation is guaranteed by state noise robustness (due to injected state noise during training).

To identify solutions with a fast-slow decomposition only rely on the generalization property of the network (in terms of the normalized mean square error for ten times longer trials).
The possible solutions that the networks can find are restricted by having a linear output mapping.
For a nonlinear output mapping, a possible solution for analog memory is the quasi-periodic toroidal attractor, but this is not possible for a linear output mapping.
Our analysis methods can identify these limit sets, but we do not have a simple way to parametrize the two dimensional torus invariant manifold.







\begin{ack}
We would like to extend our heartfelt thanks to Sukbin Lim, Srdjan Ostojic and Daniel Durstewitz for their invaluable feedback and thoughtful suggestions which significantly refined the work.
This work was supported by NIH RF1-DA056404 and the Portuguese Recovery and Resilience Plan (PPR), through project number 62, Center for Responsible AI, and the Portuguese national funds, through FCT--- Funda\c{c}\~{a}o para a Ci\^{e}ncia e a Tecnologia--- in the context of the project UIDB/04443/2020.
\end{ack}


%\section*{References}
%The natbib package will be loaded for you by default. Citations may be author/year or numeric, as
%long as you maintain internal consistency. As to the format of the references themselves, any style is
%acceptable as long as it is used consistently

%References follow the acknowledgments in the camera-ready paper. Use unnumbered first-level heading for
%the references. Any choice of citation style is acceptable as long as you are
%consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
%when listing the references.
%Note that the Reference section does not count towards the page limit.
%\medskip

\small

\newpage
\bibliographystyle{abbrvnatm}
\bibliography{../cit,../catniplab}
%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\makeatletter
\if@preprint
\makeatother
\else
\newpage
\section*{NeurIPS Paper Checklist}

%%%% BEGIN INSTRUCTIONS %%%
%The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: {\bf The papers not including the checklist will be desk rejected.} The checklist should follow the references and precede the (optional) supplemental material.  The checklist does NOT count towards the page
%limit.
%
%Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:
%\begin{itemize}
%    \item You should answer \answerYes{}, \answerNo{}, or \answerNA{}.
%    \item \answerNA{} means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
%    \item Please provide a short (12 sentence) justification right after your answer (even for NA).
%   % \item {\bf The papers not including the checklist will be desk rejected.}
%\end{itemize}
%
%{\bf The checklist answers are an integral part of your paper submission.} They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.
%
%The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While ``\answerYes{}'' is generally preferable to ``\answerNo{}'', it is perfectly acceptable to answer ``\answerNo{}'' provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering ``\answerNo{}'' or ``\answerNA{}'' is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer \answerYes{} to a question, in the justification please point to the section(s) where related material for the question can be found.
%
%IMPORTANT, please:
%\begin{itemize}
%    \item {\bf Delete this instruction block, but keep the section heading ``NeurIPS paper checklist"},
%    \item  {\bf Keep the checklist subsection headings, questions/answers and guidelines below.}
%    \item {\bf Do not modify the questions and only use the provided macros for your answers}.
%\end{itemize}
%

%%% END INSTRUCTIONS %%%


\begin{enumerate}

\item {\bf Claims}
    \item[] Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: The theoretical and experimental results match and we provide various examples that indicate that they generalize to other settings.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the abstract and introduction do not include the claims made in the paper.
        \item The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.
        \item The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.
        \item It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
    \end{itemize}

\item {\bf Limitations}
    \item[] Question: Does the paper discuss the limitations of the work performed by the authors?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: We report on the limitations of the analysis in the discussion section.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
        \item The authors are encouraged to create a separate ``Limitations'' section in their paper.
        \item The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
        \item The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
        \item The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
        \item The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
        \item If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
        \item While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
    \end{itemize}

\item {\bf Theory Assumptions and Proofs}
    \item[] Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: We provide proofs in the supplemental and provide assumptions about the applicability of the theoretical results.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include theoretical results.
        \item All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
        \item All assumptions should be clearly stated or referenced in the statement of any theorems.
        \item The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
        \item Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
        \item Theorems and Lemmas that the proof relies upon should be properly referenced.
    \end{itemize}

    \item {\bf Experimental Result Reproducibility}
    \item[] Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: We report on all the analysis step decisions that might affect the results.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
        \item If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
        \item Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
        \item While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example
        \begin{enumerate}
            \item If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.
            \item If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.
            \item If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).
            \item We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
        \end{enumerate}
    \end{itemize}


\item {\bf Open access to data and code}
    \item[] Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?
    \item[] Answer: \answerNo{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: Although we do not provide open access to the data and code, we strive to make it available as soon as possible.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that paper does not include experiments requiring code.
        \item Please see the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
        \item While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
        \item The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
        \item The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
        \item The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
        \item At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
        \item Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
    \end{itemize}


\item {\bf Experimental Setting/Details}
    \item[] Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: These are reported in detail in the Supplemental.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
        \item The full details can be provided either with the code, in appendix, or as supplemental material.
    \end{itemize}

\item {\bf Experiment Statistical Significance}
    \item[] Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?
    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: We do not consider error bars in the paper.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item The authors should answer ``Yes'' if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
        \item The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
        \item The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
        \item The assumptions made should be given (e.g., Normally distributed errors).
        \item It should be clear whether the error bar is the standard deviation or the standard error of the mean.
        \item It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96\% CI, if the hypothesis of Normality of errors is not verified.
        \item For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
        \item If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
    \end{itemize}

\item {\bf Experiments Compute Resources}
    \item[] Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: Yes, we have reported the computer resources in the Supplemental.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
        \item The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
        \item The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
    \end{itemize}

\item {\bf Code Of Ethics}
    \item[] Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics \url{https://neurips.cc/public/EthicsGuidelines}?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: We have read the NeurIPS Code of Ethics and make sure to preserve anonymity.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
        \item If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
        \item The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
    \end{itemize}


\item {\bf Broader Impacts}
    \item[] Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?
    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: The paper is very theoretical and therefore, we do not expect any direct societal impact in the forseeable future.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that there is no societal impact of the work performed.
        \item If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
        \item Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
        \item The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
        \item The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
        \item If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
    \end{itemize}

\item {\bf Safeguards}
    \item[] Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?
    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: We do not rely on any pretrained language models, image generators, or scraped datasets.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper poses no such risks.
        \item Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
        \item Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
        \item We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
    \end{itemize}

\item {\bf Licenses for existing assets}
    \item[] Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: Yes, we rely on PyTorch and we cite it.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not use existing assets.
        \item The authors should cite the original paper that produced the code package or dataset.
        \item The authors should state which version of the asset is used and, if possible, include a URL.
        \item The name of the license (e.g., CC-BY 4.0) should be included for each asset.
        \item For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
        \item If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, \url{paperswithcode.com/datasets} has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
        \item For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
        \item If this information is not available online, the authors are encouraged to reach out to the asset's creators.
    \end{itemize}

\item {\bf New Assets}
    \item[] Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?
    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: The paper does not release new assets.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not release new assets.
        \item Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
        \item The paper should discuss whether and how consent was obtained from people whose asset is used.
        \item At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
    \end{itemize}

\item {\bf Crowdsourcing and Research with Human Subjects}
    \item[] Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?
    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: The paper does not use any data from human subjects.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
        \item Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
        \item According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
    \end{itemize}

\item {\bf Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects}
    \item[] Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?
    \item[] Answer:  \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: The paper does not use any data from human subjects.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
        \item Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
        \item We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
        \item For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.
    \end{itemize}

\end{enumerate}
\fi

\newpage
\appendix
%\section{Appendix / supplemental material}
\section*{Supplemental Material}
\setcounter{section}{0}
\renewcommand{\thefigure}{S\arabic{figure}} % figure numbering for supplement
\renewcommand{\thesection}{S\arabic{section}} % figure numbering for supplement

%RNNs are capable of learning complex patterns and relationships in the data, which makes them in particular useful as models for neural computations. This is achieved through the use of non-linear activation functions and the training of the network using backpropagation through time (BPTT).
%BPTT allows the network to adjust the weights of the connections between neurons based on the error signal that is propagated backwards through time.
%%This makes them suitable models for
%
%However, training RNNs using back-propagation through time to compute error-derivatives can be difficult.  Early attempts suffered from vanishing and exploding gradients \citep{kolen2001} and this meant that they had great difficulty learning long-term dependencies.
%Many different methods have been proposed for overcoming this difficulty.


\section{Intuitive definitions of several key concepts used in our paper}\label{sec:supp:simplifications}

\paragraph{Manifold:} A part of the state-space that locally resembles a flat, ordinary space (such as a plane or a three-dimensional space, but more generally
-dimensional Euclidean space) but can have a more complicated global shape (such as a donut).\label{sec:supp:manifold}
\paragraph{Invariant set:} A property of a set of points in the state space where, if you start within the set, all future states remain within the set and all past states belong to the set as well.\label{sec:supp:invset}
\paragraph{Normally Hyperbolic Invariant Manifold:} A behavior of a dynamical system where flow in the direction orthogonal to the manifold converges (or diverges) to the manifold significantly faster than the direction that remains on the manifold.\label{sec:supp:nhim}
\paragraph{Diffeomorphism:} A diffeomorphism is a stretchable map that can be used to transform one shape into another without tearing or gluing.
A differentiable map with differentiable inverse.\label{sec:supp:diffeomorphism}
\paragraph{\(C^{1}\) neighborhood of a \(C^{1}\) function:} A set of functions that are close to the function in terms of both their values and their first derivatives.\label{sec:supp:c1neighborhood}
\paragraph{Compact Set:} A set where every sequence of points has a subsequence that converges to a point within the set. Intuitively, it means the set is closed and bounded, making it ``finite'' in a certain sense.\label{sec:supp:compactset}
\paragraph{Connecting Orbit:} A trajectory in a dynamical system that connects two different equilibrium points or periodic orbits.
 Specifically, a heteroclinic orbit connects distinct equilibrium points.\label{sec:supp:connectingorbits}

%\subsection{Visualization of geometric intuition of closeness in model space to behavioral similarty correspondence}
%\ascomment{TODO: we will include a subsection with a visual illustration of our claims, aiming to tap into human geometric intuition. In this visualization, we show how models that behave almost like a perfect analog memory system correspond to a volume of models in the space of dynamical systems.}




\newpage
\section{The bounded line attractor}\label{sec:supp:bla}
In order to demonstrate the implications of the theory of the persistence of  continuous attractors, we rigorously test the predictions of the theory on the stability of the Bounded Line Attractor (BLA).
Our objective is to assess the practical implications of the theoretical findings of bounded continuous attractors in a small and tractable system, and second, to contribute empirical evidence that can help refine and extend existing theoretical frameworks.

The BLA has a parameter that determines step size along line attractor \(\alpha\). Analogously as for UBLA, these parameters determine the capacity of the network.
The inputs push the input along the line attractor in two opposite directions, see below. The BLA needs to be initialized at \(\beta(1, 1)\) and \(\tfrac{\beta}{2}(1, 1)\), respectively, for correct decoding, i.e., output projection.
\begin{equation}\label{eq:bla}
\win = \alpha
\begin{pmatrix}
-1  &  1 \\
1  &  -1
\end{pmatrix}, \
\vW =
\begin{pmatrix}
0  &  -1 \\
-1  &  0
\end{pmatrix}, \
\wout = \frac{1}{2\alpha}
\begin{pmatrix}
1  \\  -1
\end{pmatrix}, \
\vb = \beta
\begin{pmatrix}
1 \\  1
\end{pmatrix}, \
\bout = 0.
\end{equation}
%\paragraph{Input}
Parameter that determines step size along line attractor \(\alpha\).
The size determines the maximum number of clicks as the difference between the two channels.
This pushes the input along the line ``attractor'' in two opposite directions, %what is the correct word for this type of invariant set?
see below.


The results from such low-dimensional system can be extended to higher-dimensional systems through reduction methods from center manifold theory.
On the center manifold the singular perturbation problem (as is the case for continuous attractors) restricts to a regular perturbation problem \citep{fenichel1979geometric}. Furthermore, relying on the Reduction Principle \citep{kirchgraber1990geometry}, one can always reduce all systems (independent of dimension) to the same canonical form, given that they have the same continuous attractor.


\subsection{Fast-slow form}\label{sec:supp:fast_slow_form}
First of all, we will show how to transform the BLA network to the slow-fast form in Eq.~\ref{eq:fenichel:flowtangent}-\ref{eq:fenichel:flownormal} to explicitly demonstrate that the theory applies to it.
To achieve this, we transform the state space so that the line attractor aligns with the \(y\)-axis.
So, we apply the affine transformation \(R_{\theta}(x-\frac{1}{2})\) with the rotation matrix \(R_{\theta} = \begin{bmatrix}\cos\theta &-\sin\theta\\\sin\theta&\cos\theta\end{bmatrix}= \frac{1}{\sqrt{2}}\begin{bmatrix}1 &1\\-1&1\end{bmatrix}\) where we have set \(\theta=-\frac{\pi}{4}\).
So we perform the transformation \(x\rightarrow x'= R_{\theta}(x-\frac{1}{2})\) and so we have \(x = R_{\theta}^{-1} x'+\frac{1}{2}\) with \(R_{\theta}^{-1} = R_{-\theta}\).
Then we get that
\begin{align}
R_{\theta}^{-1}\dot x' = \operatorname{ReLU}\left(W(R_{\theta}^{-1} x`+\frac{1}{2})+1\right)-R_{\theta}^{-1} x'-\frac{1}{2}.
\end{align}For a perturbed connection matrix \(W=\begin{bmatrix}\epsilon &-1\\-1&0\end{bmatrix}\) we get
\begin{align}
R_{\theta}^{-1}\dot x' &= \operatorname{ReLU}\left(\frac{1}{\sqrt{2}}\begin{bmatrix}\epsilon &-1\\-1&0\end{bmatrix}\left(\begin{bmatrix}1 &-1\\1&1\end{bmatrix} x`+\frac{1}{2}\right)+1\right)-\frac{1}{\sqrt{2}}\begin{bmatrix}1 &-1\\1&1\end{bmatrix} x'-\frac{1}{2}\\
\dot x' &=\begin{bmatrix}-1 &1\\1&1\end{bmatrix}\left(\frac{1}{2}\begin{bmatrix}\epsilon - 1 &-\epsilon - 1\\-1&1\end{bmatrix}x' + \frac{1}{2\sqrt{2}}\begin{bmatrix}\epsilon - 1 \\-1\end{bmatrix}+\begin{bmatrix}1 \\1\end{bmatrix}-\frac{1}{2}\begin{bmatrix}1 \\1\end{bmatrix}\right)-x'\\
\dot x' &=\left(\begin{bmatrix}-2 &0\\0&0\end{bmatrix}+\frac{\epsilon}{2}\begin{bmatrix}1 &-1\\-1&1\end{bmatrix}\right)x' + \frac{1}{2\sqrt{2}}\begin{bmatrix}\epsilon \\-\epsilon\end{bmatrix}
\end{align}
\subsection{Bifurcation analysis}
We will now identify all possible bifurcations from the BLA, to show that indeed all perturbations preserve the continuous attractor as an invariant manifold.

We consider all parametrized perturbations of the form \( \vW \leftarrow \vW + \vV\) for a random matrix \(\vV\in \mathbb{R}^{2 \times 2}\) to the BLA.
The BLA can bifurcate in the following systems, characterized by their invariant sets: a system with single stable fixed point, a system with three fixed points (one unstable and two stable) and  a system with two fixed points (one stable and the other a half-stable node) and a system with a (rotated) line attractor.
Only the first two bifurcations (Fig.~\ref{fig:lara_bifurcations}A) can happen with nonzero chance for the type of random perturbations we consider.
The perturbations that leave the line attractor intact or to lead to a system with two fixed points have measure zero in the parameter space.
%The types of perturbation with measure zero are codimension 2 bifurcations.
The perturbation that results in one fixed point happen with probability \(\frac{3}{4}\), while perturbations lead to a system with three fixed points with probability \(\frac{1}{4}\), see Sec.~\ref{sec:supp:probbla}.
The (local) %, in the case of the single fixed point)
 invariant manifold manifold is indeed persistent for the BLA and homeomorphic to the original (the bounded line).



\paragraph{Stabilty of the fixed point with full support}
We investigate how perturbations to the bounded line affect the Lyapunov spectrum.
We calculate the eigenspectrum of the Jacobian:
\begin{align*}
\det [W' -(1+\lambda)\mathbb{I}] &= (\epsilon_{11}-1-\lambda)(\epsilon_{22}-1-\lambda)-(\epsilon_{12}+1)(\epsilon_{21}+1)\\
&=\lambda^{2} - (2+\epsilon_{11}+\epsilon_{22})\lambda -\epsilon_{11}-\epsilon_{22}+\epsilon_{11}\epsilon_{22} -\epsilon_{12} - \epsilon_{21} - \epsilon_{12}\epsilon_{21}
\end{align*}
Let
\(u=- (2+\epsilon_{11}+\epsilon_{22})\)
and
\(v=-\epsilon_{11}-\epsilon_{22}+\epsilon_{11}\epsilon_{22} -\epsilon_{12} - \epsilon_{21} - \epsilon_{12}\epsilon_{21}\)

There are only two types of invariant set for the perturbations of the line attractor. Both have as invariant set a fixed point at the origin. What distinguishes them is that one type of perturbations leads to this fixed point being stable while the other one makes it unstable.




%Another implementation of a perfect integrator: one which is bounded.



%The parameters:
%\begin{equation}\label{eq:bla}
%\win = \alpha
%\begin{pmatrix}
%-1  &  1 \\
%1  &  -1
%\end{pmatrix}, \
%W =
%\begin{pmatrix}
%0  &  -1 \\
%-1  &  0
%\end{pmatrix}, \
%\wout = \frac{1}{2\alpha}
%\begin{pmatrix}
%1  &  -1
%\end{pmatrix}, \
%b = \beta
%\begin{pmatrix}
%1 \\  1
%\end{pmatrix}, \
%\bout = 0.
%\end{equation}%
%Needs to be initialized at \(\tfrac{\beta}{2}(1, 1)\) for correct decoding, i.e., output projection



\paragraph{Stability of the fixed points on the axes}
We perform the stability analysis for the part of the state space where \(Wx>0\).
There, the Jacobian is
\begin{equation}
J = -
\begin{pmatrix}
1  &  1 \\
1  &  1
\end{pmatrix}
\end{equation}
We apply the perturbation
\begin{equation}
W' =
\begin{pmatrix}
0  &  -1 \\
-1  &  0
\end{pmatrix}
+ \epsilon
\end{equation}with
\begin{equation}
\epsilon =
\begin{pmatrix}
\epsilon_{11}  &  \epsilon_{12} \\
\epsilon_{21}  &  \epsilon_{22}
\end{pmatrix}
\end{equation}
The eigenvalues are computed as
\begin{align*}
\det [W' -(1+\lambda)\mathbb{I}] &= (\epsilon_{11}-1-\lambda)(\epsilon_{22}-1-\lambda)-(\epsilon_{12}-1)(\epsilon_{21}-1)\\
&=\lambda^{2} + (2-\epsilon_{11}-\epsilon_{22})\lambda -\epsilon_{11}-\epsilon_{22}+\epsilon_{11}\epsilon_{22} +\epsilon_{12} + \epsilon_{21} - \epsilon_{12}\epsilon_{21}
\end{align*}
Let
\(u = 2-\epsilon_{11}-\epsilon_{22}\)
and
\(v=-\epsilon_{11}-\epsilon_{22}+\epsilon_{11}\epsilon_{22} + \epsilon_{12} + \epsilon_{21} - \epsilon_{12}\epsilon_{21}\)
\begin{equation}
\lambda = \frac{-u \pm \sqrt{u^{2} - 4v}}{2}
\end{equation}
Case 1: \(\operatorname{Re}(\sqrt{u^{2} - 4v})<-u\), then
\(\lambda_{1, 2}<0\)


Case 2:  \(\operatorname{Re}(\sqrt{u^{2} - 4v})>-u\), then
\(\lambda_{1}<0\) and \(\lambda_{2}>0\)


Case 3: \(v = 0\), then
\(\lambda=\tfrac{1}{2}(-u\pm u)\), i.e.,
\(\lambda_{1} = 0\) and  \(\lambda_{2}=-u\)
\begin{align}
\epsilon_{11} &= -\epsilon_{22}+\epsilon_{11}\epsilon_{22} + \epsilon_{12} + \epsilon_{21} - \epsilon_{12}\epsilon_{21}
\end{align}
We give some examples of the different types of perturbations to the bounded line attractor.
The first type is when the invariant set is composed of a single fixed point, for example for the perturbation:
\begin{equation}
\epsilon = \frac{1}{10}
\begin{pmatrix}
-2  &  1 \\
 1   &  -2
\end{pmatrix}
\end{equation}%See Figure~\ref{fig:bounded_lineattractor_allpert}, left upper.



The second type is when the invariant set is composed of three fixed points:
\begin{equation}
\epsilon = \frac{1}{10}
\begin{pmatrix}
1  &  -2 \\
 -2  &  1
\end{pmatrix}
\end{equation}
The third type is when the invariant set is composed of two fixed points, both with partial support.
\begin{equation}
b' =  \frac{1}{10}
\begin{pmatrix}
1 & -1
\end{pmatrix}
\end{equation}
The fourth and final type is when the line attractor is maintained but rotated:
\begin{equation}
\epsilon =  \frac{1}{20}
\begin{pmatrix}
1 & 10\\
10 & 1
\end{pmatrix}
\end{equation}
\subsubsection{Bifurcation landscape}\label{sec:supp:bifurcationlandscape}
We will now state our previous observations as a Theorem that characterizes the possible bifurcations of the BLA.
\begin{theorem}
All perturbations of the bounded line attractor are of the types as listed above.
\end{theorem}



\begin{proof}
We enumerate all possibilities for the dynamics of a ReLU activation network with two units.
First of all, note that there can be no limit cycle or chaotic orbits.

Now, we look at the different possible systems with fixed points.
There can be at most three fixed points \citep[Corollary 5.3]{morrison2024diversity}.
There has to be at least one fixed point, because the bias is non-zero.

%1 fixed point
General form (example):
\begin{equation}
\epsilon = \frac{1}{10}
\begin{pmatrix}
-2  &  1 \\
 1   &  -2
\end{pmatrix}
\end{equation}
One fixed point with full support:

In this case we can assume \(W\) to be full rank.
\begin{align*}
\dot x =
\relu\left[
\begin{pmatrix}
\epsilon_{11}  &  \epsilon_{12} \\
\epsilon_{21}  &  \epsilon_{22}
\end{pmatrix}
\begin{pmatrix}
x_{1}\\x_{2}
\end{pmatrix}
+
\begin{pmatrix}
1\\1
\end{pmatrix}
\right]
-
\begin{pmatrix}
x_{1}\\x_{2}
\end{pmatrix}
&=0
\end{align*}
Note that \(x>0\) iff \(z_{1}\coloneqq \epsilon_{11}x_{1} + (\epsilon_{12}-1)x_{2} - 1>0\). Similarly for \(x_{2}>0\).

So for a fixed point with full support, we have
\begin{equation}
\begin{pmatrix}
x_{1}\\x_{2}
\end{pmatrix}
=A^{-1}
\begin{pmatrix}
-1\\-1
\end{pmatrix}
\end{equation}with
\[A\coloneqq\begin{pmatrix}
\epsilon_{11}-1  &  \epsilon_{12}-1 \\
\epsilon_{21}-1  &  \epsilon_{22}-1
\end{pmatrix}.\]


Note that it is not possible that \(x_{1} = 0=x_{2}\).

Now define
\[
B\coloneqq A^{-1} = \frac{1}{\det A}
\begin{pmatrix}
\epsilon_{22}-1  &  1-\epsilon_{12} \\
1-\epsilon_{21}  &  \epsilon_{11}-1
\end{pmatrix}
\]
with \[\det A = \epsilon_{11}\epsilon_{22}-\epsilon_{11}-\epsilon_{22}-\epsilon_{12}\epsilon_{21}+\epsilon_{12}+\epsilon_{21}.\]

Hence, we have that \(x_{1}, x_{2}>0\) if \(B_{11}+B_{12}>0\), \(B_{21}+B_{22}>0\) and \(\det A >0\)
or if \(B_{11}+B_{12}<0\), \(B_{21}+B_{22}<0\) and \(\det A <0\).

This can be satisfied in two ways,
If \(\det A >0\), this is satisfied if \(\epsilon_{22}>\epsilon_{12}\) and \(\epsilon_{11}>\epsilon_{21}\),
while if \(\det A <0\), this is satisfied if \(\epsilon_{22}<\epsilon_{12}\) and \(\epsilon_{11}<\epsilon_{21}\).
This gives condition 1. %necessary condition (#1)



Finally, we investigate the condition that specify that there are fixed points with partial support.
%condition for no fixed points for which \(x_{i} = 0\) for i=1 or i=2 (necessary condiiton #2)
If \(x_{1} = 0\) then \((\epsilon_{22}-1)x_{2} + 1=0\) and \(z_{1}<0\).
From the equality, we get that \(x_{2}=\frac{1}{1-\epsilon_{22}}\).
From the inequality, we get  \((\epsilon_{12}-1)x_{2} + 1\geq 0\), i.e. \(\frac{1}{1-\epsilon_{12}}\geq x_{2}\).
Hence,
\begin{equation*}
\frac{1}{1-\epsilon_{12}}\geq\frac{1}{1-\epsilon_{22}}
\end{equation*}and thus
\begin{equation}\label{eq:condition2.1}
\epsilon_{22} \leq \epsilon_{12}.
\end{equation}
Similarly to have a fixed point \(x^*\) such that \(x_{2}^*=0\), we must have that
\begin{equation}\label{eq:condition2.2}
\epsilon_{11} \leq \epsilon_{21}.
\end{equation}
Equation~\ref{eq:condition2.1} and~\ref{eq:condition2.2} together form condition 2.


Then, we get the following conditions for the different types of bifurcations:
\begin{enumerate}
%2 fixed points
\item  If condition 1 is violated, but condition 2 is satisfied with exactly one strict inequality, there are two fixed points on the boundary of the admissible quadrant.
%what about the subconditions:
\item If condition 1 is violated, and only one of the subconditions of condition 2 is satisfied, there is a single fixed point on one of the axes.
\item If condition 2 is violated, there is a single fixed point with full support.
%what about the subconditions?
%3 fixed points
\item If both conditions are satisfied, there are three fixed points.
%what about the subconditions?
\end{enumerate}


We now look at the possibility of the line attractor being preserved.
This is the case if \(v = 0\).
It is not possible to have a line attractor with a fixed point off of it for as there cannot be disjoint fixed points that are linearly dependent \citep[Lemma 5.2]{morrison2024diversity}
\end{proof}




\subsubsection{Probability of bifurcation types}\label{sec:supp:probbla}
We will now calculate which proportion proportion of the bifurcation parameter space is results in the different bifurcation types.
The conditions  that result in three fixed points are
\begin{align*}
0 &< \epsilon_{11}\epsilon_{22}-\epsilon_{11}-\epsilon_{22}-\epsilon_{12}\epsilon_{21}-\epsilon_{12}-\epsilon_{21},\\
\epsilon_{22} &\leq \epsilon_{12},\\
\epsilon_{11} &\leq \epsilon_{21}.
\end{align*}Therefore, because
\begin{align*}
\epsilon_{22} &\leq \epsilon_{12},\\
\epsilon_{11} &\leq \epsilon_{21}.
\end{align*}we always have that
\begin{align*}
0 &< \epsilon_{11}\epsilon_{22}-\epsilon_{11}-\epsilon_{22}-\epsilon_{12}\epsilon_{21}-\epsilon_{12}-\epsilon_{21}.
\end{align*}This implies that this bifurcation happens with probability \(\frac{1}{4}\)  in a \(\epsilon\)-ball around the BLA neural integrator with \(\epsilon<1\).
We conclude that the single stable fixed point type perturbation happens with probability \(\frac{3}{4}\).


\subsection{Structure of the parameter space}
We will present the structure of the bifurcation space through a slice in which we fix  \(\epsilon_{11}\) and \(\epsilon_{12}\).
First, we summarize which conditions result in which bifurcation in Table~\ref{tab:bifs}.
We derive that the local bifurcation in this slice  has the structure as shown in Fig.~\ref{fig:blaparameterspace}.
\begin{table}[H]
\caption{Summary of the conditions for the different bifurcations.}\label{tab:bifs}
\centering
\bgroup
\def\arraystretch{1.52}
\begin{tabular}{|c||c|c|c|c|c|}
\hline
& 1FP (full) 		& 1FP (partial) & 3FPs & 2FPs & LA  \\\hline \hline
C1 & \cmark	 	& \xmark 	 & \cmark & \xmark & \xmark \\\hline
C2 & \xmark 		& only Eq\ref{eq:condition2.1} or~\ref{eq:condition2.2}  	 & \cmark & \cmark& \xmark \\\hline
\end{tabular}
\egroup
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{bla_parameter_space}
  \caption{A slice of the parameter space of the BLA for a fixed \(\epsilon_{11}\) and \(\epsilon_{12}\). %The different bifurcations can be found
  }\label{fig:blaparameterspace}
\end{figure}



\subsection{Smoother activation functions}
It is well-known that activation functions (\(\sigma\) in  Eqs.~\ref{eq:RNN:discrete} and~\ref{eq:RNN:continuous}), which can take many forms, play a critical role in propagating gradients effectively through the network and backwards in time \citep{jagtap2023,ramachandran2017,hayou2019}.
Activation functions that are \(C^{r}\) for \(r\geq 1\) are the ones to which the Persistence Theorem applies.
The Persistence Theorem further specifies how the smoothness of the activation can have implications on the smoothness of the persistent invariant manifold.
For situations where smoothness of the persistent invariant manifold is of importance, smoother activation functions might be preferable, such as the Exponential Linear Unit (ELU)\citep{clevert2015} or the Continuously Differentiable Exponential Linear Units (CELU) \citep{barron2017}.








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%noisy learning with irnn, ubla and bla

%\subsection{Linear temporal integration task}\label{sec:task:continuous-clicks}
%Given a sequence of scalar input, the job of the network is to accumulate the values over time and report the final value at a later time.
%In the context of perceptual decision-making, subjects can be trained to perform the Poisson clicks task where they have to count the differing number of sensory stimulus events from the left and right side and report the side~\cite{brunton2013}.
%A linear integrator as a continuous attractor is a natural solution to such a task.
%We generalize the clicks to have associated continuous-values for the training of RNNs to discourage discrete counting solutions.
%
%We used discrete time representations over \(T\) time bins and the stimulus encoded as difference of two non-negative values:
%\begin{align}
%    I_{t,i} &= m_{t,i} \cdot u_{t,i}
%             \qquad & t=1,\dots, T, \ i=1,2 &&\text{(continuous clicks)}\label{eq:input}
%    \\
%    O^\ast_{t} &= \sum_{s=0}^{t} \left(
%        I_{s,1} - I_{s,2}
%        \right)
%            \qquad  & t=1,\dots, T &&  \text{(desired output)}\label{eq:output}
%\end{align}%where \(m_{t, i}\) are independent Bernoulli random variables with probability \(0.2\) and \(u_{t, i}\) are independent random variables with uniform distribution on the unit interval.
%We used mean squared error (MSE) of the 1-dimensional output over time as the loss function over all time bins.
%We used \(T = 100\) time bins per trial unless specified otherwise.
%The gradients were computed in batch mode with \(1024\) randomly generated trials.
%% The main challenges of this task are (1) addition and subtraction and (2) maintain the accumulated clicks for an extended period of time.
%
%% We designed a simple integration task where one of the optimal solutions is the continuous attractor. % <== not necessarily true!
%
%%The inputs for the clicks task are incoming clicks in two channels (\(K = 2\)) and are given as follows.
%%The non-zero inputs follow a Poisson distribution with given rates.
%%At such time points, an input is sampled from \([0, 1]\).
%%The task is to output the difference between the two inputs in the two channels.
%
%%A noisy version of the task also includes noise added to a part of the input, without alternation to the target output, i.e., the noise added to the system should be ignored by the system.
%
%\subsubsection{RNN solutions to the linear integration task}\label{sec:rnn:integration}
%We use vanilla RNN implementations with the standard parameterization:
%%An RNN \citep{elmanFindingStructureTime1990} consists of a \(N \times N\) transition matrix \(W\), an \(L \times N\) decoder matrix \(\wout\) (where \(L\) is the output dimension), a \(N \times K\) encoder matrix \(\win\) (where \(K\) is the input dimension), and a bias \(b\) for the hidden state and \(\bout\) for the output.
%% If either the output or input is categorical, \(M\) (respectively \(N\)) is the number of classes, and we use a one-hot representation.
%%As the RNN ingests a sequence, at each timestep it updates to a hidden state \(h\), and using the hidden state and the decoder matrix, produces outputs \(y\):
%\begin{equation}
%  \begin{aligned}
%	\vx_t &= \sigma(\win \vI_t + \vW \vx_{t-1} + \vb) \label{eq:RNN:discrete}\\
%	O_t &= \wout \vx_t + \bout
%  \end{aligned}
%\end{equation}%where \(\vx_{t} \in \reals^{d}\) is the hidden state, \(\vI_{t} \in \reals^{K}\) is the input,
%\(\sigma: \reals \to \reals\) an activation function which acts on each of the hidden dimension, and
%\(\vW, \vb, \win, \wout, \bout\) are parameters.
%Assuming an Euler integration with unit time step, the discrete-time RNN of \eqref{eq:RNN:discrete} corresponds to the ODE:
%\begin{align}
%    \dot{\vx} &= -\vx + \sigma(\win \vI + \vW \vx + \vb). \label{eq:RNN:continuous}
%\end{align}%%
%For tractable analysis, we consider \(2\) dimensional systems with ReLU activation. %We include another continuous attractor, the identity RNN, which is also a popular initialization for RNN training \citep{le2015}.
%We study the three different ReLU RNN implementations of a perfect integrator in a 2 dimensional system, the Identity RNN (iRNN), UBLA and BLA (we refer to the line attractors together as LA).
%These three networks have same norm in the recurrent matrix \(\vW\) but not close in the parameter space.
%On the original clicks task the UBLA and BLA networks count the click differences directly, while iRNN counts the clicks separately and then subtracts these representations through the output mapping.
%The behaviors of UBLA and BLA in the absence of stimulus are shown in Fig.~\ref{fig:ublabla}, while the behavior of the iRNN is trivial since there is no flow. These networks are defined as follows.
%
%\paragraph{Identity RNN~\citep{le2015}}
%\label{sec:ubpa,sec:iRNN}
%%This is an RNN with the identity matrix as its recurrent weights  with two hidden units.
%\begin{equation}\label{eq:irnn}
%\win =
%\begin{pmatrix}
%1  &  0 \\
%0 &  1
%\end{pmatrix}, \
%\vW =
%\begin{pmatrix}
%1  &  0 \\
%0  &  1
%\end{pmatrix}, \
%\wout =
%\begin{pmatrix}
%-1  \\  1
%\end{pmatrix}, \
%\vb =
%\begin{pmatrix}
%0  \\ 0
%\end{pmatrix}, \
%\bout = 0.
%\end{equation}%
%%The system has the whole state space as its invariant manifold.
%
%\paragraph{Unbounded line attractor}
%We formulate this implementation of a bounded integrator with a parameter that determines step size along line attractor \(\alpha\). Together with the parameters for the output bias \(\beta\) the parameters determine the capacity of the network. While the line attractor is unbounded from above, it only extends to the center from below.  The step size along line attractor \(\alpha\) determines the maximum number of clicks as the difference between the two channels; the capacity is \(\beta/\alpha\) number of clicks.
%\label{sec:ubla}
%\begin{equation}\label{eq:ubla}
%\win = \alpha
%\begin{pmatrix}
%-1  &  1 \\
%-1  &  1
%\end{pmatrix}, \
%\vW =
%\begin{pmatrix}
%0  &  1 \\
%1  &  0
%\end{pmatrix}, \
%\wout = \frac{1}{2\alpha}
%\begin{pmatrix}
%1  \\  1
%\end{pmatrix}, \
%\vb =
%\begin{pmatrix}
%0  \\  0
%\end{pmatrix}, \
%\bout = -\frac{\beta}{\alpha}.
%\end{equation}
%\subsubsection{Asymmetric loss landscape reflecting dynamics after bifurcation}\label{sec:asymmetricloss}
%To illustrate the effect of bifurcations from the continuous attractor solution, we take a 1-dimensional slice of the loss surface, see Fig.~\ref{fig:maintenance_h0}B.
%Specifically, we continuously vary one of the entries of the self-recurrent connection matrix: \(    \vW_{1, 1} \leftarrow \vW_{1, 1} + \Delta\). % with \(\Delta\in[-\tfrac{1}{10},\tfrac{1}{10}].\)
%At any \(\Delta \neq 0\), the continuous attractor disappears and the spontaneous dynamics of the networks show convergent and/or divergent behavior at exponential rates.
%Therefore, as the number of time steps in a trial increases, the error in the output also exponentially converge or diverge in a corresponding manner.
%As can be seen in Fig.~\ref{fig:maintenance_h0}B, for UBLA and iRNN, \(\Delta > 0\)  perturbations shows exponentially increasing loss and corresponds to an exploding gradient dynamical system.
%In all other cases, including all perturbations of BLA, leads to vanishing gradient, hence the loss is bounded.
%Note also the high curvature of the loss landscape around the optimal solution indicating that the slow manifold may only be maintained in a small neighborhood around the optimal solution, especially for the LAs.

%
%\subsubsection{Maintaining a neural integrator}\label{sec:exp:maintaining}
%The theory of persistent invariant manifolds for compact continuous attractors suggests that the BLA should have bounded gradients (unlike UBLA and iRNN) and hence it should be easier to maintain it in the presence of noise.
%To investigate the differential effect of stochastic gradient descent (SGD) on the three neural integrator models, we performed three learning experiments using the continuous-valued click integration task.
%The input and output are defined as in Eqs.~\ref{eq:input} and~\ref{eq:output} with \(I_{t, i}=0\) for \(t = 11,\dots, T\).
%%T=100,500,1000
%We investigate the effects of perturbations of the recurrent matrix on the learning of the parameters during gradient descent starting from the perfect solutions to the task.  Gradient step were taken with a fixed gradient step size \(\lambda\) (learning rate).
%%MSE at last time step
%%For SGD, the output of the network over \(T\) steps was taken to calculate the loss based on the mean squared error (MSE) over a batch of 1024 trials.
% We set \(\alpha = 1\) and \(\beta = 20\) in Eq~\ref{eq:ubla} and~\ref{eq:bla}. The hidden state at the start of a trial is a learnable parameter.
%
%In the first experiment, Gaussian random noise is injected to all parameters inducing a constant diffusion of the parameters, which emulates the biological synaptic variability.
%This type of noise is directly applied to the weights as \( \vW \leftarrow \vW + \vV\) with \(\vV_{i, j}\sim\mathcal{N}(0,\sigma)\). To dissociate the effect of misadjustment from gradient descent and external perturbation, we measured the effect of a single perturbation on the learning dynamics.
%Fig.~\ref{fig:maintenance_h0}C shows that for all networks, gradient descent (with constant learning rate, chosen from a grid search) was able to counter the diffusion.
%%The distribution of MSE for the task can be used as proxy for the misadjustment from the optimal solution.
%BLA and UBLA with learning have superior misadjustment compared to iRNN and compared to perturbations without learning, while the BLA has the broadest range of learning rates that are optimal and far away from exploding gradients (Fig.~\ref{fig:maintenance_h0}A).
%BLA has a slight advantage in terms of a smaller spread of MSE compared to UBLA.
%The invariant manifold of the BLA is persistent throughout learning in many cases, see~\ref{sec:supp:learning} and Fig.~\ref{fig:vfs1}. However, the gradients are not pointing towards the BLA but to one of the bifurcations of the BLA (see Supp~Fig.~\ref{fig:wdn_mse_trajectories_F1}, Fig.~\ref{fig:vfs5} and Supp. Fig.~\ref{fig:vfs30}). We determine the alignment of the gradient as the cosine similarity of the gradient step with the vector in recurrent parameter space that points towards the initial parameters at every gradient step and use a cutoff of a maximum deviation of 45\textdegree\  as aligned gradients with the optimal solution direction.
%iRNN often finds a different optimum (it settles at a part of the state space that is at a non-zero distance from the initial recurrent matrix and bias (Fig.~\ref{fig:maintenance_h0}D and Fig~\ref{fig:vfs1}).
%UBLA can stay close to the initial solution for a small enough learning rate (Fig.~\ref{fig:maintenance_h0}D and E) and maintains a slower flow than the BLA (Fig.~\ref{fig:speeds}).
%
%
%%The noise level \(\sigma\) was chosen individually for the three networks as follows.
%We calculated the loss on a batch of inputs for various noise levels \(\sigma\) for all three noise types (Fig.~\ref{fig:matching_noise_3types_cont}).
%We chose a matched noise level per integrator that corresponded to a set average loss averaged over 200 weight perturbations (see also in Sec.~\ref{sec:supp:matching}).
%This way of matching noise level to induce the same loss should be a universal approach to be able to compare the performance of different networks.
%
%%description of finding optimal learning rate
%For the matched noise level, we find the optimal learning rate for each network separately.
%The optimal learning rates for the input-type noise experiments were chosen from a set of values
%(\(\{(1 + j\frac{1}{4}))10^{-i}\}_{i = 4,\dots 10, j = 1, 2,3}\)))  based on best performance of the task, measured as mean MSE of the last ten epochs averaged over ten runs.
%The slow manifold that is created after perturbations provides gradients that can counteract parameter diffusions for all networks (on short trials), even for the ones that have the potential for exploding gradients (Fig.~\ref{fig:maintenance_h0}A and C).
% %new conclusions
%We use the normed difference to the initial parameters at every gradient step as proxy for the misadjustment from the optimal solution (Fig.~\ref{fig:maintenance_h0}D and E).
% We further show that all networks converge to a different (from the initialization), non-optimal, solution as they settle in a regime in parameter space that has a higher norm difference with the initial parameters of the neural integrator in ten different runs with the same random seed for the noise for the three integrators (Fig.~\ref{fig:maintenance_h0}D and E).
% We conclude therefore that, in practice, it is difficult to maintain any of the continuous attractors.
%
%%gradientdistribution
%%Then we looked for networks that diverged due to exploding gradients which we identify as having a loss of 1 or higher.
%Note that exploding gradients can be seen for UBLA manifested as the bimodal distribution of the gradients in Fig.~\ref{fig:maintenance_h0}F. This does lead to faster divergence (for lower learning rate) but has on the other hand the benefit of providing useful gradients to maintain the (local) solution around the optimal solution, which explains the superior performance at the optimal learning rate for the UBLA (Fig.~\ref{fig:maintenance_h0}A), on this timescale for the trial that we investigated.
%Also in the presence of input and internal noise the UBLA has a higher tendency to have exploding gradients for lower learning rates, see Fig.~\ref{fig:all_lrs_vs_mses}.
%%This is because the noise sometimes pushes the UBLA to the regime with divergent dynamics which leads to exploding gradients, which then causes the weights to make larger jumps.
%%The fact that the iRNN has a smaller range of gradients explains exploding gradients at a higher learning rate for iRNN and the need for a higher learning rate for optimal maintenance of the solution and is explained by a more shallow loss landscape in the close vininity of the iRNN (Fig.~\ref{fig:maintenance_h0}C).
%We hypothesise that the negative effect of exploding gradients shows only for longer trials.
%
%
%\begin{figure}[H]
%  \centering
%  \includegraphics[width=\textwidth]{maintenance_h0_less}
%  \caption{
%  Comparing three continuous attractor solutions on the click integration task of \(T = 100\) time steps.
%(A) MSE distribution during learning with different learning rates.
%(B) Loss landscape is steeper around the attractors. The BLA has a bounded loss in its neighborhood.
%(C) MSE distribution during learning and in the presence of noise. Learning counteracts diffusion in all three-types of initializations.
%(D) All networks converge to local non-optimal solutions after a single perturbation in 30 gradient steps.
%(E) Distance of parameters to original during learning with noise (left) and without learning (right).
%(F) Gradient distribution at the beginning (upper) and end (lower) of trials.
%  }
%  \label{fig:maintenance_h0}
%\end{figure}


%\newpage
%\section{Internal and input noise and noise level matching}\label{sec:supp:matching}
%%Rationale: what is a universal measure of noise level
%%that is applicable to any dyn sys and makes comparisons of performance (under gradient descent) meaningful?
%
%
%We investigate the effects of two other types of noise on the learning of the parameters during gradient descent starting from the perfect solutions to the task.
%We investigated the effect of learning when perturbations are induced by the backpropagated gradient which is structured by the recurrent dynamics in the following two ways.
%The second type of noise is injected into the input \(x_{i, t}+\epsilon_{i, t}\) with \(\epsilon\sim\mathcal{N}(0,\sigma)\).
%To inject noisy gradients naturally, we added noise to the input to the first 10 time steps during the trial that were not integrated in the target output \(O_{t}^*\) (Eq.~\ref{eq:output}).
%The third type of noise is injected into the hidden state \(h_{i, t}+\epsilon_{i, t}\) with \(\epsilon\sim\mathcal{N}(0,\sigma)\) for \(t = 1,\dots, T\) and \(i = 1, 2\).
%
%\begin{figure}[thbp]
%     \centering
%    \includegraphics[width=\textwidth]{matching_noise_3types_cont}
%       \caption{For various values the loss was calculated for the three types of noise. The matched noise levels were chosen based on these curves.}
%         \label{fig:matching_noise_3types_cont}
%\end{figure}
%
%\newpage
%\section{Continuous attractor solutions in click integration tasks with noise in the weights}
%For the SGD, the last output of the network after \(T\) steps was taken to calculate the loss based on the mean squared error (MSE) over a batch of 1024 trials.
%
%\begin{figure}[thbp]
%  \centering
%  \includegraphics[width=\textwidth]{maintenance_T100}
%  \caption{
%  Comparing three continuous attractor solutions to the click integration task for a length of \(T = 100\) time steps.
%  (A) Effect of gradient descent in repairing the continuous attractor. RNNs without gradient descent (dashed line) are shown for reference. Box plots show distribution of the loss for the last 10 steps. Averages (thick lines) over 10 simulations (thin lines) are shown for each network.
%  (B) Changes to the recurrent parameters (matrix and bias), without (upper) and with (lower) learning (with the optimal learning rates). iRNN converges to a different solution.
% (C)  The distribution of the MSE for different learning rates. The dip in the MSE defines the optimal learning rate for each of the three neural integrators.
%    (D) Single parameter perturbation showing exploding gradients for iRNN and UBLA.
%  (E) Distribution of gradients shows bimodal distribution for UBLA. %Gradient distributions for all noise levels and learning rates. For some noise levels a bimodal distribution can be seen for the LAs, but exploding gradients seem to be absent for iRNN for low learning rates.
%    (F) Interleaved weight perturbations showing quick recovery for BLA and and slow for iRNN and UBLA.
%  }
%  \label{fig:maintenance}
%\end{figure}
%
%\begin{figure}[thbp]
%  \centering
%  \includegraphics[width=\textwidth]{maintenance_T1000}
%  \caption{
%  Comparing three continuous attractor solutions to the click integration task for a length of \(T = 1000\) time steps.
%  (A) Effect of gradient descent in repairing the continuous attractor. RNNs without gradient descent (dashed line) are shown for reference. Box plots show distribution of the loss for the last 10 steps.
%  (B) Changes to the recurrent parameters (matrix and bias), without (upper) and with (lower) learning (with the optimal learning rates). iRNN converges to a different solution.
% (C)  The distribution of the MSE for different learning rates. The dip in the MSE defines the optimal learning rate for each of the three neural integrators.
%    (D) Single parameter perturbation showing exploding gradients for iRNN and UBLA.
%  (E) Distribution of gradients shows bimodal distribution for UBLA.
%    (F) Interleaved weight perturbations showing quick recovery for BLA and and slow for iRNN and UBLA.
%  }
%  \label{fig:maintenance:long}
%\end{figure}
%
%\newpage
%\section{Stability of the neural integrators for different learning rates}
%
%\begin{figure}[thbp]
%     \centering
%    \includegraphics[width=\textwidth]{all_lrs_vs_mses_f1}
%       \caption{Distribution of MSE for the three noisy types for different learning rates.}
%         \label{fig:all_lrs_vs_mses}
%\end{figure}
%
%
%\newpage
%\section{Trajectories of the neural integrators in the recurrent network space}
%
%\begin{figure}[thbp]
%     \centering
%    \includegraphics[width=\textwidth]{wdn_mse_trajectories_F1}
%       \caption{Trajectories of learning in parameter space relative to the initial recurrent parameters. The LAs follow a trajectory that is orthogonal to the initial parameters, but that yet decreases the MSE.}
%         \label{fig:wdn_mse_trajectories_F1}
%\end{figure}
%
%
%\newpage
%\section{Influence of the different noise types on the found solutions for the neural integrators}
%
%\begin{figure}[H]
%     \centering
%    \includegraphics[width=\textwidth]{allnoise_contours}
%       \caption{Distribution of parameters for the three noise types for three noise levels. Because of relative scale of perturbation, iRNN is further away from the initial parameters with internal and input noise.
%        Depending on the level of the noise it performs better or worse than the LAs.}
%         \label{fig:interal_input_contours}
%\end{figure}
%
%
%\newpage
%\section{Changes to the neural integrators during learning}\label{sec:supp:learning}
%%Message: This is what happens to the different neural integrators during learning
%During learning, the various neural integrators undergo distinct changes. These alterations manifest differently depending on the specific integrator involved. Here are some examples of progresses of chaning slow manifold dynamics.
%
%\begin{figure}[H]
%     \centering
%    \includegraphics[width=\textwidth]{vfs_f100_gs1_optlrs}
%       \caption{The dynamics of the recurrent part of the integrators across learning with some example orbits (blue lines), stable (green) and unstable (red) fixed points. A gradient step is taken after every perturbation. Gradient steps 0, 1, 5, 10, 15, 20, 25 and 29 are shown. }
%         \label{fig:vfs1}
%\end{figure}
%
%\begin{figure}[H]
%     \centering
%    \includegraphics[width=\textwidth]{vfs_f100_gs5_optlrs}
%       \caption{The dynamics of the recurrent part of the integrators across learning with some example orbits (blue lines), stable (green) and unstable (red) fixed points. A gradient step is taken after every 5 perturbations. Gradient steps 0, 1, 5, 10, 15, 20, 25 and 29 are shown. }
%         \label{fig:vfs5}
%\end{figure}
%
%\begin{figure}[H]
%     \centering
%    \includegraphics[width=\textwidth]{vfs_f100_gs30_optlrs}
%       \caption{The dynamics of the recurrent part of the integrators across learning with some example orbits (blue lines), stable (green) and unstable (red) fixed points. 30 gradient steps are take after a single perturbation. Gradient steps 0, 1, 5, 10, 15, 20, 25 and 29 are shown. }
%         \label{fig:vfs30}
%\end{figure}
%
%
%\begin{figure}[H]
%     \centering
%    \includegraphics[width=\textwidth]{speeds}
%       \caption{Speed along the invariant manifold during learning. For the iRNN a slice (the diagonal) is shown.
%       %UBLA has a lower
%       }
%         \label{fig:speeds}
%\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\section{Ring attractor}
%
%%Compact
%%No boundary: in- and outflowing
%%But not \(C^{1}\)!
%%
%%
%%discrete attractors as a trivial application?
%%Discrete attractors can be seen as an approximation to continuous attractor models in the context of head direction representation \citep{zhang1996}. The discrete nature of the representation makes it more robust to small fluctuations or disturbances in neural activity.
%%However, these system have fading memory: the system eventually forgets the past, since any difference between any two neural activations eventually tends zero as they both evolve to a global resting state.
%
%We will first of all look at a simple (non-biological) system that has a ring attractor to demonstrate the consequences of the Persistence Theorem.
%The system we will analyse is defined by the following ODE: \(\dot r = r(1 - r), \ \dot \theta = 0.\)
%This system has as fixed points the ring with radius one centered around zero, i.e., \((0, 0)\cup\{(1,\theta)\ |\ \theta\in[0, 2\pi)\}\).
%
%
%\begin{figure}[H]
%     \centering
%%  \includegraphics[width=.8\textwidth]{ring_perturbations_stream}
%    \includegraphics[width=.8\textwidth]{ring_perturbations_stream_2by2}
%       \caption{Perturbations to a simple implementation of a ring attractor all leave the invariant manifold intact. % (Left)  Examples of a local perturbation to the vector field through the addition of a bump to the vector field along the ring attractor.
%              (Leftmost) An example of a bump perturbation that results in the ring breaking up and becoming diffeomorphic to a line. %slow flow in hole?
%              (Left, middle) An example of a bump perturbation that maintains the ring structure, but deforms it locally.
%              %
%      % (Right) Examples of a global perturbation to the vector field through the addition of a small term to the connectivity matrix.
%       (Right, middle) A global perturbation that results in a system with four fixed points along the persistent invariant manifold. %The two saddle nodes (yellow dots) are connected to the stable fixed points (green dots) through connecting orbits.
%       (Rightmost)   A global perturbation that results in a limit cycle.}
%         \label{fig:ring_activity_pert}
%\end{figure}

%All perturbations maintain the invariant manifold.






\newpage
\section{Ring perturbations}\label{sec:supp:ring_perturbations}


To computationally investigate the neighborhood of recurrent dynamical systems that implement continuous attractors, we investigate 5 RNNs that are known a priori to form 1 or 2 dimensional continuous attractors.
%We consider two topologically distinct temporal integration tasks: (i) linear integration, and (ii) angular integration.
%For all experiments we used single precision floating point arithmetic and PyTorch.

%\subsection{Local and global perturbations}
%Definition of a bump perturbation
We define a local perturbation (i.e., a change to the ODE with compact support) through the bump function \(\Psi(x) = \exp\left(\frac{1}{\|x\|^{2} - 1}\right)\) for \(\|x\|<1\) and zero outside, by multiplying it with a uniform, unidirectional vector field. All such perturbations leave at least a part of the continuous attractor intact and preserve the invariant manifold, i.e. the parts where the fixed points disappear a slow flow appears.

%Definition of a global perturbation
The parametrized perturbations are characterized as the addition of a random matrix to the ODE.





\subsection{Simple ring attractor}
%Ring attractor
We further analyzed a simple (non-biological)  ring attractor, defined by the following ODE: \(\dot r = r(1 - r), \ \dot \theta = 0\).
This system has as fixed points the origin and the ring with radius one centered around zero, i.e., \((0, 0)\cup\{(1,\theta)\ |\ \theta \in [0, 2\pi)\}\). % ChkTeX 9
We investigate bifurcations caused by parametric and bump perturbations of the ring invariant manifold (see Sec.~\ref{sec:supp:ring_perturbations}), which is bounded and boundaryless.
All perturbations maintain the topological structure of the invariant manifold. %(Fig.~\ref{fig:lara_bifurcations}B).


%include fig?



\subsection{Heading direction network}\label{sec:supp:headdirection}
The networks proposed in \citep{Noorman2022} are composed of \(N\) heading-tuned neurons whose preferred headings \(\theta_{j}\) uniformly tile heading space, with an angular separation of \(\Delta\theta=\nicefrac{2\pi}{N}\) radians.
These neurons can be arranged topologically in a ring according to their preferred headings, with neurons locally exciting and broadly inhibiting their neighbors.
The total input activity \(h_{j}\) of each neuron is governed by:
\begin{equation}
\tau \dot h_{j} = -h_{j} + \frac{1}{N} \sum_{k} (W_{jk}^{sym} + v_{in} W_{jk}^{asym})\phi(h_{k})+c_{ff},     j = 1,\dots, N,
\end{equation}with
\begin{equation}
W_{jk}^{sym} = J_{I} + J_{E} \cos(\theta_{j} - \theta_{k})
\end{equation}, where \(J_{E}\) and \(J_{I}\) respectively control the strength of the tuned and untuned components of recurrent connectivity between neurons with preferred headings \(\theta_{j}\) and \(\theta_{j}\)
and \(v_{in}\) is an angular velocity input which the network receives through asymmetric, velocity-modulated weights \begin{equation}W^{asym} =\sin(\theta_{j} - \theta_{k}).\end{equation}
\paragraph{Fixed points}
In the absence of an input (\(v_{in}=0\)) fixed points of the system can be found analytically by considering all submatrices \(W_{\sigma}^{sym}\) for all subsets \(\{\sigma\subset [n]\}\) with\([n]=\{1,\dots, N\}\).
A fixed point \(x^*\) needs to satisfy
\begin{equation}
x^*= -(W_{\sigma}^{sym})^{-1}c_{ff}
\end{equation}and
\begin{equation}
x^*_{i}<0 \text{   for  	 } i\in\sigma.
\end{equation}
We bruteforce check all possible supports to find all fixed points.
We use the eigenvalues of the Jacobian to identify the stability of the found fixed points.


We evaluate the effect of parametric perturbations n a network of size \(N = 6\) with \(J_{E} = 4\) and \(J_{I}=-2.4\) by identifying all bifurcations (Fig.~\ref{fig:bio_rings}A).



\paragraph{Connecting orbits}
We approximate connecting orbits through numerical integration of the ODE intialized in close to the identified saddle points along the unstable manifold.


\newpage
\paragraph{Measure zero co-dimension 1 bifurcations}
Measure zero co-dimension 1 bifurcations of the ring attractor network fall into two types, see Fig.~\ref{fig:meaure_zero_perturbations}.

\begin{figure}[tbhp]
     \centering
    \includegraphics[width=.9\textwidth]{ring_n6_perturbations_schematic}
       \caption{Measure zero co-dimension 1 bifurcations of the ring attractor network \citep{Noorman2022}.}\label{fig:meaure_zero_perturbations}
\end{figure}


\paragraph{Measure zero co-dimension \(N\) bifurcation}
The limit cycle is the only bifurcation that we found that can be achieved on only a measure zero set of parameter values around the parameter for the continuous attractor.



\paragraph{Independence of norm of perturbation on bifurcation}
As we can see in Fig.~\ref{fig:noorman_ring_allfxdpnts_allnorm}, the topology of the system is maintained through a range of bifurcation sizes when the bifurcation direction is fixed.
\begin{figure}[tbhp]
     \centering
    \includegraphics[width=\textwidth]{noorman_ring_N6_pert_allfxdpnts_allnorms}
       \caption{Rows show the bifurcations resulting from perturbations from the matrices with the same direction in Fig.~\ref{fig:bio_rings}A but with different norms (columns). }\label{fig:noorman_ring_allfxdpnts_allnorm}
\end{figure}






\newpage
\subsection{Ring attractor approximation with tanh neurons}\label{sec:supp:goodridge}


We investigated the bifurcations around the approximate ring attractor constructed with a symmetric weight matrix for a tanh network  \citep{compte2000synaptic, seeholzer2017efficient}.
The functional form of \(W\) is the sum of a constant term plus a Gaussian centered at \(\theta_{i} - \theta_{j} = 0\):
\begin{equation}
W(\theta_{i} - \theta_{j}) = J^- + (J^+ - J^-) \exp\left[ -\frac{(\theta_{i} - \theta_{j})^{2}}{2\sigma^{2}} \right],
\end{equation}with the dimensionless parameter \(J^-\) representing the strength of the weak crossdirectional connections, \(J^+\) the strength of the stronger isodirectional connections,
 and \(\sigma\) the width of the connectivity footprint.


 Such ring attractor approximations are similar to the ones in \citep{goodridge2000, samsonovich1997path, redish1996coupled, tsodyks1995associative}. %Equation 9,12
However, some have other nonlinearities, e.g.,  the sigmoid is used in \citep{goodridge2000}.
Another line of related models can be found in \citep{Burak2009}, \citep{couey2013} and \citep{spalla2021continuous}.



\paragraph{Loss of function: Sensitivity of continuous attractors to perturbations}\label{sec:supp:boa}
We will show that there are differences at how well approximations perform at different timescales.
We measure how performance of different models for the representation of an angular variable drop as a function of perturbation size Fig.~\ref{fig:performance} through the memory capacity metric (Sec.\ref{sec:supp:asymbehav}).
For each perturbation size, we sample a low rank (rank 1,2 or 3) random matrix with norm equal to that perturbation size.
We determine the location of the fixed points through the local flow direction criterion as described in Sec.~\ref{sec:fastslowmethod}
This invariant manifold was found to be consistently close the the original invariant ring attractor.
The initial ring had \(2N\) fixed points (\(N\) stable, \(N\) saddle) on this invariant ring manifold.
The memory capacity of this initial configuration is \(N\log(N)\) for the \(2N\) uniformly spaced fixed points.

\begin{figure}[tbhp]
  \centering
  \includegraphics[width=\textwidth]{performance2}
  \caption{Degradation of performance across perturbation sizes. System behavior at the asymptotic time scales measured through memory capacity. }\label{fig:performance}
\end{figure}



\subsubsection{Mean field approaches}
Another line of models	\citep{miller2006analysis,wu2008dynamics,fung2010} also relies on connection weights with translational invariance
\begin{equation}
J(x, x') = \frac{A}{\sqrt{2\pi a}} \exp\left[ -\frac{(x - x')^2}{2a^2} \right]
\end{equation}
where \(J (x, x')\) is the neural interaction from \(x'\) to \(x\) and the ensemble of infinite neurons are lined up so that \(x\in (-\infty, \infty)\).

Weak random spatial fluctuations in the connection strength are to be expected when learning the coupling function with Hebbian plasticity.
In the 1D case, it is well known that the presence of such synaptic heterogeneity causes a drift of an input-induced activity pattern to one of a finite number of attractor positions which are randomly spread over representational space \citep{Renart2003,itskov2011short}.
Similarly, in 2D, spatial fluctuations in the connection strengths cause a slight perturbation of the bump shape \citep{wojtak2023robust}.
%infinite network size limit
Frozen stabilisation has been proposed as alternative method to construct a neural networks to self-organise to a state exhibiting (high-dimensional) memory manifolds with arbitrarily large time constants \citep{can2021emergence}. % The principle works by spontaneously freezing/slowing the dynamics of a (random) subpopulation of neurons, thereby creating a static background input which serves to stabilise the remaining population. 



\citep{kuhn2023information} analyzes an Ising network perturbed with a specially structured noise at the thermodynamic limit.
Although their analysis elegantly shows that the population activity of the perturbed system does not destroy the Fisher information about the input, they do not consider a scenario where the ring attractor is used as a working memory mechanism, it is rather used to encode instantaneous representation.
In contrast, our analysis involves understanding how the working memory content degrades over time due to the dynamics. We are not aware of any mean field analysis that covers this aspect.




%Networks
%  \citep{seeholzer2017efficient} %tanh
%  \citep{goodridge2000}		%sigmoid

%\subsection{Couey}\label{sec:supp:couey}

%\subsection{NEF}
%\citep{barak2021mapping}


\subsection{Ring attractor approximation with a low-rank network}\label{sec:supp:lowrank}
The networks consisted of \(N\) firing rate units with a sigmoid input-output transfer function \citep{mastrogiuseppe2018}:
\begin{equation}
\dot \xi_{i}(t) = - \xi_{i}(t) + \sum_{j = 1}^{N} J_{ij}\phi(x_{j}(t)) + I_{i},
\label{eq:1}
\end{equation}where \(x_{i}(t)\) is the total input current to unit \(i\),
\( J_{ij} = g\chi_{ij} + P_{ij}\) is the connectivity matrix,
\(\phi(x) = \tanh(x)\) is the current-to-rate transfer function, and \(I_{i}\) is the external, feedforward input to unit \(i\).
The random component \(g\chi\) is considered unknown except for its statistics (mean 0, variance \(g^{2}/N\)).
A general structured component of rank \(r\ll N\) can be written as a superposition of \(r\) independent unit-rank terms
\begin{equation}
P_{ij} = \frac{m_{i}^{(1)} n_{j}^{(1)}}{N} + \cdots + \frac{m_{i}^{(r)} n_{j}^{(r)}}{N},
\end{equation} and is in principle characterized by \(2r\) vectors \(m^{(k)}\) and \(n^{(k)}\).





%Rank-two structures with internal pairwise overlap
To approximate a ring attractor we can consider structured matrices where the two connectivity pairs \( m^{(1)} \) and \( n^{(1)} \), \( m^{(2)} \) and \( n^{(2)} \) share two different overlap directions, defined by vectors \( y_{1} \) and \( y_{2} \). We set:
\begin{align}
    m^{(1)} &= \sqrt{\Sigma^{2} - r_{1}^{2}} \, x_{1} + r_{1} y_{1}, \\
    m^{(2)} &= \sqrt{\Sigma^{2} - r_{2}^{2}} \, x_{2} + r_{2} y_{2}, \\
    n^{(1)} &= \sqrt{\Sigma^{2} - r_{1}^{2}} \, x_{3} + r_{1} y_{1}, \\
    n^{(2)} &= \sqrt{\Sigma^{2} - r_{2}^{2}} \, x_{4} + r_{2} y_{2},
\end{align}
where \( \Sigma^{2} \) is the variance of the connectivity vectors and \( r_{1}^{2} \) and \( r_{2}^{2} \) quantify the overlaps along the directions \( y_{1} \) and \( y_{2} \).

We keep the following parameters for the analysis:
\(\Sigma = 2\),
\(\rho = 1.9\) and
\(g=0.1\).


\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{N100_si2_rho1.9_g0_fp4.8.12_s}
\caption{Some examples of networks dynamics for sizes \(N = 10, 100, 1000\).}\label{fig:low_rank_examples}
\end{figure}




Our theory explains the phenomenon of the existence of a ring invariant manifold in these low-rank networks as follows. %MOVE TO LATER
We can think of the finite size realization as a small perturbation to the infinite size network on the reduced dynamics in the \(m^{1}, m^{2}\) plane (independent of the parameter \(g\) for the random part of the matrix)  (Fig.~\ref{fig:bio_rings}B).
%This implies that the ring attractor network
 For very small networks the ring structure is destroyed and only the plane persists as a slow manifold.




%EMPJ
\subsection{Embedding Manifolds with Population-level Jacobians}\label{sec:supp:empj}
We fit three networks with the Embedding Manifolds with Population-level Jacobians (EMPJ) method \citep{pollock2020}.

The networks are RNNs of \(N = 10\) neurons with a \(tanh\) activation function and \(\tau = 0.05\) time constant.
 To use EMPJ we need to specify our desired \(k\) fixed points and \(m\) eigenvectors and eigenvalues per fixed point.

To do this we choose \(k = 3\) points \(\{(x_{i}, y_{i})\}_{i = 1}^{N}\) in the 2D ring with radius \(r = 1\) that are equally spaced from \(0.1\) to \(2 \pi\) radians. Then we specify the vectors orthogonal to the ring in those points (\(V_{o} = \{(x_{i}, y_{i})\}_{i = 1}^{N}\), same as the points) and tangent (\(V_{t} = \{(-y_{i}, x_{i})\}_{i = 1}^{N}\)).
Finally we associate the negative eigenvalue \(-1/\tau\) to the orthogonal eigenvectors and different eigenvalues to the tangent eigenvectors. We use \(-1\), \(0\) or \(1\) depending on whether we want the points to be stable, center or unstable in the direction of the slow ring manifold.

Since these determined dynamics are only in a 2D plane, we use a random linear mapping \(D\colon\reals^{2}\rightarrow\reals^{N}\) to map the fixed points and the eigenvectors to a \(10\)-dimensional space. First, we sample a random from orthogonal matrix \(A\) from the \(O(N)\) Haar distribution and then we take the first two columns of this matrix to be \(D=[A]_{12}\).

EMPJ then returns a network parameters that satisfy these constraints. Furthermore, due to the particular solver used, that regularizes the magnitude of the parameters, we get that all the other eigenvalues not specified are also set to \(-\nicefrac{1}{\tau}\) (for details see \citep{pollock2020}).






\paragraph{Finding fixed points}
As we remark in Sec.~\ref{sec:empjnonrobust}, EMPJ networks are not robust to S-type noise, therefore we cannot apply our analysis of identifying the invariant set through the convergence criterion of numerically integrated trajectories.
We therefore find fixed points through the Newton-Raphson method.
We iteratively solve
\begin{align}
 \vJ(\vx_{i})\vd\vx_{i} = \vx_{i}
\end{align}
where \(\vJ(\vx_{i})\) is the Jacobian of the system at \(\vx_{i}\) and \(\vx_{i + 1}=\vx_{i}-\vd\vx_{i}\).
The iteration stops when \(|\vd\vx|<\delta\) for a tolerance threshold \(\delta\).
We initialize \(\vx_{0}\) on the invariant ring uniformly.
The maximum number of iterations was set to 10.000 and the tolerance level to \(\delta = 10^{-8}\).

\subsubsection{Lack of S-type robustness}\label{sec:empjnonrobust}
We remark that the resulting invariant manifold is not robust to S-type perturbations.
Although the fixed points that are constrained in the fitting procedure are attractive in all directions, some of the points along the ring might not be.
For on-manifold perturbations (in the plane in which the ring is embedded), S-type perturbations do lead to flow towards the invariant ring (Fig.~\ref{fig:empj_onoff_perturbation}A).
However, for (small) off-manifold perturbations, the trajectories typically diverge away from the invariant ring (Fig.~\ref{fig:empj_onoff_perturbation}B).
This indicates that the basin of attration is very small and hence this approximation is not robust to S-type noise.


All of the perturbations were sampled as
\(x(0) = x(0) + \eta\) with \(x(0)\in \operatorname{span}(D)\).
For the on-manifold perturbations \(\eta\in \operatorname{span}(D)\)  and \(\|\eta\|_{2} = 10^{-2}\).
For the off-manifold perturbations \(\eta\in \reals^{10}\) and \(\|\eta\|_{2} = 10^{-5}\).


\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{empj_onoff_perturbation_s}
\caption{Trajectories of the third network in Fig.~\ref{fig:bio_rings}C. Starting point in red, end of trajectory in blue.
(A) On-manifold S-type perturbations from the ring.
(B) An example of an off-manifold  S-type perturbation from the ring.
}\label{fig:empj_onoff_perturbation}
\end{figure}




\subsubsection{Higher dimensional manifolds}
We furthermore fit a torus and a sphere continuous attractor with the EMPJ.
The networks we used have \(N = 100\) neurons.
For finding fixed points with the Newton-Raphson method, we used 1.000  as the maximum number of iterations  and a tolerance level of \(\delta = 10^{-5}\).

\paragraph{Torus}
%3D
%4D
Figure \ref{fig:torus_empj} illustrates the stability structures of  the approximate torus attractor fitted with EMPJ.
The ratio of the radii of the two rings is adjusted for visualization purposes.
\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{torus_empj_ss}
\caption{The approximate torus attractor.
(A) Points initialized on a grid off of the torus (blue) converge onto the torus attractor (red).
(B) The found fixed points on the approximate torus (green: stable, red: saddle).
(C) The found fixed points projected onto the two 2D subspaces that defined the two rings of the torus.
}\label{fig:torus_empj}
\end{figure}



\paragraph{Sphere}
Fig.~\ref{fig:sphere_empj} illustrates the stability structures of  the approximate sphere  attractor fitted with EMPJ.
Similar to the torus,  points initialized off the sphere converge onto the sphere attractor (Fig.~\ref{fig:sphere_empj}).
\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{sphere_empj_ss}
\caption{The approximate sphere attractor.
(A)  Points initialized on a grid off of the sphere (blue) converge onto the torus attractor (red).
(B) The found fixed points on the approximate sphere (green: stable, red: saddle).
 The two subfigures show rotated versions of the location of the fixed points on the sphere.
}\label{fig:sphere_empj}
\end{figure}





\subsubsection{Other fixed point fitting methods}

Storing multiple continuous attractors has been worked out in \citep{battista2020capacity}, with a similar approximation as our theory suggests. The different stored patterns form a discrete approximation of the continuous map (resulting in quasicontinuous maps, which corrspond to the approximate contintuous attractors in our theory).


%\citep{darshan2022}




%\newpage
%%\subsection{Biswas}\label{sec:supp:biswas}
%
%
%
%
%\subsection{}
%Some concluding remarks




%\newpage
% \section{Infinite horizon computation in a bounded state space}\label{sec:inhoco}
% Both sequential and nearly persistent solutions are part of a spectrum that emerges naturally in trained networks under different conditions~\citep{orhan2019diverse}.
% %We describe the general implementational principles of robust, timing-independent (infinite horizon) neural computation.
% If a dynamical system is implementing a timing-independent (infinite horizon) computation, on a compact domain, the state will evolve to an attractor state inside the chain-recurrent recurrent set~\citep{conley1978}.
% This implies that the only possible implementation of a continuous valued memory that can be decoded linearly is the (approximate) continuous attractor.
%
%\begin{proof}
%
%
%\end{proof}


\newpage
\section{Persistence Theorem}\label{sec:supp:persistence_extra}
Understanding the long-term behavior of dynamical systems is a fundamental question in various fields, including mathematical biology, ecological modeling, and neuroscience.
 Fenichel's Persistence Theorem provides critical insights into the behavior of such systems, particularly in relation to the stability and persistence of invariant manifolds under perturbations.

Fenichel's Persistence Theorem extends the classical theory of invariant manifolds, offering conditions under which normally hyperbolic invariant manifolds persist despite small perturbations.
This theorem is particularly powerful in analyzing systems where perturbations are inevitable, providing a  framework for understanding how qualitative features of the system's dynamics are maintained.
In this section, we delve into the specifics of Fenichel's Persistence Theorem, outlining its key components, assumptions, and implications.


\paragraph{Invariance}
One of the main concepts in the Persistence Theorem is the notion of an invariant manifold.
Intuitively, this just means that trajectories stay inside the manifold for all time.
Local invariance is a bit more involved and allows for the possibility of leaving the manifold, but only through it's boundary.

\begin{definition}
A set \(\manifold\) is \emph{locally invariant} under the flow from Eq.~\ref{eq:fenichel:flowtangent}-\ref{eq:fenichel:flownormal} if it has neighborhood \(V\) so that no trajectory can leave \(\manifold\) without also leaving \(V\).
In other words, it is locally invariant if for all \(x \in \manifold, \varphi(x, [0, t] \subset V\) implies that  \(\varphi(x,[0, t]) \subset M\), similarly with \([0, t]\) replaced by \([t, 0]\) when \(t < 0\).
\end{definition}

\begin{definition}
A set \( S \) is said to be \emph{forward invariant} under a flow \( \varphi_{t} \) if for every point \( x \) in \( S \) and for all \( t \geq 0 \), the image of \( x \) under the flow at time \( t \), denoted \( \varphi_{t}(x) \), remains in \( S \). This can be written as:

\[ \varphi_{t}(x) \in S \quad \text{for all} \quad x \in S \quad \text{and} \quad t \geq 0. \]
\end{definition}

\paragraph{Small perturbation}
In the context of Fenichel's Persistence Theorem, a ``sufficiently small \( C^{r} \) perturbation of the vector field \( f \)'' refers to perturbations that are small in the \( C^{r} \) norm. The \( C^{r} \) norm measures the size of a function and its derivatives up to order \( r \).

Formally, let \( f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{n} \) be the original smooth vector field and let \( \tilde{f} \) be a perturbed vector field. The perturbation \( \tilde{f} \) is a sufficiently small \( C^{r} \) perturbation if the difference \( \tilde{f} - f \) has a small \( C^{r} \) norm. Mathematically, this can be expressed as:

\[ \left\| \tilde{f} - f \right\|_{C^{r}} < \epsilon, \]
where \( \epsilon \) is a small positive number, and the \( C^{r} \) norm is defined as:
\[ \left\| \tilde{f} - f \right\|_{C^{r}} = \max_{0 \leq k \leq r} \sup_{x \in \mathbb{R}^{n}} \left\| D^{k} (\tilde{f}(x) - f(x)) \right\|. \]
Which gives a constraint on how much each of the \(k\)-th derivatives \( D^{k} \) of the perturbed vector field can differ from the original.

In summary, a perturbation \( \tilde{f} \) is considered sufficiently small in the \( C^{r} \) sense if the difference between \( \tilde{f} \) and \( f \), along with their derivatives up to order \( r \), is uniformly small across the domain.
For \(C^{1}\) perturbations, this defines a \(C^{1}\)  neighborhood of functions, within which the persistence of the manifold is ensured by Fenichel's theorem.
For $r=0$, we get the uniform norm (defined as \(\|f\|)\infty\coloneqq \sup(|f|)\), see also Sec.\ref{sec:supp:ub}).


\paragraph{Closeness}
Finally, to formalize what is meant by that the persistent invariant manifold is very close to the original continuous attractor, we need a notion of distance between the manifolds.
\begin{definition}
Let \((M, d)\) be a metric space. For each pair of non-empty subsets \(X \subset M\) and \(Y \subset M\), the \emph{Hausdorff distance} between \(X\) and \(Y\) is defined as
\[
d_{H}(X, Y) \coloneqq \max \left\{ \sup_{x \in X} d(x, Y), \ \sup_{y \in Y} d(X, y) \right\},
\]
where \(\sup\) represents the supremum operator, \(\inf\) the infimum operator, and where
\[
d(a, B) \coloneqq \inf_{b \in B} d(a, b)
\]
quantifies the distance from a point \(a \in X\) to the subset \(B \subseteq X\).
\end{definition}

In conclusion, the Hausdorff distance provides a rigorous mathematical framework to quantify the ``closeness'' or ``similarity'' between two sets.




\subsection{Fenichel's Persistent Manifold Theorem}

This section will introduce the original Fenichel's Persistent Manifold Theorem, laying the groundwork for understanding how normally hyperbolic invariant manifolds persist under perturbations in systems with distinct time scales.
 By examining this foundational theorem, we can build a deeper understanding of the stability and behavior of complex dynamical systems.

In the study of dynamical systems, particularly those involving multiple time scales, understanding the behavior of solutions near invariant manifolds is crucial.
These manifolds often determine the long-term dynamics of the system and their stability properties.
 The Fenichel's Persistent Manifold Theorem provides a powerful framework for analyzing such systems by demonstrating the persistence of normally hyperbolic invariant manifolds under small perturbations. This theorem is particularly relevant in systems where variables evolve on different time scales--- typically referred to as ``slow'' and ``fast'' dynamics.
 By reformulating the system with a change of time-scale, we can explore how the dynamics on these manifolds behave, especially when perturbed.
Consider the system given by Equations~\ref{eq:fenichel:flowtangent}-\ref{eq:fenichel:flownormal}, which can be rewritten using a rescaled time variable, \(\tau\), to distinguish between the fast and slow dynamics as follows:
\begin{align}\label{eq:slow}
\begin{cases}
\epsilon x' &= g (x, y , \epsilon) \\
y'&= h(x, y, \epsilon)
\end{cases}
\end{align}
where  \('=\frac{d}{d\tau}\) and \(\tau=\nicefrac{t}{\epsilon}\).
 The time scale given by \(\tau\) is said to be fast whereas that for \(t\) is slow, as long as \(\epsilon\neq 0\) the two systems are equivalent.


%Assumptions
The functions \(g\) and \(h\)in Eq.~\ref{eq:fenichel:flowtangent}-\ref{eq:fenichel:flownormal} are both assumed to be \(C^{r}\) (for \(r>0\) on a set \(U \times I\) where \(U\subset\reals^{d}\) is open, and \(I\) is an open interval, containing \(0\).

Suppose that the set \(\manifold_{0}\)  is a subset of the set \(\{h(x, y, 0) = 0\}\) and is a compact manifold, possibly with boundary, and is normally hyperbolic relative to~\ref{eq:slow}.


\begin{theorem}[Theorem 1 in \citep{Jones1995}]\label{theorem:originalpersistent}
If \(\epsilon >0\), but sufficiently small, there exists a manifold \(\manifold_{\epsilon}\) that lies within \(\mathcal{O}(\epsilon)\) of \(\manifold_{0}\) and is diffeomorphic to \(\manifold_{0}\).
 Moreover it is locally invariant under the flow of Eq.~\ref{eq:fenichel:flowtangent}-\ref{eq:fenichel:flownormal}, and \(C^{r}\), including in \(\epsilon\), for any \(0<r< \infty\).
 Finally, \(\manifold_{\epsilon}\) has \(\mathcal{O}(\epsilon)\) Hausdroff distance to \(\manifold_{0}\) and has the same smoothness as \(g\) and \(h\).
\end{theorem}


If the invariant manifold \(\manifold_{0}\) is attractive, then the only way trajectories can escape the invariant set \(\manifold_{\epsilon}\) after perturbation, is in negative time through the boundaries.
This guarantees that the persistent manifold \(\manifold_{\epsilon}\) is still attractive.


%-  page 6: Although we have seen numerous topologies in Sec. 2, dynamical systems theory says that they are fundamentally limited, especially in low dimensions. - it would be super useful to add a reference, or even unpack this statement a little more
\subsection{Fundamental limitations of the topology of bifurcated continuous attractors}


\subsubsection{Flow on a line}
\begin{theorem}[One Dimensional Equivalence]
Two flows and in are topologically equivalent iff their equilibria, ordered on the line, can be put into one-to-one correspondence and have the same topological type (sink, source or semistable).
\end{theorem}

\subsubsection{Flow on a ring}
The flow on a ring can be described by the differential equation\citep{hirsch2013differential}:
\begin{equation}
\frac{d\theta}{dt} = f(\theta),
\end{equation}with \(\theta\in[0, 2\pi].\)

The simplest type of flow on a ring is the uniform circular flow, where each point moves with a constant angular velocity \(f(\theta)=\omega\).
If \(|f(\theta)|>0\) for all \(\theta\), there is a circular flow with a variable speed.

There is a fixed point for each unique \(\theta\) for which \(f(\theta)=0\).
The nature of the flow around these fixed points can be classified into:
\begin{itemize}
\item\textbf{Stable fixed points}: Points where the flow tends to as time progresses (\(f(\theta)>0\) for all \( \theta\in[0,\theta]\cap V\) and \(f(\theta)<0\) for all \( \theta\in[\theta, 2\pi]\cap V\) for some open ball \(V\) around \(\theta\)).
\item\textbf{Unstable fixed points}: Points from which the flow diverges (\(f(\theta)<0\) for all \( \theta\in[0,\theta]\cap V\) and \(f(\theta)>0\) for all \( \theta\in[\theta, 2\pi]\cap V\) for some open ball \(V\) around \(\theta\)).
\end{itemize}


For a detailed discussion of how bifurcations of a continuous attractors depend on the symmetry of the continuous attractor, see for example The Equivariant Branching Lemma (Lemma 1.31 in \citep{golubitsky2002symmetry}).
If the solutions are symmetric under rotations (a circular symmetry such as for a ring attractor).
As you change the bifurcation parameter, you find that new solutions appear that also respect this rotational symmetry.
The lemma tells you that these new solutions will align with specific symmetries (like different rotation angles), which are described by the irreducible representations of the symmetry group.



%\subsubsection{Flow on a torus}
%\ascomment{Comment on torus}


\subsection{Consequences to system identification}
Fenichel's Persistence Theorem has several significant implications for modelling and system identification in dynamical systems.
Because the theorem provides a guarantee that small perturbations in the system do not lead to significant changes in the qualitative behavior of the system,
we can be (slightly) wrong for example about the exact nonlinearity of a neuron's transform function.
For example, if neurons are only approximately ReLU the theory developed in \citep{biswas2022geometric} still holds (at the behaviorally relevant timescales).
More generally, when reconstructing computational system dynamics to understand how cognitive functions are implemented in the brain, our theory shows that small deviations in the identified system can still lead to behaviorally equivalent models for neural computation \citep{durstewitz2023reconstructing}.



\newpage
\section{Near Perfect Analog Memory Systems are close to Continuous Attractors}
%some intro comments
%definitions:
%%Near Perfect Analog Memory Systems
We will give some clarifications and proofs of the claims on systems near perfect analog memory systems.

\subsection{Revival of the continuous attractor from a slow manifold}\label{sec:supp:proofprop1}

We  provide a proof of Prop.~\ref{prop:revival} which is dependent on the \(\epsilon\)-Neighborhood Theorem\footnote{Not to be confused with the perturbation parameter \(\epsilon\).}, which we state here.

\begin{theorem}[\(\epsilon\)-Neighborhood Theorem\citep{guillemin2010differential}]
Let \( Y \subseteq \mathbb{R}^{n} \) be a smooth, compact manifold. Then for a small enough \(\epsilon > 0\),
each \( w \in Y_{\epsilon} \coloneqq \{ w \in \mathbb{R}^{n} \mid \exists y \in Y : \|y - w\| < \epsilon \} \)
has a unique closest point in \( Y \), denoted \( \pi(w) \).
Moreover, \( \pi: Y_{\epsilon} \to Y \) is a submersion with \( \pi|_{Y} = \text{id} \).
\end{theorem}

%Use Extension Lemma for Vector Fields on Submanifolds?

We now prove  Prop.~\ref{prop:revival}.
\begin{proof}%[Prop.~\ref{prop:revival}]
It is sufficient to show that there is a perturbation \(\vp\) that has zero flow off of \(\manifold_{\epsilon}\) but for which \(\vf+\vp = 0\) on \(\manifold_{\epsilon}\) for the full system \(\vf\)  as defined in Eq.~\ref{eq:perturbation:nonparam}.
Define
\[\vp(\vx) = \int_{\manifold_{\epsilon}} - \delta(\vx-\vy)\vf(\vy)\vd\vy,\]
 with the Dirac delta function \(\delta(\vx)=\delta(x_{1})\delta(x_{2})\dots\delta(x_{d})\).
It is then easy to check that \(\vf(\vx)+\vp(\vx)=0\) for all \(\vx\in\manifold_{\epsilon}\) and \(\vf(\vx)+\vp(\vx)=\vf(\vx)\) for all \(\vx\not\in\manifold_{\epsilon}\).
Hence, we have a continuous attractor at \(\manifold_{\epsilon}\).
%\end{proof}
%
%\begin{remark}
If smoothness is important, we can construct the following perturbation.
From the \(\epsilon\)-Neighborhood Theorem\citep{folland1999real}, we get that  there exists a smooth positive function \(\delta\colon \manifold_{\epsilon} \rightarrow \reals^+\),
 such that if we let \(N_{\delta}\) be the
\(\delta\)-neighborhood of \(\manifold_{\epsilon}\),
\[
M_{\epsilon}\coloneqq \{y  \in \reals^{n} : |y - x| < \delta(x) \text{ for some }  x \in M_{\epsilon}\},
\]
then each \(y\in N_{\delta}\) possesses a unique closest point \(\pi_{\delta}(y)\) in \(\manifold_{\epsilon}\) with the map \(\pi_{\delta}\colon \manifold_{\delta} \rightarrow \manifold_{\epsilon}\) being a submersion.

We can then define a bump function
\begin{equation}
\psi(y) =
\begin{cases}
\exp\left(-\frac{1}{1-(\pi_{\delta}(y)-y)^{2}}\right) &\text{  if  } y\in(-\delta-\pi_{\delta}(y), \delta-\pi_{\delta}(y))\\
0 & \text{  otherwise }
\end{cases}
\end{equation}
Then the perturbation
\[
\vp(\vy) = - \psi(\vy)\vf(\pi_{\delta}(\vy))
\]
is smooth and creates a continuous attractor at \(\manifold_{\epsilon}\).
\end{proof}

%Setup for case where smoothness matters:
%\begin{itemize}
%\item Define tubular neighbourhood
%\begin{itemize}
%    \item Define the normal bundle \(N_{X}\) of \(X\) in \(Y\).
%    \item Use the exponential map restricted to \(N_{X}\) to map small normal vectors to \(Y\).
%    \item Ensure the map is injective on a sufficiently small neighborhood around the zero section of \(N_{X}\).
%    \item The image of this neighborhood forms the tubular neighborhood \(N\).
%    \item Define a retraction map from \(N\) to \(X\) using the projection along normal fibers.
%\end{itemize}
%\item Use exponential map restricted to \(N_{X}\) to map small normal vectors to \(Y\) to define vector field (bump around CA)
%\begin{itemize}
%\item Scale this map with bump function
%\item
%\begin{equation}
%\psi_x^i(y) =
%\begin{cases}
%\exp\left(-\frac{1}{1-(x_i-y_i)^2)}\right) &\text{  if  } y_i\in(-\delta-x_i, \delta-x_i)\\
%0 & \text{  otherwise }
%\end{cases}
%\end{equation}%\end{itemize}
%\item Take a small enough neighbourhood such that
%\begin{itemize}
%\item Topology is unchanged
%\item How can we guarantee that no invariant manifolds, especially equilibira, are created next to the CA
%\end{itemize}
%\end{itemize}


\subsection{Output mapping} %move to next subsection?
This paper focuses on a linear output mapping for simplicity.
Errors can be minimized with a nonlinear mapping, if there is an output that is mapped off of the output manifold, we can always adjust the output mapping to correct for this, if the space of output mappings is general enough.
However, this can only be applied for errors off of the output manifold.
If there is memory degradation along the output manifold, it is not possible to choose another mapping that corrects for this error.
%prove?
Therefore, we choose a linear output mapping for our analysis.
In some cases, linear output mappings are found to support neural computation, for example for motion direction-discrimination \citep{yates2020simple}.


\subsection{Approximate solutions  to an analog working memory problem}\label{sec:condition_clarifications}
We will now discuss the conditions for when approximate solutions to an analog working memory problem are near a continuous attractor.
We consider approximate solutions to an analog working memory problem to be systems of the form~\ref{eq:RNN:discrete} or~\ref{eq:RNN:continuous} (in both cases following a linear decoder), which have a small memory error over time in output space.


%\paragraph{Approximate bijection}\label{sec:bijection}



\subsubsection{Robustness}
Noise, practically defined as unpredictable components of the system's behavior, comes from many sources.
The concept of S- and D-type noise is based on \citep{Park2023a}.


\paragraph{S-type robustness}\label{sec:stype}
S-type noise encapsulates reversible changes in the neural state such that the deterministic part of the dynamics itself remains unchanged.
Neural dynamics must be robust to perturbations and stimuli that push the neuronal activity away from the continuous attractor \citep{durstewitz2000neurocomputational}.


\paragraph{D-type robustness}\label{sec:persitencempliesnh}
%What counts as D-type robustness: persistence (+attractiveness)
D-type noise, in the space of recurrent network dynamics parameterized by the synaptic weights.
This corresponds to slight changes to the ODE, i.e. perturbations.
Previously, it robustness to D-type noise was considered to correspond to structurally stability \citep{Park2023a}.

Here we shortly discuss what we consider to be a necessary and sufficient condition for D-type robustness.
We can use the concept of Lipschitz persistence to define D-type robustness.
Ma\~{n}e showed that if an invariant manifold is Lipschitz persistent then it must be normally hyperbolic \citep{mane1978persistent}.
To understand the concept of Lipschitz persistence, we need to define the Lipschitz section and Lipschitz constant.

\begin{definition}
Let \(M\) be a \(C^{\infty}\) boundaryless manifold and \(V \subset M\) a \(C^{1}\) compact boundaryless submanifold.
Assume that \(M\) is a submanifold of \(\mathbb{R}^{n}\). Let \(NV\) be a \(C^{1}\) subbundle of \(TM|_{V}\) satisfying \(TV \oplus NV = TM|_{V}\).
If \(\eta\) is a section of \(NV\), define the Lipschitz constant of \(\eta\) by
\[
\mathrm{Lip}(\eta) = \sup \left\{ \frac{\|\eta(x) - \eta(y)\|}{\|x - y\|} \mid x, y \in V, x \neq y \right\}.
\]
We say that \(\eta\) is a \emph{Lipschitz section} if \(\mathrm{Lip}(\eta) < +\infty\).
Let \(\Gamma_{\mathcal{L}}(NV)\) be the space of Lipschitz sections of \(NV\) endowed with the norm
\[
\|\eta\|_{\mathcal{L}} = \sup \left\{ \|\eta(x)\| \mid x \in V \right\} + \mathrm{Lip}(\eta).
\]
Let \(\mathrm{Diff}^{1}(M)\) be the space of \(C^{1}\) diffeomorphisms with the topology of the \(C^{1}\) convergence on compact subsets.
\end{definition}

\begin{definition}
 Let \(f \in \mathrm{Diff}^{1}(M)\).
 We say that \(V\) is a Lipschitz persistent invariant manifold of \(f\) if there exists a neighborhood \(U\) of \(V\) such that for all \(\delta > 0\) there exists a neighborhood \(\mathcal{U}_{\delta}\) of \(f\) such that if \(g \in \mathcal{U}_{\delta}\) there exists \(\eta \in \Gamma_{\mathcal{L}}(NV)\) with \(\|\eta\|_{\mathcal{L}} < \delta\) satisfying \(V_{g} = \mathrm{graph}(\eta)\), where \(\mathrm{graph}(\eta) = \{\exp_{x}(\eta(x)) \mid x \in V\}\), \(V_{g} = g(U)\).

Observe that this definition implies \(V_{f} = V\), hence \(f(V) = V\). Moreover, the Lipschitz persistence is independent of the bundle \(NV\).
\end{definition}


For a flow \(\varphi_{t}\) (coming from the solutions of an ODE) we can fix \(t=\tau\in\reals_{>0}\) so that we get a homeomorphism \(\varphi_{\tau}\).
This allows us to apply this result to apply to our case.


In the case where \(V\) is a point, then it is a hyperbolic fixed point and the persistence follows trivially from the implicit function theorem.


\begin{remark}
It is sufficient to take a persistent manifold that is uniformly locally maximal.
If \(V\) is persistent and uniformly locally maximal, then \(V\) has to be normally hyperbolic \citep{mane1978persistent}.


\begin{definition}[Uniformly locally maximal]
 There exist neighborhoods \(U\) of \(V\) in \(M\) and \(U\) of \(f\) in the space \(\mathrm{Diff}^{1}(M)\) of \(C^{1}\)-diffeomorphisms of \(M\), such that for any \(g \in U\), \(N_{g} = \bigcap_{k \in \mathbb{Z}} g^{k}(U)\) is a \(C^{1}\)-submanifold close to \(V\), with \(N_{f} = N\).
 The latter property implies the uniqueness of the invariant submanifold.
\end{definition}
\end{remark}



\subsection{Near Perfect Analog Memory Systems are close to Decomposable Systems with a Continuous Attractor}\label{sec:near_ca}

We now prove a more general statement about the kind of systems that are close to perfect analog memory systems.
The theory guarantees that a system that satisfies conditions~\ref{converse:bijection}-\ref{converse:Dtype}  will have a continuous attractor in the following sense.
For such a system there exists a decomposition such that the system can be effectively decomposed into a continuous attractor (attractive invariant manifold with zero flow) and a component on which a (possibly ``fast'' flow) can exist buy which get quenched by the out put projection.
These additional dynamics orthogonal to decoding have been observed for motor movement preparation \citep{kaufman2014null, churchland2024preparatory}.

For this part of the theory, we need to consider the output manifold \(\manifold_{\text{output}}\), the manifold on which we determine the error over time as in Sec.~\ref{sec:supp:ub}.
For this section, we will consider the output mapping to be a smooth (possibly nonlinear) mapping \(g\colon X\rightarrow Y\) between the neural state space \(X=\reals^{d_{X}}\) and the output space \(Y=\reals^{d_{Y}}\).
For a circular variable this will be the ring \(S^{1}\).

The construction of a perturbation relies on finding the necessary minimal structure in the invariant manifold for which we can guarantee closeness to a continuous attractor.
Therefore, first of all, we need this closeness in terms of the geometry of the manifold, which we guarantee through the notion of a fibration of the output mapping.
Second, we need to guarantee that the flow is bounded in a sense so that our perturbation is also bounded by this amount.
We will characterize this by the vector field normal to the fibers of the fiber bundle.
A fiber bundle is a mathematical structure that allows us to study spaces that are locally like a product space but globally may have a different structure.
So we first state the definition of a fiber bundle and a trivial fibration.

\begin{definition}
A \emph{fiber bundle} is a structure \((E, B, \pi, F)\) where:
\begin{itemize}
    \item \(E\) is the \emph{total space},
    \item \(B\) is the \emph{base space},
    \item \(\pi: E \to B\) is a continuous surjection called the \emph{projection map}, and
    \item \(F\) is a topological space called the \emph{fiber}.
\end{itemize}
This structure must satisfy the local triviality condition: for each \(b \in B\), there exists an open neighborhood \(U\) of \(b\) such that there is a homeomorphism
\[
\varphi: \pi^{-1}(U_{b}) \to U_{b} \times F_{b}
\]
that commutes with the projection onto \(U\), meaning that the following diagram commutes:
\begin{equation*}
    \begin{tikzcd}
        \pi^{-1}(U_{b}) \arrow[rightarrow]{r}{\varphi}	\arrow[swap]{d}{\pi} & U_{b} \times F_{b}	\arrow[rightarrow]{d}{\operatorname{pr}_{1}} 	\\
        U_{b} \arrow[rightarrow]{r}{id}  &	 	U_{b}
      \end{tikzcd}
\end{equation*}% \[
% \begin{array}{ccc}
% \pi^{-1}(U) & \xrightarrow{\varphi} & U \times F \\
% \pi \downarrow &  & \downarrow \text{pr}_1 \\
% U & = & U
% \end{array}
% \]
where \(\text{pr}_{1}: U_{b} \times F_{b} \to U_{b}\) is the projection onto the first factor.
\end{definition}

So, around every point in the base space, you can ``zoom in'' and see that the bundle looks like a straightforward product of the base space and the fiber and \(\varphi\) is providing this trivialization.

\begin{definition}
We say that a projection map \(\pi:E \rightarrow  B\) is \emph{locally trivial} if each point \(b \in B\) is contained in an open set \(U\) having the property that \(E_{U}\coloneqq \pi^{-1}(U)\) is trivial over \(U\).
\end{definition}

We will rely on the concept of a submersion to characterize how the output space needs to relate to our invariant manifold.
\begin{definition}[Submersion]
Let \(M\) and \(N\) be differentiable manifolds and \(f\colon M\to N\) be a differentiable map between them.
 The map \(f\) is a \emph{submersion} at a point \(p\in M\) if its differential \(Df_{p}\colon T_{p}M\to T_{f(p)}N\) is a surjective linear map.
\end{definition}



This allows us to characterize what kind of structure the invariant manifold needs to have, namely it can be see as the direct product of the output manifold and some other manifold which described over what part of the invariant manifold the output mapping is invariant.
\begin{theorem}[Ehresmann's lemma\citep{ehresmann1950connexions}]
If a smooth mapping \(f\colon M\rightarrow N\), where \(M\) and \(N\) are smooth manifolds, is
 \begin{enumerate}
\item a surjective submersion, and
\item a proper map
\end{enumerate}
then it is a locally trivial fibration.
\end{theorem}

\begin{remark}
If a manifold \(M\) is compact, then the above smooth map  is a proper map.
If we do not have a submersion or that the output mapping is transversal to the output manifold, we have a situation in which the flow on the invariant manifold can be arbitrarily fast  (even though memory is degrading slowly).
This happens when the flow is in a singularity of the output mapping.
\end{remark}

For our statement we want that the invariant manifold can be decomposed into a space that is diffeomorphic to the output manifold and another compact manifold: \(\manifold_{\epsilon}= \manifold_{\text{slow}}\times \manifold_{\text{null}}\) with \(\manifold_{slow}\simeq \manifold_{Y}\). We can relax this to allow for the possibility of some torsion along the invariant manifold.
For this to hold, it is sufficient that the output mapping must be a submersion because this makes it a locally trivial fibration.
So we get that \(g\colon X\rightarrow Y\) defines a locally trivial fiber mapping.


The second assumption we need is to have a bound for the speed of trajectories along the fibers, which will correspond to a speed along the output manifold, resulting in memory degradation.
We need to assume that there is a slow flow (in the direction of \(\manifold_\text{slow}\)).
We characterize the relevant maximal size of the vector field that needs to be perturbed as the  supremum over the uniform norms of the vector field normal to the fibers of the fiber bundle.


\begin{theorem}\label{thrm:near_ca}
Let \(\manifold_{\epsilon}\) be a connected, compact, normally hyperbolic slow manifold  (as parametrized by Eq.~\eqref{eq:fenichel:flowtangent}-\eqref{eq:fenichel:flownormal}) with the real part of the eigenvalues of \(\nabla_{\vz} \vh\) all negative. % in the neighborhood of a continuous attractor.
Further, assume that this manifold can be decomposed \(\manifold_{\epsilon}= \manifold_{\text{slow}}\times \manifold_{\text{null}}\) with \(\manifold_{slow}\simeq \manifold_{Y}\) and that
 the uniform norm of the flow tangent to \(\manifold_{\epsilon}\) restricted to \(\manifold_{\text{slow}}\) be \(\|\dot{(\vy)}_{\text{slow}}\|_{\infty}=\eta\).
 %eta should be the uniform norm of the projected vector field on the output manifold
 %actually it should be the supremum over all possible pullback sections
Then, there exists a perturbation with uniform norm at most \(\eta\) that induces a bifurcation to a system that is decomposable into a continuous attractor and a system with a non-zero flow.
% eta/||g||_\infty for the output mapping g
\end{theorem}

In other words, after applying this perturbation, the slow component of the perturbed system satisfies \(\dot{\vx}'|_{\text{slow}} = 0\) for the system \(\dot{\vx}' = \vf(\vx) + \vp(\vx)\).
Furthermore, the trajectories of the resulting system form a fiber bundle where the output projection serves as the bundle projection.
Each fiber consists of trajectories that are mapped to the same value in \(\manifold_{\text{output}}\), meaning that the fibers describe an invariance under the output projection. % equivalence classes under the output projection.


\begin{proof}
Assume that the output mapping is a submersion from the invariant manifold \(\manifold_{\epsilon}\) to the output manifold \(\manifold_{\text{output}}\).
Ehresmann's lemma implies that this implies that we have a locally trivial fibration \((\manifold_{\epsilon}, \manifold_{\text{output}}, g,\manifold_{\text{null}})\).
We construct the perturbation \(\vp\) as the part of the vector field that us normal to the fibers \(g^{-1}(y)\) for each \(y\in\manifold_{\text{output}}\).
By construction, this perturbation has uniform norm at most \(\eta\).
This perturbation makes the vector field normal to each fiber zero.
That implies that each fiber is an invariant submanifold of \(\manifold_{\epsilon}\).
From the structure of the fiber bundle it follows that there is a continuum of such invariant submanifolds.
Hence, the perturbed system is decomposable into a system that is a continuous attractor and a system with a non-zero flow.
\end{proof}


\paragraph{Remark}
This perturbation can be made smoothness, along similar lines as above.
We can take a\(\epsilon\)-Neighborhood around the invariant manifold on which we extend the above vector field with bump functions to get a smooth vector field that still results in an attractive invariant manifold.


\paragraph{Example}
An example of an approximate continuous attractor solution of the form with a decomposable system of which one of the subsystems is close to a continuous attractor is the torus solution in Fig.~\ref{fig:fastslow_decomposition}D.
In this case, the system can be perturbed slightly such that there exists a continuum of limit cycles laid out over a ring.




%\paragraph{} More general:
%Prove (?): There is a point \(y\in Y\) for which \(g^{-1}(y)={x}\times \manifold_{null}\)
%With some implicit function theorem?
%+the fact that the invariant manifold is compact
%
%This would imply that the invariant manifold \( \manifold_{\text{slow}}\times \manifold_{\text{null}}\) hold for every point because \(\manifold_{X}\) is a manifold











%%%%%%%%%%
\newpage
\section{Upper bound for the memory performance on a short time scale}\label{sec:supp:ub}
We will now formalize the statement about an upper bound dependent on the uniform norm of the vector field on the slow manifold in Sec.\ref{sec:revival} and provide a proof.
\begin{prop}\label{prop:ub}
Let \(\manifold\) be a normally hyperbolic slow manifold as in Prop.~\ref{sec:revival}.
Let \(\vx_{0} \in \manifold\), and \(\varphi = \vf\vert_{\manifold}\) be the flow restricted to the manifold.
The average deviation from initial memory \(\vx_{0}\) over time is bounded linearly
\begin{align}
\frac{1}{\operatorname{vol}\manifold}\int_{\manifold}
\abs{\vx(t, \vx_{0}) - \vx_{0}}\,
\dm{\vx_{0}}
\leq t\uniformNorm{\varphi}.
\end{align}\end{prop}

\begin{proof}
Numerical integration of the ODE gives
\begin{align*}
x(t,\vx_{0}) &= \int_{0}^{t}\varphi(x(\tau))d\tau + \vx_{0} \\
&\leq \int_{0}^{t}\|\varphi\|_{\infty} d\tau + \vx_{0} \\
&= t\|\varphi\|_{\infty}+ \vx_{0}
\end{align*}From this, we get
\begin{align*}
\frac{1}{\operatorname{vol}\manifold}\int_{\manifold}
\abs{\vx(t, \vx_{0}) - \vx_{0}}\,
\dm{\vx_{0}} & \leq \frac{1}{\operatorname{vol}\manifold}\int_{\manifold} t\|\varphi\|_{\infty}\\
&= t\uniformNorm{\varphi}.
\end{align*}\end{proof}



We formulate here a theory of continuous attractor approximations in terms of memory loss over time.
It can be used uniform norm of vector field on the manifold to bound the memory performance on the short-time scale. % based on the uniform bound of the slow flow.
Let \(\vx_{0} \in \manifold\), and \(\varphi = \vp\vert_{\manifold}\) be the flow restricted to the manifold.
We will show that the average deviation from initial memory \(\vx_{0}\) over time is bounded linearly as in Eq.~\ref{eq:distance:ub}.

\subsection{Ring attractor}
For a ring attractor we can give more tight bounds on the accumulated error for the angular memory.
Suppose we have a dynamical system \(\bm{x} \in \mathbb{R}^{N}\) with autonomous dynamics \(\dot{ \bm{x}} = F_{\boldsymbol{\theta}} (\bm{x})\) and solutions \(\vx(t, \vx_{0})\) uniquely defined for each \(\vx_{0}\).
Let us define the error (i.e.the  average deviation from initial memory) for an attractor as
\begin{align}
\mathcal{L}(T) \coloneqq
\frac{1}{\operatorname{vol}\manifold}\int_{\manifold}
%\frac{1}{T}\int_0^T
\abs{\vx(t, \vx_{0}) - \vx_{0}}\,
\dm{\vx_{0}}
\end{align}
If we further assume that the memory is not simply the state of the network, we need to take into consideration a decoder of the memory.
Suppose that there is an invertible decoder mapping \(f: \mathcal{U} \rightarrow \mathbb{R}^{N}\). %what happens if it is not invertible?
For a ring variable, we can take this to be the projection onto the plane \(\wout\) and then applying the \(\arctan\):
\begin{equation}
g(x) = \arctan(\wout x). %hat alpha?
\end{equation}%Then, let us define a memory loss in a time \(T\) as:
%\begin{equation}
%    \mathcal{L}(T) = \frac{1}{|\mathcal{U}|} \int_\mathcal{U}  \frac{1}{|f(\bm{\alpha})|} \int_{f(\bm{\alpha})} \frac{1}{T} \int_0^T d(f^{-1}(\phi_{\bm{\theta}}(f(\bm{\alpha}),t),\bm{\alpha}) dt df(\bm{\alpha}) d\bm{\alpha}
%\end{equation}
In the case of the ring attractor, the memory we would like to encode is \(\alpha \in \mathcal{U} = [0, 2 \pi)\), the error is defined as \(|x - y|_{o} = o_{\pi}(|x - y|)\) where: % ChkTeX 9
\begin{equation}
    o_{\pi}(x) = \begin{cases}
    x & \text{if } x < \pi \\
    2 \pi - x & \text{if } x \geq \pi
    \end{cases}
\end{equation}
If we call \(\hat{\alpha}_{\boldsymbol{\theta}}(\alpha_{0}, t) = g(\varphi_{\bm{\theta}}(f(\alpha),t))\) we get the expression of the memory loss for this kind of memory as:
\begin{equation}
    \mathcal{L}(T) =  \frac{1}{2 \pi} \int_{0}^{2 \pi}    \left( \left| \hat{\alpha}_{\boldsymbol{\theta}}(\alpha_{0}, t) - \alpha_{0}  \right| \right)  d\alpha_{0}
\end{equation}
%Let \(\mathcal{U}\subset \mathbb{R}^{D}\) be a subamifold and  with metric \(d(x, y): \mathcal{U} \times \mathcal{U} \rightarrow \mathbb{R}\).
%%

%If \(D = 1\), then without loss of generality we can take \(\mathcal{U} = [a, b]\) (or open/closed combinations and/or \(a = -\infty\) and/or \(b = +\infty\)):
%
%\begin{equation}
%    \mathcal{L}(T) = \left(\lim_{a(,b) \rightarrow -\infty(,+\infty)}\right) \frac{1}{b - a} \int_a^b  \frac{1}{|f(\alpha)|} \int_{f(\alpha)} \frac{1}{T} \int_0^T d(f^{-1}(\phi_{\bm{\theta}}(f(\alpha),t)),\alpha) dt df(\alpha) d\alpha
%\end{equation}
%\subsubsection{Bounds}

\textbf{General bounds}
Define the following functions:
\begin{equation}
\begin{split}
    \epsilon^+(t) &= \sup_{\alpha_{0}} \; o_{\pi} \left( \left| \hat{\alpha}_{\boldsymbol{\theta}}(\alpha_{0}, t) - \alpha_{0}  \right| \right) \geq \frac{1}{2 \pi} \int_{0}^{2 \pi}  o_{\pi} \left( \left| \hat{\alpha}_{\boldsymbol{\theta}}(\alpha_{0}, t) - \alpha_{0}  \right| \right)  \\
     \epsilon^{m}(t) &= \frac{1}{2 \pi} \int_{0}^{2 \pi}  o_{\pi} \left( \left| \hat{\alpha}_{\boldsymbol{\theta}}(\alpha_{0}, t) - \alpha_{0}  \right| \right) d\alpha_{0} \\
    \epsilon^-(t) &= \inf_{\alpha_{0}} \; o_{\pi} \left( \left| \hat{\alpha}_{\boldsymbol{\theta}}(\alpha_{0}, t) - \alpha_{0}  \right| \right) \leq \frac{1}{2 \pi} \int_{0}^{2 \pi}  o_{\pi} \left( \left| \hat{\alpha}_{\boldsymbol{\theta}}(\alpha_{0}, t) - \alpha_{0}  \right| \right)
\end{split}
\end{equation}
Then we get the bounds for the loss \(\mathcal{L}(T)\) as:
\begin{equation}
    \frac{1}{T} \int_{0}^{T} \epsilon^-(t)dt \leq \mathcal{L}(T) = \frac{1}{T} \int_{0}^{T} \epsilon^{m}(t) dt \leq  \frac{1}{T} \int_{0}^{T} \epsilon^+(t) dt
\end{equation}
\textbf{Speed bounds}

We can define the maximum, average and minimum memory error speed as:
\begin{equation}
\begin{split}
    v_{\epsilon^+} &= \sup_{\alpha_{0}} \; \frac{d}{dt} (o_{\pi} \left( \left| \hat{\alpha}_{\boldsymbol{\theta}}(\alpha_{0}, t) - \alpha_{0}  \right| \right)) |_{t = 0}  \\
    v_{\epsilon^{m}} &= \frac{1}{2 \pi} \int_{0}^{2 \pi} \frac{d}{dt} (o_{\pi} \left( \left| \hat{\alpha}_{\boldsymbol{\theta}}(\alpha_{0}, t) - \alpha_{0}  \right| \right))|_{t = 0} d\alpha_{0} \\
    v_{\epsilon^-} &= \inf_{\alpha_{0}} \; \frac{d}{dt} (o_{\pi} \left( \left| \hat{\alpha}_{\boldsymbol{\theta}}(\alpha_{0}, t) - \alpha_{0}  \right| \right))|_{t = 0}
\end{split}
\end{equation}
Notice then that since:
\begin{equation}
\begin{split}
    \epsilon^+(t) &\leq \min(t v_{\epsilon^+}, \pi) \\
    \epsilon^-(t) &\geq t v_{\epsilon^-}
\end{split}
\end{equation}then,
\begin{equation}
\begin{split}
    \frac{1}{T} \int_{0}^{T} \epsilon^+(t)dt &\leq  \min \left( \frac{1}{T} \int_{0}^{T} t v_{\epsilon^+} dt, \pi \right) = \min \left( \frac{T v_{\epsilon^+}}{2}, \pi \right) \\
    \frac{1}{T} \int_{0}^{T} \epsilon^-(t)dt &\geq  \frac{1}{T} \int_{0}^{T} tv_{\epsilon^-}dt  = \frac{T v_{\epsilon^-}}{2} \\
\end{split}
\end{equation}and we get:
\begin{equation}
    \frac{T v_{\epsilon^-}}{2} \leq \frac{1}{T} \int_{0}^{T} \epsilon^-(t)dt \leq \mathcal{L}(T) = \frac{1}{T} \int_{0}^{T} \epsilon^{m}(t) dt \leq  \frac{1}{T} \int_{0}^{T} \epsilon^+(t) dt \leq \min \left( \frac{T v_{\epsilon^+}}{2}, \pi \right)
\end{equation}
Finally, if the error is uniform enough we can expect \(\epsilon^{m}(t) \approx tv_{\epsilon^{m}}\) and
\begin{equation}
    \mathcal{L}(T) = \frac{1}{T} \int_{0}^{T} \epsilon^{m}(t) dt \approx  \frac{1}{T} \int_{0}^{T} t v_{\epsilon^{m}}dt = \frac{T v_{\epsilon^{m}}}{2}
\end{equation}
\paragraph{Within manifold case}
Let's assume that we have managed the system \(F_{\boldsymbol{\theta}}\)  have a slow manifold \(\manifold \in \mathbb{R}^{N}\) in bijection with \(\mathcal{U}\), i.e. \(f|_{\manifold}\) is not a mapping but a bijective function and:
\begin{equation}
\forall \bm{x} \in \manifold \quad \dot{\bm{x}} = \epsilon_{\bm{\theta}}(\bm{x})\frac{\frac{\partial f}{\partial \alpha}(f^{-1}(\bm{x}))}{||\frac{\partial f}{\partial \alpha}(f^{-1}(\bm{x}))||}
\end{equation}
% In this case:
% \begin{equation}
% f^{-1}(\phi_{\bm{\theta}}(f(\alpha),t)) = g(\alpha, t, \epsilon_{\bm{\theta}})
% \end{equation}
Then we have a slow manifold in the form of a ring attractor, we have \(f(0) = f(2 \pi)\) and:
\begin{equation}
    \hat{\alpha}_{\boldsymbol{\theta}}(\alpha_{0}, t) = \left(\alpha_{0} + \int_{0}^{t} \epsilon_{\bm{\theta}}(\alpha_{0}, s) ds \right)\mod 2 \pi
\end{equation}
Then:
\begin{equation}
\begin{split}
    \hat{\alpha}_{\boldsymbol{\theta}}(\alpha_{0}, t) &= o_{\pi} \left( \left| \left(\alpha_{0} + \int_{0}^{t} \epsilon_{\bm{\theta}}(\alpha_{0}, s) ds \right)\mod 2 \pi - \alpha_{0} \right| \right)  \\
    &= o_{\pi} \left( \left| \left(\alpha_{0} + \int_{0}^{t} \epsilon_{\bm{\theta}}(\alpha_{0}, s) ds \right)\mod 2 \pi - \alpha_{0} \mod 2 \pi \right| \right) \\
    &= o_{\pi} \left( \left| \int_{0}^{t} \epsilon_{\bm{\theta}}(\alpha_{0}, s) ds \mod 2 \pi \right| \right)
\end{split}
\end{equation}where we used that \(\alpha_{0} \in [0, 2 \pi) \Rightarrow \alpha_{0} = \alpha_{0} \mod 2 \pi\) and that \(|x \mod 2 \pi - y \mod 2 \pi| = |(x - y) \mod 2 \pi|\). % ChkTeX 9

The final equation of the loss in this case has the form:
\begin{equation}
    \mathcal{L}(T) =  \frac{1}{2 \pi} \int_{0}^{2 \pi} \frac{1}{T} \int_{0}^{T} o_{\pi} \left( \left| \int_{0}^{t} \epsilon_{\bm{\theta}}(\alpha_{0}, s) ds \mod 2 \pi \right| \right) dt d\alpha_{0}.
\end{equation}
\textbf{Slow manifold bounds}

In this case, if we have \(N\) fixed points in the ring-like slow manifold, we know that:
\begin{equation}
\begin{split}
     \epsilon^+(t) &\leq \min \left( \frac{2 \pi}{N}, \pi \right),
\end{split}
\end{equation}and therefore:
\begin{equation}
    \mathcal{L}(T) \leq \min \left( \frac{2 \pi}{N}, \pi \right).
\end{equation}
\newpage
\section{Slow manifold in trained RNNs}
We will provide a detailed description of the tasks, architectures, training methods, and analysis techniques used in our numerical experiments with trained RNNs.
\subsection{Tasks}\label{sec:supp:tasks}
\paragraph{Memory guided saccade task}

The total time length of a trial is 512 steps.
The time delay to the output cue was sampled from
\begin{align}
T_{delay} \sim \mathcal{U}(50, 400).
\end{align}We applied a mask \(m_{i, t}=0\) for 5 time steps (\(t = T_{delay}+j\) for \(j = 0,\dots 4\)) after the go cue (Eq.~\ref{eq:loss}).



\paragraph{Angular velocity integration task}
The time length of a trial is 256 steps.
The input is an angular velocity and the target output is the sine and cosine of the integrated angular velocity.
Velocity at every timestep  is sampled from as a Gaussian Process (GP) for smooth movement trajectories, consistent with the observed animal behavior in flies and rodents.
%These parameters are set to roughly match the angular velocity distribution of the rat and fly (Stackman & Taube, 1998; Sharp et al., 2001; Bender & Dickinson, 2006; Raudies & Hasselmo, 2012).
\begin{equation}
k(x, y)=\exp\left(-\frac{\|x - y\|}{2\ell^{2}}\right),
\end{equation}with length scale \(\ell\).
 The length scale of the kernel was fixed at 1.

 \begin{figure}[tbhp]
     \centering
    \includegraphics[width=\textwidth]{task_fig}
       \caption{Description of the angular velocity integration task.
        (A) The angular velocity integration task.
        (B) The output of the angular velocity integration in the output space, color coded according to the integrated angle. An example of an input is shown with constant velocity and it is provided until one turn is completed.
        }\label{fig:angular_task}
\end{figure}

\paragraph{Double angular velocity integration task}
The Double Angular Velocity Integration Task is an extension of the Angular Velocity Integration Task, where two independent instances of the task are performed simultaneously. In this case, you have two separate angular velocities, each sampled from its own Gaussian Process, representing two distinct movement trajectories.

For each of the two angular velocities, the integration over time is performed separately, resulting in two sets of outputs: one for each angular velocity, making the output space four dimensional.

%\citep{Burak2009}
Grid cells are known to exhibit periodic firing patterns that form a hexagonal grid across an environment, and these patterns are often modeled as existing on a toroidal surface (a doughnut-shaped surface)\citep{hermansen2024uncovering}. The reason for this is that the activity patterns of grid cells are continuous and wrap around seamlessly, meaning that if you move far enough in one direction, the grid pattern will repeat itself. This toroidal structure allows for the continuous representation of space without boundaries, which is crucial for efficient path integration.
In the context of the Double Angular Velocity Integration Task, where two independent angular velocities are integrated, the resulting four-dimensional output space can be considered as two 2D subspaces (one for each angular velocity).

\paragraph{Unbounded tasks}
Regarding tasks with an unbounded range, such as navigation tasks, two points bear mentioning.

For planar attractors are diffeomorphic to \(\reals^{2}\), note that they do not conform to the assumptions on normally hyperbolic invariant manifolds, since \(\reals^{2}\) is not compact.
There are suitable generalizations of this theory to noncompact manifolds \citep{eldering2013normally}, but we do not pursue them since they require more refined tools, which would only obscure the point that we are trying to make.
Tangentially, we would also like to point out that we assume that neural dynamics are naturally bounded (e.g. by energy constraints) and hence sufficiently well described by compact invariant manifolds.


\subsection{Output projection of the invariant manifold}
%Fig 4

\begin{figure}[tbhp]
  \centering
  \includegraphics[width=\textwidth]{fastslow_decomposition_outputs_s}
  \caption{Output projection of slow manifold approximation of the trained networks of Fig.~\ref{fig:fastslow_decomposition}. All stability strutures are colored according to the decoded angle shown in Fig.~\ref{fig:angular_task}B, target output circle shown in red.
 (A) An example fixed-point type solution to the memory-guided saccade task (Fig.~\ref{fig:fastslow_decomposition}B).
 (B) An example of a found solution to the angular velocity integration task (Fig.~\ref{fig:fastslow_decomposition}C).
 (C) An example slow-torus type solution to the memory-guided saccade task. The colored curves indicate stable limit cycles of the system (Fig.~\ref{fig:fastslow_decomposition}D).
}\label{fig:fastslow_decomposition_otuput}
\end{figure}

The output projection of the invariant manifolds and stability structures (fixed points and limit cycles) is very close to the target output circle, as shown in Fig.~\ref{fig:fastslow_decomposition_otuput}.



\subsection{Discretization}\label{sec:supp:discretization}


Computational neuroscientists often train RNNs as models of neural computation and interpret them as dynamical systems \citep{mante2013context,sussillo2013blackbox,xie2022neural}.
Our experiments connect to existing literature.
We examined vanilla continuous-time RNNs for this work.
The time-discretized version of RNN network activity is given by
\begin{equation}
  \begin{aligned}
	\vx_{t} &= \phi(\win \vI_{t} + \vW \vx_{t - 1} + \vb) + \zeta_{t} \label{eq:RNN:discrete}\\
	\vy_{t} &= \wout \vx_{t} + \bout
  \end{aligned}
\end{equation}where \(\vx_{t} \in \reals^{d}\) is the hidden state,
\(\vy_{t} \) is the readout,
\(\vI_{t} \in \reals^{K}\) is the input,
\(\phi\colon \reals \to \reals\) is an activation function that acts on each of the hidden dimensions,
 \(\zeta_{t}\iidsample\mathcal{N}(\mathbf{0},\sigma^{2} = \nicefrac{1}{100}\identity)\) is a state noise variable, and %\(\zeta\sim \mathcal{N}(0, .001)\)
\(\vW, \vb, \win, \wout, \bout\) are parameters.
We will shortly explain the discretization procedure, i.e., the steps for going from Eq.~\ref{eq:RNN:continuous} to Eq.~\ref{eq:RNN:discrete}.
Let \(t_{n} = n \Delta t\).



The Euler-Maruyama method for a stochastic differential equation (Eq.~\ref{eq:RNN:continuous})
\[\mathrm{d}{\mathbf{x}} = \left(-\mathbf{x} + \phi\mathbf{W}_{\text{in}} \mathbf{I}(t) + \mathbf{W} \mathbf{x} + \mathbf{b})\right)\mathrm{d}{t} + \sigma\mathrm{d}{W}_{t}\] is given by :
\[\mathbf{x}_{n + 1} = \mathbf{x}_{n} + \left( -\mathbf{x}_{n} + \phi(\mathbf{W}_{\text{in}} \mathbf{I}_{n} + \mathbf{W} \mathbf{x}_{n} + \mathbf{b}) \right) \Delta t + \sigma \Delta W_{n},\]
with \(\Delta W_{n}=W_{(n + 1)\Delta t}-W_{n\Delta t}\sim \mathcal{N}(0,\Delta t).\)

Now subsitute \(\Delta t = 1\):
\begin{align}
 \mathbf{x}_{t + 1} &= \mathbf{x}_{t} + \left( -\mathbf{x}_{t} + \phi(\mathbf{W}_{\text{in}} \mathbf{I}_{t} + \mathbf{W} \mathbf{x}_{t} + \mathbf{b}) \right) + \sigma \Delta W_{t}, \\
 &= \phi(\mathbf{W}_{\text{in}} \mathbf{I}_{t} + \mathbf{W} \mathbf{x}_{t} + \mathbf{b}) + \sigma \Delta W_{t}.
 \end{align}
If we introduce the noise term \(\zeta_{t} = \sigma \Delta W_{t}\), which represents the discrete-time noise, we have derived the discrete-time equation: \[ \vx_{t} = \phi(\win \vI_{t} + \vW \vx_{t - 1} + \vb) + \zeta_{t}. \]

So, assuming Euler-Maruyama integration with unit time step, the discrete-time RNN of~\eqref{eq:RNN:discrete} corresponds to the stochastic differential equation:
\begin{align}
    \dm{\vx} &= -\vx\,\dm{t} + \phi(\win \vI(t) + \vW \vx + \vb)\,\dm{t} + \sigma\,\dm{W}. \label{eq:RNN:continuous}
\end{align}where \(\dm{W}\) is a Wiener process that models the intrinsic state noise in the brain.
See for more detail on correspondences between discrete- and continuous-time RNNs in \citep{monfared2020transformation} and \citep{ozaki2012time}.
Our experiments connect to existing literature. In future studies, it would be interesting to perform experiments with Neural SDEs \citep{tzen2019neural}

%\subsection{Discrepancies between discrete and continuous RNNs}
%\label{sec:discrepancies}
%In the discrete time version of vanilla RNN \eqref{eq:RNN:discrete}, there can exist 2-period orbits, which are absent in the \eqref{eq:RNN:continuous} version.

%\subsection{Unbounded line attractor}
%\label{sec:ubla}
%
%The parameters:
%\begin{equation}\label{eq:bla}
%\win = \alpha
%\begin{pmatrix}
%-1  &  1 \\
%1  &  -1
%\end{pmatrix}, \
%W =
%\begin{pmatrix}
%0  &  1 \\
%1  &  0
%\end{pmatrix}, \
%\wout = \frac{1}{2\alpha}
%\begin{pmatrix}
%1  &  1
%\end{pmatrix}, \
%\bout = -\frac{\beta}{\alpha}.
%\end{equation}%
%The bias to the recurrent units is zero.

\subsection{Network architectures}
In all network architectures a linear output is used.
Furthermore, for the angular velocity integration tasks we used an additional mapping from the output to the hidden layer to initialize the hidden state on the initial position along the ring from which the network needed to integrate from.

\paragraph{Vanilla}
We used vanilla RNNs with different nonlinearities (ReLU, tanh and rectified tanh) for the recurrent layer.


\paragraph{LSTM}
The number of units for the trained LSTMs was half of that of vanilla RNNs to match the number of paramters \citep{Hochreiter1997}.


\paragraph{GRU}
We also trained Gated Recurrent Units (GRU) \citep{cho2014learning} for which we used the same number of hidden units as the vanilla RNNs.




\subsection{Training methods}
We trained  RNNs with PyTorch \citep{paszke2017automatic}  on the three tasks Fig.~\ref{fig:angular_task}.
For the vanilla RNNs, we used a time step of \(\Delta t = 0.1\).
The parameters were initialized were initialized using the Xavier normal distribution \citep{Glorot2010}.
For the recurrent weights we used \(W_{ij}\sim\mathcal{N}(0, \nicefrac{g}{\sqrt{N}})\) with a high gain \(g = 1.5\).
The initial hidden state was initialized using the output to recurrent mapping matrix \(W_{otr}\colon\reals^{2}\rightarrow\reals^{N}\) which was trained together with the other parameters.

Adam optimization with \(\beta_{1} = 0.9\) and \(\beta_{2} = 0.999\) was employed with a batch size of 64 and training was run for 5000 gradient updates.
The batches were generated on-line, similar to how animals are trained with a new trial instead of iterating through a dataset of trials.

The best learning rate \(10^{-2}\) was chosen from a set of values \(\{10^{-2},10^{-3},10^{-4},10^{-5}\}\) by 5 initial runs for all nonlinearity and size pairings with the lowest average loss after 100 gradient steps.
Training a single network took around 10 minutes on a CPU and occupied 10 percent of an 8GB RAM.

We numerically minimized the loss \(L\) which was the mean squared error (MSE) between the network output \(\mathbf{y}(t)\) and the target output \(\hat{\mathbf{y}(t)}\):
\begin{equation}\label{eq:loss}
L_{MSE} \coloneqq \langle m_{i, t}(y_{i, t}-\hat y_{i, t})^{2}\rangle_{i, t}, 
\end{equation}
with a mask \(m_{i, t}\) with \(i\) the index of the output units and \(t\)  the index for time.
We implemented a mask, \(m_{i, t}\), for modulating the loss with respect to certain time intervals for the memory guided saccade task (see Sec.~\ref{sec:supp:tasks}).

Although some of the models did not learn the task, most networks converged to a loss below \(10^{-2}\) (Fig.~\ref{fig:training_losses}).
 \begin{figure}[tbhp]
     \centering
    \includegraphics[width=0.75\textwidth]{training_losses_s}
       \caption{Training loss across gradient steps.}\label{fig:training_losses}
\end{figure}






\newpage
 \subsection{RNN analysis methods}


\subsubsection{Evaluation Metric}
After training, we report the normalized mean squared error (NMSE) to asses the how good a found solution is:
\begin{equation}
\operatorname{NMSE} = \frac{\mathbb{E}[(y-\hat y)^{2}]}{\mathbb{E}[y]},
\end{equation}where \(y\) is the target and \(\hat y\) is the prediction.


\subsubsection{Asymptotic behavior and memory capacity}\label{sec:supp:asymbehav}
%and we identify the network behavior through its \(\omega\)-limit sets.
We determine the location of the fixed points through the local flow direction criterion as described in Sec.~\ref{sec:fastslowmethod}
and determine the basin of attraction
\begin{equation}
\basin(x^*) \coloneqq \{x\in \manifold \ | \lim_{t\rightarrow\infty}\varphi(t, x)=\{x^*\}\}.
\end{equation}through assesing the local flow direction for 1024 sample points in the found invariant manifold.

We construct a probability distribution of what part of state space we end up in an infinite time through the calculation of the size of the basins of attraction of stable fixed points as a proportion of the ring.
Finally, we characterize the memory capacity of the network by calculating the entropy of this probability distribution of the network.


 \subsubsection{Fast-slow decomposition}\label{sec:fsdecmethod}
 We simulated 1024 trajectories without noise with inputs from the task and let the networks evolve for 16 times the task definition lengths.
 We took the cutoff to identify the slow manifold to be \(10^{-3}\) of the highest speed along each trajectory.
 We believe that this guarantees the identification of the slow manifold in a system that has a fast-slow decomposition.
 We sampled 1024 points from these points to fit a periodic, cubic spline (black line in Fig.~\ref{fig:fastslow_decomposition},~\ref{fig:im_rep},~\ref{fig:im_all}).



\paragraph{Finding fixed points}\label{sec:supp:fpf}
 We then find fixed points by identifying where the flow reverses by sampling the direction of the local flow for 1024 sample points along the found invariant manifold.
 We assess the direction through projection onto the output mapping and calculate the angular flow.
If the flow is pointing towards a point where the flow reverses then we consider there to be a stable fixed point.
If the flow s pointing away from a reversal point then we consider the fixed point there to be a saddle.
We find that long integrated trajectories of the network converge to the found stable fixed points through this independent method.


\paragraph{Eigenspectrum along the invariant manifold}
We use the eigenvalue spectrum as evidence for normal hyperbolicity.
Normal hyperbolicity of an attractive ring invariant manifold implies that the eigenvalue spectrum has a gap in its eigenvalue spectrum.
To measure this, we linearize at reference points on the invariant manifold (calculate the Jacobian) and calculate the eigenvalues.
The largest  eigenvalue (real part) for such a manifold needs to be much closer to zero than the second largest.
For LSTMs and GRUs the  eigenspectrum was approximated by autodifferentiation of the networks w.r.t. the states on the identified invariant manifold.

%\begin{figure}[tbhp]
%     \centering
%    \includegraphics[width=\textwidth]{eigenvalue_gap}
%       \caption{The eigenvalue spectrum for the trained networks in Fig~\ref{fig:fastslow_decomposition})B and C show a gap between the first two largest eigenvalues.}
%       \label{fig:eigenvalue_gap}
%\end{figure}

For a stable system, where the eigenvalues have negative real parts, the time constant \(\tau\) is given by the negative inverse of the eigenvalue's real part: \(\tau = -\frac{1}{\Re(\lambda)}\),
where \(\Re(\lambda)\) denotes the real part of the eigenvalue \(\lambda\).
For the two example networks in Fig.\ref{fig:fastslow_decomposition}, there is a time scale separation between the dynamics on and off the invariant manifold because there is only one eigenvalue close to zero.

 \paragraph{Vector field on invariant manifold}\label{sec:supp:vf}

 We assess the vector field for the ODE (Eq.~\ref{eq:RNN:continuous}) without noise and input) on the found invariant manifold \(\manifold\) by calculating it in the state space
 and then projecting it onto the output space:
 \begin{equation}
\dot \alpha =  \wout f(\hat \alpha) %=: g(\alpha)
\end{equation}for sampled points \(\hat \alpha\in \manifold\).
These points \(\hat \alpha\in \manifold\) on the manifold are associated with the points on the ring through the mapping \(\alpha = \wout\hat\alpha\).


 \begin{figure}[tbhp]
     \centering
    \includegraphics[width=\textwidth]{vf_on_ring}
       \caption{The projected vector field on the found invariant manifold for the system Fig~\ref{fig:fastslow_decomposition}C.
       (A) The found vector field aligns well with the ring in the projected output space.
       (B) The norm of the vector field is low around found fixed points as expected, but is higher for points that are just slow points.
       }\label{fig:vf_on_ring}
\end{figure}

This vector field in the output space captures in what direction and how quickly angular memory will decay.
 The vector field  suggests that the system indeed has an invariant manifold (Fig.~\ref{fig:vf_on_ring}).
 Furthermore, the vector field and fixed points are consistent with each other, as the vector field flips direction around found fixed points.


 There are some inconsistencies around saddle nodes, where the vector field seems to point off of the manifold.
 This is probably just inaccuracies  coming from numerically calculating the vector field and the exact location of the invariant manifold.
For the bound discussed in Sec.~\ref{sec:revival}, we calculate the uniform norm of the found vector field
\begin{equation}\|f\|_{\infty} = \sup_{\alpha} \wout f(\alpha),\end{equation} see also Sec.~\ref{sec:supp:ub}.

For LSTMs and GRUs the  vector field was approximated by taking the difference the initial and the next state after initializing the network from states on the identified invariant manifold.



\newpage
\subsection{LSTM and GRU results}\label{sec:supp:lstmgru}

The trained LSTMs and GRUs share the same pattern observed in the trained vanilla RNNs: a ring slow invariant manifold (Fig.~\ref{fig:angular_losses_lstm_gru}C and D).
The fixed point topologies in the LSTM and GRU networks show a lot of variation in the number of fixed points, paralleling the systems adjacent to continuous attractors from Fig.~\ref{fig:lara_bifurcations}, as seen in Figure~\ref{fig:angular_loss}D. These variations are similar to those discussed in Figures~\ref{fig:angular_losses_lstm_gru}B.
Additionally, the angular error and memory capacity measures across different time scales are comparable to those illustrated in Figure~\ref{fig:angular_losses_lstm_gru}A, highlighting the generalization properties influenced by the topology of the solutions.
These results underline the universality of our findings beyond vanilla RNNs.

\begin{figure}[tbhp]
  \centering
  \includegraphics[width=\textwidth]{angular_losses_lstm_gru}
  \caption{%\textbf{GRU and LSTM results.}
      The different measures for memory capacity reflect the generalization properties implied by the topology of the found solution.
    \textbf{(A)} The average accumulated angular error vs. the uniform norm on the vector field shown.
     Angular error at \(T_{1} =\) trial length (filled markers) and \(\lim T_{1} \to \infty\)  (hollow markers).
      % The number of fixed point is an integer number.
      Points are jittered to aid legibility.
    \textbf{(B)} The number of fixed points vs. average accumulated angular error, with the average distance between neighboring fixed points (magenta).
    \textbf{(C,D)} Invariant manifold (black) of a trained LSTM (C) and GRU (D) with stable fixed points (green) and saddle nodes (red).
}\label{fig:angular_losses_lstm_gru}
\end{figure}



\newpage
 \subsection{Identified invariant manifold output projections}\label{sec:inv_man_projections}
 The identified invariant manifolds in the trained networks (Fig.~\ref{fig:im_rep} and Fig.~\ref{fig:im_all}).
 However, not all solutions can be meaningfully analyzed with the slow-fast decomposition method.
 For example, the solution at the center of the \(\tanh, N = 256\) block, the found invariant manifold is not correctly captured.
 This is true for the networks that have not learned the task correctly (networks with a NMSE higher than -20dB).

 \begin{figure}[tbhp]
     \centering
    \includegraphics[width=\textwidth]{im_all_last2}
       \caption{Representative identified invariant manifolds (projected onto the output space, in black) with the fixed points (cyan for stable, orange for saddle and black for unstable).
        The reference target ring is shown in grey.}\label{fig:im_rep}
\end{figure}

\newpage
\begin{figure}[tbhp]
     \centering
    \includegraphics[width=\textwidth]{im_all_3x3_s}
       \caption{The identified invariant manifolds with the fixed points (cyan for stable, orange for saddle and black for unstable) for all inferred networks (except the ones in Fig~\ref{fig:im_rep}).}\label{fig:im_all}
\end{figure}



\newpage
\subsection{Double angular integration task}\label{sec:supp:davit}

The analysis of networks trained on the double angular velocity integration task indicates a torus shaped invariant slow manifold as predicted by our theory (Fig.\ref{fig:davit}A and B).
Furthermore, the angular memory error (measured as the sum of the two separate angular errors) of the trained networks show the same conformity to the theoretical bound as defined by the uniform norm of the vector field on the identified invariant manifold.
These results underline the universality of our findings beyond 1D tasks.

\begin{figure}[tbhp]
  \centering
  \includegraphics[width=\textwidth]{davit_ss}
  \caption{%\textbf{2D continuous attractor in RNNs.}
      Networks trained on a double angular velocity integration task.
    \textbf{(A)} Initializations (blue) and fixed points (orange) of an example network.
    \textbf{(B)} Fixed points on a 2D parametrization of the torus for the example network.
    \textbf{(C)} The sum of the total mean angular error (sum of the two seperate angular errors over the two rings) is bounded by the uniform norm of the vector field.
    \textbf{(D)} Generalization for longer memory depends on the number of fixed points in the network.
}\label{fig:davit}
\end{figure}




\subsubsection{Methods}\label{sec:supp:fastslowmethod}
\paragraph{Fixed point search}
In our study, we implemented a  method to analyze the convergence and uniqueness of fixed points within our data.
 Specifically, we considered a convergence threshold of \(10^{-4}\), meaning that the iterative process was halted when the change in the solution between consecutive iterations fell below this value, indicating convergence. Additionally, to assess the uniqueness of the fixed points, we applied a broader threshold of  \(10^{-2}\), ensuring that any fixed points identified within this margin were considered distinct.


\paragraph{Double Mean Angular Error}
The Mean Angular Error (MAE) was computed as the sum of the individual angular errors observed in the data.
 This measure provides an aggregate view of the angular errors for the two separate subtasks.



\paragraph{Uniform Norm of the (Projected) Vector Field}
To evaluate the uniform norm of the (projected) vector field, we calculated the sum of the individual uniform norms of the vector field components.


\newpage
\subsection{Comparison to other methods}

\paragraph{Fixed point analysis}

\citep{sussillo2013blackbox} and \citep{golub2018fixedpointfinder} are primarily concerned with a pointwise definition of slowness, by comparison normal hyperbolicity requires a uniform separation of timescales over the entire invariant manifold.
In \citep{sussillo2013blackbox}, it was observed that structural perturbations  (random gaussian noise in the parameters with zero mean and standard deviation) still leads to the same approximate plane attractor dynamical structure is still in place, however no explanation is provided for these observations.
Our theory can explain why perturbations to the trained RNN.
The Persistence Manifold Theorem (Theorem~\ref{theorem:persistent}) guarantees that for small perturbations (of size \(\epsilon\)) the persistent invariant manifold will be at the approximate same place (it will be at a distance of order \(\mathcal{O}(\epsilon)\)).





\paragraph{Piecewise linear recurrent neural network}
\citep{schmidt2019identifying} identifies asymptotic behaviors in dynamical systems, fixed point dynamics and more general cases cycles and chaos.
We look beyond asymptotic behavior and characterize attractive invariant manifolds, thereby also identifying connecting orbits (or heteroclinic orbits) between fixed points.
Although we developed new analysis methods for dynamical systems to find slow manifolds in them, we do not propose a new general framework for analysis of all dynamical systems.
Finally, \citep{schmidt2019identifying} provides analysis tools for Piecewise-Linear Dynamical Systems, while our methods are generally applicable to RNNs with any activation function.



%\paragraph{DSA}
%\citep{ostrow2024beyond}
%
%The DSA score is relative.
%
%DSA essentially requires fitting a separate model, which implicitly requires performing model selection with respect to the delay embedding, rank of the linear operator.

%\ascomment{Include table?}



%
%
%\begin{figure}[tbhp]
%     \centering
%    \includegraphics[width=\textwidth]{analysis_small}
%       \caption{Analysis steps for the distillation of the implemented computation in a trained RNN. A) Input driven hidden trajectories for constant inputs of different magnitudes in the left (blue) and right (red) direction. B) Projection onto the output space of the attractors found by simulation until convergence to periodic solutions (color indicates the angular direction it maps to) with slow points found by minimizing the speed of the hidden (square) or output (cross) dynamics. Stability is indicated by green for stable, pink and red for saddles with 1 and 2 unstable dimensions, respectively. C) Effective input drive shown as average vector fields for the hidden dynamics projected onto the output space. Averages taken for a single constant input in left (blue) and right (red) directions.}
%         \label{fig:asymptotic_analysis}
%\end{figure}
%
%\begin{figure}[tbhp]
%     \centering
%    \includegraphics[width=\textwidth]{example_solutions}
%       \caption{A) A solution with a single limit cycle (light blue) that gets mapped onto a small subset of the output space. B) A solution with multiple limit cycles spread around the ring attractor. C) A solution with only fixed points spread around a ring like attractor with slow dynamics.}
%         \label{fig:angular_solutions}
%\end{figure}
%
%
%For the angular velocity integration task, typical solutions have two limit cycles corresponding to the two directions of constant inputs. The autonomous dynamics can be characterized by an (approximate) line attractor with two (approximate) ring attractors at the ends.
%The found solutions Fig.\ref{fig:angular_solutions}A-C all show bounded ring-like attractors. These solutions are all composed of two rings (Fig.\ref{fig:asymptotic_analysis}A) connected by an (approximate) line attractor.
%
%The vector field (Fig.~\ref{fig:asymptotic_analysis}C) suggests that the system exhibits input driven dynamics  corresponding to a limit cycle, which would mean that the invariant manifold of the input-driven dynamics is compact.
%%\include{iclr2024_supplementary.tex}







%\section{Appendix / supplemental material}
%
%
%Optionally include supplemental material (complete proofs, additional experiments and plots) in appendix.
%All such materials \textbf{SHOULD be included in the main submission.}
%
%


\end{document}
