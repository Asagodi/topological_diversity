\documentclass{article} % For LaTeX2e
\usepackage{neurips_2024,times}
\pdfminorversion=6
\usepackage[utf8]{inputenc}
\usepackage{amssymb, amsmath, amsthm}
\usepackage{thmtools, mathtools, mathrsfs}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{forloop}
%\usepackage{natbib}
%\setcitestyle{square, comma, numbers,sort&compress, super}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage{pifont} 				%xmark
\usepackage{caption}
\usepackage{float}
\usepackage{bm}
%\usepackage{tikz}
%\usepackage{tikz-cd}
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[colorlinks=true,linkcolor=MidnightBlue,citecolor=ForestGreen,filecolor=TealBlue,urlcolor=Plum]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
%\usepackage{xr-hyper}

\DeclareGraphicsExtensions{.png,.pdf,.jpg,.mps,.eps,.ps}
\graphicspath{{../figures/}, {../figures/inv_man/}}

\newcommand{\defvec}[1]{\expandafter\newcommand\csname v#1\endcsname{{\mathbf{#1}}}}
\newcounter{ct}
\forLoop{1}{26}{ct}{
    \edef\letter{\alph{ct}}
    \expandafter\defvec\letter
}

% captial \vA
\forLoop{1}{26}{ct}{
    \edef\letter{\Alph{ct}}
    \expandafter\defvec\letter
}

\newcommand{\dm}[1]{\ensuremath{\mathrm{d}{#1}}} % dx dy dz dmu
\newcommand{\RN}[2]{\frac{\dm{#1}}{\dm{#2}}} % (Radon-Nikodym) derivative
\newcommand{\PD}[2]{\frac{\partial #1}{\partial #2}} % partial derivative
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\newcommand{\win}{\vW_{\text{in}}}
\newcommand{\wout}{\vW_{\text{out}}}
\newcommand{\bout}{\vb_{\text{out}}}
\newcommand{\reals}{\mathbb{R}}

\newcommand{\manifold}{\mathcal{M}}

\DeclareMathOperator{\relu}{ReLU}

\newcommand{\iidsample}{\stackrel{iid}{\sim}}

\newcommand{\probP}{\text{I\kern-0.15em P}}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\theoremstyle{remark}
\newtheorem{remark}{Remark}

%\title{RNNs with gracefully degrading continuous attractors}

\title{Biologically plausible approximations of continuous attractors}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\renewcommand{\cite}{\citep}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\author{%
    \'Abel ~S\'agodi, Piotr Sok\'o\l, Il Memming Park \\
    %\thanks{Use footnote for providing further information about author (webpage, alternative address).} \\
    \\
    Champalimaud Centre for the Unknown\\
    Champalimaud Foundation, Lisbon, Portugal\\
    \texttt{\{abel.sagodi,piotr.sokol,memming.park\}@research.fchampalimaud.org} \\
}

\begin{document}

%keywords: 
%neural computation, robustness, bifurcation analysis, exploding gradient problem, continuous attractors, persistence of invariant manifolds 
    
%TLDR


\maketitle

\begin{abstract}
Attractors are essential theoretical components in recurrent networks for memory, computation, and learning.
However, the continuous attractors essential for continuous-valued memory suffer from severe structural instability---infinitesimal changes in the parameters typically destroy the continuous attractor.
This fragility poses a problem in biological neural networks, as they are constantly subjected to perturbations caused by noise.
To address the issue of how animals robustly store continuous variables, we use Fenichel's persistence theorem from dynamical systems theory.
This theory shows that normally hyperbolic continuous attractors are robust to perturbations. 
We derive that attractive slow manifolds are universal as approximations of continuous attractors.
In simple line and ring attractors, we verify that all perturbations preserve the invariant manifold and demonstrate the principle numerically in various ring attractor systems.
Finally, we show in trained RNNs that they all approximate continuous attractors as slow manifolds and characterize the dynamics through a fast-slow decomposition.
\end{abstract}


\section{Introduction}

%par1: animals can represent CVs
The natural behavior of animals, including their ability to navigate without sensory cues, implies the presence of a robust internal representation of key variables such as location and head direction.
This robustness is achieved despite the constantly changing synaptic connections \citep{shimizu2021}. %\citep{gallego2020, kim2019generation, flesch2023continual}.





%par2: proposals how this might happen

%Recurrent neural networks (RNNs) can process sequential observations and model temporal dependencies of arbitrary length.
%At the same time, they are fundamentally limited by their finite-sized hidden states which form the only channel between the past and the future.
To store information over a long period of time, as many difficult tasks demand, Recurrent neural networks (RNNs) can learn or be designed to have ``persistent memory''.
Cotninuous attractors are the ideal solution to the problem of how to hold continuous valued variables.
When the information of interest is continuous-valued, a natural solution is to use continuous attractors.
%CA
This made them prevalent in theoretical neuroscience and they have been extensively used as tools to model neural representation and computation ranging from internal representations of head direction \citep{Skaggs1995, goodridge2000} and eye positions \citep{seung1996} to perceptual decision-making and working memory dependent on continuous variables~\cite{Khona2022}.
%QPTA


Continuous attractors are biologically plausible, theoretically elegant, consistent with neural recordings, and avoids the asymptotic exploding and vanishing gradient problem~\cite{Park2023a}.
As a conceptual tool in computational and theoretical neuroscience, continuous attractors are widely used when working memory of continuous values is needed~\cite{Dayan2001,Burak2009,Khona2022}.
When used to accumulate stimuli, continuous attractors are also called neural integrators that are hypothesized to be the underlying computation for the maintenance of eye positions, heading direction, self-location, target location, sensory evidence, working memory, decision variables, to name a few~\cite{seung1996,Seung2000,Romo1999}.
Neural representation of continuous values have been observed as persistent activity in the prefrontal cortex of primates, ellipsoid body of the fly, and hypothalamus~\cite{Romo1999,Noorman2022,Nair2023}.
A typical computational implementation of a continuous attractor is a bump attractor network model which requires a mean-field limit~\cite{Skaggs1995,Camperi1998,Renart2003} and finite sized networks with threshold linear units \cite{Noorman2022,Spalla2021}, see also Sec.~\ref{sec:hd}.

%However, the so-called ``fine-tuning problem'' describing the theoretical and practical brittleness of continuous attractors has long been recognized \citep{seung1998,Park2023a}.

%par3: problems with CAs
Learning generally induces stochasticity in parameters, while spontaneous synaptic fluctuations are present in biological neuronal networks (Sec.~\ref{sec:imp:neuroscience}).
For continuous attractor networks, even the smallest amount of changes in the synaptic connections are problematic, as it destroys the continuum of fixed points -- this is referred to as the ``fine-tuning problem'' (Figure 1) \citep{seung1996, Renart2003}.
Since biological neural systems have constantly fluctuating synaptic weights~\cite{shimizu2021}, this has been a big puzzle in the field.
There have been efforts and remedies to lessen the degradation for particular implementations, often focusing on keeping the short-term behavior close to the continuous attractor case~\cite{Lim2012,Lim2013,Boerlin2013,Koulakov2002,Renart2003,gu2022}.
This discrepancy between biological neural networks and theoretical models highlights a mismatch and thus poses a challenge for modeling working memory and related cognitive processes \citep{Renart2003, seeholzer2019}.
This seemingly makes continuous attractors impractical and hence irrelenvant to be used as a memory of continuous valued variables.
However, as we will show, not all RNN implementations of continuous attractors behave similarly in their brittleness.


%The critical weakness of continuous attractors is their inherent brittleness as they are rare in the parameter space, i.e., infinitesimal changes in parameters destroy the continuous attractor implemented in RNNs~\cite{seung1996,Renart2003}, even if biologically plausible asymmetric connections are used to construct them \citep{darshan2022}.





%par4: observation: CAs bifurcate into invariant manifolds topologically equivalent to the original CA
%Fig1

%We found that in the space of RNNs, some have neighbourhoods with highly undesirable exploding gradients. 
We will describe the bifurcation space of some continuous attractors.  
Consider an RNN (without input or output for now) expressed in continuous time as an ordinary differential equation:
\begin{align}\label{eq:TLN}
    \dot{\vx} = -\vx + \left[ \vW \vx + \vb \right]_{+}
\end{align}
where $\vx \in \reals^d$ is the hidden state of the network, $\vb$ is the bias, and $[\cdot]_{+} = \max(0,\cdot)$ is the threshold nonlinearity per unit.
In discrete time, this corresponds to a ReLU RNN (see Sec.~\ref{sec:rnn:integration}).
The non-trivial activity of this network is limited to the (non-negative) first quadrant.

When $d=2$, we can build a line continuous attractor as follows.
%First, through positive feedback, $\vW = [0, 1; 1, 0]$ and no bias $\vb = \mathbf{0}$, we can create a continuous attractor, i.e., $\dot{\vx} = 0$ on the $x_1 = x_2 \geq 0$ half-line, and surrounding attractive flow (Fig.~\ref{fig:ublabla}A left).
%We refer to it as an \textbf{unbounded line attractor (UBLA)}.
%For any point on the line attractor, linearization results in eigenvalues $0$ and $-2$, corresponding to the zero flow and attractive flow respectively.
%When $\vW$ is perturbed, the null eigenvalue can easily become non-zero and the continuous line attractor disappears.
%If it becomes negative, the system bifurcates to a stable fixed point at the origin (Fig.~\ref{fig:ublabla}A bottom).
%However, if it becomes positive (Fig.~\ref{fig:ublabla}A top), \emph{the resulting flow diverges to infinity along the diagonal}.
%Corresponding to the divergent flow, the backpropagating gradient over time exponentially grows in magnitude, thus rendering gradient descent impractical without truncation in time.
%
%The second kind of continuous attractor is created through negative feedback.
By choosing $\vW = [0, -1; -1, 0]$ and $\vb = [1; 1]$, we get $\dot{\vx} = 0$ on the $x_1 = -x_2 + 1$ line segment in the first quadrant as the continuous attractor.
We refer to it as the \emph{bounded line attractor (BLA)}.
Again, linearization on the attractor shows two eigenvalues, $0$ and $-2$, and perturbations again cause the null eigenvalue to be non-zero and the line attractor disappears.
However, surprisingly, the bifurcations are  qualitatively different.
It either bifurcates into a single stable fixed point (Fig.~\ref{fig:lara_bifurcations}A top) or two stable fixed points separated with a saddle node in between (Fig.~\ref{fig:lara_bifurcations}A bottom).
Neither of these two cases show a divergent flow, but rather consists of one or two basins of attraction.
%It implies only vanishing gradients for this system and \textbf{exploding gradients will not be present for an arbitrarily long time}.

%Ring attractor
We further analyzed a simple (non-biological) system that has a ring attractor, defined by the following ODE: $\dot r = r(1-r), \ \dot \theta = 0.$
This system has as fixed points the origin and the ring with radius one centered around zero, i.e., $(0,0)\cup\{(1,\theta)\ |\ \theta\in[0,2\pi)\}$.
We investigate bifurcations caused by parametric and bump perturbations of the ring invariant manifold (see Sec.~\ref{sec:supp:ring_perturbations}), which is bounded and boundaryless.
All perturbations maintain the topological structure of the invariant manifold (Fig.~\ref{fig:bio_rings}).

This type of behavior (an invariant ring manifold with fixed points on it) has been observed in finite dimensional low-rank attractors \citep{mastrogiuseppe2018, beiran2021}.

%\begin{figure}[tbhp]
%  \centering
%  \includegraphics[width=\textwidth]{UBLABLA}
%  \caption{The critical weakness of continuous attractors is their inherent brittleness as they are rare in the parameter space, i.e., infinitesimal changes in parameters destroy the continuous attractor implemented in RNNs~\cite{seung1996,Renart2003}.
%  %Motivating case study of the two systems implementing the same computation but one near exploding gradients.
%    Phase portraits for unbounded and bounded linear attractors~\eqref{eq:TLN}.
%    Under perturbation of parameters, each of them can bifurcate to one of the two potential systems without the continuous line attractor.
%    Note that the parameters for the UBLA are near a diverging system associated with exploding gradient behavior.
%}
%  \label{fig:ublabla}
%\end{figure}

\begin{figure}[tbhp]
  \centering
  \includegraphics[width=\textwidth]{lara_bifurcations}
  \caption{The critical weakness of continuous attractors is their inherent brittleness as they are rare in the parameter space, i.e., infinitesimal changes in parameters destroy the continuous attractor implemented in RNNs~\cite{seung1996,Renart2003}.
  Some of the structure seems to remain, there is an invariant manifold that is topologically equivalent to the original continuous attractor.
    Phase portraits for the bounded line attractor ~\eqref{eq:TLN} and the simple ring attractor.
    Under perturbation of parameters, each of them bifurcates to systems without the continuous attractor.
}
  \label{fig:lara_bifurcations}
\end{figure}


Is this only true for ReLU parameterized RNNs, or does it generalize?


In this paper, we lay out a new theory of general continuous-valued memory to answer the following questions:
\begin{enumerate}
%    \item Can we avoid exploding gradients under parameter perturbation?
    \item Do we need to worry about the brittleness of the continuous attractor solutions in practice?
    \item How can biological networks approximate the representation of continuous variables?
%    \item 
\end{enumerate}
Our theory provides answers to both questions under mild assumptions in an architecture agnostic manner.
%Using Fenichel's invariant manifold theorem, we derive a sufficient condition for RNNs implementing continuous attractors to remain free of exploding gradients.
%Moreover, even after a bifurcation, these RNNs still approximately behave like the original continuous attractor (for a while).
Using Fenichel's invariant manifold theorem, we derive a universality statement for the bifurcation landscape for one dimensional continuous attractors.
We verify our theoretical insights with numerical experiments.






%\section{Theory of gracefully degrading continuous attractors}\label{sec:theory}
\section{Theory of continuous attractors approximations}\label{sec:theory}
In this section, we apply invariant manifold theory to RNNs and translate the results for the machine learning and theoretical neuroscience audience.
Our emphasis in this paper centers on investigating the distinctive properties of continuous attractors that prove essential for specific tasks, with a deliberate exclusion of considerations related to learning chaotic dynamics.



\subsection{Invariant Continuous Attractor Manifold Theory}\label{sec:imt}
We start by formulating RNNs implementing a continuous attractor in continuous time: $\dot{\vx} = \vf(\vx)$.
Let $l$ be the intrinsic dimension of the manifold of equilibria that defines the continuous attractor.
We will reparameterize the dynamics around the manifold with coordinates $\vy \in \reals^l$ and the remaining ambient space with $\vz \in \reals^{d-l}$.
To describe an arbitrary bifurcation of interest, we introduce a sufficiently smooth function $g$ and a bifurcation parameter $\epsilon \geq 0$, such that the following system is equivalent to the original ODE:
\begin{align}\label{eq:fenichel:flow}
    \dot{\vy} &=           \epsilon  \vg(\vy, \vz, \epsilon) \qquad \text{(tangent)}\\
    \dot{\vz} &= \hphantom{\epsilon} \vh(\vy, \vz, \epsilon) \qquad \text{(normal)}
\end{align}
where $\epsilon = 0$ gives the condition for the continuous attractor $\dot{\vy} = \mathbf{0}$.
We denote the corresponding manifold of $l$ dimensions $\manifold_0 = \{(\vy,\vz) \mid \vh(\vy,\vz,0) = 0\}$.

We need the flow normal to the manifold to be hyperbolic, that is \emph{normally hyperbolic}, meaning that the Jacobians $\nabla_\vz \vh$ evaluated on any point on the $\manifold_0$ has $d-l$ eigenvalues with their real part uniformly away from zero, and $\nabla_\vy \vg$ has $l$ eigenvalues with zero real parts.
More specifically, for continuous attractors, the real part of the eigenvalues of $\nabla_\vz \vh$ will be negative, representing sufficiently strong attractive flow toward the manifold.
Equivalently, for the ODE, $\dot{\vx} = \vf(\vx)$, the variational system is of constant rank, and has exactly $(d-l)$ eigenvalues with negative real parts and $l$ eigenvalues with zero real parts everywhere along the continuous attractor.

\begin{figure}[bthp]
  \centering
  \includegraphics[width=0.5\textwidth]{FenichelThm}
  \caption{
    Fenichel's invariant manifold theorem applied to compact continuous attractor guarantees the flow on the slow manifold is locally invariant and continues to be attractive.
    The dashed line is a trajectory ``trapped'' in the slow manifold (locally invariant).
  }
  \label{fig:fenichel}
\end{figure}

When $\epsilon > 0$, the continuous attractor bifurcates away.
What can we say about the fate of the perturbed system?
The continuous dependence theorem~\citep{Chicone2006} says that the trajectories will change continuously as a function of $\epsilon$ without a guarantee on how quickly they change.
Moreover, the topological structure and the asymptotic behavior of trajectories change discontinuously due to the bifurcation.
Surprisingly there is a strong connection in the geometry due to Fenichel's theorem~\cite{fenichel1971}.
We informally present a special case due to~\cite{Jones1995}:
\begin{theorem}[Fenichel's Invariant Manifold Theorem]
Let $\manifold_0$ be a connected, compact, normally hyperbolic manifold of equilibria originating from a sufficiently smooth ODE.
For a sufficiently small perturbation $\epsilon > 0$, there exists a manifold $\manifold_\epsilon$ diffeomorphic to $\manifold_0$ and locally invariant under the flow of \eqref{eq:fenichel:flow}.
Moreover, $\manifold_\epsilon$ has $\mathcal{O}(\epsilon)$ Hausdoff distance to $\manifold_0$ and has the same smoothness as $g$ and $h$ in (\ref{eq:fenichel:flow}).
\end{theorem}

The manifold $\manifold_\epsilon$ is called the \emph{slow manifold} which is no longer necessarily a continuum of equilibria.
However, the local invariance implies that trajectories remain within the manifold except potentially at the boundary.
Furthermore, the non-zero flow on the slow manifold is slow and given in the $\epsilon \to 0$ limit as $\RN{\vy}{\tau} = \vg(c^\epsilon(\vy), \vy, 0)$ where $\tau = \epsilon t$ is a rescaled time and $c^\epsilon(\cdot)$ parameterizes the $l$ dimensional slow manifold.
In addition, the stable manifold of $\manifold_0$ is similarly approximately persistent~\cite{Jones1995}, allowing the manifold $\manifold_\epsilon$ to remain attractive.

These conditions are met  for the examples in Fig.~\ref{fig:ublabla} (see Sec.~\ref{sec:supp:fast_slow_form} for the rerparametrization of the BLA to this form).
 As a technical note, for the theory apply to a continuous piecewise-linear system, it is required that the invariant manifold is globally attracting \cite{simpson2018}, which is also the case for the BLA.
%applicability of Fenichel’s invariant manifold theorem on continuous piecewise linear (PWL) dynamical systems.
As the theory predicts, BLA bifurcates into a 1-dimensional slow manifold (dark colored regions) that contains fixed points, and overall still attractive.
%On the contrary, the UBLA does not satisfy the compactness condition, hence the theory does not predict its persistence. %extension: 
%Importantly, the ``slow'' flow on the perturbed system is not bounded.

% the compact case is that you have that the invariant manifolds are diffeomorphic to the original one. So neural states aren’t too far off from where they would have been before the perturbation.

%In practice, the sufficient conditions for RNNs implementing continuous attractors to have this graceful breakdown (like BLA but not UBLA) is for the continuous attractor manifold to be of finite dimension throughout, connected, and bounded.
%However, in systems with an invariant manifold with dimension at least three, it is possible that a slow manifold with chaotic dynamics is created through a perturbation.
%This would have as consequence that the perturbed system acquires positive Lyapunov exponents (corresponding to the chaotic orbit), which then can still lead to exploding gradients albeit with very slow flow that has little practical consequence in finite time experiments.
%The probability of such a bifurcation is low.


%\subsection{Implications on Machine Learning}\label{sec:imp:ML}
%Extending the memory time constant of RNNs have long been an important area of research with much focus on random weights~\cite{Legenstein2007,Goldman2009,Toyoizumi2011,Kerg2019,Chen2018,Henaff2016,Rusch2021,arjovsky2016}.
%Various initializations for the recurrent weights have been proposed to help learning: initialization with the identity matrix \citep{le2015}, with a random orthogonal matrix \citep{saxe2014,Henaff2016}, with a unitary matrix \citep{arjovsky2016} and with a block diagonal weight matrix that creates a quasi-periodic system with limit cycles \citep{Sokol2019a}.
%However, despite the capacity to maintain representation of continuous quantities for arbitrary duration of time, continuous attractor mechanism has not been pursued in machine learning research because of its brittleness.
%The stochasticity in gradients inherited from the training data, regularization strategy, and multi-task learning objectives act as a perturbation on the recurrent dynamics, and continuous attractors break down even if it could be learned.
%Remedies emerged in machine learning to hard-code continuous-valued memory structures within the RNNs---e.g., the cell state in vanilla LSTM.
%However, our theory shows that the geometric structure of the manifold and the flow around the manifold play a critical role in enabling gradient descent learning of continuous attractors using standard methods such as backpropagation through time (BPTT)~\cite{Toomarian1991}. 
%
%It is well known that asymptotic exploding gradients comes from positive Lyapunov exponents~\cite{Mikhaeil2022,Vogt2022,Engelken2023}.
%It has also been pointed out that bifurcations can cause arbitrarily large gradients~\cite{doya1993} as well as discontinuity in the Lyapunov spectrum~\cite{Park2023a}.
%These gradient propagation theories suggested that bifurcations should be avoided, including the continuous attractors.
%
%As far as we know, there is no architecture agnostic theory describing the loss landscape around RNN solutions.
%We remark that due to the singular nature of the center manifold that supports the continuous attractor, the usual analysis approach of linearization fails.
%\emph{Our theory successfully connects the invariant manifold theory and the gradient signal propagation theory in RNNs to describe two types of loss landscape around continuous attractor solutions.}
%In one case, when the theorem holds, the landscape is shallow in all directions due to (asymptotically) vanishing gradients induced by the attractor structure---we have the gracefully degrading continuous attractor.
%In the other case, we can find examples where the theorem does not hold, and the continuous attractor solution is at the boundary of network configurations with exploding gradients, meaning the loss landscape is very steep in some directions.
%While exploding gradients would prevent gradient descent to correct for deviations from the optima,
%for gracefully degrading ones, one can apply restorative forces via gradient descent to be in the vicinity of the brittle continuous attractor solution (see Sec.~\ref{sec:exp:maintaining}).


\subsection{}\label{sec:}


The theory implies that systems in the neighbourhood of a CAN in the space of dynamical systems all share the same topology for the attractive persistent invariant manifold.
For a 1 dimensional attractor this implies that the stability structure of the invariant manifold is either 
(1)  composed of an equal number of stable fixed points and saddle nodes, placed alternatingly and with connecting orbits 
or (2) a limit cycle.
%saddle node with homoclinic orbit: with noise equivalent to a limit cycle
For 2 dimensional attractors  stable fixed points and saddle nodes can coexist with limit cycles.


\subsection{Implications for Neuroscience}\label{sec:imp:neuroscience}


Our theory shows that not all continuous attractors are born equal, and there are gracefully degrading continuous attractors.
In finite time, trajectories are well-behaved, contrary to the asymptotic behavior captured by the Lyapunov exponents.
Animal behavior is finite time in nature and the longer the temporal distance the harder it is to learn in general.
The conditions are favorable in the recurrent neuronal networks: (1) mutual inhibition is widely present and evidence points to inhibition dominated dynamics,
(2) the neural state space is bounded due to physiological constraints, namely by a non-negative firing rate below and a maximum firing rate above.

%Given an $d$-dimensional continuous attractor manifold embedded within a recurrent dynamics of $n$-dimensions, it supports persistent continuous memory of $d$-dimension.
%There are $d$ zero Lyapunov exponents corresponding to the perturbations tangent to the manifold, coinciding with the memory representation, and $(n-d)$ negative Lyapunov exponents that expresses the attractive nature.
%In theory, the topology of the manifold can be arbitrary, ideally matching the desired structure of the target variables.








\section{Experiments}

\subsection{Line attractors}



\subsection{Ring attractors}\label{sec:ras}
\subsubsection{Angular integration networks}\label{sec:hd}
For circular variables such as the goal-direction or head-direction needed for spatial navigation in 2D, the temporal integration and working memory functions are naturally solved by a ring attractor (continuous attractor with a ring topology).
Continuous attractor models of the head-direction representation suggest that the representation emerges from the interactions of recurrent connections among neurons that form a ring-like structure~\citep{zhang1996,Noorman2022,ajabi2023}.
Since continuous attractors are susceptible to noise and perturbations the precise representation of the head direction can in principle be disrupted easily.
We demonstrate the consequences of the Persistence Theorem in two models with a continuous ring attractor.

Firstly, we investigated perturbations of a continuous ring attractor proposed as a model for the head direction representation in fruitflies~\citep{Noorman2022}.
As this continuous ring attractor is bounded its invariant manifold persists and, hence, no divergent orbits are created under small perturbations to this system. 
Furthermore, because the ring attractor is boundaryless it is both forward and backward invariant, i.e. hence it is invariant and trajectories never leave the persistent invariant manifold~\citep{wiggins1994}.
This model is composed of $N$ heading-tuned neurons  with preferred headings $\theta_j \in \{\frac{2\pi i}{N}\}_{i=1\dots N}$ radians (see Supp. Sec \ref{sec:supp:headdirection}).
For sufficiently strong local excitation (given by the parameter $J_E$) and broad inhibition ($J_I$), this network will generate a stable bump of activity, one corresponding to each head direction. This continuum of fixed points forms a one dimensional manifold homeomorphic to the circle. 

\begin{figure}[tbhp]
     \centering
  \includegraphics[width=\textwidth]{bio_rings}
       \caption{Characterization of bifurcations of two ring attractors. 
       (A)       Perturbations to the ring attractor \citep{Noorman2022}. The ring attractor can be perturbed in systems with an even number of fixed points (FPs)  up to $2N$ (stable and saddle points are paired). 
       (B) Perturbations to a simple implementation of a ring attractor lead to bifurcations that all leave the invariant manifold intact.
       }
         \label{fig:bio_rings}
\end{figure}

We evaluate the effect of parametric perturbations of the form $ \vW \leftarrow \vW + \vV$ with $\vV_{i,j}\iidsample\mathcal{N}(0,\frac{1}{100})$ on a network of size $N = 6$ with $J_E= 4$ and $J_I=-2.4$ by identifying all bifurcations (Sec.~\ref{sec:supp:ring_perturbations}).
We found that the ring (consisting of infinite fixed points) can be perturbed into systems with between 2 and 12 fixed points (Fig.~\ref{fig:bio_rings}A).
As far as we know, this bifurcation from a ring of equilibria to a saddle and node has not been described previously in the literature.
The probability of each type of bifurcation was numerically estimated.
There are several additional co-dim 1 bifurcations with measure zero (see Fig.~\ref{fig:meaure_zero_perturbations}).
%The space of possible perturbations in Sec. ~\ref{sec:imt} is very large. To be able to form an image of what happens in the theorem we will work out the effect of some examples of perturbations.


%mastrogiuseppe, beiran, ostojic
Low-rank networks can be used to approximate ring attractors~\citep{mastrogiuseppe2018, beiran2021}.
The Dynamical Mean Field (DMF) tell us that in the limit of infinite-size networks one can construct a ring attractor through a rank 2 network by choosing the overlap of the right- and left-connectivity vectors.
However, in simulations of finite-size networks, the dynamics instead always converge on a small number of equilibrium spontaneous states located on the ring \citep{mastrogiuseppe2018}.
Our theory explains this phenomenon as follows.
We can think of the finite size realization as a small perturbation to the infinite size network on the reduced dynamics in the $m^1, m^2$ plane (independent of the parameter $g$ for the random part of the matrix).
This implies that the ring attractor network 
For very small networks the ring structure is destroyed. Only the plane persist as a slow manifold.



We will assess performance at two different time-scales, very-long or asymptotic and a shorter one.
For the asymptotic time-scale the asymptotic behavior of the system dominates.
For a one dimensional system this will either be fixed points or limit cycles.


\subsection{Unpacking slow manifolds} %from trained RNNs

\begin{figure}[tbhp]
  \centering
  \includegraphics[width=\textwidth]{centerout_decomposition}
  \caption{s
}
  \label{fig:centerout_decomp}
\end{figure}


\section{Discussion}
The attractive manifold of equilibria in continuous attractor networks provides two key functions continuous memory and propagation of gradient through time.
Although the corresponding configurations are measure zero, we showed that the when the invariant manifold theorem holds, the finite time behavior of the trajectories and the gradient through time only slowly breakdown.
We investigated the neighborhood of the continuous attractor networks and analyzed diverse bifurcations in 5 example systems.
There were surprisingly diverse bifurcations which provide additional insights to vanishing and exploding gradient regimes that the network visits in the presence of stochasticity in synapses and learning signals.
In particular, we showed that some RNNs are devoid of bifurcations that lead to exploding gradients with non-zero measure.

As the theory predicts, our numerical experiments demonstrate the properties of loss landscape and gradient near the fine-tuned system.
However, after small perturbations, plain gradient descent typically converges to a non-continuous attractor solution, indicating that the homeostatic restoration of continuous attractor may be challenging.
Based on our observations, we cannot conclude that continuous attractors solutions are universally brittle.
Further research on finding continuous attractor networks that may allow restorative learning for larger perturbations in the parameter space is needed.

% We have derived a general theory that describes the loss landscape of RNNs around continuous attractor solutions.
% This theory implies that backpropagating gradient does not explode for systems near compact continuous attractors because of divergent dynamics.
% This insight also suggests that there can exist homeostatic mechanisms for certain implementations of continuous attractors that maintain the structure of the attractor sufficiently for the neural computation it is used in, which we demonstrate in a simple network. 

%\begin{ack}
%Use unnumbered first level headings for the acknowledgments. All acknowledgments go at the end of the paper before the list of references. Moreover, you are required to declare
%funding (financial activities supporting the submitted work) and competing interests (related financial activities outside the submitted work).
%More information about this disclosure can be found at: \url{https://neurips.cc/Conferences/2023/PaperInformation/FundingDisclosure}.
%
%
%Do {\bf not} include this section in the anonymized submission, only in the final paper. You can use the \texttt{ack} environment provided in the style file to autmoatically hide this section in the anonymized submission.
%\end{ack}


%\section*{References}
%The natbib package will be loaded for you by default. Citations may be author/year or numeric, as
%long as you maintain internal consistency. As to the format of the references themselves, any style is
%acceptable as long as it is used consistently

%References follow the acknowledgments in the camera-ready paper. Use unnumbered first-level heading for
%the references. Any choice of citation style is acceptable as long as you are
%consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
%when listing the references.
%Note that the Reference section does not count towards the page limit.
%\medskip

\small

\newpage
\bibliographystyle{abbrvnat}
\bibliography{../cit,../catniplab}
%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section*{NeurIPS Paper Checklist}

%%% BEGIN INSTRUCTIONS %%%
The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: {\bf The papers not including the checklist will be desk rejected.} The checklist should follow the references and precede the (optional) supplemental material.  The checklist does NOT count towards the page
limit. 

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:
\begin{itemize}
    \item You should answer \answerYes{}, \answerNo{}, or \answerNA{}.
    \item \answerNA{} means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
    \item Please provide a short (1–2 sentence) justification right after your answer (even for NA). 
   % \item {\bf The papers not including the checklist will be desk rejected.}
\end{itemize}

{\bf The checklist answers are an integral part of your paper submission.} They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "\answerYes{}" is generally preferable to "\answerNo{}", it is perfectly acceptable to answer "\answerNo{}" provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "\answerNo{}" or "\answerNA{}" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer \answerYes{} to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:
\begin{itemize}
    \item {\bf Delete this instruction block, but keep the section heading ``NeurIPS paper checklist"},
    \item  {\bf Keep the checklist subsection headings, questions/answers and guidelines below.}
    \item {\bf Do not modify the questions and only use the provided macros for your answers}.
\end{itemize} 
 

%%% END INSTRUCTIONS %%%


\begin{enumerate}

\item {\bf Claims}
    \item[] Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the abstract and introduction do not include the claims made in the paper.
        \item The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. 
        \item The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. 
        \item It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 
    \end{itemize}

\item {\bf Limitations}
    \item[] Question: Does the paper discuss the limitations of the work performed by the authors?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. 
        \item The authors are encouraged to create a separate "Limitations" section in their paper.
        \item The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
        \item The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
        \item The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
        \item The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
        \item If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
        \item While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
    \end{itemize}

\item {\bf Theory Assumptions and Proofs}
    \item[] Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include theoretical results. 
        \item All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
        \item All assumptions should be clearly stated or referenced in the statement of any theorems.
        \item The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. 
        \item Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
        \item Theorems and Lemmas that the proof relies upon should be properly referenced. 
    \end{itemize}

    \item {\bf Experimental Result Reproducibility}
    \item[] Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
        \item If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. 
        \item Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
        \item While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example
        \begin{enumerate}
            \item If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.
            \item If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.
            \item If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).
            \item We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
        \end{enumerate}
    \end{itemize}


\item {\bf Open access to data and code}
    \item[] Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that paper does not include experiments requiring code.
        \item Please see the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
        \item While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
        \item The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
        \item The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
        \item The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
        \item At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
        \item Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
    \end{itemize}


\item {\bf Experimental Setting/Details}
    \item[] Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
        \item The full details can be provided either with the code, in appendix, or as supplemental material.
    \end{itemize}

\item {\bf Experiment Statistical Significance}
    \item[] Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
        \item The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
        \item The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
        \item The assumptions made should be given (e.g., Normally distributed errors).
        \item It should be clear whether the error bar is the standard deviation or the standard error of the mean.
        \item It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96\% CI, if the hypothesis of Normality of errors is not verified.
        \item For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
        \item If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
    \end{itemize}

\item {\bf Experiments Compute Resources}
    \item[] Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
        \item The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. 
        \item The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). 
    \end{itemize}
    
\item {\bf Code Of Ethics}
    \item[] Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics \url{https://neurips.cc/public/EthicsGuidelines}?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
        \item If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
        \item The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
    \end{itemize}


\item {\bf Broader Impacts}
    \item[] Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that there is no societal impact of the work performed.
        \item If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
        \item Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
        \item The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
        \item The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
        \item If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
    \end{itemize}
    
\item {\bf Safeguards}
    \item[] Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper poses no such risks.
        \item Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. 
        \item Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
        \item We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
    \end{itemize}

\item {\bf Licenses for existing assets}
    \item[] Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not use existing assets.
        \item The authors should cite the original paper that produced the code package or dataset.
        \item The authors should state which version of the asset is used and, if possible, include a URL.
        \item The name of the license (e.g., CC-BY 4.0) should be included for each asset.
        \item For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
        \item If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, \url{paperswithcode.com/datasets} has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
        \item For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
        \item If this information is not available online, the authors are encouraged to reach out to the asset's creators.
    \end{itemize}

\item {\bf New Assets}
    \item[] Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not release new assets.
        \item Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. 
        \item The paper should discuss whether and how consent was obtained from people whose asset is used.
        \item At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
    \end{itemize}

\item {\bf Crowdsourcing and Research with Human Subjects}
    \item[] Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? 
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
        \item Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. 
        \item According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 
    \end{itemize}

\item {\bf Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects}
    \item[] Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
        \item Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. 
        \item We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. 
        \item For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.
    \end{itemize}

\end{enumerate}

\newpage
\appendix
\section*{Supplementary Material}
\setcounter{section}{0}
\renewcommand{\thefigure}{S\arabic{figure}} % figure numbering for supplement
\renewcommand{\thesection}{S\arabic{section}} % figure numbering for supplement

%RNNs are capable of learning complex patterns and relationships in the data, which makes them in particular useful as models for neural computations. This is achieved through the use of non-linear activation functions and the training of the network using backpropagation through time (BPTT).
%BPTT allows the network to adjust the weights of the connections between neurons based on the error signal that is propagated backwards through time. 
%%This makes them suitable models for 
%
%However, training RNNs using back-propagation through time to compute error-derivatives can be difficult.  Early attempts suffered from vanishing and exploding gradients \citep{kolen2001} and this meant that they had great difficulty learning long-term dependencies. 
%Many different methods have been proposed for overcoming this difficulty.


%\section{Persistence in non-differentiable vector fields}
%The applicability of the Persistence Theorem to non-differentiable vector fields would depend on the specific characteristics of the system and the nature of the non-differentiability.
%

\section{Experiments}
To computationally investigate the neighborhood of recurrent dynamical systems that implement continuous attractors, we investigate 5 RNNs that are known a priori to form 1 or 2 dimensional continuous attractors.
We consider two topologically distinct temporal integration tasks: (i) linear integration, and (ii) angular integration.
For all experiments we used single precision floating point arithmetic and PyTorch.
%In order to demonstrate the implications of the theory of the persistence of bounded continuous attractors, we rigorously test the predictions of the theory on the stability of the BLA.
% Our objective is to assess the practical implications of the theoretical findings of bounded continuous attractors in a small and tractable system, and second, to contribute empirical evidence that can help refine and extend existing theoretical frameworks. 
%
\subsection{Linear temporal integration task}\label{sec:task:continuous-clicks}
Given a sequence of scalar input, the job of the network is to accumulate the values over time and report the final value at a later time.
In the context of perceptual decision-making, subjects can be trained to perform the Poisson clicks task where they have to count the differing number of sensory stimulus events from the left and right side and report the side~\cite{brunton2013}.
A linear integrator as a continuous attractor is a natural solution to such a task.
We generalize the clicks to have associated continuous-values for the training of RNNs to discourage discrete counting solutions.

We used discrete time representations over $T$ time bins and the stimulus encoded as difference of two non-negative values:
\begin{align}
    I_{t,i} &= m_{t,i} \cdot u_{t,i} 
             \qquad & t=1,\dots, T, \ i=1,2 &&\text{(continuous clicks)}	\label{eq:input}
    \\
    O^\ast_{t} &= \sum_{s=0}^{t} \left(
        I_{s,1} - I_{s,2}
        \right)
            \qquad  & t=1,\dots, T &&  \text{(desired output)} 			\label{eq:output}
\end{align}
where $m_{t,i}$ are independent Bernoulli random variables with probability $0.2$ and $u_{t,i}$ are independent random variables with uniform distribution on the unit interval.
We used mean squared error (MSE) of the 1-dimensional output over time as the loss function over all time bins.
We used $T=100$ time bins per trial unless specified otherwise.
The gradients were computed in batch mode with $1024$ randomly generated trials.
% The main challenges of this task are (1) addition and subtraction and (2) maintain the accumulated clicks for an extended period of time.

% We designed a simple integration task where one of the optimal solutions is the continuous attractor. % <== not necessarily true!

%The inputs for the clicks task are incoming clicks in two channels ($K=2$) and are given as follows.
%The non-zero inputs follow a Poisson distribution with given rates.
%At such time points, an input is sampled from $[0,1]$.
%The task is to output the difference between the two inputs in the two channels. 

%A noisy version of the task also includes noise added to a part of the input, without alternation to the target output, i.e., the noise added to the system should be ignored by the system.

\subsubsection{RNN solutions to the linear integration task}\label{sec:rnn:integration}
We use vanilla RNN implementations with the standard parameterization:
%An RNN \citep{elmanFindingStructureTime1990} consists of a $N \times N$ transition matrix $W$, an $L \times N$ decoder matrix $\wout$ (where $L$ is the output dimension), a $N \times K$ encoder matrix $\win$ (where $K$ is the input dimension), and a bias $b$ for the hidden state and $\bout$ for the output.
% If either the output or input is categorical, $M$ (respectively $N$) is the number of classes, and we use a one-hot representation. 
%As the RNN ingests a sequence, at each timestep it updates to a hidden state $h$, and using the hidden state and the decoder matrix, produces outputs $y$:
\begin{equation}
  \begin{aligned}
	\vx_t &= \sigma(\win \vI_t + \vW \vx_{t-1} + \vb) \label{eq:RNN:discrete}\\
	O_t &= \wout \vx_t + \bout
  \end{aligned}
\end{equation}
where $\vx_t \in \reals^d$ is the hidden state, $\vI_t \in \reals^K$ is the input,
$\sigma: \reals \to \reals$ an activation function which acts on each of the hidden dimension, and
$\vW, \vb, \win, \wout, \bout$ are parameters.
Assuming an Euler integration with unit time step, the discrete-time RNN of \eqref{eq:RNN:discrete} corresponds to the ODE:
\begin{align}
    \dot{\vx} &= -\vx + \sigma(\win \vI + \vW \vx + \vb). \label{eq:RNN:continuous}
\end{align}
%
For tractable analysis, we consider $2$ dimensional systems with ReLU activation. %We include another continuous attractor, the identity RNN, which is also a popular initialization for RNN training \citep{le2015}.
We study the three different ReLU RNN implementations of a perfect integrator in a 2 dimensional system, the Identity RNN (iRNN), UBLA and BLA (we refer to the line attractors together as LA).
These three networks have same norm in the recurrent matrix $\vW$ but not close in the parameter space.
On the original clicks task the UBLA and BLA networks count the click differences directly, while iRNN counts the clicks separately and then subtracts these representations through the output mapping.
The behaviors of UBLA and BLA in the absence of stimulus are shown in Fig.~\ref{fig:ublabla}, while the behavior of the iRNN is trivial since there is no flow. These networks are defined as follows.

\paragraph{Identity RNN~\citep{le2015}}
\label{sec:ubpa,sec:iRNN}
%This is an RNN with the identity matrix as its recurrent weights  with two hidden units. 
\begin{equation}\label{eq:irnn}
\win = 
\begin{pmatrix}
1  &  0 \\
0 &  1
\end{pmatrix}, \
\vW = 
\begin{pmatrix}
1  &  0 \\
0  &  1
\end{pmatrix}, \
\wout = 
\begin{pmatrix}
-1  \\  1 
\end{pmatrix}, \
\vb = 
\begin{pmatrix}
0  \\ 0
\end{pmatrix}, \
\bout = 0.
\end{equation}

%The system has the whole state space as its invariant manifold.

\paragraph{Unbounded line attractor}
We formulate this implementation of a bounded integrator with a parameter that determines step size along line attractor $\alpha$. Together with the parameters for the output bias $\beta$ the parameters determine the capacity of the network. While the line attractor is unbounded from above, it only extends to the center from below.  The step size along line attractor $\alpha$ determines the maximum number of clicks as the difference between the two channels; the capacity is $\beta/\alpha$ number of clicks.
\label{sec:ubla}
\begin{equation}\label{eq:ubla}
\win = \alpha
\begin{pmatrix}
-1  &  1 \\
-1  &  1
\end{pmatrix}, \
\vW = 
\begin{pmatrix}
0  &  1 \\
1  &  0
\end{pmatrix}, \
\wout = \frac{1}{2\alpha}
\begin{pmatrix}
1  \\  1 
\end{pmatrix}, \
\vb = 
\begin{pmatrix}
0  \\  0
\end{pmatrix}, \
\bout = -\frac{\beta}{\alpha}.
\end{equation}



\paragraph{Bounded line attractor}\label{sec:bla}
Similarly as for UBLA, the BLA has a parameter that determines step size along line attractor $\alpha$. Analogously as for UBLA, these parameters determine the capacity of the network.
The inputs push the input along the line attractor in two opposite directions, see below. UBLA and BLA need to be initialized at $\beta(1,1)$ and $\tfrac{\beta}{2}(1,1)$, respectively, for correct decoding, i.e., output projection.
\begin{equation}\label{eq:bla}
\win = \alpha
\begin{pmatrix}
-1  &  1 \\
1  &  -1
\end{pmatrix}, \
\vW = 
\begin{pmatrix}
0  &  -1 \\
-1  &  0
\end{pmatrix}, \
\wout = \frac{1}{2\alpha}
\begin{pmatrix}
1  \\  -1 
\end{pmatrix}, \
\vb = \beta
\begin{pmatrix}
1 \\  1 
\end{pmatrix}, \
\bout = 0.
\end{equation}

\subsubsection{Asymmetric loss landscape reflecting dynamics after bifurcation}\label{sec:asymmetricloss}
To illustrate the effect of bifurcations from the continuous attractor solution, we take a 1-dimensional slice of the loss surface, see Fig.~\ref{fig:maintenance_h0}B.
Specifically, we continuously vary one of the entries of the self-recurrent connection matrix: $    \vW_{1,1} \leftarrow \vW_{1,1} + \Delta$. % with $\Delta\in[-\tfrac{1}{10},\tfrac{1}{10}].$
At any $\Delta \neq 0$, the continuous attractor disappears and the spontaneous dynamics of the networks show convergent and/or divergent behavior at exponential rates.
Therefore, as the number of time steps in a trial increases, the error in the output also exponentially converge or diverge in a corresponding manner.
As can be seen in Fig.~\ref{fig:maintenance_h0}B, for UBLA and iRNN, $\Delta > 0$  perturbations shows exponentially increasing loss and corresponds to an exploding gradient dynamical system.
In all other cases, including all perturbations of BLA, leads to vanishing gradient, hence the loss is bounded.
Note also the high curvature of the loss landscape around the optimal solution indicating that the slow manifold may only be maintained in a small neighborhood around the optimal solution, especially for the LAs.

\subsubsection{Bifurcation probability and random perturbations of BLA}
We consider all parametrized perturbations of the form $ \vW \leftarrow \vW + \vV$ for a random matrix $\vV\in \mathbb{R}^{2\times 2}$ to the BLA.
The BLA can bifurcate in the following systems, characterized by their invariant sets: a system with single stable fixed point, a system with three fixed points (one unstable and two stable) and  a system with two fixed points (one stable and the other a half-stable node) and a system with a (rotated) line attractor. 
Only the first two bifurcations (Fig.~\ref{fig:ublabla}) can happen with nonzero chance for the type of random perturbations we consider.
The perturbations that leave the line attractor intact or to lead to a system with two fixed points have measure zero in the parameter space.
%The types of perturbation with measure zero are codimension 2 bifurcations.
The perturbation that results in one fixed point happen with probability $\frac{3}{4}$, while perturbations lead to a system with three fixed points with probability $\frac{1}{4}$, see Sec.~\ref{sec:supp:bla}.
The (local) %, in the case of the single fixed point)
 invariant manifold manifold is indeed persistent for the BLA and homeomorphic to the original (the bounded line).



\subsubsection{Maintaining a neural integrator}\label{sec:exp:maintaining}
The theory of persistent invariant manifolds for compact continuous attractors suggests that the BLA should have bounded gradients (unlike UBLA and iRNN) and hence it should be easier to maintain it in the presence of noise.
To investigate the differential effect of stochastic gradient descent (SGD) on the three neural integrator models, we performed three learning experiments using the continuous-valued click integration task.
The input and output are defined as in Eqs.~\ref{eq:input} and \ref{eq:output} with $I_{t,i}=0$ for $t=11,\dots,T$.
%T=100,500,1000
We investigate the effects of perturbations of the recurrent matrix on the learning of the parameters during gradient descent starting from the perfect solutions to the task.  Gradient step were taken with a fixed gradient step size $\lambda$ (learning rate).
%MSE at last time step
%For SGD, the output of the network over $T$ steps was taken to calculate the loss based on the mean squared error (MSE) over a batch of 1024 trials.
 We set $\alpha=1$ and $\beta=20$ in Eq \ref{eq:ubla} and \ref{eq:bla}. The hidden state at the start of a trial is a learnable parameter. 

In the first experiment, Gaussian random noise is injected to all parameters inducing a constant diffusion of the parameters, which emulates the biological synaptic variability.
This type of noise is directly applied to the weights as $ \vW \leftarrow \vW + \vV$ with $\vV_{i,j}\sim\mathcal{N}(0,\sigma)$. To dissociate the effect of misadjustment from gradient descent and external perturbation, we measured the effect of a single perturbation on the learning dynamics.
Fig.~\ref{fig:maintenance_h0}C shows that for all networks, gradient descent (with constant learning rate, chosen from a grid search) was able to counter the diffusion.
%The distribution of MSE for the task can be used as proxy for the misadjustment from the optimal solution.
BLA and UBLA with learning have superior misadjustment compared to iRNN and compared to perturbations without learning, while the BLA has the broadest range of learning rates that are optimal and far away from exploding gradients (Fig.~\ref{fig:maintenance_h0}A).
BLA has a slight advantage in terms of a smaller spread of MSE compared to UBLA.
The invariant manifold of the BLA is persistent throughout learning in many cases, see \ref{sec:supp:learning} and Fig.~\ref{fig:vfs1}. However, the gradients are not pointing towards the BLA but to one of the bifurcations of the BLA (see Supp~Fig.~\ref{fig:wdn_mse_trajectories_F1}, Fig.~\ref{fig:vfs5} and Supp. Fig.~\ref{fig:vfs30}). We determine the alignment of the gradient as the cosine similarity of the gradient step with the vector in recurrent parameter space that points towards the initial parameters at every gradient step and use a cutoff of a maximum deviation of 45\textdegree\  as aligned gradients with the optimal solution direction.
iRNN often finds a different optimum (it settles at a part of the state space that is at a non-zero distance from the initial recurrent matrix and bias (Fig.~\ref{fig:maintenance_h0}D and Fig~\ref{fig:vfs1}).
UBLA can stay close to the initial solution for a small enough learning rate (Fig.~\ref{fig:maintenance_h0}D and E) and maintains a slower flow than the BLA (Fig.~\ref{fig:speeds}).


%The noise level $\sigma$ was chosen individually for the three networks as follows. 
We calculated the loss on a batch of inputs for various noise levels $\sigma$ for all three noise types (Fig.~\ref{fig:matching_noise_3types_cont}).
We chose a matched noise level per integrator that corresponded to a set average loss averaged over 200 weight perturbations (see also in Sec.~\ref{sec:supp:matching}).
This way of matching noise level to induce the same loss should be a universal approach to be able to compare the performance of different networks.

%description of finding optimal learning rate
For the matched noise level, we find the optimal learning rate for each network separately.
The optimal learning rates for the input-type noise experiments were chosen from a set of values 
($\{(1+j\frac{1}{4}))10^{-i}\}_{i=4,\dots 10, j=1,2,3}$))  based on best performance of the task, measured as mean MSE of the last ten epochs averaged over ten runs.
The slow manifold that is created after perturbations provides gradients that can counteract parameter diffusions for all networks (on short trials), even for the ones that have the potential for exploding gradients (Fig.~\ref{fig:maintenance_h0}A and C).
 %new conclusions
We use the normed difference to the initial parameters at every gradient step as proxy for the misadjustment from the optimal solution (Fig.~\ref{fig:maintenance_h0}D and E).
 We further show that all networks converge to a different (from the initialization), non-optimal, solution as they settle in a regime in parameter space that has a higher norm difference with the initial parameters of the neural integrator in ten different runs with the same random seed for the noise for the three integrators (Fig.~\ref{fig:maintenance_h0}D and E).
 We conclude therefore that, in practice, it is difficult to maintain any of the continuous attractors.

%gradientdistribution 
%Then we looked for networks that diverged due to exploding gradients which we identify as having a loss of 1 or higher. 
Note that exploding gradients can be seen for UBLA manifested as the bimodal distribution of the gradients in Fig.~\ref{fig:maintenance_h0}F. This does lead to faster divergence (for lower learning rate) but has on the other hand the benefit of providing useful gradients to maintain the (local) solution around the optimal solution, which explains the superior performance at the optimal learning rate for the UBLA (Fig.~\ref{fig:maintenance_h0}A), on this timescale for the trial that we investigated.
Also in the presence of input and internal noise the UBLA has a higher tendency to have exploding gradients for lower learning rates, see Fig.~\ref{fig:all_lrs_vs_mses}. 
%This is because the noise sometimes pushes the UBLA to the regime with divergent dynamics which leads to exploding gradients, which then causes the weights to make bigger jumps.
%The fact that the iRNN has a smaller range of gradients explains exploding gradients at a higher learning rate for iRNN and the need for a higher learning rate for optimal maintenance of the solution and is explained by a more shallow loss landscape in the close vininity of the iRNN (Fig.~\ref{fig:maintenance_h0}C).
We hypothesise that the negative effect of exploding gradients shows only for longer trials.


\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{maintenance_h0_less}
  \caption{
  Comparing three continuous attractor solutions on the click integration task of $T=100$ time steps.
(A) MSE distribution during learning with different learning rates.
(B) Loss landscape is steeper around the attractors. The BLA has a bounded loss in its neighborhood.
(C) MSE distribution during learning and in the presence of noise. Learning counteracts diffusion in all three-types of initializations.
(D) All networks converge to local non-optimal solutions after a single perturbation in 30 gradient steps.
(E) Distance of parameters to original during learning with noise (left) and without learning (right).
(F) Gradient distribution at the beginning (upper) and end (lower) of trials.
  }
  \label{fig:maintenance_h0}
\end{figure}


\section{Bifurcation analysis of the line attractors}

%\subsection{Discrepancies between discrete and continuous RNNs}
%\label{sec:discrepancies}
%In the discrete time version of vanilla RNN \eqref{eq:RNN:discrete}, there can exist 2-period orbits, which are absent in the \eqref{eq:RNN:continuous} version.

\subsection{Unbounded line attractor}
%\label{sec:ubla}
%
%The parameters:
%\begin{equation}\label{eq:bla}
%\win = \alpha
%\begin{pmatrix}
%-1  &  1 \\
%1  &  -1
%\end{pmatrix}, \
%W = 
%\begin{pmatrix}
%0  &  1 \\
%1  &  0
%\end{pmatrix}, \
%\wout = \frac{1}{2\alpha}
%\begin{pmatrix}
%1  &  1 
%\end{pmatrix}, \
%\bout = -\frac{\beta}{\alpha}.
%\end{equation}
%
%The bias to the recurrent units is zero.


\paragraph{Stabilty of the fixed point with full support}
We investigate how perturbations to the bounded line affect the Lyapunov spectrum.
We calculate the eigenspectrum of the Jacobian:
\begin{align*}
\det [W' -(1+\lambda)\mathbb{I}] &= (\epsilon_{11}-1-\lambda)(\epsilon_{22}-1-\lambda)-(\epsilon_{12}+1)(\epsilon_{21}+1)\\
&=\lambda^2 - (2+\epsilon_{11}+\epsilon_{22})\lambda -\epsilon_{11}-\epsilon_{22}+\epsilon_{11}\epsilon_{22} -\epsilon_{12} - \epsilon_{21} - \epsilon_{12}\epsilon_{21}
\end{align*}

Let 
$u=- (2+\epsilon_{11}+\epsilon_{22})$
and 
$v=-\epsilon_{11}-\epsilon_{22}+\epsilon_{11}\epsilon_{22} -\epsilon_{12} - \epsilon_{21} - \epsilon_{12}\epsilon_{21}$

There are only two types of invariant set for the perturbations of the line attractor. Both have as invariant set a fixed point at the origin. What distinguishes them is that one type of perturbations leads to this fixed point being stable while the other one makes it unstable.



\subsection{Bounded line attractor}\label{sec:supp:bla}
%Another implementation of a perfect integrator: one which is bounded.

\paragraph{Input}
Parameter that determines step size along line attractor $\alpha$.
The size determines the maximum number of clicks as the difference between the two channels. 
This pushes the input along the line ``attractor" in two opposite directions, %what is the correct word for this type of invariant set?
see below.

%The parameters:
%\begin{equation}\label{eq:bla}
%\win = \alpha
%\begin{pmatrix}
%-1  &  1 \\
%1  &  -1
%\end{pmatrix}, \
%W = 
%\begin{pmatrix}
%0  &  -1 \\
%-1  &  0
%\end{pmatrix}, \
%\wout = \frac{1}{2\alpha}
%\begin{pmatrix}
%1  &  -1 
%\end{pmatrix}, \
%b = \beta
%\begin{pmatrix}
%1 \\  1 
%\end{pmatrix}, \
%\bout = 0.
%\end{equation}
%
%Needs to be initialized at $\tfrac{\beta}{2}(1,1)$ for correct decoding, i.e., output projection



\paragraph{Stability of the fixed points}
We perform the stability analysis for the part of the state space where $Wx>0$.
There, the Jacobian is
\begin{equation}
J = -
\begin{pmatrix}
1  &  1 \\
1  &  1
\end{pmatrix}
\end{equation}

We apply the perturbation
\begin{equation}
W' = 
\begin{pmatrix}
0  &  -1 \\
-1  &  0
\end{pmatrix}
+ \epsilon
\end{equation}
with 
\begin{equation}
\epsilon = 
\begin{pmatrix}
\epsilon_{11}  &  \epsilon_{12} \\
\epsilon_{21}  &  \epsilon_{22}
\end{pmatrix}
\end{equation}

The eigenvalues are computed as
\begin{align*}
\det [W' -(1+\lambda)\mathbb{I}] &= (\epsilon_{11}-1-\lambda)(\epsilon_{22}-1-\lambda)-(\epsilon_{12}-1)(\epsilon_{21}-1)\\
&=\lambda^2 + (2-\epsilon_{11}-\epsilon_{22})\lambda -\epsilon_{11}-\epsilon_{22}+\epsilon_{11}\epsilon_{22} +\epsilon_{12} + \epsilon_{21} - \epsilon_{12}\epsilon_{21}
\end{align*}

Let 
$u=2-\epsilon_{11}-\epsilon_{22}$
and 
$v=-\epsilon_{11}-\epsilon_{22}+\epsilon_{11}\epsilon_{22} + \epsilon_{12} + \epsilon_{21} - \epsilon_{12}\epsilon_{21}$

\begin{equation}
\lambda = \frac{-u \pm \sqrt{u^2-4v}}{2}
\end{equation}



Case 1: $\operatorname{Re}(\sqrt{u^2-4v})<-u$, then 
$\lambda_{1,2}<0$


Case 2:  $\operatorname{Re}(\sqrt{u^2-4v})>-u$, then 
$\lambda_{1}<0$ and $\lambda_{2}>0$


Case 3: $v=0$, then 
$\lambda=\tfrac{1}{2}(-u\pm u)$, i.e.,
$\lambda_1=0$ and  $\lambda_2=-u$

\begin{align}
\epsilon_{11} &= -\epsilon_{22}+\epsilon_{11}\epsilon_{22} + \epsilon_{12} + \epsilon_{21} - \epsilon_{12}\epsilon_{21}
\end{align}



We give some examples of the different types of perturbations to the bounded line attractor.
The first type is when the invariant set is composed of a single fixed point, for example for the perturbation:
\begin{equation}
\epsilon = \frac{1}{10}
\begin{pmatrix}
-2  &  1 \\
 1   &  -2
\end{pmatrix}
\end{equation}
%See Figure \ref{fig:bounded_lineattractor_allpert}, left upper.



The second type is when the invariant set is composed of three fixed points:
\begin{equation}
\epsilon = \frac{1}{10}
\begin{pmatrix}
1  &  -2 \\
 -2  &  1
\end{pmatrix}
\end{equation}

The third type is when the invariant set is composed of two fixed points, both with partial support.
\begin{equation}
b' =  \frac{1}{10}
\begin{pmatrix}
1 & -1
\end{pmatrix}
\end{equation}

The fourth and final type is when the line attractor is maintained but rotated:
\begin{equation}
\epsilon =  \frac{1}{20}
\begin{pmatrix}
1 & 10\\
10 & 1
\end{pmatrix}
\end{equation}

\begin{theorem}
All perturbations of the bounded line attractor are of the types as listed above.
\end{theorem}



\begin{proof}
We enumerate all possibilities for the dynamics of a ReLU activation network with two units.
First of all, note that there can be no limit cycle or chaotic orbits.

Now, we look at the different possible systems with fixed points.
There can be at most three fixed points \citep[Corollary 5.3]{morrisonDiversityEmergentDynamics2022}.
There has to be at least one fixed point, because the bias is non-zero.

%1 fixed point
General form (example):
\begin{equation}
\epsilon = \frac{1}{10}
\begin{pmatrix}
-2  &  1 \\
 1   &  -2
\end{pmatrix}
\end{equation}

One fixed point with full support:

In this case we can assume $W$ to be full rank.

\begin{align*}
\dot x = 
\relu\left[
\begin{pmatrix}
\epsilon_{11}  &  \epsilon_{12} \\
\epsilon_{21}  &  \epsilon_{22}
\end{pmatrix}
\begin{pmatrix}
x_1\\x_2
\end{pmatrix}
+
\begin{pmatrix}
1\\1
\end{pmatrix}
\right]
-
\begin{pmatrix}
x_1\\x_2
\end{pmatrix}
&=0
\end{align*}


Note that $x>0$ iff $z_1\coloneqq \epsilon_{11}x_1 + (\epsilon_{12}-1)x_2-1>0$. Similarly for $x_2>0$.

So for a fixed point with full support, we have 
\begin{equation}
\begin{pmatrix}
x_1\\x_2
\end{pmatrix}
=A^{-1}
\begin{pmatrix}
-1\\-1
\end{pmatrix}
\end{equation}
with 
\[A\coloneqq\begin{pmatrix}
\epsilon_{11}-1  &  \epsilon_{12}-1 \\
\epsilon_{21}-1  &  \epsilon_{22}-1
\end{pmatrix}.\]


Note that it is not possible that $x_1=0=x_2$.

Now define
\[
B\coloneqq A^{-1} = \frac{1}{\det A}
\begin{pmatrix}
\epsilon_{22}-1  &  1-\epsilon_{12} \\
1-\epsilon_{21}  &  \epsilon_{11}-1
\end{pmatrix}
\]
with \[\det A = \epsilon_{11}\epsilon_{22}-\epsilon_{11}-\epsilon_{22}-\epsilon_{12}\epsilon_{21}+\epsilon_{12}+\epsilon_{21}.\]

Hence, we have that $x_1,x_2>0$ if $B_{11}+B_{12}>0$, $B_{21}+B_{22}>0$ and $\det A >0$ 
or if $B_{11}+B_{12}<0$, $B_{21}+B_{22}<0$ and $\det A <0$.

This can be satisfied in two ways, 
If $\det A >0$, this is satisfied if $\epsilon_{22}>\epsilon_{12}$ and $\epsilon_{11}>\epsilon_{21}$,
while if $\det A <0$, this is satisfied if $\epsilon_{22}<\epsilon_{12}$ and $\epsilon_{11}<\epsilon_{21}$.
This gives condition 1. %necessary condition (#1)



Finally, we investigate the condition that specify that there are fixed points with partial support.
%condition for no fixed points for which $x_i=0$ for i=1 or i=2 (necessary condiiton #2)
If $x_1=0$ then $(\epsilon_{22}-1)x_2+1=0$ and $z_1<0$. 
From the equality, we get that $x_{2}=\frac{1}{1-\epsilon_{22}}$.
From the inequality, we get  $(\epsilon_{12}-1)x_2+1\geq 0$, i.e. $\frac{1}{1-\epsilon_{12}}\geq x_2$.
Hence, 
\begin{equation*}
\frac{1}{1-\epsilon_{12}}\geq\frac{1}{1-\epsilon_{22}}
\end{equation*}
and thus
\begin{equation}\label{eq:condition2.1}
\epsilon_{22} \leq \epsilon_{12}.
\end{equation}

Similarly to have a fixed point $x^*$ such that $x_2^*=0$, we must have that 
\begin{equation}\label{eq:condition2.2}
\epsilon_{11} \leq \epsilon_{21}.
\end{equation}

Equation \ref{eq:condition2.1} and \ref{eq:condition2.2} together form condition 2.


Then, we get the following conditions for the different types of bifurcations:
\begin{enumerate}
%2 fixed points
\item  If condition 1 is violated, but condition 2 is satisfied with exactly oner strict inequality, there are two fixed points on the boundary of the admissible quadrant.
%what about the subconditions:
\item If condition 1 is violated, and only one of the subconditions of condition 2 is satisfied, there is a single fixed point on one of the axes.
\item If condition 2 is violated, there is a single fixed point with full support.
%what about the subconditions?
%3 fixed points
\item If both conditions are satisfied, there are three fixed points.
%what about the subconditions?
\end{enumerate}


We now look at the possibility of the line attractor being preserved. 
This is the case if $v=0$.
It is not possible to have a line attractor with a fixed point off of it for as there cannot be disjoint fixed points that are linearly dependent \citep[Lemma 5.2]{morrison2016a}.
\end{proof}

\subsection{Structure of the parameter space}
\begin{table}[H]
\caption{Summary of the conditions for the different bifurcations.}\label{tab:bifs}
\centering
\bgroup
\def\arraystretch{1.52}
\begin{tabular}{|c||c|c|c|c|c|}
\hline
& 1FP (full) 		& 1FP (partial) & 3FPs & 2FPs & LA  \\\hline \hline
C1 & \cmark	 	& \xmark 	 & \cmark & \xmark & \xmark \\\hline 
C2 & \xmark 		& only Eq\ref{eq:condition2.1} or \ref{eq:condition2.2}  	 & \cmark & \cmark& \xmark \\\hline 
\end{tabular}
\egroup
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{bla_parameter_space}
  \caption{A slice of the parameter space of the BLA for a fixed $\epsilon_{11}$ and $\epsilon_{12}$. %The different bifurcations can be found 
  }
  \label{fig:blaparameterspace}
\end{figure}


\subsubsection{Probability of bifurcation types}
We check what proportion of the bifurcation parameter space is constituted with bifurcations of the type that result in three fixed points.

The conditions are 
\begin{align*}
0 &< \epsilon_{11}\epsilon_{22}-\epsilon_{11}-\epsilon_{22}-\epsilon_{12}\epsilon_{21}-\epsilon_{12}-\epsilon_{21},\\
\epsilon_{22} &\leq \epsilon_{12},\\
\epsilon_{11} &\leq \epsilon_{21}.
\end{align*}


Because
\begin{align*}
\epsilon_{22} &\leq \epsilon_{12},\\
\epsilon_{11} &\leq \epsilon_{21}.
\end{align*}
we always have that
\begin{align*}
0 &< \epsilon_{11}\epsilon_{22}-\epsilon_{11}-\epsilon_{22}-\epsilon_{12}\epsilon_{21}-\epsilon_{12}-\epsilon_{21}.
\end{align*}


This implies that this bifurcation happens with $\frac{1}{4}$ probability in a $\epsilon$-ball around the BLA neural integrator with $\epsilon<1$.


\subsection{Fast-slow form}\label{sec:supp:fast_slow_form}

We transform the state space so that the line attractor aligns with the $y$-axis.
So, we apply the affine transformation $R_\theta(x-\frac{1}{2})$ with the rotation matrix $R_\theta = \begin{bmatrix}\cos\theta &-\sin\theta\\\sin\theta&\cos\theta\end{bmatrix}= \frac{1}{\sqrt{2}}\begin{bmatrix}1 &1\\-1&1\end{bmatrix}$ where we have set $\theta=-\frac{\pi}{4}$.
So we perform the transformation $x\rightarrow x'= R_\theta(x-\frac{1}{2})$ and so we have $x=R^{-1}_\theta x'+\frac{1}{2}$ with $R^{-1}_\theta = R_{-\theta}$.
Then we get that 
\begin{align}
R_{\theta}^{-1}\dot x' = \operatorname{ReLU}\left(W(R^{-1}_\theta x'+\frac{1}{2})+1\right)-R^{-1}_\theta x'-\frac{1}{2}.
\end{align}
For a perturbed connection matrix $W=\begin{bmatrix}\epsilon &-1\\-1&0\end{bmatrix}$ we get 
\begin{align}
R_{\theta}^{-1}\dot x' &= \operatorname{ReLU}\left(\frac{1}{\sqrt{2}}\begin{bmatrix}\epsilon &-1\\-1&0\end{bmatrix}\left(\begin{bmatrix}1 &-1\\1&1\end{bmatrix} x'+\frac{1}{2}\right)+1\right)-\frac{1}{\sqrt{2}}\begin{bmatrix}1 &-1\\1&1\end{bmatrix} x'-\frac{1}{2}\\
\dot x' &=\begin{bmatrix}-1 &1\\1&1\end{bmatrix}\left(\frac{1}{2}\begin{bmatrix}\epsilon-1 &-\epsilon-1\\-1&1\end{bmatrix}x' + \frac{1}{2\sqrt{2}}\begin{bmatrix}\epsilon-1 \\-1\end{bmatrix}+\begin{bmatrix}1 \\1\end{bmatrix}-\frac{1}{2}\begin{bmatrix}1 \\1\end{bmatrix}\right)-x'\\
\dot x' &=\left(\begin{bmatrix}-2 &0\\0&0\end{bmatrix}+\frac{\epsilon}{2}\begin{bmatrix}1 &-1\\-1&1\end{bmatrix}\right)x' + \frac{1}{2\sqrt{2}}\begin{bmatrix}\epsilon \\-\epsilon\end{bmatrix}
\end{align}

\section{Smoother activation functions}
It is well-known that activation functions ($\sigma$ in  Eqs.~\ref{eq:RNN:discrete} and  \ref{eq:RNN:continuous}), which can take many forms, play a critical role in propagating gradients effectively through the network and backwards in time \citep{jagtap2023,ramachandran2017,hayou2019}.
Activation functions that are $C^r$ for $r\geq 1$ are the ones to which the Persistence Theorem applies. 
The Persistence Theorem further specifies how the smoothness of the activation can have implications on the smoothness of the persistent invariant manifold.
For situations where smoothness of the persistent invariant manifold is of importance, smoother activation functions might be preferable, such as the Exponential Linear Unit (ELU)\citep{clevert2015} or the Continuously Differentiable Exponential Linear Units (CELU) \citep{barron2017}.

\newpage
\section{Internal and input noise and noise level matching}\label{sec:supp:matching}
%Rationale: what is a universal measure of noise level
%that is applicable to any dyn sys and makes comparisons of performance (under gradient descent) meaningful?


We investigate the effects of two other types of noise on the learning of the parameters during gradient descent starting from the perfect solutions to the task.
We investigated the effect of learning when perturbations are induced by the backpropagated gradient which is structured by the recurrent dynamics in the following two ways.
The second type of noise is injected into the input $x_{i,t}+\epsilon_{i,t}$ with $\epsilon\sim\mathcal{N}(0,\sigma)$.
To inject noisy gradients naturally, we added noise to the input to the first 10 time steps during the trial that were not integrated in the target output $O_t^*$ (Eq.~\ref{eq:output}).
The third type of noise is injected into the hidden state $h_{i,t}+\epsilon_{i,t}$ with $\epsilon\sim\mathcal{N}(0,\sigma)$ for $t=1,\dots, T$ and $i=1,2$. 

\begin{figure}[thbp]
     \centering
    \includegraphics[width=\textwidth]{matching_noise_3types_cont}
       \caption{For various values the loss was calculated for the three types of noise. The matched noise levels were chosen based on these curves.}
         \label{fig:matching_noise_3types_cont}
\end{figure}

\newpage
\section{Continuous attractor solutions in click integration tasks with noise in the weights}
For the SGD, the last output of the network after $T$ steps was taken to calculate the loss based on the mean squared error (MSE) over a batch of 1024 trials. 

\begin{figure}[thbp]
  \centering
  \includegraphics[width=\textwidth]{maintenance_T100}
  \caption{
  Comparing three continuous attractor solutions to the click integration task for a length of $T=100$ time steps.
  (A) Effect of gradient descent in repairing the continuous attractor. RNNs without gradient descent (dashed line) are shown for reference. Box plots show distribution of the loss for the last 10 steps. Averages (thick lines) over 10 simulations (thin lines) are shown for each network.
  (B) Changes to the recurrent parameters (matrix and bias), without (upper) and with (lower) learning (with the optimal learning rates). iRNN converges to a different solution.
 (C)  The distribution of the MSE for different learning rates. The dip in the MSE defines the optimal learning rate for each of the three neural integrators. 
    (D) Single parameter perturbation showing exploding gradients for iRNN and UBLA.
  (E) Distribution of gradients shows bimodal distribution for UBLA. %Gradient distributions for all noise levels and learning rates. For some noise levels a bimodal distribution can be seen for the LAs, but exploding gradients seem to be absent for iRNN for low learning rates.
    (F) Interleaved weight perturbations showing quick recovery for BLA and and slow for iRNN and UBLA.
  }
  \label{fig:maintenance}
\end{figure}

\begin{figure}[thbp]
  \centering
  \includegraphics[width=\textwidth]{maintenance_T1000}
  \caption{
  Comparing three continuous attractor solutions to the click integration task for a length of $T=1000$ time steps.
  (A) Effect of gradient descent in repairing the continuous attractor. RNNs without gradient descent (dashed line) are shown for reference. Box plots show distribution of the loss for the last 10 steps.
  (B) Changes to the recurrent parameters (matrix and bias), without (upper) and with (lower) learning (with the optimal learning rates). iRNN converges to a different solution.
 (C)  The distribution of the MSE for different learning rates. The dip in the MSE defines the optimal learning rate for each of the three neural integrators. 
    (D) Single parameter perturbation showing exploding gradients for iRNN and UBLA.
  (E) Distribution of gradients shows bimodal distribution for UBLA.
    (F) Interleaved weight perturbations showing quick recovery for BLA and and slow for iRNN and UBLA.
  }
  \label{fig:maintenance:long}
\end{figure}

\newpage
\section{Stability of the neural integrators for different learning rates}

\begin{figure}[thbp]
     \centering
    \includegraphics[width=\textwidth]{all_lrs_vs_mses_f1}
       \caption{Distribution of MSE for the three noisy types for different learning rates.}
         \label{fig:all_lrs_vs_mses}
\end{figure}


\newpage
\section{Trajectories of the neural integrators in the recurrent network space}

\begin{figure}[thbp]
     \centering
    \includegraphics[width=\textwidth]{wdn_mse_trajectories_F1}
       \caption{Trajectories of learning in parameter space relative to the initial recurrent parameters. The LAs follow a trajectory that is orthogonal to the initial parameters, but that yet decreases the MSE.}
         \label{fig:wdn_mse_trajectories_F1}
\end{figure}


\newpage
\section{Influence of the different noise types on the found solutions for the neural integrators}

\begin{figure}[H]
     \centering
    \includegraphics[width=\textwidth]{allnoise_contours}
       \caption{Distribution of parameters for the three noise types for three noise levels. Because of relative scale of perturbation, iRNN is further away from the initial parameters with internal and input noise.
        Depending on the level of the noise it performs better or worse than the LAs.}
         \label{fig:interal_input_contours}
\end{figure}


\newpage
\section{Changes to the neural integrators during learning}\label{sec:supp:learning}
%Message: This is what happens to the different neural integrators during learning 
During learning, the various neural integrators undergo distinct changes. These alterations manifest differently depending on the specific integrator involved. Here are some examples of progresses of chaning slow manifold dynamics.

\begin{figure}[H]
     \centering
    \includegraphics[width=\textwidth]{vfs_f100_gs1_optlrs}
       \caption{The dynamics of the recurrent part of the integrators across learning with some example orbits (blue lines), stable (green) and unstable (red) fixed points. A gradient step is taken after every perturbation. Gradient steps 0, 1, 5, 10, 15, 20, 25 and 29 are shown. }
         \label{fig:vfs1}
\end{figure}

\begin{figure}[H]
     \centering
    \includegraphics[width=\textwidth]{vfs_f100_gs5_optlrs}
       \caption{The dynamics of the recurrent part of the integrators across learning with some example orbits (blue lines), stable (green) and unstable (red) fixed points. A gradient step is taken after every 5 perturbations. Gradient steps 0, 1, 5, 10, 15, 20, 25 and 29 are shown. }
         \label{fig:vfs5}
\end{figure}

\begin{figure}[H]
     \centering
    \includegraphics[width=\textwidth]{vfs_f100_gs30_optlrs}
       \caption{The dynamics of the recurrent part of the integrators across learning with some example orbits (blue lines), stable (green) and unstable (red) fixed points. 30 gradient steps are take after a single perturbation. Gradient steps 0, 1, 5, 10, 15, 20, 25 and 29 are shown. }
         \label{fig:vfs30}
\end{figure}


\begin{figure}[H]
     \centering
    \includegraphics[width=\textwidth]{speeds}
       \caption{Speed along the invariant manifold during learning. For the iRNN a slice (the diagonal) is shown.
       %UBLA has a lower
       }
         \label{fig:speeds}
\end{figure}

%\section{Ring attractor}
%
%%Compact
%%No boundary: in- and outflowing
%%But not $C^1$!
%%
%%
%%discrete attractors as a trivial application?
%%Discrete attractors can be seen as an approximation to continuous attractor models in the context of head direction representation \citep{zhang1996}. The discrete nature of the representation makes it more robust to small fluctuations or disturbances in neural activity.
%%However, these system have fading memory: the system eventually forgets the past, since any difference between any two neural activations eventually tends zero as they both evolve to a global resting state.
%
%We will first of all look at a simple (non-biological) system that has a ring attractor to demonstrate the consequences of the Persistence Theorem.
%The system we will analyse is defined by the following ODE: $\dot r = r(1-r), \ \dot \theta = 0.$
%This system has as fixed points the ring with radius one centered around zero, i.e., $(0,0)\cup\{(1,\theta)\ |\ \theta\in[0,2\pi)\}$.
%
%
%\begin{figure}[H]
%     \centering
%%  \includegraphics[width=.8\textwidth]{ring_perturbations_stream}
%    \includegraphics[width=.8\textwidth]{ring_perturbations_stream_2by2}
%       \caption{Perturbations to a simple implementation of a ring attractor all leave the invariant manifold intact. % (Left)  Examples of a local perturbation to the vector field through the addition of a bump to the vector field along the ring attractor.
%              (Leftmost) An example of a bump perturbation that results in the ring breaking up and becoming diffeomorphic to a line. %slow flow in hole?
%              (Left, middle) An example of a bump perturbation that maintains the ring structure, but deforms it locally.
%              %
%      % (Right) Examples of a global perturbation to the vector field through the addition of a small term to the connectivity matrix. 
%       (Right, middle) A global perturbation that results in a system with four fixed points along the persistent invariant manifold. %The two saddle nodes (yellow dots) are connected to the stable fixed points (green dots) through connecting orbits.
%       (Rightmost)   A global perturbation that results in a limit cycle.}
%         \label{fig:ring_activity_pert}
%\end{figure}

%All perturbations maintain the invariant manifold.






\newpage
\section{Ring perturbations}\label{sec:supp:ring_perturbations}

%Definition of a bump perturbation
We define a local perturbation (i.e., a change to the ODE with compact support) through the bump function $\Psi(x) = \exp\left(\frac{1}{\|x\|^2-1}\right)$ for $\|x\|<1$ and zero outside, by multiplying it with a uniform, unidirectional vector field. All such perturbations leave at least a part of the continuous attractor intact and preserve the invariant manifold, i.e. the parts where the fixed points disappear a slow flow appears.
%Definition of a global perturbation
The parametrized perturbations are characterized as the addition of random matrix to the connection matrix. 



\subsection{Heading direction network}\label{sec:supp:headdirection}

\begin{equation}
\tau \dot h_j = -h_j + \frac{1}{N} \sum_k (W^{sym}_{jk} + v_{in} W^{asym}_{jk})\phi(h_k)+c_{ff},     j=1,\dots,N,
\end{equation}

In the absence of an input ($v_{in}=0$) fixed points of the system can be found analytically by considering all submatrices $W^{sym}_\sigma$ for all subsets $\{\sigma\subset [n]\}$ with$[n]=\{1,\dots, N\}$.
A fixed point $x^*$ needs to satisfy
\begin{equation}
x^*= -(W^{sym}_\sigma)^{-1}c_{ff}
\end{equation}
and 
\begin{equation}
x^*_i<0 \text{   for  	 } i\in\sigma.
\end{equation}


\subsubsection{Measure zero co-dimension 1 bifurcations}

\begin{figure}[tbhp]
     \centering
    \includegraphics[width=\textwidth]{ring_n6_perturbations_schematic}
       \caption{Measure zero co-dimension 1 bifurcations of the ring attractor network \citep{Noorman2022}.}
         \label{fig:meaure_zero_perturbations}
\end{figure}


\newpage
\subsubsection{Independence of norm of perturbation on bifurcation}

\begin{figure}[tbhp]
     \centering
    \includegraphics[width=\textwidth]{noorman_ring_N6_pert_allfxdpnts_allnorms}
       \caption{Rows show the bifurcations resulting from perturbations from the matrices with the same direction in Fig.~\ref{fig:both_rings}A but with different norms (columns). }
         \label{fig:noorman_ring_allfxdpnts_allnorm}
\end{figure}



\newpage
\subsection{Goodridge}\label{sec:supp:goodridge}
\citep{goodridge2000}


\newpage
\subsection{Couey}\label{sec:supp:couey}
\citep{couey2013}


\newpage
\subsection{Low-rank}\label{sec:supp:lowrank}


The networks consisted of $N$ firing rate units with a sigmoid inputoutput transfer function \citep{mastrogiuseppe2018}
\begin{equation}
\xi_i(t) = - \xi_i(t) + \sum_{j=1}^{N} J_{ij}\phi(x_j(t)) + I_i
\label{eq:1}
\end{equation}
where $x_i(t)$ is the total input current to unit $i$,
$ J_{ij} = g\chi_{ij} + P_{ij}$ is the connectivity matrix, 
$\phi(x) = tanh(x)$ is the current-to-rate transfer function, and $I_i$ is the external, feedforward input to unit $i$.

This far we focused on unit-rank connectivity structure, but our framework can be directly extended to higher-rank structure. A more general structured component of rank $r\ll N$ can be written as a superposition of $r$ independent unit-rank terms 
\begin{equation}
P_{ij} = \frac{m^{(1)}_i n^{(1)}_j}{N} + \dots + \frac{m^{(r)}_i n^{(r)}_j}{N},
\end{equation} and is in principle characterized by $2r$ vectors $m^{(k)}$ and $n^{(k)}$.
$g\chi$ is considered unknown except for its statistics (mean 0, variance $g^2/N$).

$N=10,100,1000$
$\Sigma=2$
$\rho=1.9$
$g=0, 0.1$


\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{N100_si2_rho1.9_g0_fp4.8.12}
\caption{Your caption here}
\label{fig:my_label}
\end{figure}


\newpage
\subsection{Biswas}\label{sec:supp:biswas}




\newpage
\section{Training RNNs on an integration task from scratch}

We trained vanilla RNNs with a ReLU nonlinearity for the recurrent layer and a linear output layer on the angular velocity integration task Fig.~\ref{fig:angular_task}. The network size varies between 50 and 200 units, initialized using a normal distribution for the parameters. Adam optimization with $\beta_1=0.9$ and $\beta_2=0.99$ was employed with a batch size of 512 and training was run until no decrease in the loss occurred for 100 epochs. The task has 256 time steps for the training samples, and training was based on the mean squared error loss.

\begin{figure}[tbhp]
     \centering
    \includegraphics[width=\textwidth]{line_attractor_like_solutions}
       \caption{The two types of found solutions. A) A line attractor with hyperbolically stable fixed points at the end of the line. B) Saddle nodes at the ends of the line.
}
         \label{fig:line_att_sols}
\end{figure}

From an arbitrary initialization, we find that a line attractor-like structure often (8 out of 10 runs) emerged with hyperbolically stable fixed points (Fig.\ref{fig:line_att_sols}A) when trained on a longer version of the task. For shorter trial lengths, saddle nodes are more likely to emerge (6 out of 10 runs) at the ends of the line (Fig.~\ref{fig:line_att_sols}B), meaning that the resulting structure is not an attractor. 




\newpage
\section{Training RNNs on an angular velocity integration task from scratch}

We trained vanilla RNNs with a tanh nonlinearity for the recurrent layer and a linear output layer on the angular velocity integration task Fig.~\ref{fig:angular_task}. The network size is 20 units, initialized using a normal distribution for the parameters. Adam optimization with $\beta_1=0.9$ and $\beta_2=0.99$ was employed with a batch size of 512 and training was run until no decrease in the loss occurred for 100 epochs. The task has 100 time steps for the training samples, and the mean squared error loss was used.

\begin{figure}[tbhp]
     \centering
    \includegraphics[width=0.75\textwidth]{task_fig}
       \caption{Description of the tasks. A) The angular velocity integration task. B) The output of the angular velocity integration in the output space, color coded according to the integrated angle. An example of an input is shown with constant velocity and it is provided until one turn is completed.}
         \label{fig:angular_task}
\end{figure}


\begin{figure}[tbhp]
     \centering
    \includegraphics[width=\textwidth]{analysis_small}
       \caption{Analysis steps for the distillation of the implemented computation in a trained RNN. A) Input driven hidden trajectories for constant inputs of different magnitudes in the left (blue) and right (red) direction. B) Projection onto the output space of the attractors found by simulation until convergence to periodic solutions (color indicates the angular direction it maps to) with slow points found by minimizing the speed of the hidden (square) or output (cross) dynamics. Stability is indicated by green for stable, pink and red for saddles with 1 and 2 unstable dimensions, respectively. C) Effective input drive shown as average vector fields for the hidden dynamics projected onto the output space. Averages taken for a single constant input in left (blue) and right (red) directions.}
         \label{fig:asymptotic_analysis}
\end{figure}

\begin{figure}[tbhp]
     \centering
    \includegraphics[width=\textwidth]{example_solutions}
       \caption{A) A solution with a single limit cycle (light blue) that gets mapped onto a small subset of the output space. B) A solution with multiple limit cycles spread around the ring attractor. C) A solution with only fixed points spread around a ring like attractor with slow dynamics.}
         \label{fig:angular_solutions}
\end{figure}


For the angular velocity integration task, typical solutions have two limit cycles corresponding to the two directions of constant inputs. The autonomous dynamics can be characterized by an (approximate) line attractor with two (approximate) ring attractors at the ends. 
The found solutions Fig.\ref{fig:angular_solutions}A-C all show bounded ring-like attractors. These solutions are all composed of two rings (Fig.\ref{fig:asymptotic_analysis}A) connected by an (approximate) line attractor.

The vector field (Fig. \ref{fig:asymptotic_analysis}C) suggests that the system exhibits input driven dynamics  corresponding to a limit cycle, which would mean that the invariant manifold of the input-driven dynamics is compact.
%\include{iclr2024_supplementary.tex}




\appendix

\section{Appendix / supplemental material}


Optionally include supplemental material (complete proofs, additional experiments and plots) in appendix.
All such materials \textbf{SHOULD be included in the main submission.}




\end{document}