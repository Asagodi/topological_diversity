### Weaknesses:

> The presentation in section 5 becomes unclear, and perhaps too dense. The authors may want to expand on this section significantly.
We agree with the reviewer--- the updated manuscript will include a substantial revision of section 5. In it, we will focus on clarity and walking the reader through the theoretical analysis as well as how our numerical experiments corroborate it.
> Some more control experiments need to be added for proving the generality (See below).

We address the specific questions below, but would like to emphasize that the main contribution of this submission is theoretical.
The numerical experiments are meant to illustrate our results, rather than prove them.

### Questions:

> I believe the work warrants a borderline accept as is, yet I would feel very supportive of its publication (up to a strong accept) if the authors performed the following changes:
I believe the slow manifold picture the authors are introducing here is not too different from [1], specifically Fig. 7. Can the authors please clarify the differences in the main text?

We appreciate the remark, and will revise the text to include the following:
1. The two approaches are indeed similar, however there are subtle technical differences.
1. In [1], Sussillo and Barak are primarily concerned with a pointwise definition of slowness, by comparison normal hyperbolicity requires a uniform separation of timescales over the entire invariant manifold.
<!-- 1. Our theory predicts that, should the normal direction become slow compared to the tangent direction, the invariant manifold might 'lose stability'. To the best of our understanding, these kinds of conclusions can't be made directly from looking at the collection of slow points. -->


> The fast-slow decomposition does not seem to be specific to vanilla RNNs. Can the authors please include experiments with LSTMs and GRUs, which would support their claims on generality.

We're in the process of training, and analyzing both RNN cells. We include our preliminary results on LSTMs in a separate document uploaded to OpenReview.

> Similarly, the experiments on Section 4 are centered around ring-like attractors. Can you please show an example with other structures?
> For example, you can consider the delayed addition/multiplication tasks discussed in [2].

We appreciate the comment, and are currently performing additional experiments. Also please see the shared reply to all reviewers.
Specifically regarding addition or multiplication tasks, an idealized solution to either would require that the NN represent $G = (\mathbb{R},+)$ or $G = (\mathbb{R}_{+},\times)$, which are not compact.
Because this contradicts our technical assumptions, we opt to focus on tasks where the invariant manifolds are naturally compact.

> Relatedly, similar slow dynamics arguments are made in [2], which I believe the authors should list as a close related work and explain the differences in their approach.

We thank the reviewer for pointing out this work. We'll reference it accordingly.

> A more detailed discussion of Section 5 is desired. I was able to understand the main takeaways, but could not evaluate the validity of the claims. Perhaps the authors may want to explain in simpler terms the evidence presented in Fig. 5.

We agree with the comment, and will rewrite this section accordingly. Also please see the shared reply to all reviewers.

> The title is not descriptive of the manuscript's content and feels like as if it belongs to a blog post. Could you please update the title to be representative of the paper's conclusions?

ðŸ¤£

> Citations
[1] Opening the Black Box: Low-Dimensional Dynamics in High-Dimensional Recurrent Neural Networks, David Sussillo and Omri Barak, Neural Computation, 2013
> [2] Schmidt, D., Koppe, G., Monfared, Z., Beutelspacher, M., & Durstewitz, D. (2019). Identifying nonlinear dynamical systems with multiple time scales and long-range dependencies. arXiv preprint arXiv:1910.03471.