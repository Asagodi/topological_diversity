We would like to thank the reviewer for their valuable comments and suggestions. They have significantly enhanced the quality of our manuscript.

### Weaknesses:

> Some more control experiments need to be added for proving the generality (See below).
We address the specific questions below, but would like to emphasize that the main contribution of this submission is theoretical.
The numerical experiments are meant to illustrate our results, rather than prove them.

### Questions:


> I believe the slow manifold picture the authors are introducing here is not too different from [1], specifically Fig. 7. Can the authors please clarify the differences in the main text?
We appreciate the remark, and will revise the text to include the following:
1. The two approaches are indeed similar, however there are subtle technical differences.
1. In [1], Sussillo and Barak are primarily concerned with a pointwise definition of slowness, by comparison normal hyperbolicity requires a uniform separation of timescales over the entire invariant manifold. 
1. Our theory can explain why perturbations to the trained RNN (with random gaussian noise with zero mean and standard deviation) still lead to the same approximate plane attractor dynamical structure is still in place. The Persistence Theorem guarantees that for small perturbations (of size $\epsilon$) the persistent invariant manifold will be at the approximate same place (it will be at a distance of order $\mathcal{O}(\epsilon))$. See Figure 9 for the experiments of the structural perturbations in [1]. They do not provide an explanation for their observations.


> The fast-slow decomposition does not seem to be specific to vanilla RNNs. Can the authors please include experiments with LSTMs and GRUs, which would support their claims on generality.
We have now trained and analyzed both LSTMs and GRUs on the angular integration task.
We include our preliminary results in a separate document uploaded to OpenReview.


> Similarly, the experiments on Section 4 are centered around ring-like attractors...
We appreciate the comment, and have included an additional task where the approximate continuous attractor is of higher dimension, namely a double angular velocity integration task. Please see the shared reply to all reviewers on our new findings.
Specifically regarding addition or multiplication tasks, an idealized solution to either would require that the RNN represent $G = (\mathbb{R},+)$ or $G = (\mathbb{R}_{+},\times)$, which are not compact.
Because this contradicts our technical assumptions, we opt to focus on tasks where the invariant manifolds are naturally compact.


> similar slow dynamics arguments are made in [2]...
We thank the reviewer for pointing out this work, we will reference it accordingly.
This work identifies asymptotic behaviors in dynamical systems, fixed point dynamics and more general cases cycles and chaos. 
We look beyond asymptotic behavior and characterize attractive invariant manifolds, thereby also identifying connecting orbits (or heteroclinic orbits) between fixed points.
We would like to reiterate that we believe that the main contribution of the paper is a new theory of approximations of continuous attractors. 
Although we developed new analysis methods for dynamical systems to find slow manifolds in them, we do not propose a new general framework for analysis of all dynamical systems.
Finally, [2] provides analysis tools for Piecewise-Linear Dynamical Systems, while our methods are generally applicable to RNNs with any activation function.  

> The presentation in section 5 becomes unclear, and perhaps too dense...
We agree with the reviewer, the updated manuscript will include a substantial revision of section 5. In it, we will focus on clarity.
Please, see the shared rebuttal for our clarification of Section 5.2.


> A more detailed discussion of Section 5 is desired


> the evidence presented in Fig. 5.
The main message of Fig.5 is to show the validity of our theoretical predictions about the bound to the memory based on the uniform norm of the flow.
Besides, we can demonstrate that even though all networks learned a continuous attractor approximation, they are distinguished from one another by their fixed point topology, which determines their asymptotic behavior and hence generalization properties (Fig.5D and E).
These results should indicate the distance to a continuous attractor as was discussed in Sec.3.2. 


> The title is not descriptive of the manuscript's content and feels like as if it belongs to a blog post. Could you please update the title to be representative of the paper's conclusions?
The title references the reversal of the Persistence Manifold theorem, i.e., how to get back to a continuous attractor.
Furthermore, it references that we can return to continuous attractors as a useful concept to describe neural computation because of the deep connection of all continuous attractor approximations. 
We can however propose to revise the title to: A theory of analog memory approximation


### References
[1] Opening the Black Box: Low-Dimensional Dynamics in High-Dimensional Recurrent Neural Networks, David Sussillo and Omri Barak, Neural Computation, 2013
[2] Schmidt, D., Koppe, G., Monfared, Z., Beutelspacher, M., & Durstewitz, D. (2019). Identifying nonlinear dynamical systems with multiple time scales and long-range dependencies. arXiv preprint arXiv:1910.03471.
[3] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, ≈Å., & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.
