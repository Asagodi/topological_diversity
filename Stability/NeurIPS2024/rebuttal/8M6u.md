### Weaknesses:

> The presentation in section 5 becomes unclear, and perhaps too dense. The authors may want to expand on this section significantly.
We agree with the reviewer--- the updated manuscript will include a substantial revision of section 5. In it, we will focus on clarity and walking the reader through the theoretical analysis as well as how our numerical experiments corroborate it.
> Some more control experiments need to be added for proving the generality (See below).

We address the specific questions below, but would like to emphasize that the main contribution of this submission is theoretical.
The numerical experiments are meant to illustrate our results, rather than prove them.

### Questions:

> I believe the work warrants a borderline accept as is, yet I would feel very supportive of its publication (up to a strong accept) if the authors performed the following changes:
I believe the slow manifold picture the authors are introducing here is not too different from [1], specifically Fig. 7. Can the authors please clarify the differences in the main text?

We appreciate the remark, and will revise the text to include the following:
1. The two approaches are indeed similar, however there are subtle technical differences.
1. In [1], Sussillo and Barak are primarily concerned with a pointwise definition of slowness, by comparison normal hyperbolicity requires a uniform separation of timescales over the entire invariant manifold. 
1. Our theory can explain why perturbations to the trained RNN (with random gaussian noise with zero mean and standard deviation) still lead to the same approximate plane attractor dynamical structure is still in place. The Persistence Theorem guarantees that for small perturbations (of size $\epsilon$) the persistent invariant manifold will at the approximate same place (it will be at a distance of order $\mathcal{O}(\epsilon))$. See Figure 9 for the experiments of the structural perturbations in [1]. They do not provide an explanation for their observations.
<!-- 1. Our theory predicts that, should the normal direction become slow compared to the tangent direction, the invariant manifold might 'lose stability'. To the best of our understanding, these kinds of conclusions can't be made directly from looking at the collection of slow points. -->


> The fast-slow decomposition does not seem to be specific to vanilla RNNs. Can the authors please include experiments with LSTMs and GRUs, which would support their claims on generality.

We trained and analyzing both types of RNN cells on the angular integration task.
We include our preliminary results on LSTMs and GRUs in a separate document uploaded to OpenReview.


> Similarly, the experiments on Section 4 are centered around ring-like attractors. Can you please show an example with other structures?
> For example, you can consider the delayed addition/multiplication tasks discussed in [2].

We appreciate the comment, and are currently performing additional experiments. Also please see the shared reply to all reviewers.
Specifically regarding addition or multiplication tasks, an idealized solution to either would require that the NN represent $G = (\mathbb{R},+)$ or $G = (\mathbb{R}_{+},\times)$, which are not compact.
Because this contradicts our technical assumptions, we opt to focus on tasks where the invariant manifolds are naturally compact.


> Relatedly, similar slow dynamics arguments are made in [2], which I believe the authors should list as a close related work and explain the differences in their approach.

We thank the reviewer for pointing out this work. We'll reference it accordingly.
This work identifies asymptotic behaviors in dynamical systems, fixed point dynamics and more general cases cycles and chaos. 
We look beyond asymptotic behavior and characterize attractive invariant manifolds, thereby also identifying connecting orbits (or heteroclinic orbits) between fixed points.
We would like to reiterate that we believe that the main contribution of the paper is a new theory of approximations of continuous attractors. 
Although we developed new analysis methods for dynamical systems to find slow manifolds in them, we do not propose a new general framework for analysis of all dynamical systems.
Finally, [2] provides analysis tools for Piecewise-Linear Dynamical Systems, while our methods are generally applicable to RNNs with any activation function.  



> A more detailed discussion of Section 5 is desired. I was able to understand the main takeaways, but could not evaluate the validity of the claims. Perhaps the authors may want to explain in simpler terms the evidence presented in Fig. 5.

We agree with the comment, and will rewrite this section accordingly. Also please see the shared reply to all reviewers.


> The title is not descriptive of the manuscript's content and feels like as if it belongs to a blog post. Could you please update the title to be representative of the paper's conclusions?

In the history of NeurIPS many titles that belong to blog posts have been accepted (e.g. All you need is attention [3]). Does this make these papers less important or relevant for NeurIPS?



> Citations
[1] Opening the Black Box: Low-Dimensional Dynamics in High-Dimensional Recurrent Neural Networks, David Sussillo and Omri Barak, Neural Computation, 2013
> [2] Schmidt, D., Koppe, G., Monfared, Z., Beutelspacher, M., & Durstewitz, D. (2019). Identifying nonlinear dynamical systems with multiple time scales and long-range dependencies. arXiv preprint arXiv:1910.03471.

[3] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, ≈Å., & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.