### Weaknesses:

> The presentation in section 5 becomes unclear, and perhaps too dense. The authors may want to expand on this section significantly.

> Some more control experiments need to be added for proving the generality (See below).
>

### Questions:

> I believe the work warrants a borderline accept as is, yet I would feel very supportive of its publication (up to a strong accept) if the authors performed the following changes:
I believe the slow manifold picture the authors are introducing
here is not too different from [1], specifically Fig. 7. Can the authors please clarify the differences in the main text?
>

> The fast-slow decomposition does not seem to be specific to
vanilla RNNs. Can the authors please include experiments with LSTMs and GRUs, which would support their claims on generality.
>

> Similarly, the experiments on Section 4 are centered around
ring-like attractors. Can you please show an example with other
structures? For example, you can consider the delayed
addition/multiplication tasks discussed in [2]. Relatedly, similar slow
dynamics arguments are made in [2], which I believe the authors should
list as a close related work and explain the differences in their
approach.
>

> A more detailed discussion of Section 5 is desired. I was able to understand the main takeaways, but could not evaluate the validity of the claims. Perhaps the authors may want to explain in simpler terms the evidence presented in Fig. 5.
>

> The title is not descriptive of the manuscript's content and feels like as if it belongs to a blog post. Could you please update the title to be representative of the paper's conclusions?

> Citations
[1] Opening the Black Box: Low-Dimensional Dynamics in
High-Dimensional Recurrent Neural Networks, David Sussillo and Omri
Barak, Neural Computation, 2013
[2] Schmidt, D., Koppe, G., Monfared, Z., Beutelspacher, M., &
Durstewitz, D. (2019). Identifying nonlinear dynamical systems with
multiple time scales and long-range dependencies. arXiv preprint
arXiv:1910.03471.
>