@article{ajabi2023,
  title = {Population Dynamics of Head-Direction Neurons during Drift and Reorientation},
  author = {Ajabi, Zaki and Keinath, Alexandra T. and Wei, Xue-Xin and Brandon, Mark P.},
  date = {2023-03-22},
  journaltitle = {Nature},
  shortjournal = {Nature},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-023-05813-2},
  url = {https://www.nature.com/articles/s41586-023-05813-2},
  urldate = {2023-03-27},
  abstract = {Abstract                            The head direction (HD) system functions as the brain’s internal compass               1,2               , classically formalized as a one-dimensional ring attractor network               3,4               . In contrast to a globally consistent magnetic compass, the HD system does not have a universal reference frame. Instead, it anchors to local cues, maintaining a stable offset when cues rotate               5–8               and drifting in the absence of referents               5,8–10               . However, questions about the mechanisms that underlie anchoring and drift remain unresolved and are best addressed at the population level. For example, the extent to which the one-dimensional description of population activity holds under conditions of reorientation and drift is unclear. Here we performed population recordings of thalamic HD cells using calcium imaging during controlled rotations of a visual landmark. Across experiments, population activity varied along a second dimension, which we refer to as network gain, especially under circumstances of cue conflict and ambiguity. Activity along this dimension predicted realignment and drift dynamics, including the speed of network realignment. In the dark, network gain maintained a ‘memory trace’ of the previously displayed landmark. Further experiments demonstrated that the HD network returned to its baseline orientation after brief, but not longer, exposures to a rotated cue. This experience dependence suggests that memory of previous associations between HD neurons and allocentric cues is maintained and influences the internal HD representation. Building on these results, we show that continuous rotation of a visual landmark induced rotation of the HD representation that persisted in darkness, demonstrating experience-dependent recalibration of the HD system. Finally, we propose a computational model to formalize how the neural compass flexibly adapts to changing environmental cues to maintain a reliable representation of HD. These results challenge classical one-dimensional interpretations of the HD system and provide insights into the interactions between this system and the cues to which it anchors.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Population dynamics of head-direction neurons during drift and reorientation.pdf}
}

@article{araiDatabaseSchemaAnalysis2009,
  title = {A {{Database Schema}} for the {{Analysis}} of {{Global Dynamics}} of {{Multiparameter Systems}}},
  author = {Arai, Zin and Kalies, William and Kokubu, Hiroshi and Mischaikow, Konstantin and Oka, Hiroe and Pilarczyk, Pawel},
  date = {2009-01},
  journaltitle = {SIAM Journal on Applied Dynamical Systems},
  shortjournal = {SIAM J. Appl. Dyn. Syst.},
  volume = {8},
  number = {3},
  pages = {757--789},
  issn = {1536-0040},
  doi = {10.1137/080734935},
  url = {http://epubs.siam.org/doi/10.1137/080734935},
  urldate = {2023-03-26},
  abstract = {A generally applicable, automatic method for the efficient computation of a database of global dynamics of a multiparameter dynamical system is introduced. An outer approximation of the dynamics for each subset of the parameter range is computed using rigorous numerical methods and is represented by means of a directed graph. The dynamics is then decomposed into the recurrent and gradient-like parts by fast combinatorial algorithms and is classified via Morse decompositions. These Morse decompositions are compared at adjacent parameter sets via continuation to detect possible changes in the dynamics. The Conley index is used to study the structure of isolated invariant sets associated with the computed Morse decompositions and to detect the existence of certain types of dynamics. The power of the developed method is illustrated with an application to the two-dimensional density-dependent Leslie population model. An interactive visualization of the results of computations discussed in the paper can be accessed at the Web site http://chomp.rutgers.edu/database/, and the source code of the software used to obtain these results has also been made freely available.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Arai-2009-A Database Schema for the Analysis of Global Dynamics of Multiparameter Systems.pdf}
}

@online{arjovskyUnitaryEvolutionRecurrent2016,
  title = {Unitary {{Evolution Recurrent Neural Networks}}},
  author = {Arjovsky, Martin and Shah, Amar and Bengio, Yoshua},
  date = {2016-05-25},
  eprint = {1511.06464},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1511.06464},
  urldate = {2023-05-07},
  abstract = {Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to the well studied issue of vanishing and exploding gradients, especially when trying to learn long-term dependencies. To circumvent this problem, we propose a new architecture that learns a unitary weight matrix, with eigenvalues of absolute value exactly 1. The challenge we address is that of parametrizing unitary matrices in a way that does not require expensive computations (such as eigendecomposition) after each weight update. We construct an expressive unitary weight matrix by composing several structured matrices that act as building blocks with parameters to be learned. Optimization with this parameterization becomes feasible only when considering hidden states in the complex domain. We demonstrate the potential of this architecture by achieving state of the art results in several hard tasks involving very longterm dependencies.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Arjovsky-2015-Unitary Evolution Recurrent Neural Networks.pdf}
}

@article{barakWorkingModelsWorking2014,
  title = {Working Models of Working Memory},
  author = {Barak, Omri and Tsodyks, Misha},
  date = {2014-04},
  journaltitle = {Current Opinion in Neurobiology},
  shortjournal = {Current Opinion in Neurobiology},
  volume = {25},
  pages = {20--24},
  issn = {09594388},
  doi = {10.1016/j.conb.2013.10.008},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0959438813002158},
  urldate = {2023-04-10},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\DC835MXD\\Barak and Tsodyks - 2014 - Working models of working memory.pdf}
}

@online{barron2017,
  title = {Continuously {{Differentiable Exponential Linear Units}}},
  author = {Barron, Jonathan T.},
  date = {2017-04-24},
  eprint = {1704.07483},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1704.07483},
  urldate = {2023-05-08},
  abstract = {Exponential Linear Units (ELUs) are a useful rectifier for constructing deep learning architectures, as they may speed up and otherwise improve learning by virtue of not have vanishing gradients and by having mean activations near zero [1]. However, the ELU activation as parametrized in [1] is not continuously differentiable with respect to its input when the shape parameter α is not equal to 1. We present an alternative parametrization which is C1 continuous for all values of α, making the rectifier easier to reason about and making α easier to tune. This alternative parametrization has several other useful properties that the original parametrization of ELU does not: 1) its derivative with respect to x is bounded, 2) it contains both the linear transfer function and ReLU as special cases, and 3) it is scale-similar with respect to α.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Barron-2017-Continuously Differentiable Exponential Linear Units.pdf}
}

@article{ben-yishaiTheoryOrientationTuning1995,
  title = {Theory of Orientation Tuning in Visual Cortex.},
  author = {Ben-Yishai, R and Bar-Or, R L and Sompolinsky, H},
  date = {1995-04-25},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {92},
  number = {9},
  pages = {3844--3848},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.92.9.3844},
  url = {https://pnas.org/doi/full/10.1073/pnas.92.9.3844},
  urldate = {2023-04-27},
  abstract = {The role of intrinsic cortical connections in processing sensory input and in generating behavioral output is poorly understood. We have examined this issue in the context of the tuning of neuronal responses in cortex to the orientation of a visual stimulus. We analytically study a simple network model that incorporates both orientationselective input from the lateral geniculate nucleus and orientation-specific cortical interactions. Depending on the model parameters, the network exhibits orientation selectivity that originates from within the cortex, by a symmetrybreaking mechanism. In this case, the width of the orientation tuning can be sharp even if the lateral geniculate nucleus inputs are only weakly anisotropic. By using our model, several experimental consequences of this cortical mechanism of orientation tuning are derived. The tuning width is relatively independent of the contrast and angular anisotropy of the visual stimulus. The transient population response to changing of the stimulus orientation exhibits a slow "virtual rotation." Neuronal cross-correlations exhibit long time tails, the sign of which depends on the preferred orientations of the cells and the stimulus orientation.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Ben-Yishai-1995-Theory of orientation tuning in visual cortex.pdf}
}

@report{bittnerInterrogatingTheoreticalModels2019,
  type = {preprint},
  title = {Interrogating Theoretical Models of Neural Computation with Emergent Property Inference},
  author = {Bittner, Sean R. and Palmigiano, Agostina and Piet, Alex T. and Duan, Chunyu A. and Brody, Carlos D. and Miller, Kenneth D. and Cunningham, John P.},
  date = {2019-11-11},
  institution = {{Neuroscience}},
  doi = {10.1101/837567},
  url = {http://biorxiv.org/lookup/doi/10.1101/837567},
  urldate = {2023-04-10},
  abstract = {1           Abstract           A cornerstone of theoretical neuroscience is the circuit model: a system of equations that captures a hypothesized neural mechanism. Such models are valuable when they give rise to an experimentally observed phenomenon – whether behavioral or a pattern of neural activity – and thus can offer insights into neural computation. The operation of these circuits, like all models, critically depends on the choice of model parameters. A key step is then to identify the model parameters consistent with observed phenomena: to solve the inverse problem. In this work, we present a novel technique, emergent property inference (EPI), that brings the modern probabilistic modeling toolkit to theoretical neuroscience. When theorizing circuit models, theoreticians predominantly focus on reproducing computational properties rather than a particular dataset. Our method uses deep neural networks to learn parameter distributions with these computational properties. This methodology is introduced through a motivational example inferring conductance parameters in a circuit model of the stomatogastric ganglion. Then, with recurrent neural networks of increasing size, we show that EPI allows precise control over the behavior of inferred parameters, and that EPI scales better in parameter dimension than alternative techniques. In the remainder of this work, we present novel theoretical findings gained through the examination of complex parametric structure captured by EPI. In a model of primary visual cortex, we discovered how connectivity with multiple inhibitory subtypes shapes variability in the excitatory population. Finally, in a model of superior colliculus, we identified and characterized two distinct regimes of connectivity that facilitate switching between opposite tasks amidst interleaved trials, characterized each regime via insights afforded by EPI, and found conditions where these circuit models reproduce results from optogenetic silencing experiments. Beyond its scientific contribution, this work illustrates the variety of analyses possible once deep learning is harnessed towards solving theoretical inverse problems.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\LD6IPSNE\\Bittner et al. - 2019 - Interrogating theoretical models of neural computa.pdf}
}

@article{bogaczPhysicsOptimalDecision2006,
  title = {The Physics of Optimal Decision Making: {{A}} Formal Analysis of Models of Performance in Two-Alternative Forced-Choice Tasks.},
  shorttitle = {The Physics of Optimal Decision Making},
  author = {Bogacz, Rafal and Brown, Eric and Moehlis, Jeff and Holmes, Philip and Cohen, Jonathan D.},
  date = {2006},
  journaltitle = {Psychological Review},
  shortjournal = {Psychological Review},
  volume = {113},
  number = {4},
  pages = {700--765},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/0033-295X.113.4.700},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.113.4.700},
  urldate = {2023-05-03},
  abstract = {In this article, the authors consider optimal decision making in two-alternative forced-choice (TAFC) tasks. They begin by analyzing 6 models of TAFC decision making and show that all but one can be reduced to the drift diffusion model, implementing the statistically optimal algorithm (most accurate for a given speed or fastest for a given accuracy). They prove further that there is always an optimal trade-off between speed and accuracy that maximizes various reward functions, including reward rate (percentage of correct responses per unit time), as well as several other objective functions, including ones weighted for accuracy. They use these findings to address empirical data and make novel predictions about performance under optimality.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Bogacz-2006-The physics of optimal decision making.pdf}
}

@article{bouchenyContinuousAttractorNetwork2005,
  title = {A {{Continuous Attractor Network Model Without Recurrent Excitation}}: {{Maintenance}} and {{Integration}} in the {{Head Direction Cell System}}},
  shorttitle = {A {{Continuous Attractor Network Model Without Recurrent Excitation}}},
  author = {Boucheny, Christian and Brunel, Nicolas and Arleo, Angelo},
  date = {2005-03-01},
  journaltitle = {Journal of computational neuroscience},
  shortjournal = {Journal of computational neuroscience},
  volume = {18},
  pages = {205--27},
  doi = {10.1007/s10827-005-6559-y},
  abstract = {Motivated by experimental observations of the head direction system, we study a three population network model that operates as a continuous attractor network. This network is able to store in a short-term memory an angular variable (the head direction) as a spatial profile of activity across neurons in the absence of selective external inputs, and to accurately update this variable on the basis of angular velocity inputs. The network is composed of one excitatory population and two inhibitory populations, with inter-connections between populations but no connections within the neurons of a same population. In particular, there are no excitatory-to-excitatory connections. Angular velocity signals are represented as inputs in one inhibitory population (clockwise turns) or the other (counterclockwise turns). The system is studied using a combination of analytical and numerical methods. Analysis of a simplified model composed of threshold-linear neurons gives the conditions on the connectivity for (i) the emergence of the spatially selective profile, (ii) reliable integration of angular velocity inputs, and (iii) the range of angular velocities that can be accurately integrated by the model. Numerical simulations allow us to study the proposed scenario in a large network of spiking neurons and compare their dynamics with that of head direction cells recorded in the rat limbic system. In particular, we show that the directional representation encoded by the attractor network can be rapidly updated by external cues, consistent with the very short update latencies observed experimentally by Zugaro et al. (2003) in thalamic head direction cells.}
}

@article{braunAttractorsNoiseTwin2010,
  title = {Attractors and Noise: {{Twin}} Drivers of Decisions and Multistability},
  shorttitle = {Attractors and Noise},
  author = {Braun, Jochen and Mattia, Maurizio},
  date = {2010-09},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  volume = {52},
  number = {3},
  pages = {740--751},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2009.12.126},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811910000637},
  urldate = {2023-04-10},
  abstract = {Perceptual decisions are made not only during goal-directed behavior such as choice tasks, but also occur spontaneously while multistable stimuli are being viewed. In both contexts, the formation of a perceptual decision is best captured by noisy attractor dynamics. Noise-driven attractor transitions can accommodate a wide range of timescales and a hierarchical arrangement with “nested attractors” harbors even more dynamical possibilities. The attractor framework seems particularly promising for understanding higherlevel mental states that combine heterogeneous information from a distributed set of brain areas.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\VXBPDR54\\Braun and Mattia - 2010 - Attractors and noise Twin drivers of decisions an.pdf}
}

@article{cannon1983,
  title = {A Proposed Neural Network for the Integrator of the Oculomotor System},
  author = {Cannon, Stephen C. and Robinson, David A. and Shamma, Shihab},
  date = {1983-12},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol. Cybernetics},
  volume = {49},
  number = {2},
  pages = {127--136},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/BF00320393},
  url = {http://link.springer.com/10.1007/BF00320393},
  urldate = {2023-05-08},
  abstract = {Single-unit recordings, stimulation studies, and eye movement measurements all indicate that the firing patterns of many oculomotor neurons in the brain stem encode eye-velocitycommands in premotor circuits while the firing patterns of extraocular motoneurons contain both eye-velocity and eye-position components. It is necessary to propose that the eyeposition component is generated from the eye-velocity signal by a leaky hold element or temporal integrator. Prior models of this integrator suffer from two important problems. Since cells appear to have a steady, background signal when eye position and velocity are zero, how does the integrator avoid integrating this background rate? Most models employ some form of lumped, positive feedback the gain of which must be kept within totally unreasonable limits for proper operation. We propose a lateral inhibitory network of homogeneous neurons as a model for the neural integrator that solves both problems. Parameter sensitivity studies and lesion simulations are presented to demonstrate robustness of the model with respect to both the choice of parameter values and the consequences of pathological changes in a portion of the neural integrator pool.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Cannon-1983-A proposed neural network for the integrator of the oculomotor system.pdf}
}

@article{carandiniNormalizationCanonicalNeural2011,
  title = {Normalization as a Canonical Neural Computation},
  author = {Carandini, Matteo and Heeger, David J.},
  date = {2011-11-23},
  journaltitle = {Nature reviews. Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {13},
  number = {1},
  eprint = {22108672},
  eprinttype = {pmid},
  pages = {51--62},
  issn = {1471-003X},
  doi = {10.1038/nrn3136},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3273486/},
  urldate = {2023-03-06},
  abstract = {There is increasing evidence that the brain relies on a set of canonical neural computations, repeating them across brain regions and modalities to apply similar operations to different problems. A promising candidate for such a computation is normalization, in which the responses of neurons are divided by a common factor that typically includes the summed activity of a pool of neurons. Normalization was developed to explain responses in the primary visual cortex and is now thought to operate throughout the visual system, and in many other sensory modalities and brain regions. Normalization may underlie operations such as the representation of odours, the modulatory effects of visual attention, the encoding of value and the integration of multisensory information. Its presence in such a diversity of neural systems in multiple species, from invertebrates to mammals, suggests that it serves as a canonical neural computation.},
  pmcid = {PMC3273486},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\LIR7SXJ2\\Carandini and Heeger - 2011 - Normalization as a canonical neural computation.pdf}
}

@article{chaudhuriComputationalPrinciplesMemory2016,
  title = {Computational Principles of Memory},
  author = {Chaudhuri, Rishidev and Fiete, Ila},
  date = {2016-03},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {19},
  number = {3},
  pages = {394--403},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4237},
  url = {http://www.nature.com/articles/nn.4237},
  urldate = {2023-03-26},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Chaudhuri-2016-Computational principles of memory.pdf}
}

@article{chaudhuriIntrinsicAttractorManifold2019,
  title = {The Intrinsic Attractor Manifold and Population Dynamics of a Canonical Cognitive Circuit across Waking and Sleep},
  author = {Chaudhuri, Rishidev and Gerçek, Berk and Pandey, Biraj and Peyrache, Adrien and Fiete, Ila},
  date = {2019-09},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {22},
  number = {9},
  pages = {1512--1520},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-019-0460-x},
  url = {http://www.nature.com/articles/s41593-019-0460-x},
  urldate = {2023-03-26},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Chaudhuri-The intrinsic attractor manifold and population dynamics of a canonical cognitive circuit across waking and sleep.pdf}
}

@online{clevert2016,
  title = {Fast and {{Accurate Deep Network Learning}} by {{Exponential Linear Units}} ({{ELUs}})},
  author = {Clevert, Djork-Arné and Unterthiner, Thomas and Hochreiter, Sepp},
  date = {2016-02-22},
  eprint = {1511.07289},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1511.07289},
  urldate = {2023-04-23},
  abstract = {We introduce the “exponential linear unit” (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Clever-2016-Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs).pdf}
}

@online{dabagiaComparingHighdimensionalNeural2022,
  title = {Comparing High-Dimensional Neural Recordings by Aligning Their Low-Dimensional Latent Representations},
  author = {Dabagia, Max and Kording, Konrad P. and Dyer, Eva L.},
  date = {2022-05-17},
  eprint = {2205.08413},
  eprinttype = {arxiv},
  eprintclass = {q-bio},
  url = {http://arxiv.org/abs/2205.08413},
  urldate = {2023-03-23},
  abstract = {Many questions in neuroscience involve understanding of the responses of large populations of neurons. However, when dealing with large-scale neural activity, interpretation becomes difficult, and comparisons between two animals, or across different time points becomes challenging. One major challenge that we face in modern neuroscience is that of correspondence, e.g. we do not record the exact same neurons at the exact same times. Without some way to link two or more datasets, comparing different collections of neural activity patterns becomes impossible. Here, we describe approaches for leveraging shared latent structure across neural recordings to tackle this correspondence challenge. We review algorithms that map two datasets into a shared space where they can be directly compared, and argue that alignment is key for comparing high-dimensional neural activities across times, subsets of neurons, and individuals.},
  pubstate = {preprint},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\P4CZJM9G\\Dabagia et al. - 2022 - Comparing high-dimensional neural recordings by al.pdf;C\:\\Users\\abel_\\Zotero\\storage\\H27VS6HC\\2205.html}
}

@article{depasqualeCentralityPopulationlevelFactors2023,
  title = {The Centrality of Population-Level Factors to Network Computation Is Demonstrated by a Versatile Approach for Training Spiking Networks},
  author = {DePasquale, Brian and Sussillo, David and Abbott, L.F. and Churchland, Mark M.},
  date = {2023-01},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  pages = {S0896627322010807},
  issn = {08966273},
  doi = {10.1016/j.neuron.2022.12.007},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627322010807},
  urldate = {2023-02-06},
  abstract = {Neural activity is often described in terms of population-level factors extracted from the responses of many neurons. Factors provide a lower-dimensional description with the aim of shedding light on network computations. Yet, mechanistically, computations are performed not by continuously valued factors but by interactions among neurons that spike discretely and variably. Models provide a means of bridging these levels of description. We developed a general method for training model networks of spiking neurons by leveraging factors extracted from either data or firing-rate-based networks. In addition to providing a useful modelbuilding framework, this formalism illustrates how reliable and continuously valued factors can arise from seemingly stochastic spiking. Our framework establishes procedures for embedding this property in network models with different levels of realism. The relationship between spikes and factors in such networks provides a foundation for interpreting (and subtly redefining) commonly used quantities such as firing rates.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\DePasquale-2023-The centrality of population-level factors to network computation is demonstrated by a versatile approach for training spiking networks.pdf}
}

@article{deutsch2020,
  title = {The Neural Basis for a Persistent Internal State in {{Drosophila}} Females},
  author = {Deutsch, David and Pacheco, Diego and Encarnacion-Rivera, Lucas and Pereira, Talmo and Fathy, Ramie and Clemens, Jan and Girardin, Cyrille and Calhoun, Adam and Ireland, Elise and Burke, Austin and Dorkenwald, Sven and McKellar, Claire and Macrina, Thomas and Lu, Ran and Lee, Kisuk and Kemnitz, Nico and Ih, Dodam and Castro, Manuel and Halageri, Akhilesh and Jordan, Chris and Silversmith, William and Wu, Jingpeng and Seung, H Sebastian and Murthy, Mala},
  date = {2020-11-23},
  journaltitle = {eLife},
  volume = {9},
  pages = {e59502},
  issn = {2050-084X},
  doi = {10.7554/eLife.59502},
  url = {https://elifesciences.org/articles/59502},
  urldate = {2023-05-08},
  abstract = {Sustained changes in mood or action require persistent changes in neural activity, but it has been difficult to identify the neural circuit mechanisms that underlie persistent activity and contribute to long-lasting changes in behavior. Here, we show that a subset of Doublesex+ pC1 neurons in the Drosophila female brain, called pC1d/e, can drive minutes-long changes in female behavior in the presence of males. Using automated reconstruction of a volume electron microscopic (EM) image of the female brain, we map all inputs and outputs to both pC1d and pC1e. This reveals strong recurrent connectivity between, in particular, pC1d/e neurons and a specific subset of Fruitless+ neurons called aIPg. We additionally find that pC1d/e activation drives long-lasting persistent neural activity in brain areas and cells overlapping with the pC1d/e neural network, including both Doublesex+ and Fruitless+ neurons. Our work thus links minutes-long persistent changes in behavior with persistent neural activity and recurrent circuit architecture in the female brain.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Deutsch-2020-The neural basis for a persistent internal state in Drosophila females.pdf}
}

@book{dibernardoPiecewisesmoothDynamicalSystems2008,
  title = {Piecewise-Smooth Dynamical Systems: Theory and Applications},
  shorttitle = {Piecewise-Smooth Dynamical Systems},
  editor = {Di Bernardo, M.},
  date = {2008},
  series = {Applied Mathematical Sciences},
  number = {163},
  publisher = {{Springer}},
  location = {{London}},
  isbn = {978-1-84628-039-9},
  langid = {english},
  pagetotal = {481},
  keywords = {Bifurcation theory,Differentiable dynamical systems},
  annotation = {OCLC: ocn144515657},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Piecewise-smooth dynamical systems theory and applications.pdf}
}

@article{doya1993,
  title = {Bifurcations of {{Recurrent Neural Networks}} in {{Gradient Descent Learning}} 3},
  author = {Doya, Kenji},
  abstract = {Asymptotic behavior of a recurrent neural network changes qualitatively at certain points in the parameter space, which are known as \textbackslash bifurcation points". At bifurcation points, the output of a network can change discontinuously with the change of parameters and therefore convergence of gradient descent algorithms is not guaranteed. Furthermore, learning equations used for error gradient estimation can be unstable. However, some kinds of bifurcations are inevitable in training a recurrent network as an automaton or an oscillator. Some of the factors underlying successful training of recurrent networks are investigated, such as choice of initial connections, choice of input patterns, teacher forcing, and truncated learning equations.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Doya - Bifurcations of Recurrent Neural Networks in Gradi.pdf}
}

@article{durstewitzNeurocomputationalModelsWorking2000,
  title = {Neurocomputational Models of Working Memory [{{Supplement}}]},
  author = {Durstewitz, Daniel and Seamans, Jeremy and Sejnowski, Terrence},
  date = {2000-12-01},
  journaltitle = {Nature neuroscience},
  shortjournal = {Nature neuroscience},
  volume = {3 Suppl},
  pages = {1184--91},
  doi = {10.1038/81460},
  abstract = {During working memory tasks, the firing rates of single neurons recorded in behaving monkeys remain elevated without external cues. Modeling studies have explored different mechanisms that could underlie this selective persistent activity, including recurrent excitation within cell assemblies, synfire chains and single-cell bistability. The models show how sustained activity can be stable in the presence of noise and distractors, how different synaptic and voltage-gated conductances contribute to persistent activity, how neuromodulation could influence its robustness, how completely novel items could be maintained, and how continuous attractor states might be achieved. More work is needed to address the full repertoire of neural dynamics observed during working memory tasks.}
}

@article{durstewitzNeurocomputationalModelsWorking2000a,
  title = {Neurocomputational Models of Working Memory},
  author = {Durstewitz, Daniel and Seamans, Jeremy K. and Sejnowski, Terrence J.},
  date = {2000-11},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {3},
  number = {S11},
  pages = {1184--1191},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/81460},
  url = {http://www.nature.com/articles/nn1100_1184},
  urldate = {2023-05-08},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Durstewitz-2000-Neurocomputational models of working memory.pdf}
}

@online{elfwingSigmoidWeightedLinearUnits2017,
  title = {Sigmoid-{{Weighted Linear Units}} for {{Neural Network Function Approximation}} in {{Reinforcement Learning}}},
  author = {Elfwing, Stefan and Uchibe, Eiji and Doya, Kenji},
  date = {2017-11-01},
  eprint = {1702.03118},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1702.03118},
  urldate = {2023-04-27},
  abstract = {In recent years, neural networks have enjoyed a renaissance as function approximators in reinforcement learning. Two decades after Tesauro’s TD-Gammon achieved near toplevel human performance in backgammon, the deep reinforcement learning algorithm DQN achieved human-level performance in many Atari 2600 games. The purpose of this study is twofold. First, we propose two activation functions for neural network function approximation in reinforcement learning: the sigmoid-weighted linear unit (SiLU) and its derivative function (dSiLU). The activation of the SiLU is computed by the sigmoid function multiplied by its input. Second, we suggest that the more traditional approach of using on-policy learning with eligibility traces, instead of experience replay, and softmax action selection with simple annealing can be competitive with DQN, without the need for a separate target network. We validate our proposed approach by, first, achieving new state-of-the-art results in both stochastic SZ-Tetris and Tetris with a small 10×10 board, using TD(λ) learning and shallow dSiLU network agents, and, then, by outperforming DQN in the Atari 2600 domain by using a deep Sarsa(λ) agent with SiLU and dSiLU hidden units.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Elfwing-2017-Sigmoid-Weighted Linear Units for Neural Network Function Approximation in RL.pdf}
}

@book{eliasmithNeuralEngineeringComputation2003,
  title = {Neural Engineering: Computation, Representation, and Dynamics in Neurobiological Systems},
  shorttitle = {Neural Engineering},
  author = {Eliasmith, Chris and Anderson, C. H.},
  date = {2003},
  series = {Computational Neuroscience},
  publisher = {{MIT Press}},
  location = {{Cambridge, Mass}},
  isbn = {978-0-262-05071-5},
  langid = {english},
  pagetotal = {356},
  keywords = {Computational neuroscience,Neural networks (Computer science),Neural networks (Neurobiology)},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Eliasmith-Neural engineering.pdf}
}

@article{elmanFindingStructureTime1990,
  title = {Finding {{Structure}} in {{Time}}},
  author = {Elman, Jeffrey L.},
  date = {1990-03},
  journaltitle = {Cognitive Science},
  volume = {14},
  number = {2},
  pages = {179--211},
  issn = {03640213},
  doi = {10.1207/s15516709cog1402_1},
  url = {http://doi.wiley.com/10.1207/s15516709cog1402_1},
  urldate = {2023-05-07},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Elman-1990-Finding Structure in Time.pdf}
}

@online{ElsevierEnhancedReader,
  title = {Elsevier {{Enhanced Reader}}},
  url = {https://reader.elsevier.com/reader/sd/pii/0022039676900048?token=F4DF8D1C7662C630D86174CEB2AF329EF1DAD085B13BD22FB4FA3A3C48D6FD2A5080ECD3731E7161424ECE92AC9D63DA&originRegion=eu-west-1&originCreation=20230320081411},
  urldate = {2023-03-20}
}

@book{ermentroutMathematicalFoundationsNeuroscience2010,
  title = {Mathematical {{Foundations}} of {{Neuroscience}}},
  author = {Ermentrout, G. Bard and Terman, David H.},
  date = {2010},
  series = {Interdisciplinary {{Applied Mathematics}}},
  volume = {35},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-0-387-87708-2},
  url = {http://link.springer.com/10.1007/978-0-387-87708-2},
  urldate = {2023-03-26},
  isbn = {978-0-387-87707-5 978-0-387-87708-2},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Ermentrout-Mathematical foundations of neuroscience.pdf}
}

@article{esnaola-acebesFlexibleIntegrationContinuous2022,
  title = {Flexible Integration of Continuous Sensory Evidence in Perceptual Estimation Tasks},
  author = {Esnaola-Acebes, Jose M. and Roxin, Alex and Wimmer, Klaus},
  date = {2022-11-08},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {119},
  number = {45},
  pages = {e2214441119},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2214441119},
  url = {https://pnas.org/doi/10.1073/pnas.2214441119},
  urldate = {2023-05-04},
  abstract = {Temporal accumulation of evidence is crucial for making accurate judgments based on noisy or ambiguous sensory input. The integration process leading to categorical decisions is thought to rely on competition between neural populations, each encoding a discrete categorical choice. How recurrent neural circuits integrate evidence for continuous perceptual judgments is unknown. Here, we show that a continuous bump attractor network can integrate a circular feature, such as stimulus direction, nearly optimally. As required by optimal integration, the population activity of the network unfolds on a two-dimensional manifold, in which the position of the network’s activity bump tracks the stimulus average, and, simultaneously, the bump amplitude tracks stimulus uncertainty. Moreover, the temporal weighting of sensory evidence by the network depends on the relative strength of the stimulus compared to the internally generated bump dynamics, yielding either early (primacy), uniform, or late (recency) weighting. The model can flexibly switch between these regimes by changing a single control parameter, the global excitatory drive. We show that this mechanism can quantitatively explain individual temporal weighting profiles of human observers, and we validate the model prediction that temporal weighting impacts reaction times. Our findings point to continuous attractor dynamics as a plausible neural mechanism underlying stimulus integration in perceptual estimation tasks.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Anscombe-Flexible integration of continuous sensory evidence in perceptual estimation tasks.pdf}
}

@online{ExtractingDynamicsBehavior,
  title = {Extracting the Dynamics of Behavior in Sensory Decision-Making Experiments | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.neuron.2020.12.004},
  url = {https://reader.elsevier.com/reader/sd/pii/S0896627320309636?token=191FC208450F2B40980200386DDB05D1AA51F3B029846D5F2F101FDB96AE40EB041629833B2A4B8785323873A75BC2D8&originRegion=eu-west-1&originCreation=20230410100107},
  urldate = {2023-04-10},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\9C8JPQ2Y\\Extracting the dynamics of behavior in sensory dec.pdf}
}

@article{gaoSimplicityComplexityBrave2015,
  title = {On Simplicity and Complexity in the Brave New World of Large-Scale Neuroscience},
  author = {Gao, Peiran and Ganguli, Surya},
  date = {2015-06},
  journaltitle = {Current Opinion in Neurobiology},
  shortjournal = {Current Opinion in Neurobiology},
  volume = {32},
  pages = {148--155},
  issn = {09594388},
  doi = {10.1016/j.conb.2015.04.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0959438815000768},
  urldate = {2023-02-26},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\BNZ8ATJQ\\Gao and Ganguli - 2015 - On simplicity and complexity in the brave new worl.pdf}
}

@report{gaoTheoryMultineuronalDimensionality2017,
  type = {preprint},
  title = {A Theory of Multineuronal Dimensionality, Dynamics and Measurement},
  author = {Gao, Peiran and Trautmann, Eric and Yu, Byron and Santhanam, Gopal and Ryu, Stephen and Shenoy, Krishna and Ganguli, Surya},
  date = {2017-11-05},
  institution = {{Neuroscience}},
  doi = {10.1101/214262},
  url = {http://biorxiv.org/lookup/doi/10.1101/214262},
  urldate = {2023-05-03},
  abstract = {Abstract           In many experiments, neuroscientists tightly control behavior, record many trials, and obtain trial-averaged firing rates from hundreds of neurons in circuits containing billions of behaviorally relevant neurons. Di-mensionality reduction methods reveal a striking simplicity underlying such multi-neuronal data: they can be reduced to a low-dimensional space, and the resulting neural trajectories in this space yield a remarkably insightful dynamical portrait of circuit computation. This simplicity raises profound and timely conceptual questions. What are its origins and its implications for the complexity of neural dynamics? How would the situation change if we recorded more neurons? When, if at all, can we trust dynamical portraits obtained from measuring an infinitesimal fraction of task relevant neurons? We present a theory that answers these questions, and test it using physiological recordings from reaching monkeys. This theory reveals conceptual insights into how task complexity governs both neural dimensionality and accurate recovery of dynamic portraits, thereby providing quantitative guidelines for future large-scale experimental design.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Gao-2017-A theory of multineuronal dimensionality, dynamics and measurement.pdf}
}

@article{gardnerToroidalTopologyPopulation2022,
  title = {Toroidal Topology of Population Activity in Grid Cells},
  author = {Gardner, Richard J. and Hermansen, Erik and Pachitariu, Marius and Burak, Yoram and Baas, Nils A. and Dunn, Benjamin A. and Moser, May-Britt and Moser, Edvard I.},
  date = {2022-02-03},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {602},
  number = {7895},
  pages = {123--128},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-021-04268-7},
  url = {https://www.nature.com/articles/s41586-021-04268-7},
  urldate = {2023-03-06},
  abstract = {Abstract                            The medial entorhinal cortex is part of a neural system for mapping the position of an individual within a physical environment               1               . Grid cells, a key component of this system, fire in a characteristic hexagonal pattern of locations               2               , and are organized in modules               3               that collectively form a population code for the animal’s allocentric position               1               . The invariance of the correlation structure of this population code across environments               4,5               and behavioural states               6,7               , independent of specific sensory inputs, has pointed to intrinsic, recurrently connected continuous attractor networks (CANs) as a possible substrate of the grid pattern               1,8–11               . However, whether grid cell networks show continuous attractor dynamics, and how they interface with inputs from the environment, has remained unclear owing to the small samples of cells obtained so far. Here, using simultaneous recordings from many hundreds of grid cells and subsequent topological data analysis, we show that the joint activity of grid cells from an individual module resides on a toroidal manifold, as expected in a two-dimensional CAN. Positions on the torus correspond to positions of the moving animal in the environment. Individual cells are preferentially active at singular positions on the torus. Their positions are maintained between environments and from wakefulness to sleep, as predicted by CAN models for grid cells but not by alternative feedforward models               12               . This demonstration of network dynamics on a toroidal manifold provides a population-level visualization of CAN dynamics in grid cells.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Gardner-2022-Toroidal topology of population activity in grid cells.pdf}
}

@online{Glossary,
  title = {Glossary},
  url = {https://www.overleaf.com/project/63da4305987ab6d5b40d21f9},
  urldate = {2023-04-17},
  abstract = {An online LaTeX editor that’s easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\IEH2FL3I\\63da4305987ab6d5b40d21f9.html}
}

@article{goncalvesTrainingDeepNeural2020,
  title = {Training Deep Neural Density Estimators to Identify Mechanistic Models of Neural Dynamics},
  author = {Gonçalves, Pedro J and Lueckmann, Jan-Matthis and Deistler, Michael and Nonnenmacher, Marcel and Öcal, Kaan and Bassetto, Giacomo and Chintaluri, Chaitanya and Podlaski, William F and Haddad, Sara A and Vogels, Tim P and Greenberg, David S and Macke, Jakob H},
  date = {2020-09-17},
  journaltitle = {eLife},
  volume = {9},
  pages = {e56261},
  issn = {2050-084X},
  doi = {10.7554/eLife.56261},
  url = {https://elifesciences.org/articles/56261},
  urldate = {2023-04-10},
  abstract = {Mechanistic modeling in neuroscience aims to explain observed phenomena in terms of underlying causes. However, determining which model parameters agree with complex and stochastic neural data presents a significant challenge. We address this challenge with a machine learning tool which uses deep neural density estimators—trained using model simulations—to carry out Bayesian inference and retrieve the full space of parameters compatible with raw data or selected data features. Our method is scalable in parameters and data features and can rapidly analyze new data after initial training. We demonstrate the power and flexibility of our approach on receptive fields, ion channels, and Hodgkin–Huxley models. We also characterize the space of circuit configurations giving rise to rhythmic activity in the crustacean stomatogastric ganglion, and use these results to derive hypotheses for underlying compensation mechanisms. Our approach will help close the gap between data-driven and theory-driven models of neural dynamics.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\N8FNGG33\\Gonçalves et al. - 2020 - Training deep neural density estimators to identif.pdf}
}

@online{gosztolaiInterpretableStatisticalRepresentations2023,
  title = {Interpretable Statistical Representations of Neural Population Dynamics and Geometry},
  author = {Gosztolai, Adam and Peach, Robert L. and Arnaudon, Alexis and Barahona, Mauricio and Vandergheynst, Pierre},
  date = {2023-04-06},
  eprint = {2304.03376},
  eprinttype = {arxiv},
  eprintclass = {cs, math, q-bio},
  url = {http://arxiv.org/abs/2304.03376},
  urldate = {2023-04-10},
  abstract = {The dynamics of neuron populations during diverse tasks often evolve on low-dimensional manifolds. However, it remains challenging to discern the contributions of geometry and dynamics for encoding relevant behavioural variables. Here, we introduce an unsupervised geometric deep learning framework for representing non-linear dynamical systems based on statistical distributions of local phase portrait features. Our method provides robust geometry-aware or geometry-agnostic representations for the unbiased comparison of dynamics based on measured trajectories. We demonstrate that our statistical representation can generalise across neural network instances to discriminate computational mechanisms, obtain interpretable embeddings of neural dynamics in a primate reaching task with geometric correspondence to hand kinematics, and develop a decoding algorithm with state-of-the-art accuracy. Our results highlight the importance of using the intrinsic manifold structure over temporal information to develop better decoding algorithms and assimilate data across experiments.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Mathematics - Dynamical Systems,Quantitative Biology - Neurons and Cognition,Quantitative Biology - Quantitative Methods},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\XUXHCLE2\\Gosztolai et al. - 2023 - Interpretable statistical representations of neura.pdf;C\:\\Users\\abel_\\Zotero\\storage\\JLG8TM2M\\2304.html}
}

@article{grillnerMeasuredMotionSearching2009,
  title = {Measured {{Motion}}: {{Searching}} for {{Simplicity}} in {{Spinal Locomotor Networks}}},
  shorttitle = {Measured {{Motion}}},
  author = {Grillner, Sten and Jessell, Thomas},
  date = {2009-12},
  journaltitle = {Current opinion in neurobiology},
  shortjournal = {Curr Opin Neurobiol},
  volume = {19},
  number = {6},
  eprint = {19896834},
  eprinttype = {pmid},
  pages = {572--586},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2009.10.011},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2951840/},
  urldate = {2023-04-10},
  pmcid = {PMC2951840}
}

@article{hahnloser2003,
  title = {Permitted and {{Forbidden Sets}} in {{Symmetric Threshold-Linear Networks}}},
  author = {Hahnloser, Richard H. R. and Seung, H. Sebastian and Slotine, Jean-Jacques},
  date = {2003-03-01},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {15},
  number = {3},
  pages = {621--638},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976603321192103},
  url = {https://direct.mit.edu/neco/article/15/3/621-638/6716},
  urldate = {2023-05-08},
  abstract = {Ascribing computational principles to neural feedback circuits is an important problem in theoretical neuroscience. We study symmetric threshold-linear networks and derive stability results that go beyond the insights that can be gained from Lyapunov theory or energy functions. By applying linear analysis to subnetworks composed of coactive neurons, we determine the stability of potential steady states. We find that stability depends on two types of eigenmodes. One type determines global stability and the other type determines whether or not multistability is possible. We can prove the equivalence of our stability criteria with criteria taken from quadratic programming. Also, we show that there are permitted sets of neurons that can be coactive at a steady state and forbidden sets that cannot. Permitted sets are clustered in the sense that subsets of permitted sets are permitted and supersets of forbidden sets are forbidden. By viewing permitted sets as memories stored in the synaptic connections, we can provide a formulation of longterm memory that is more general than the traditional perspective of fixed point attractor networks.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Hanhloser-2001-Permitted and forbidden sets in symmetric threshold-linear network.pdf}
}

@book{haleOrdinaryDifferentialEquations1980,
  title = {Ordinary Differential Equations},
  author = {Hale, Jack K.},
  date = {1980},
  edition = {2d ed},
  publisher = {{R. E. Krieger Pub. Co}},
  location = {{Huntington, N.Y}},
  isbn = {978-0-89874-011-0},
  langid = {english},
  pagetotal = {361},
  keywords = {Differential equations},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Hale-ODEs.pdf}
}

@article{hanksDistinctRelationshipsParietal2015,
  title = {Distinct Relationships of Parietal and Prefrontal Cortices to Evidence Accumulation},
  author = {Hanks, Timothy and Kopec, Charles D. and Brunton, Bingni W. and Duan, Chunyu A. and Erlich, Jeffrey C. and Brody, Carlos D.},
  date = {2015-04-09},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {520},
  number = {7546},
  eprint = {25600270},
  eprinttype = {pmid},
  pages = {220--223},
  issn = {0028-0836},
  doi = {10.1038/nature14066},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4835184/},
  urldate = {2023-04-10},
  abstract = {Gradual accumulation of evidence is thought to be fundamental for decision-making, and its neural correlates have been found in multiple brain regions–. Here we develop a generalizable method to measure tuning curves that specify the relationship between neural responses and mentally-accumulated evidence, and apply it to distinguish the encoding of decision variables in posterior parietal cortex (PPC) and prefrontal cortex (frontal orienting fields, FOF). We recorded the firing rates of neurons in PPC and FOF from rats performing a perceptual decision-making task. Classical analyses uncovered correlates of accumulating evidence, similar to previous observations in primates and also similar across the two regions. However, tuning curve assays revealed that while the PPC encodes a graded value of the accumulating evidence, the FOF has a more categorical encoding that indicates, throughout the trial, the decision provisionally favored by the evidence accumulated so far. Contrary to current views,,–, this suggests that premotor activity in frontal cortex does not play a role in the accumulation process, but instead has a more categorical function, such as transforming accumulated evidence into a discrete choice. To causally probe the role of FOF activity, we optogenetically silenced it during different timepoints of the trial. Consistent with a role in committing to a categorical choice at the end of the evidence accumulation process, but not consistent with a role during the accumulation itself, a behavioral effect was observed only when FOF silencing occurred at the end of the perceptual stimulus. Our results place important constraints on the circuit logic of brain regions involved in decision-making.},
  pmcid = {PMC4835184},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\GY8J8AJE\\Hanks et al. - 2015 - Distinct relationships of parietal and prefrontal .pdf}
}

@article{hartRecurrentCircuitDynamics2020,
  title = {Recurrent Circuit Dynamics Underlie Persistent Activity in the Macaque Frontoparietal Network},
  author = {Hart, Eric and Huk, Alexander C},
  date = {2020-05-07},
  journaltitle = {eLife},
  volume = {9},
  pages = {e52460},
  issn = {2050-084X},
  doi = {10.7554/eLife.52460},
  url = {https://elifesciences.org/articles/52460},
  urldate = {2023-02-06},
  abstract = {During delayed oculomotor response tasks, neurons in the lateral intraparietal area (LIP) and the frontal eye fields (FEF) exhibit persistent activity that reflects the active maintenance of behaviorally relevant information. Despite many computational models of the mechanisms of persistent activity, there is a lack of circuit-level data from the primate to inform the theories. To fill this gap, we simultaneously recorded ensembles of neurons in both LIP and FEF while macaques performed a memory-guided saccade task. A population encoding model revealed strong and symmetric long-timescale recurrent excitation between LIP and FEF. Unexpectedly, LIP exhibited stronger local functional connectivity than FEF, and many neurons in LIP had longer network and intrinsic timescales. The differences in connectivity could be explained by the strength of recurrent dynamics in attractor networks. These findings reveal reciprocal multi-area circuit dynamics in the frontoparietal network during persistent activity and lay the groundwork for quantitative comparisons to theoretical models.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Hart-2020-Recurrent circuit dynamics underlie persistent activity in the macaque frontoparietal network.pdf}
}

@article{hass2022,
  title = {Constraints on Persistent Activity in a Biologically Detailed Network Model of the Prefrontal Cortex with Heterogeneities},
  author = {Hass, Joachim and Ardid, Salva and Sherfey, Jason and Kopell, Nancy},
  date = {2022-08},
  journaltitle = {Progress in Neurobiology},
  shortjournal = {Progress in Neurobiology},
  volume = {215},
  pages = {102287},
  issn = {03010082},
  doi = {10.1016/j.pneurobio.2022.102287},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0301008222000739},
  urldate = {2023-05-14},
  abstract = {Persistent activity, the maintenance of neural activation over short periods of time in cortical networks, is widely thought to underlie the cognitive function of working memory. A large body of modeling studies has reproduced this kind of activity using cell assemblies with strengthened synaptic connections. However, almost all of these studies have considered persistent activity within networks with homogeneous neurons and synapses, making it difficult to judge the validity of such model results for cortical dynamics, which is based on highly heterogeneous neurons. Here, we consider persistent activity in a detailed, strongly data-driven network model of the prefrontal cortex with heterogeneous neuron and synapse parameters. Surprisingly, persistent activity could not be reproduced in this model without incorporating further constraints. We identified three factors that prevent successful persistent activity: heterogeneity in the cell parameters of interneurons, heterogeneity in the pa­ rameters of short-term synaptic plasticity and heterogeneity in the synaptic weights. We also discovered a general dynamic mechanism that prevents persistent activity in the presence of heterogeneities, namely a gradual drop-out of cell assembly neurons out of a bistable regime as input variability increases. Based on this mechanism, we found that persistent activity is recovered if heterogeneity is compensated, e.g., by a homeostatic plasticity mechanism. Cell assemblies shaped in this way may be potentially targeted by distinct inputs or become more responsive to specific tuning or spectral properties. Finally, we show that persistent activity in the model is robust against external noise, but the compensation of heterogeneities may prevent the dynamic gen­ eration of intrinsic in vivo-like irregular activity. These results may help informing the ongoing debate about the neural basis of working memory.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Hass-2022-Constraints on persistent activity in a biologically detailed network model of the prefrontal cortex with heterogeneities.pdf}
}

@online{hayou2019,
  title = {On the {{Impact}} of the {{Activation Function}} on {{Deep Neural Networks Training}}},
  author = {Hayou, Soufiane and Doucet, Arnaud and Rousseau, Judith},
  date = {2019-05-26},
  eprint = {1902.06853},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1902.06853},
  urldate = {2023-05-09},
  abstract = {The weight initialization and the activation function of deep neural networks have a crucial impact on the performance of the training procedure. An inappropriate selection can lead to the loss of information of the input during forward propagation and the exponential vanishing/exploding of gradients during back-propagation. Understanding the theoretical properties of untrained random networks is key to identifying which deep networks may be trained successfully as recently demonstrated by (Schoenholz et al., 2017) who showed that for deep feedforward neural networks only a specific choice of hyperparameters known as the ‘Edge of Chaos’ can lead to good performance. While the work by (Schoenholz et al., 2017) discuss trainability issues, we focus here on training acceleration and overall performance. We give a comprehensive theoretical analysis of the Edge of Chaos and show that we can indeed tune the initialization parameters and the activation function in order to accelerate the training and improve performance.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Hayou-2019-OntheImpactoftheActivationFunctiononDeepNeuralNetworksTraining.pdf}
}

@article{henaff2016,
  title = {Recurrent {{Orthogonal Networks}} and {{Long-Memory Tasks}}},
  author = {Henaff, Mikael and Szlam, Arthur and LeCun, Yann},
  abstract = {Although RNNs have been shown to be powerful tools for processing sequential data, finding architectures or optimization strategies that allow them to model very long term dependencies is still an active area of research. In this work, we carefully analyze two synthetic datasets originally outlined in (Hochreiter \& Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store information over many time steps. We explicitly construct RNN solutions to these problems, and using these constructions, illuminate both the problems themselves and the way in which RNNs store different types of information in their hidden states. These constructions furthermore explain the success of recent methods that specify unitary initializations or constraints on the transition matrices.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Henaff-Recurrent orthogonal networks.pdf}
}

@article{hintonSpikingBoltzmannMachines,
  title = {Spiking {{Boltzmann Machines}}},
  author = {Hinton, Geoffrey E and Brown, Andrew D},
  abstract = {We first show how to represent sharp posterior probability distributions using real valued coefficients on broadly-tuned basis functions. Then we show how the precise times of spikes can be used to convey the real-valued coefficients on the basis functions quickly and accurately. Finally we describe a simple simulation in which spiking neurons learn to model an image sequence by fitting a dynamic generative model.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Hinton-1999-Spiking Boltzmann Machines.pdf}
}

@article{hopfieldNeuralNetworksPhysical1982,
  title = {Neural Networks and Physical Systems with Emergent Collective Computational Abilities.},
  author = {Hopfield, J J},
  date = {1982-04},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {79},
  number = {8},
  pages = {2554--2558},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.79.8.2554},
  url = {https://pnas.org/doi/full/10.1073/pnas.79.8.2554},
  urldate = {2023-03-26},
  abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\5TGPQJ6X\\Hopfield - 1982 - Neural networks and physical systems with emergent.pdf}
}

@article{hulseMechanismsUnderlyingNeural2020,
  title = {Mechanisms {{Underlying}} the {{Neural Computation}} of {{Head Direction}}},
  author = {Hulse, Brad K. and Jayaraman, Vivek},
  date = {2020-07-08},
  journaltitle = {Annual Review of Neuroscience},
  shortjournal = {Annu. Rev. Neurosci.},
  volume = {43},
  number = {1},
  pages = {31--54},
  issn = {0147-006X, 1545-4126},
  doi = {10.1146/annurev-neuro-072116-031516},
  url = {https://www.annualreviews.org/doi/10.1146/annurev-neuro-072116-031516},
  urldate = {2023-04-10},
  abstract = {Many animals use an internal sense of direction to guide their movements through the world. Neurons selective to head direction are thought to support this directional sense and have been found in a diverse range of species, from insects to primates, highlighting their evolutionary importance. Across species, most head-direction networks share four key properties: a unique representation of direction at all times, persistent activity in the absence of movement, integration of angular velocity to update the representation, and the use of directional cues to correct drift. The dynamics of theorized network structures called ring attractors elegantly account for these properties, but their relationship to brain circuits is unclear. Here, we review experiments in rodents and flies that offer insights into potential neural implementations of ring attractor networks. We suggest that a theory-guided search across model systems for biological mechanisms that enable such dynamics would uncover general principles underlying head-direction circuit function.},
  langid = {english}
}

@online{hurwitzBuildingPopulationModels2021,
  title = {Building Population Models for Large-Scale Neural Recordings: Opportunities and Pitfalls},
  shorttitle = {Building Population Models for Large-Scale Neural Recordings},
  author = {Hurwitz, Cole and Kudryashova, Nina and Onken, Arno and Hennig, Matthias H.},
  date = {2021-07-10},
  eprint = {2102.01807},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  url = {http://arxiv.org/abs/2102.01807},
  urldate = {2023-03-02},
  abstract = {Modern extracellular recording technologies now enable simultaneous recording from large numbers of neurons. This has driven the development of new statistical models for analyzing and interpreting neural population activity. Here we provide a broad overview of recent developments in this area. We compare and contrast di erent approaches, highlight strengths and limitations, and discuss biological and mechanistic insights that these methods provide. While still an area of active development, there are already a number of powerful models for interpreting large scale neural recordings even in complex experimental settings.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Hurwitz-2021-Building population models for large-scale neural recordings opportunities and pitfalls.pdf}
}

@article{hurwitzTargetedNeuralDynamical2021,
  title = {Targeted {{Neural Dynamical Modeling}}},
  author = {Hurwitz, Cole and Srivastava, Akash and Xu, Kai and Jude, Justin and Perich, Matthew G and Miller, Lee E and Hennig, Matthias H},
  date = {2021},
  abstract = {Latent dynamics models have emerged as powerful tools for modeling and interpreting neural population activity. Recently, there has been a focus on incorporating simultaneously measured behaviour into these models to further disentangle sources of neural variability in their latent space. These approaches, however, are limited in their ability to capture the underlying neural dynamics (e.g. linear) and in their ability to relate the learned dynamics back to the observed behaviour (e.g. no time lag). To this end, we introduce Targeted Neural Dynamical Modeling (TNDM), a nonlinear state-space model that jointly models the neural activity and external behavioural variables. TNDM decomposes neural dynamics into behaviourally relevant and behaviourally irrelevant dynamics; the relevant dynamics are used to reconstruct the behaviour through a flexible linear decoder and both sets of dynamics are used to reconstruct the neural activity through a linear decoder with no time lag. We implement TNDM as a sequential variational autoencoder and validate it on simulated recordings and recordings taken from the premotor and motor cortex of a monkey performing a center-out reaching task. We show that TNDM is able to learn low-dimensional latent dynamics that are highly predictive of behaviour without sacrificing its fit to the neural data.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Hurwitz-2021-targeted-neural-dynamical-modeling.pdf}
}

@article{ikegamiExistenceToleranceStable1983,
  title = {On Existence of Tolerance Stable Diffeomorphisms},
  author = {Ikegami, Gikō},
  date = {1983-06},
  journaltitle = {Nagoya Mathematical Journal},
  shortjournal = {Nagoya Mathematical Journal},
  volume = {90},
  pages = {63--76},
  issn = {0027-7630, 2152-6842},
  doi = {10.1017/S0027763000020341},
  url = {https://www.cambridge.org/core/product/identifier/S0027763000020341/type/journal_article},
  urldate = {2023-04-09},
  abstract = {We consider a compact smooth manifold               M               . Diff               1               (               M               ) denotes the space of               C               1               -diffeomorphisms of               M               onto itself with the usual               C               1               -topology. In the research of the qualitative theory of dynamical systems there is a desire to find a concept of stability of geometric global structure of orbits such that this stable systems are dense in the space of dynamical systems on               M               . Structural stability does not satisfy the density condition in Diff               1               (               M               ). Tolerance stability (see Section 2 for definition) is a candidate for the density property [7, p. 294]. Concerning tolerance stability there are researches as [6], [7], [8], and [2].},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\M58MPEDA\\Ikegami - 1983 - On existence of tolerance stable diffeomorphisms.pdf}
}

@article{inagakiDiscreteAttractorDynamics2019,
  title = {Discrete Attractor Dynamics Underlies Persistent Activity in the Frontal Cortex},
  author = {Inagaki, Hidehiko K. and Fontolan, Lorenzo and Romani, Sandro and Svoboda, Karel},
  date = {2019-02},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {566},
  number = {7743},
  pages = {212--217},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-019-0919-7},
  url = {http://www.nature.com/articles/s41586-019-0919-7},
  urldate = {2023-02-06},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Inagaki-Discrete attractor dynamics underlies persistent activity in the frontal cortex.pdf}
}

@online{jagtap2022,
  title = {How Important Are Activation Functions in Regression and Classification? {{A}} Survey, Performance Comparison, and Future Directions},
  shorttitle = {How Important Are Activation Functions in Regression and Classification?},
  author = {Jagtap, Ameya D. and Karniadakis, George Em},
  date = {2022-12-28},
  eprint = {2209.02681},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2209.02681},
  urldate = {2023-05-09},
  abstract = {Inspired by biological neurons, the activation functions play an essential part in the learning process of any artificial neural network commonly used in many real-world problems. Various activation functions have been proposed in the literature for classification as well as regression tasks. In this work, we survey the activation functions that have been employed in the past as well as the current state-of-the-art. In particular, we present various developments in activation functions over the years and the advantages as well as disadvantages or limitations of these activation functions. We also discuss classical (fixed) activation functions, including rectifier units, and adaptive activation functions. In addition to discussing the taxonomy of activation functions based on characterization, a taxonomy of activation functions based on applications is presented. To this end, the systematic comparison of various fixed and adaptive activation functions is performed for classification data sets such as the MNIST, CIFAR-10, and CIFAR-100. In recent years, a physics-informed machine learning framework has emerged for solving problems related to scientific computations. For this purpose, we also discuss various requirements for activation functions that have been used in the physics-informed machine learning framework. Furthermore, various comparisons are made among different fixed and adaptive activation functions using various machine learning libraries such as TensorFlow, Pytorch, and JAX. Our findings show that activation functions such as ReLU and its variants, which are currently the stateof-the-art for many classification problems, do not work well in physics-informed machine learning frameworks due to the stringent requirement of the existence of derivatives, whereas other activation functions such as hyperbolic tangent, swish and sine give better performance, with superior results achieved with adaptive activation functions, especially for multiscale problems.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Jagtap-2022-Howimportantareactivationfunctionsinregression andclassification.pdf}
}

@article{jazayeriInterpretingNeuralComputations2021,
  title = {Interpreting Neural Computations by Examining Intrinsic and Embedding Dimensionality of Neural Activity},
  author = {Jazayeri, Mehrdad and Ostojic, Srdjan},
  date = {2021-10},
  journaltitle = {Current Opinion in Neurobiology},
  shortjournal = {Current Opinion in Neurobiology},
  volume = {70},
  pages = {113--120},
  issn = {09594388},
  doi = {10.1016/j.conb.2021.08.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0959438821000933},
  urldate = {2023-02-06},
  abstract = {The ongoing exponential rise in recording capacity calls for new approaches for analysing and interpreting neural data. Effective dimensionality has emerged as an important property of neural activity across populations of neurons, yet different studies rely on different definitions and interpretations of this quantity. Here we focus on intrinsic and embedding dimensionality, and discuss how they might reveal computational principles from data. Reviewing recent works, we propose that the intrinsic dimensionality reflects information about the latent variables encoded in collective activity, while embedding dimensionality reveals the manner in which this information is processed. We conclude by highlighting the role of network models as an ideal substrate for testing more specifically various hypotheses on the computational principles reflected through intrinsic and embedding dimensionality.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Jazayeri-2021-Interpreting neural computations by examining intrinsic and embedding dimensionality of neural activity.pdf}
}

@article{jazayeriOptimalRepresentationSensory2006,
  title = {Optimal Representation of Sensory Information by Neural Populations},
  author = {Jazayeri, Mehrdad and Movshon, J Anthony},
  date = {2006-05},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {9},
  number = {5},
  pages = {690--696},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn1691},
  url = {http://www.nature.com/articles/nn1691},
  urldate = {2023-05-03},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Jazayeri-2006-Optimal representation of sensory information by neural populations.pdf}
}

@article{jazayeriTemporalContextCalibrates2010,
  title = {Temporal Context Calibrates Interval Timing},
  author = {Jazayeri, Mehrdad and Shadlen, Michael N.},
  date = {2010-08},
  journaltitle = {Nature neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {13},
  number = {8},
  eprint = {20581842},
  eprinttype = {pmid},
  pages = {1020--1026},
  issn = {1097-6256},
  doi = {10.1038/nn.2590},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2916084/},
  urldate = {2023-04-10},
  abstract = {We use our sense of time to identify temporal relationships between events and to anticipate actions. How well we can exploit temporal contingencies depends on the variability of our measurements of time. We asked humans to reproduce time intervals drawn from different underlying distributions. As expected, production times were more variable for longer intervals. Surprisingly however, production times exhibited a systematic regression towards the mean. Consequently, estimates for a sample interval differed depending on the distribution from which it was drawn. A performance-optimizing Bayesian model that takes the underlying distribution of samples into account provided an accurate description of subjects’ performance, variability and bias. This finding suggests that the central nervous system incorporates knowledge about temporal uncertainty to adapt internal timing mechanisms to the temporal statistics of the environment.},
  pmcid = {PMC2916084},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\TJM2E9CT\\Jazayeri and Shadlen - 2010 - Temporal context calibrates interval timing.pdf}
}

@article{kakariaRingAttractorDynamics2017,
  title = {Ring {{Attractor Dynamics Emerge}} from a {{Spiking Model}} of the {{Entire Protocerebral Bridge}}},
  author = {Kakaria, Kyobi S. and De Bivort, Benjamin L.},
  date = {2017-02-14},
  journaltitle = {Frontiers in Behavioral Neuroscience},
  shortjournal = {Front. Behav. Neurosci.},
  volume = {11},
  issn = {1662-5153},
  doi = {10.3389/fnbeh.2017.00008},
  url = {http://journal.frontiersin.org/article/10.3389/fnbeh.2017.00008/full},
  urldate = {2023-04-27},
  abstract = {Animal navigation is accomplished by a combination of landmark-following and dead reckoning based on estimates of self motion. Both of these approaches require the encoding of heading information, which can be represented as an allocentric or egocentric azimuthal angle. Recently, Ca2+ correlates of landmark position and heading direction, in egocentric coordinates, were observed in the ellipsoid body (EB), a ring-shaped processing unit in the fly central complex (CX; Seelig and Jayaraman, 2015). These correlates displayed key dynamics of so-called ring attractors, namely: (1) responsiveness to the position of external stimuli; (2) persistence in the absence of external stimuli; (3) locking onto a single external stimulus when presented with two competitors; (4) stochastically switching between competitors with low probability; and (5) sliding or jumping between positions when an external stimulus moves. We hypothesized that ring attractor-like activity in the EB arises from reciprocal neuronal connections to a related structure, the protocerebral bridge (PB). Using recent lightmicroscopy resolution catalogs of neuronal cell types in the PB (Lin et al., 2013; Wolff et al., 2015), we determined a connectivity matrix for the PB-EB circuit. When activity in this network was simulated using a leaky-integrate-and-fire model, we observed patterns of activity that closely resemble the reported Ca2+ phenomena. All qualitative ring attractor behaviors were recapitulated in our model, allowing us to predict failure modes of the putative PB-EB ring attractor and the circuit dynamics phenotypes of thermogenetic or optogenetic manipulations. Ring attractor dynamics emerged under a wide variety of parameter configurations, even including non-spiking leaky-integrator implementations. This suggests that the ring-attractor computation is a robust output of this circuit, apparently arising from its high-level network properties (topological configuration, local excitation and long-range inhibition) rather than fine-scale biological detail.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Kakaria-2017-Ring Attractor Dynamics Emerge from a Spiking Model of the Entire Protocerebral Bridge.pdf}
}

@article{kayTasksTheirRole2023,
  title = {Tasks and Their Role in Visual Neuroscience},
  author = {Kay, Kendrick and Bonnen, Kathryn and Denison, Rachel N. and Arcaro, Mike J. and Barack, David L.},
  date = {2023-04},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  pages = {S0896627323002180},
  issn = {08966273},
  doi = {10.1016/j.neuron.2023.03.022},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627323002180},
  urldate = {2023-04-11},
  abstract = {Vision is widely used as a model system to gain insights into how sensory inputs are processed and interpreted by the brain. Historically, careful quantification and control of visual stimuli have served as the backbone of visual neuroscience. There has been less emphasis, however, on how an observer’s task influences the processing of sensory inputs. Motivated by diverse observations of task-dependent activity in the visual system, we propose a framework for thinking about tasks, their role in sensory processing, and how we might formally incorporate tasks into our models of vision.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Kay-2023-Tasks and their role in visual neuroscience.pdf}
}

@inproceedings{kimExamplesAreNot2016,
  title = {Examples Are Not Enough, Learn to Criticize! {{Criticism}} for {{Interpretability}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kim, Been and Khanna, Rajiv and Koyejo, Oluwasanmi O},
  date = {2016},
  volume = {29},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2016/hash/5680522b8e2bb01943234bce7bf84534-Abstract.html},
  urldate = {2023-03-06},
  abstract = {Example-based explanations are widely used in the effort to improve the interpretability of highly complex distributions. However, prototypes alone are rarely sufficient to represent the gist of the complexity. In order for users to construct better mental models and understand complex data distributions, we also need \{\textbackslash em criticism\} to explain what are \textbackslash textit\{not\} captured by prototypes.  Motivated by the Bayesian model criticism framework, we develop \textbackslash texttt\{MMD-critic\} which efficiently learns prototypes and criticism, designed to aid human interpretability. A human subject pilot study shows that the \textbackslash texttt\{MMD-critic\} selects prototypes and criticism that are useful to facilitate human understanding and reasoning. We also evaluate the prototypes selected by \textbackslash texttt\{MMD-critic\} via a nearest prototype classifier, showing competitive performance compared to baselines.},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\ZGJ3YSUZ\\Kim et al. - 2016 - Examples are not enough, learn to criticize! Criti.pdf}
}

@article{kimInferringLatentDynamics,
  title = {Inferring {{Latent Dynamics Underlying Neural Population Activity}}  via {{Neural Differential Equations}}},
  author = {Kim, Timothy Doyeon and Luo, Thomas Zhihao and Pillow, Jonathan W and Brody, Carlos D},
  abstract = {An important problem in systems neuroscience is to identify the latent dynamics underlying neural population activity. Here we address this problem by introducing a low-dimensional nonlinear model for latent neural population dynamics using neural ordinary differential equations (neural ODEs), with noisy sensory inputs and Poisson spike train outputs. We refer to this as the Poisson Latent Neural Differential Equations (PLNDE) model. We apply the PLNDE framework to a variety of synthetic datasets, and show that it accurately infers the phase portraits and fixed points of nonlinear systems augmented to produce spike train data, including the FitzHugh-Nagumo oscillator, a 3-dimensional nonlinear spiral, and a nonlinear sensory decision-making model with attractor dynamics. Our model significantly outperforms existing methods at inferring single-trial neural firing rates and the corresponding latent trajectories that generated them, especially in the regime where the spike counts and number of trials are low. We then apply our model to multi-region neural population recordings from medial frontal cortex of rats performing an auditory decision-making task. Our model provides a general, interpretable framework for investigating the neural mechanisms of decision-making and other cognitive computations through the lens of dynamical systems.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Readinglist\\Kim-2021-Inferring Latent Dynamics Underlying Neural Population Activity via Neural Differential Equations.pdf}
}

@article{kimZeemanToleranceStability,
  title = {Zeeman’s {{Tolerance Stability Conjecture}} and {{Takens}}’ {{Conjecture}}},
  author = {Kim, Jong Myung},
  abstract = {We do an attempt to solve the Zeeman’s tolerance stability conjecture and Takens’ conjecture using the concepts of chains.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\HPLPWZNL\\Kim - Zeeman’s Tolerance Stability Conjecture and Takens.pdf}
}

@inbook{kolenGradientFlowRecurrent2009,
  title = {Gradient {{Flow}} in {{Recurrent Nets}}: {{The Difficulty}} of {{Learning LongTerm Dependencies}}},
  shorttitle = {Gradient {{Flow}} in {{Recurrent Nets}}},
  booktitle = {A {{Field Guide}} to {{Dynamical Recurrent Networks}}},
  date = {2009},
  publisher = {{IEEE}},
  doi = {10.1109/9780470544037.ch14},
  url = {http://ieeexplore.ieee.org/search/srchabstract.jsp?arnumber=5264952},
  urldate = {2023-04-20},
  bookauthor = {Kolen, John F. and Kremer, Stefan C.},
  isbn = {978-0-470-54403-7},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Hochreiter-Gradient Flow in Recurrent Nets.pdf}
}

@article{kornblith,
  title = {Similarity of {{Neural Network Representations Revisited}}},
  author = {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  abstract = {Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Kornblith-2019-Similarity of Neural Network Representations Revisited.pdf}
}

@article{kriegeskorteCognitiveComputationalNeuroscience2018,
  title = {Cognitive Computational Neuroscience},
  author = {Kriegeskorte, Nikolaus and Douglas, Pamela K.},
  date = {2018-09},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {21},
  number = {9},
  pages = {1148--1160},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-018-0210-5},
  url = {http://www.nature.com/articles/s41593-018-0210-5},
  urldate = {2023-03-26},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Kriegeskorte-Cognitive computational neuroscience.pdf}
}

@article{kriegeskortePeelingOnionBrain2019,
  title = {Peeling the {{Onion}} of {{Brain Representations}}},
  author = {Kriegeskorte, Nikolaus and Diedrichsen, Jörn},
  date = {2019-07-08},
  journaltitle = {Annual Review of Neuroscience},
  shortjournal = {Annu. Rev. Neurosci.},
  volume = {42},
  number = {1},
  pages = {407--432},
  issn = {0147-006X, 1545-4126},
  doi = {10.1146/annurev-neuro-080317-061906},
  url = {https://www.annualreviews.org/doi/10.1146/annurev-neuro-080317-061906},
  urldate = {2023-03-19},
  abstract = {The brain’s function is to enable adaptive behavior in the world. To this end, the brain processes information about the world. The concept of representation links the information processed by the brain back to the world and enables us to understand what the brain does at a functional level. The appeal of making the connection between brain activity and what it represents has been irresistible to neuroscience, despite the fact that representational interpretations pose several challenges: We must define which aspects of brain activity matter, how the code works, and how it supports computations that contribute to adaptive behavior. It has been suggested that we might drop representational language altogether and seek to understand the brain, more simply, as a dynamical system. In this review, we argue that the concept of representation provides a useful link between dynamics and computational function and ask which aspects of brain activity should be analyzed to achieve a representational understanding. We peel the onion of brain representations in search of the layers (the aspects of brain activity) that matter to computation. The article provides an introduction to the motivation and mathematics of representational models, a critical discussion of their assumptions and limitations, and a preview of future directions in this area.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\NEPE8Q7X\\Kriegeskorte and Diedrichsen - 2019 - Peeling the Onion of Brain Representations.pdf}
}

@article{kriegeskorteRepresentationalGeometryIntegrating2013,
  title = {Representational Geometry: Integrating Cognition, Computation, and the Brain},
  shorttitle = {Representational Geometry},
  author = {Kriegeskorte, Nikolaus and Kievit, Rogier A.},
  date = {2013-08},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {17},
  number = {8},
  pages = {401--412},
  issn = {13646613},
  doi = {10.1016/j.tics.2013.06.007},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661313001277},
  urldate = {2023-03-26},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Kriegeskorte-Representational geometry.pdf}
}

@article{kuznetsovElementsAppliedBifurcation,
  title = {Elements of {{Applied Bifurcation Theory}}, {{Second Edition}}},
  author = {Kuznetsov, Yuri A},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Kuznetsov - Elements of applied bifurcation theory.pdf}
}

@online{le2015,
  title = {A {{Simple Way}} to {{Initialize Recurrent Networks}} of {{Rectified Linear Units}}},
  author = {Le, Quoc V. and Jaitly, Navdeep and Hinton, Geoffrey E.},
  date = {2015-04-07},
  eprint = {1504.00941},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1504.00941},
  urldate = {2023-05-05},
  abstract = {Learning long term dependencies in recurrent networks is difficult due to vanishing and exploding gradients. To overcome this difficulty, researchers have developed sophisticated optimization techniques and network architectures. In this paper, we propose a simpler solution that use recurrent neural networks composed of rectified linear units. Key to our solution is the use of the identity matrix or its scaled version to initialize the recurrent weight matrix. We find that our solution is comparable to a standard implementation of LSTMs on our four benchmarks: two toy problems involving long-range temporal structures, a large language modeling problem and a benchmark speech recognition problem.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Le-2015-A Simple Way to Initialize Recurrent Networks of Rectified Linear Units.pdf}
}

@online{lipton2015,
  title = {A {{Critical Review}} of {{Recurrent Neural Networks}} for {{Sequence Learning}}},
  author = {Lipton, Zachary C. and Berkowitz, John and Elkan, Charles},
  date = {2015-10-17},
  eprint = {1506.00019},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1506.00019},
  urldate = {2023-05-07},
  abstract = {Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences. In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences. Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities. Recurrent neural networks (RNNs) are connectionist models that capture the dynamics of sequences via cycles in the network of nodes. Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window. Although recurrent neural networks have traditionally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them. In recent years, systems based on long short-term memory (LSTM) and bidirectional (BRNN) architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition. In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models. When appropriate, we reconcile conflicting notation and nomenclature. Our goal is to provide a selfcontained explication of the state of the art together with a historical perspective and references to primary research.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Lipton-2015-A Critical Review of Recurrent Neural Networks for Sequence Learning.pdf}
}

@book{marrVisionComputationalInvestigation2010,
  title = {Vision: A Computational Investigation into the Human Representation and Processing of Visual Information},
  shorttitle = {Vision},
  author = {Marr, David},
  date = {2010},
  publisher = {{MIT Press}},
  location = {{Cambridge, Mass}},
  isbn = {978-0-262-51462-0},
  langid = {english},
  pagetotal = {403},
  keywords = {Data processing,Human information processing,Mathematical models,Vision},
  annotation = {OCLC: ocn472791457},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Marr - Vision A Computational Investigation into the Human Representation and Processing of Visual Information.pdf}
}

@article{mazurek2003,
  title = {A {{Role}} for {{Neural Integrators}} in {{Perceptual Decision Making}}},
  author = {Mazurek, Mark E. and Roitman, Jamie D. and Ditterich, Jochen and Shadlen, Michael N.},
  date = {2003-11-01},
  journaltitle = {Cerebral Cortex},
  volume = {13},
  number = {11},
  pages = {1257--1269},
  issn = {1460-2199, 1047-3211},
  doi = {10.1093/cercor/bhg097},
  url = {https://academic.oup.com/cercor/article/13/11/1257/274091},
  urldate = {2023-05-03},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Mazurek-A Role for Neural Integrators in Perceptual Decision Making.pdf}
}

@inbook{mischaikowConleyIndexTheory1995,
  title = {Conley Index Theory},
  booktitle = {Dynamical {{Systems}}},
  author = {Mischaikow, Konstantin},
  editor = {Johnson, Russell},
  date = {1995},
  volume = {1609},
  pages = {119--207},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/BFb0095240},
  url = {http://link.springer.com/10.1007/BFb0095240},
  urldate = {2023-03-26},
  bookauthor = {Arnold, Ludwig and Jones, Christopher K. R. T. and Mischaikow, Konstantin and Raugel, Geneviève},
  isbn = {978-3-540-60047-3 978-3-540-49415-7},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Mischaikow-1999-Conley Index Theory.pdf}
}

@article{mischaikowConleyTheoryCombinatorial,
  title = {Conley {{Theory}}: {{A}} Combinatorial Approach to Dynamics},
  author = {Mischaikow, Konstantin},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Mischaikow-2010-Conley Theory_ A combinatorial approach to dynamics.pdf}
}

@online{monfaredTransformationReLUbasedRecurrent2020,
  title = {Transformation of {{ReLU-based}} Recurrent Neural Networks from Discrete-Time to Continuous-Time},
  author = {Monfared, Zahra and Durstewitz, Daniel},
  date = {2020-07-01},
  eprint = {2007.00321},
  eprinttype = {arxiv},
  eprintclass = {math},
  url = {http://arxiv.org/abs/2007.00321},
  urldate = {2023-04-11},
  abstract = {Recurrent neural networks (RNN) as used in machine learning are commonly formulated in discrete time, i.e. as recursive maps. This brings a lot of advantages for training models on data, e.g. for the purpose of time series prediction or dynamical systems identification, as powerful and efficient inference algorithms exist for discrete time systems and numerical integration of differential equations is not necessary. On the other hand, mathematical analysis of dynamical systems inferred from data is often more convenient and enables additional insights if these are formulated in continuous time, i.e. as systems of ordinary (or partial) differential equations (ODE). Here we show how to perform such a translation from discrete to continuous time for a particular class of ReLU-based RNN. We prove three theorems on the mathematical equivalence between the discrete and continuous time formulations under a variety of conditions, and illustrate how to use our mathematical results on different machine learning and nonlinear dynamical systems examples.},
  pubstate = {preprint},
  keywords = {Mathematics - Dynamical Systems},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\A7DQABKI\\Monfared and Durstewitz - 2020 - Transformation of ReLU-based recurrent neural netw.pdf;C\:\\Users\\abel_\\Zotero\\storage\\A32BPCDT\\2007.html}
}

@online{morrison2022,
  title = {Diversity of Emergent Dynamics in Competitive Threshold-Linear Networks},
  author = {Morrison, Katherine and Degeratu, Anda and Itskov, Vladimir and Curto, Carina},
  date = {2022-10-15},
  eprint = {1605.04463},
  eprinttype = {arxiv},
  eprintclass = {nlin, q-bio},
  url = {http://arxiv.org/abs/1605.04463},
  urldate = {2023-04-14},
  abstract = {Threshold-linear networks consist of simple units interacting in the presence of a threshold nonlinearity. Competitive threshold-linear networks have long been known to exhibit multistability, where the activity of the network settles into one of potentially many steady states. In this work, we find conditions that guarantee the absence of steady states, while maintaining bounded activity. These conditions lead us to define a combinatorial family of competitive threshold-linear networks, parametrized by a simple directed graph. By exploring this family, we discover that threshold-linear networks are capable of displaying a surprisingly rich variety of nonlinear dynamics, including limit cycles, quasiperiodic attractors, and chaos. In particular, several types of nonlinear behaviors can co-exist in the same network. Our mathematical results also enable us to engineer networks with multiple dynamic patterns. Taken together, these theoretical and computational findings suggest that threshold-linear networks may be a valuable tool for understanding the relationship between network connectivity and emergent dynamics.},
  pubstate = {preprint},
  keywords = {Nonlinear Sciences - Adaptation and Self-Organizing Systems,Quantitative Biology - Neurons and Cognition}
}

@report{nairApproximateLineAttractor2022,
  type = {preprint},
  title = {An Approximate Line Attractor in the Hypothalamus That Encodes an Aggressive Internal State},
  author = {Nair, Aditya and Karigo, Tomomi and Yang, Bin and Linderman, Scott W and Anderson, David J and Kennedy, Ann},
  date = {2022-04-19},
  institution = {{Neuroscience}},
  doi = {10.1101/2022.04.19.488776},
  url = {http://biorxiv.org/lookup/doi/10.1101/2022.04.19.488776},
  urldate = {2023-02-06},
  abstract = {Summary                        The hypothalamus plays a key role in regulating innate behaviors. It is widely believed to function as a system of ‘labeled lines’, containing behavior-specific neurons with characteristic transcriptomic and connectomic profiles. This view however fails to explain why, although activation of estrogen receptor-1 (Esr1) expressing neurons in the ventromedial hypothalamus (VMHvl) promotes aggression, few VMHvl neurons are tuned to attack. To address this paradox, we adopted an unsupervised dynamical systems framework to analyze population activity among VMHvl             Esr1             neurons during aggression. We discovered that this activity contains an “integration” dimension exhibiting slow-ramping dynamics and persistent activity that correlates with escalating aggressiveness. These dynamics are implemented as an approximate line attractor in state space. Our analysis suggests a function for VMHvl to encode the intensity of behavior-relevant motive states using line attractors. This view reconciles observational and perturbational studies of VMHvl, and reveals a new mode of neural computation in the hypothalamus.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Nair-2022-An approximate line attractor in the hypothalamus that encodes an aggressive internal state.pdf}
}

@inproceedings{nassarLearningStructuredNeural2018,
  title = {Learning {{Structured Neural Dynamics From Single Trial Population Recording}}},
  booktitle = {2018 52nd {{Asilomar Conference}} on {{Signals}}, {{Systems}}, and {{Computers}}},
  author = {Nassar, Josue and Linderman, Scott W. and Zhao, Yuan and Bugallo, Monica and Park, Il Memming},
  date = {2018-10},
  pages = {666--670},
  publisher = {{IEEE}},
  location = {{Pacific Grove, CA, USA}},
  doi = {10.1109/ACSSC.2018.8645122},
  url = {https://ieeexplore.ieee.org/document/8645122/},
  urldate = {2023-04-10},
  abstract = {To understand the complex nonlinear dynamics of neural circuits, we fit a structured state-space model called tree-structured recurrent switching linear dynamical system (TrSLDS) to noisy high-dimensional neural time series. TrSLDS is a multi-scale hierarchical generative model for the state-space dynamics where each node of the latent tree captures locally linear dynamics. TrSLDS can be learned efficiently and in a fully Bayesian manner using Gibbs sampling. We showcase TrSLDS’ potential of inferring lowdimensional interpretable dynamical systems on a variety of examples.},
  eventtitle = {2018 52nd {{Asilomar Conference}} on {{Signals}}, {{Systems}}, and {{Computers}}},
  isbn = {978-1-5386-9218-9},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\UCAK5CJX\\Nassar et al. - 2018 - Learning Structured Neural Dynamics From Single Tr.pdf}
}

@article{niederNeuronalCodeNumber2016,
  title = {The Neuronal Code for Number},
  author = {Nieder, Andreas},
  date = {2016-06},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {17},
  number = {6},
  pages = {366--382},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn.2016.40},
  url = {http://www.nature.com/articles/nrn.2016.40},
  urldate = {2023-05-08},
  abstract = {Humans and non-human primates share an elemental quantification system that resides in a dedicated neural network in the parietal and frontal lobes. In this cortical network, ‘number neurons’ encode the number of elements in a set, its cardinality or numerosity, irrespective of stimulus appearance across sensory motor systems, and from both spatial and temporal presentation arrays. After numbers have been extracted from sensory input, they need to be processed to support goal-directed behaviour. Studying number neurons provides insights into how information is maintained in working memory and transformed in tasks that require rule-based decisions. Beyond an understanding of how cardinal numbers are encoded, number processing provides a window into the neuronal mechanisms of high-level brain functions.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Nieder-2016-The neural code for number.pdf}
}

@online{noorman2022,
  type = {preprint},
  title = {Accurate Angular Integration with Only a Handful of Neurons},
  author = {Noorman, Marcella and Hulse, Brad K and Jayaraman, Vivek and Romani, Sandro and Hermundstad, Ann M},
  date = {2022-05-25},
  eprinttype = {Neuroscience},
  doi = {10.1101/2022.05.23.493052},
  url = {http://biorxiv.org/lookup/doi/10.1101/2022.05.23.493052},
  urldate = {2023-05-04},
  abstract = {To flexibly navigate, many animals rely on internal spatial representations that persist when the animal is standing still in darkness, and update accurately by integrating the animal’s movements in the absence of localizing sensory cues. Theories of mammalian head direction cells have proposed that these dynamics can be realized in a special class of networks that maintain a localized bump of activity via structured recurrent connectivity, and that shift this bump of activity via angular velocity input. Although there are many different variants of these so-called ring attractor networks, they all rely on large numbers of neurons to generate representations that persist in the absence of input and accurately integrate angular velocity input. Surprisingly, in the fly, Drosophila melanogaster, a head direction representation is maintained by a much smaller number of neurons whose dynamics and connectivity resemble those of a ring attractor network. These findings challenge our understanding of ring attractors and their putative implementation in neural circuits. Here, we analyzed failures of angular velocity integration that emerge in small attractor networks with only a few computational units. Motivated by the peak performance of the fly head direction system in darkness, we mathematically derived conditions under which small networks, even with as few as 4 neurons, achieve the performance of much larger networks. The resulting description reveals that by appropriately tuning the network connectivity, the network can maintain persistent representations over the continuum of head directions, and it can accurately integrate angular velocity inputs. We then analytically determined how performance degrades as the connectivity deviates from this optimally-tuned setting, and we find a trade-off between network size and the tuning precision needed to achieve persistence and accurate integration. This work shows how even small networks can accurately track an animal’s movements to guide navigation, and it informs our understanding of the functional capabilities of discrete systems more broadly.},
  langid = {english},
  pubstate = {preprint},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Noorman-2022-Accurate angular integration with only a handful of neurons.pdf}
}

@article{oconnellBridgingNeuralComputational2018,
  title = {Bridging {{Neural}} and {{Computational Viewpoints}} on {{Perceptual Decision-Making}}},
  author = {O’Connell, Redmond G. and Shadlen, Michael N. and Wong-Lin, KongFatt and Kelly, Simon P.},
  date = {2018-11},
  journaltitle = {Trends in Neurosciences},
  shortjournal = {Trends in Neurosciences},
  volume = {41},
  number = {11},
  pages = {838--852},
  issn = {01662236},
  doi = {10.1016/j.tins.2018.06.005},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0166223618301668},
  urldate = {2023-05-03},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\OConnel-2018-Bridging Neural and Computational Viewpoints on Perceptual Decision-Making.pdf}
}

@report{oldenburgLogicRecurrentCircuits2022,
  type = {preprint},
  title = {The Logic of Recurrent Circuits in the Primary Visual Cortex},
  author = {Oldenburg, Ian Antón and Hendricks, William D. and Handy, Gregory and Shamardani, Kiarash and Bounds, Hayley A. and Doiron, Brent and Adesnik, Hillel},
  date = {2022-09-22},
  institution = {{Neuroscience}},
  doi = {10.1101/2022.09.20.508739},
  url = {http://biorxiv.org/lookup/doi/10.1101/2022.09.20.508739},
  urldate = {2023-04-11},
  abstract = {Abstract           Recurrent cortical activity sculpts visual perception by refining, amplifying, or suppressing incoming visual signals. Despite the importance of recurrent circuits for cortical processing, the basic rules that govern how nearby cortical neurons influence each other remains enigmatic. We used two-photon holographic optogenetics to activate ensembles of neurons in Layer 2/3 of the primary visual cortex (V1) in the absence of external stimuli to isolate the impact of local recurrence from external inputs. We find that the spatial arrangement and the stimulus feature preference of both the stimulated and the target ensemble jointly determine the net effect of recurrent activity, defining the cortical activity patterns that drive competition versus facilitation in L2/3 circuits. Computational modeling suggests that a combination of highly local recurrent excitatory connectivity and selective convergence onto inhibitory neurons give rise to these principles of recurrent activity. Our data and modeling reveal that recurrent activity can have varied impact, but a logic emerges through an understanding of the precise spatial distribution and feature preference of the multicellular pattern of activity.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Oldenburg-2020-The logic of recurrent circuits in the primary visual cortex.pdf}
}

@article{pandarinathInferringSingletrialNeural2018,
  title = {Inferring Single-Trial Neural Population Dynamics Using Sequential Auto-Encoders},
  author = {Pandarinath, Chethan and O’Shea, Daniel J. and Collins, Jasmine and Jozefowicz, Rafal and Stavisky, Sergey D. and Kao, Jonathan C. and Trautmann, Eric M. and Kaufman, Matthew T. and Ryu, Stephen I. and Hochberg, Leigh R. and Henderson, Jaimie M. and Shenoy, Krishna V. and Abbott, L. F. and Sussillo, David},
  date = {2018-10},
  journaltitle = {Nature Methods},
  shortjournal = {Nat Methods},
  volume = {15},
  number = {10},
  pages = {805--815},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-018-0109-9},
  url = {http://www.nature.com/articles/s41592-018-0109-9},
  urldate = {2023-05-03},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Pandarinath-Inferring single-trial neural population dynamics using sequential auto-encoders.pdf}
}

@article{pargaEmergentComputationsTrained2023,
  title = {Emergent {{Computations}} in {{Trained Artificial Neural Networks}} and {{Real Brains}}},
  author = {Parga, Néstor and Serrano-Fernández, Luis and Falcó-Roget, Joan},
  date = {2023-02-01},
  journaltitle = {Journal of Instrumentation},
  shortjournal = {J. Inst.},
  volume = {18},
  number = {02},
  eprint = {2212.04938},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  pages = {C02060},
  issn = {1748-0221},
  doi = {10.1088/1748-0221/18/02/C02060},
  url = {http://arxiv.org/abs/2212.04938},
  urldate = {2023-03-28},
  abstract = {Synaptic plasticity allows cortical circuits to learn new tasks and to adapt to changing environments. How do cortical circuits use plasticity to acquire functions such as decision-making or working memory? Neurons are connected in complex ways, forming recurrent neural networks, and learning modifies the strength of their connections. Moreover, neurons communicate emitting brief discrete electric signals. Here we describe how to train recurrent neural networks in tasks like those used to train animals in neuroscience laboratories, and how computations emerge in the trained networks. Surprisingly, artificial networks and real brains can use similar computational strategies.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\5RZL52ZM\\Parga et al. - 2023 - Emergent Computations in Trained Artificial Neural.pdf;C\:\\Users\\abel_\\Zotero\\storage\\X5V4MVFC\\2212.html}
}

@online{parkRecurrentNeuralNetworks2022,
  title = {Recurrent {{Neural Networks}} for {{Dynamical Systems}}: {{Applications}} to {{Ordinary Differential Equations}}, {{Collective Motion}}, and {{Hydrological Modeling}}},
  shorttitle = {Recurrent {{Neural Networks}} for {{Dynamical Systems}}},
  author = {Park, Yonggi and Gajamannage, Kelum and Jayathilake, Dilhani I. and Bollt, Erik M.},
  date = {2022-02-14},
  eprint = {2202.07022},
  eprinttype = {arxiv},
  eprintclass = {cs, math},
  url = {http://arxiv.org/abs/2202.07022},
  urldate = {2023-04-11},
  abstract = {Classical methods of solving spatiotemporal dynamical systems include statistical approaches such as autoregressive integrated moving average, which assume linear and stationary relationships between systems' previous outputs. Development and implementation of linear methods are relatively simple, but they often do not capture non-linear relationships in the data. Thus, artificial neural networks (ANNs) are receiving attention from researchers in analyzing and forecasting dynamical systems. Recurrent neural networks (RNN), derived from feed-forward ANNs, use internal memory to process variable-length sequences of inputs. This allows RNNs to applicable for finding solutions for a vast variety of problems in spatiotemporal dynamical systems. Thus, in this paper, we utilize RNNs to treat some specific issues associated with dynamical systems. Specifically, we analyze the performance of RNNs applied to three tasks: reconstruction of correct Lorenz solutions for a system with a formulation error, reconstruction of corrupted collective motion trajectories, and forecasting of streamflow time series possessing spikes, representing three fields, namely, ordinary differential equations, collective motion, and hydrological modeling, respectively. We train and test RNNs uniquely in each task to demonstrate the broad applicability of RNNs in reconstruction and forecasting the dynamics of dynamical systems.},
  pubstate = {preprint},
  keywords = {37M99,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,I.2.1,Mathematics - Dynamical Systems},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\FHTJX78I\\Park et al. - 2022 - Recurrent Neural Networks for Dynamical Systems A.pdf;C\:\\Users\\abel_\\Zotero\\storage\\YXTUK9EW\\2202.html}
}

@online{pascanuDifficultyTrainingRecurrent2013,
  title = {On the Difficulty of Training {{Recurrent Neural Networks}}},
  author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  date = {2013-02-15},
  eprint = {1211.5063},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1211.5063},
  urldate = {2023-04-09},
  abstract = {There are two widely known issues with properly training Recurrent Neural Networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\HFA3D563\\Pascanu et al. - 2013 - On the difficulty of training Recurrent Neural Net.pdf;C\:\\Users\\abel_\\Zotero\\storage\\PK6MURWE\\1211.html}
}

@online{peiNeuralLatentsBenchmark2022,
  title = {Neural {{Latents Benchmark}} '21: {{Evaluating}} Latent Variable Models of Neural Population Activity},
  shorttitle = {Neural {{Latents Benchmark}} '21},
  author = {Pei, Felix and Ye, Joel and Zoltowski, David and Wu, Anqi and Chowdhury, Raeed H. and Sohn, Hansem and O'Doherty, Joseph E. and Shenoy, Krishna V. and Kaufman, Matthew T. and Churchland, Mark and Jazayeri, Mehrdad and Miller, Lee E. and Pillow, Jonathan and Park, Il Memming and Dyer, Eva L. and Pandarinath, Chethan},
  date = {2022-01-17},
  eprint = {2109.04463},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  url = {http://arxiv.org/abs/2109.04463},
  urldate = {2023-03-03},
  abstract = {Advances in neural recording present increasing opportunities to study neural activity in unprecedented detail. Latent variable models (LVMs) are promising tools for analyzing this rich activity across diverse neural systems and behaviors, as LVMs do not depend on known relationships between the activity and external experimental variables. However, progress with LVMs for neuronal population activity is currently impeded by a lack of standardization, resulting in methods being developed and compared in an ad hoc manner. To coordinate these modeling efforts, we introduce a benchmark suite for latent variable modeling of neural population activity. We curate four datasets of neural spiking activity from cognitive, sensory, and motor areas to promote models that apply to the wide variety of activity seen across these areas. We identify unsupervised evaluation as a common framework for evaluating models across datasets, and apply several baselines that demonstrate benchmark diversity. We release this benchmark through EvalAI. http://neurallatents.github.io},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\XFE62PY6\\Pei et al. - 2022 - Neural Latents Benchmark '21 Evaluating latent va.pdf;C\:\\Users\\abel_\\Zotero\\storage\\ICHCXTYB\\2109.html}
}

@article{pereiraAttractorDynamicsNetworks2018,
  title = {Attractor {{Dynamics}} in {{Networks}} with {{Learning Rules Inferred}} from {{In Vivo Data}}},
  author = {Pereira, Ulises and Brunel, Nicolas},
  date = {2018-07},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {99},
  number = {1},
  pages = {227-238.e4},
  issn = {08966273},
  doi = {10.1016/j.neuron.2018.05.038},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627318304367},
  urldate = {2023-03-26},
  abstract = {The attractor neural network scenario is a popular scenario for memory storage in the association cortex, but there is still a large gap between models based on this scenario and experimental data. We study a recurrent network model in which both learning rules and distribution of stored patterns are inferred from distributions of visual responses for novel and familiar images in the inferior temporal cortex (ITC). Unlike classical attractor neural network models, our model exhibits graded activity in retrieval states, with distributions of firing rates that are close to lognormal. Inferred learning rules are close to maximizing the number of stored patterns within a family of unsupervised Hebbian learning rules, suggesting that learning rules in ITC are optimized to store a large number of attractor states. Finally, we show that there exist two types of retrieval states: one in which firing rates are constant in time and another in which firing rates fluctuate chaotically.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Pereira-2020-Attractor Dynamics in Networks with Learning Rules Inferred from In Vivo Data.pdf}
}

@online{ramachandran2017,
  title = {Searching for {{Activation Functions}}},
  author = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V.},
  date = {2017-10-27},
  eprint = {1710.05941},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1710.05941},
  urldate = {2023-05-09},
  abstract = {The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, f (x) = x · sigmoid(βx), which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9\% for Mobile NASNet-A and 0.6\% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Ramachandran-2017-Searching for Activation Functions.pdf}
}

@online{RepresentationalDriftEmerging,
  title = {Representational Drift: {{Emerging}} Theories for Continual Learning and Experimental Future Directions | {{Elsevier Enhanced Reader}}},
  shorttitle = {Representational Drift},
  doi = {10.1016/j.conb.2022.102609},
  url = {https://reader.elsevier.com/reader/sd/pii/S0959438822001039?token=A9893BEFE702591071CFC0B97A9E2604EF6A726AA2457F66F967A20A1701AC7119A9DCBAE56EB2C34722BA669BAA3EB1&originRegion=eu-west-1&originCreation=20230410100042},
  urldate = {2023-04-10},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\JSDYYIQV\\Representational drift Emerging theories for cont.pdf}
}

@article{robinsonStructuralStabilityC11976,
  title = {Structural Stability of {{C1}} Diffeomorphisms},
  author = {Robinson, Clark},
  date = {1976-09},
  journaltitle = {Journal of Differential Equations},
  shortjournal = {Journal of Differential Equations},
  volume = {22},
  number = {1},
  pages = {28--73},
  issn = {00220396},
  doi = {10.1016/0022-0396(76)90004-8},
  url = {https://linkinghub.elsevier.com/retrieve/pii/0022039676900048},
  urldate = {2023-03-26},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Robinson-1976-Structural stability of C1 diffeomorphisms.pdf}
}

@inproceedings{rodriguezLyaNetLyapunovFramework2022,
  title = {{{LyaNet}}: {{A Lyapunov Framework}} for {{Training Neural ODEs}}},
  shorttitle = {{{LyaNet}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Rodriguez, Ivan Dario Jimenez and Ames, Aaron and Yue, Yisong},
  date = {2022-06-28},
  pages = {18687--18703},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/rodriguez22a.html},
  urldate = {2023-03-07},
  abstract = {We propose a method for training ordinary differential equations by using a control-theoretic Lyapunov condition for stability. Our approach, called LyaNet, is based on a novel Lyapunov loss formulation that encourages the inference dynamics to converge quickly to the correct prediction. Theoretically, we show that minimizing Lyapunov loss guarantees exponential convergence to the correct solution and enables a novel robustness guarantee. We also provide practical algorithms, including one that avoids the cost of backpropagating through a solver or using the adjoint method. Relative to standard Neural ODE training, we empirically find that LyaNet can offer improved prediction performance, faster convergence of inference dynamics, and improved adversarial robustness. Our code is available at https://github.com/ivandariojr/LyapunovLearning.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\EVEKX7AE\\Rodriguez et al. - 2022 - LyaNet A Lyapunov Framework for Training Neural O.pdf}
}

@article{romoNeuronalCorrelatesParametric1999,
  title = {Neuronal Correlates of Parametric Working Memory in the Prefrontal Cortex},
  author = {Romo, Ranulfo and Brody, Carlos D. and Hernández, Adrián and Lemus, Luis},
  date = {1999-06},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {399},
  number = {6735},
  pages = {470--473},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/20939},
  url = {https://www.nature.com/articles/20939},
  urldate = {2023-05-06},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Romo-1999-Neuronal correlates of.pdf}
}

@report{saniWhereAllNonlinearity2021,
  type = {preprint},
  title = {Where Is All the Nonlinearity: Flexible Nonlinear Modeling of Behaviorally Relevant Neural Dynamics Using Recurrent Neural Networks},
  shorttitle = {Where Is All the Nonlinearity},
  author = {Sani, Omid G. and Pesaran, Bijan and Shanechi, Maryam M.},
  date = {2021-09-06},
  institution = {{Neuroscience}},
  doi = {10.1101/2021.09.03.458628},
  url = {http://biorxiv.org/lookup/doi/10.1101/2021.09.03.458628},
  urldate = {2023-04-23},
  abstract = {Understanding the dynamical transformation of neural activity to behavior requires modeling this transformation while both dissecting its potential nonlinearities and dissociating and preserving its nonlinear behaviorally relevant neural dynamics, which remain unaddressed. We present RNN PSID, a nonlinear dynamic modeling method that enables flexible dissection of nonlinearities, dissociation and preferential learning of neural dynamics relevant to specific behaviors, and causal decoding. We first validate RNN PSID in simulations and then use it to investigate nonlinearities in monkey spiking and LFP activity across four tasks and different brain regions. Nonlinear RNN PSID successfully dissociated and preserved nonlinear behaviorally relevant dynamics, thus outperforming linear and non-preferential nonlinear learning methods in behavior decoding while reaching similar neural prediction. Strikingly, dissecting the nonlinearities with RNN PSID revealed that consistently across all tasks, summarizing the nonlinearity only in the mapping from the latent dynamics to behavior was largely sufficient for predicting behavior and neural activity. RNN PSID provides a novel tool to reveal new characteristics of nonlinear neural dynamics underlying behavior.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Sani-2021-Where is all the nonlinearity.pdf}
}

@online{saxeExactSolutionsNonlinear2014,
  title = {Exact Solutions to the Nonlinear Dynamics of Learning in Deep Linear Neural Networks},
  author = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya},
  date = {2014-02-19},
  eprint = {1312.6120},
  eprinttype = {arxiv},
  eprintclass = {cond-mat, q-bio, stat},
  url = {http://arxiv.org/abs/1312.6120},
  urldate = {2023-05-05},
  abstract = {Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Saxe-2014-Exact solutions to the nonlinear dynamics of learning in deep linear neural networks.pdf}
}

@article{seungStabilityMemoryEye2000,
  title = {Stability of the {{Memory}} of {{Eye Position}} in a {{Recurrent Network}} of {{Conductance-Based Model Neurons}}},
  author = {Seung, H.Sebastian and Lee, Daniel D. and Reis, Ben Y. and Tank, David W.},
  date = {2000-04},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {26},
  number = {1},
  pages = {259--271},
  issn = {08966273},
  doi = {10.1016/S0896-6273(00)81155-1},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627300811551},
  urldate = {2023-05-08},
  abstract = {Studies of the neural correlates of short-term memory in a wide variety of brain areas have found that transient inputs can cause persistent changes in rates of action potential firing, through a mechanism that remains unknown. In a premotor area that is responsible for holding the eyes still during fixation, persistent neural firing encodes the angular position of the eyes in a characteristic manner: below a threshold position the neuron is silent, and above it the firing rate is linearly related to position. Both the threshold and linear slope vary from neuron to neuron. We have reproduced this behavior in a biophysically plausible network model. Persistence depends on precise tuning of the strength of synaptic feedback, and a relatively long synaptic time constant improves the robustness to mistuning.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Seung-2002- Stability of the memory of eye position in a recurrent network of conductance-based model neurons.pdf}
}

@online{shang2016,
  title = {Understanding and {{Improving Convolutional Neural Networks}} via {{Concatenated Rectified Linear Units}}},
  author = {Shang, Wenling and Sohn, Kihyuk and Almeida, Diogo and Lee, Honglak},
  date = {2016-07-19},
  eprint = {1603.05201},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1603.05201},
  urldate = {2023-05-09},
  abstract = {Recently, convolutional neural networks (CNNs) have been used as a powerful tool to solve many problems of machine learning and computer vision. In this paper, we aim to provide insight on the property of convolutional neural networks, as well as a generic method to improve the performance of many CNN architectures. Specifically, we first examine existing CNN models and observe an intriguing property that the filters in the lower layers form pairs (i.e., filters with opposite phase). Inspired by our observation, we propose a novel, simple yet effective activation scheme called concatenated ReLU (CReLU) and theoretically analyze its reconstruction property in CNNs. We integrate CReLU into several state-of-the-art CNN architectures and demonstrate improvement in their recognition performance on CIFAR-10/100 and ImageNet datasets with fewer trainable parameters. Our results suggest that better understanding of the properties of CNNs can lead to significant performance improvement with a simple modification.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Shang-2016-Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units.pdf}
}

@article{sherstinsky2020,
  title = {Fundamentals of {{Recurrent Neural Network}} ({{RNN}}) and {{Long Short-Term Memory}} ({{LSTM}}) {{Network}}},
  author = {Sherstinsky, Alex},
  date = {2020-03},
  journaltitle = {Physica D: Nonlinear Phenomena},
  shortjournal = {Physica D: Nonlinear Phenomena},
  volume = {404},
  eprint = {1808.03314},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {132306},
  issn = {01672789},
  doi = {10.1016/j.physd.2019.132306},
  url = {http://arxiv.org/abs/1808.03314},
  urldate = {2023-05-07},
  abstract = {Because of their effectiveness in broad practical applications, LSTM networks have received a wealth of coverage in scientific journals, technical blogs, and implementation guides. However, in most articles, the inference formulas for the LSTM network and its parent, RNN, are stated axiomatically, while the training formulas are omitted altogether. In addition, the technique of “unrolling” an RNN is routinely presented without justification throughout the literature. The goal of this tutorial is to explain the essential RNN and LSTM fundamentals in a single document. Drawing from concepts in Signal Processing, we formally derive the canonical RNN formulation from differential equations. We then propose and prove a precise statement, which yields the RNN unrolling technique. We also review the difficulties with training the standard RNN and address them by transforming the RNN into the “Vanilla LSTM”1 network through a series of logical arguments. We provide all equations pertaining to the LSTM system together with detailed descriptions of its constituent entities. Albeit unconventional, our choice of notation and the method for presenting the LSTM system emphasizes ease of understanding. As part of the analysis, we identify new opportunities to enrich the LSTM system and incorporate these extensions into the Vanilla LSTM network, producing the most general LSTM variant to date. The target reader has already been exposed to RNNs and LSTM networks through numerous available resources and is open to an alternative pedagogical approach. A Machine Learning practitioner seeking guidance for implementing our new augmented LSTM model in software for experimentation and research will find the insights and derivations in this treatise valuable as well.},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Sherstinsky-2020-Fundamentals of Recurrent Neural Network.pdf}
}

@article{shimizu2021,
  title = {Computational Roles of Intrinsic Synaptic Dynamics},
  author = {Shimizu, Genki and Yoshida, Kensuke and Kasai, Haruo and Toyoizumi, Taro},
  date = {2021-10},
  journaltitle = {Current Opinion in Neurobiology},
  shortjournal = {Current Opinion in Neurobiology},
  volume = {70},
  pages = {34--42},
  issn = {09594388},
  doi = {10.1016/j.conb.2021.06.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0959438821000672},
  urldate = {2023-05-08},
  abstract = {Conventional theories assume that long-term information storage in the brain is implemented by modifying synaptic efficacy. Recent experimental findings challenge this view by demonstrating that dendritic spine sizes, or their corresponding synaptic weights, are highly volatile even in the absence of neural activity. Here, we review previous computational works on the roles of these intrinsic synaptic dynamics. We first present the possibility for neuronal networks to sustain stable performance in their presence, and we then hypothesize that intrinsic dynamics could be more than mere noise to withstand, but they may improve information processing in the brain.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Shimizu-2021-Computational roles of intrinsic synaptic dynamics.pdf}
}

@online{TemporallyIrregularMnemonic,
  title = {Temporally {{Irregular Mnemonic Persistent Activity}} in {{Prefrontal Neurons}} of {{Monkeys During}} a {{Delayed Response Task}}},
  doi = {10.1152/jn.00949.2002},
  url = {https://journals.physiology.org/doi/epdf/10.1152/jn.00949.2002},
  urldate = {2023-03-26},
  langid = {english}
}

@report{teradaChaoticNeuralDynamics2023,
  type = {preprint},
  title = {Chaotic Neural Dynamics Facilitate Probabilistic Computations through Sampling},
  author = {Terada, Yu and Toyoizumi, Taro},
  date = {2023-05-05},
  institution = {{Neuroscience}},
  doi = {10.1101/2023.05.04.539470},
  url = {http://biorxiv.org/lookup/doi/10.1101/2023.05.04.539470},
  urldate = {2023-05-08},
  abstract = {Cortical neurons exhibit highly variable responses over trials and time. Theoretical works posit that this variability arises potentially from chaotic network dynamics of recurrently connected neurons. Here we demonstrate that chaotic neural dynamics, formed through synaptic learning, allow networks to perform sensory cue integration in a sampling-based implementation. We show that the emergent chaotic dynamics provide neural substrates for generating samples not only of a static variable but also of a dynamical trajectory, where generic recurrent networks acquire these abilities with a biologically-plausible learning rule through trial and error. Furthermore, the networks generalize their experience in the stimulus-evoked samples to the inference without partial or all sensory information, which suggests a computational role of spontaneous activity as a representation of the priors as well as a tractable biological computation for marginal distributions. These \hspace{0.6em}ndings suggest that chaotic neural dynamics may serve for the brain function as a Bayesian generative model.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\YuTerada-2023-Chaotic neural dynamics facilitate probabilistic computations through sampling.pdf}
}

@inproceedings{teshimaCouplingbasedInvertibleNeural2020,
  title = {Coupling-Based {{Invertible Neural Networks Are Universal Diffeomorphism Approximators}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Teshima, Takeshi and Ishikawa, Isao and Tojo, Koichi and Oono, Kenta and Ikeda, Masahiro and Sugiyama, Masashi},
  date = {2020},
  volume = {33},
  pages = {3362--3373},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/2290a7385ed77cc5592dc2153229f082-Abstract.html},
  urldate = {2023-04-11},
  abstract = {Invertible neural networks based on coupling flows (CF-INNs) have various machine learning applications such as image synthesis and representation learning. However, their desirable characteristics such as analytic invertibility come at the cost of restricting the functional forms. This poses a question on their representation power: are CF-INNs universal approximators for invertible functions? Without a universality, there could be a well-behaved invertible transformation that the CF-INN can never approximate, hence it would render the model class unreliable. We answer this question by showing a convenient criterion: a CF-INN is universal if its layers contain affine coupling and invertible linear functions as special cases. As its corollary, we can affirmatively resolve a previously unsolved problem: whether normalizing flow models based on affine coupling can be universal distributional approximators. In the course of proving the universality, we prove a general theorem to show the equivalence of the universality for certain diffeomorphism classes, a theoretical insight that is of interest by itself.},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\DJRCTACU\\Teshima et al. - 2020 - Coupling-based Invertible Neural Networks Are Univ.pdf}
}

@article{vafidisLearningAccuratePath2022,
  title = {Learning Accurate Path Integration in Ring Attractor Models of the Head Direction System},
  author = {Vafidis, Pantelis and Owald, David and D'Albis, Tiziano and Kempter, Richard},
  date = {2022-06-20},
  journaltitle = {eLife},
  volume = {11},
  pages = {e69841},
  issn = {2050-084X},
  doi = {10.7554/eLife.69841},
  url = {https://elifesciences.org/articles/69841},
  urldate = {2023-04-10},
  abstract = {Ring attractor models for angular path integration have received strong experimental support. To function as integrators, head direction circuits require precisely tuned connectivity, but it is currently unknown how such tuning could be achieved. Here, we propose a network model in which a local, biologically plausible learning rule adjusts synaptic efficacies during development, guided by supervisory allothetic cues. Applied to the Drosophila head direction system, the model learns to path-i­ntegrate accurately and develops a connectivity strikingly similar to the one reported in experiments. The mature network is a quasi-­continuous attractor and reproduces key experiments in which optogenetic stimulation controls the internal representation of heading in flies, and where the network remaps to integrate with different gains in rodents. Our model predicts that path integration requires self-­supervised learning during a developmental phase, and proposes a general framework to learn to path-i­ntegrate with gain-1­ even in architectures that lack the physical topography of a ring.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\TEU29BNV\\Vafidis et al. - 2022 - Learning accurate path integration in ring attract.pdf}
}

@article{vogtLyapunovExponentsRNNs2022,
  title = {On {{Lyapunov Exponents}} for {{RNNs}}: {{Understanding Information Propagation Using Dynamical Systems Tools}}},
  shorttitle = {On {{Lyapunov Exponents}} for {{RNNs}}},
  author = {Vogt, Ryan and Puelma Touzel, Maximilian and Shlizerman, Eli and Lajoie, Guillaume},
  date = {2022-03-02},
  journaltitle = {Frontiers in Applied Mathematics and Statistics},
  shortjournal = {Front. Appl. Math. Stat.},
  volume = {8},
  pages = {818799},
  issn = {2297-4687},
  doi = {10.3389/fams.2022.818799},
  url = {https://www.frontiersin.org/articles/10.3389/fams.2022.818799/full},
  urldate = {2023-04-23},
  abstract = {Recurrent neural networks (RNNs) have been successfully applied to a variety of problems involving sequential data, but their optimization is sensitive to parameter initialization, architecture, and optimizer hyperparameters. Considering RNNs as dynamical systems, a natural way to capture stability, i.e., the growth and decay over long iterates, are the Lyapunov Exponents (LEs), which form the Lyapunov spectrum. The LEs have a bearing on stability of RNN training dynamics since forward propagation of information is related to the backward propagation of error gradients. LEs measure the asymptotic rates of expansion and contraction of non-linear system trajectories, and generalize stability analysis to the time-varying attractors structuring the non-autonomous dynamics of data-driven RNNs. As a tool to understand and exploit stability of training dynamics, the Lyapunov spectrum fills an existing gap between prescriptive mathematical approaches of limited scope and computationally-expensive empirical approaches. To leverage this tool, we implement an efficient way to compute LEs for RNNs during training, discuss the aspects specific to standard RNN architectures driven by typical sequential datasets, and show that the Lyapunov spectrum can serve as a robust readout of training stability across hyperparameters. With this expositionoriented contribution, we hope to draw attention to this under-studied, but theoretically grounded tool for understanding training stability in RNNs.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Vogt-2022-On Lyapunov Exponents for RNNs Understanding Information Propagation Using Dynamical Systems Tools.pdf}
}

@article{wang2022,
  title = {Multiple Bumps Can Enhance Robustness to Noise in Continuous Attractor Networks},
  author = {Wang, Raymond and Kang, Louis},
  editor = {Wei, Xuexin},
  date = {2022-10-10},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLoS Comput Biol},
  volume = {18},
  number = {10},
  pages = {e1010547},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1010547},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1010547},
  urldate = {2023-05-09},
  abstract = {A central function of continuous attractor networks is encoding coordinates and accurately updating their values through path integration. To do so, these networks produce localized bumps of activity that move coherently in response to velocity inputs. In the brain, continuous attractors are believed to underlie grid cells and head direction cells, which maintain periodic representations of position and orientation, respectively. These representations can be achieved with any number of activity bumps, and the consequences of having more or fewer bumps are unclear. We address this knowledge gap by constructing 1D ring attractor networks with different bump numbers and characterizing their responses to three types of noise: fluctuating inputs, spiking noise, and deviations in connectivity away from ideal attractor configurations. Across all three types, networks with more bumps experience less noise-driven deviations in bump motion. This translates to more robust encodings of linear coordinates, like position, assuming that each neuron represents a fixed length no matter the bump number. Alternatively, we consider encoding a circular coordinate, like orientation, such that the network distance between adjacent bumps always maps onto 360 degrees. Under this mapping, bump number does not significantly affect the amount of error in the coordinate readout. Our simulation results are intuitively explained and quantitatively matched by a unified theory for path integration and noise in multi-bump networks. Thus, to suppress the effects of biologically relevant noise, continuous attractor networks can employ more bumps when encoding linear coordinates; this advantage disappears when encoding circular coordinates. Our findings provide motivation for multiple bumps in the mammalian grid network.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Wang-2022-Multiple bumps can enhance robustness to noise in continuous attractor networks.pdf}
}

@article{wangProbabilisticDecisionMaking2002,
  title = {Probabilistic {{Decision Making}} by {{Slow Reverberation}} in {{Cortical Circuits}}},
  author = {Wang, Xiao-Jing},
  date = {2002-12},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {36},
  number = {5},
  pages = {955--968},
  issn = {08966273},
  doi = {10.1016/S0896-6273(02)01092-9},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627302010929},
  urldate = {2023-05-05},
  abstract = {Recent physiological studies of alert primates have revealed cortical neural correlates of key steps in a perceptual decision-making process. To elucidate synaptic mechanisms of decision making, I investigated a biophysically realistic cortical network model for a visual discrimination experiment. In the model, slow recurrent excitation and feedback inhibition produce attractor dynamics that amplify the difference between conflicting inputs and generates a binary choice. The model is shown to account for salient characteristics of the observed decision-correlated neural activity, as well as the animal’s psychometric function and reaction times. These results suggest that recurrent excitation mediated by NMDA receptors provides a candidate cellular mechanism for the slow time integration of sensory stimuli and the formation of categorical choices in a decision-making neocortical network.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Wang-2002-Probabilistic Decision Making by Slow Reverberation in Cortical Circuits.pdf}
}

@article{wangRobustnessPerturbationAnalysis,
  title = {Robustness and {{Perturbation Analysis}} of a {{Class}} of {{Nonlinear Systems}} with {{Applications}} to {{Neural Networks}}},
  author = {Wang, Kaining and Michel, Anthony N},
  abstract = {In this paper we study the robustness properties of a large class of nonlinear systems by addressing the following question: given a nonlinear system with specified asymptotically stable equilibria, under what conditions will a perturbed model of the system possess asymptotically stable equilibria that are close (in distance) to the asymptotically stable equilibria of the unperturbed system? In arriving at our results, we establish robustness stabfity results for the perturbed systems considered, and we determine conditions that ensure the existence of asymptotically stable equilibria of the perturbed system that are near the asymptotically stable equilibria of the original unperturbed system. These results involve quantitative estimates of the distance between the corresponding equilibrium points of the unperturbed and perturbed systems. We apply the above results in the qualitative analysis of a large class of artificial neural networks.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Wang-1994-Robustness and perturbation analysis of a class of nonlinear systems with applications to neural networks.pdf}
}

@article{wimmer2014,
  title = {Bump Attractor Dynamics in Prefrontal Cortex Explains Behavioral Precision in Spatial Working Memory},
  author = {Wimmer, Klaus and Nykamp, Duane Q and Constantinidis, Christos and Compte, Albert},
  date = {2014-03},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {17},
  number = {3},
  pages = {431--439},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3645},
  url = {http://www.nature.com/articles/nn.3645},
  urldate = {2023-03-26},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\ANRPCTPQ\\Wimmer et al. - 2014 - Bump attractor dynamics in prefrontal cortex expla.pdf}
}

@article{wuDynamicsComputationContinuous2008,
  title = {Dynamics and {{Computation}} of {{Continuous Attractors}}},
  author = {Wu, Si and Hamaguchi, Kosuke and Amari, Shun-ichi},
  date = {2008-04},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {20},
  number = {4},
  pages = {994--1025},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.2008.10-06-378},
  url = {https://direct.mit.edu/neco/article/20/4/994-1025/7303},
  urldate = {2023-03-06},
  abstract = {Continuous attractor is a promising model for describing the encoding of continuous stimuli in neural systems. In a continuous attractor, the stationary states of the neural system form a continuous parameter space, on which the system is neutrally stable. This property enables the neutral system to track time-varying stimuli smoothly, but it also degrades the accuracy of information retrieval, since these stationary states are easily disturbed by external noise. In this work, based on a simple model, we systematically investigate the dynamics and the computational properties of continuous attractors. In order to analyze the dynamics of a large-size network, which is otherwise extremely complicated, we develop a strategy to reduce its dimensionality by utilizing the fact that a continuous attractor can eliminate the noise components perpendicular to the attractor space very quickly. We therefore project the network dynamics onto the tangent of the attractor space and simplify it successfully as a one-dimensional Ornstein-Uhlenbeck process. Based on this simplified model, we investigate (1) the decoding error of a continuous attractor under the driving of external noisy inputs, (2) the tracking speed of a continuous attractor when external stimulus experiences abrupt changes, (3) the neural correlation structure associated with the specific dynamics of a continuous attractor, and (4) the consequence of asymmetric neural correlation on statistical population decoding. The potential implications of these results on our understanding of neural information processing are also discussed.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\VQJQU8ZT\\Wu et al. - 2008 - Dynamics and Computation of Continuous Attractors.pdf}
}

@article{yampolskiyUnexplainabilityIncomprehensibilityArtificial,
  title = {Unexplainability and {{Incomprehensibility}} of {{Artificial Intelligence}}},
  author = {Yampolskiy, Roman V},
  abstract = {Explainability and comprehensibility of AI are important requirements for intelligent systems deployed in real-world domains. Users want and frequently need to understand how decisions impacting them are made. Similarly it is important to understand how an intelligent system functions for safety and security reasons. In this paper, we describe two complementary impossibility results (Unexplainability and Incomprehensibility), essentially showing that advanced AIs would not be able to accurately explain some of their decisions and for the decisions they could explain people would not understand some of those explanations.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\9RUNDJQD\\Yampolskiy - Unexplainability and Incomprehensibility of Artifi.pdf}
}

@article{yangArtificialNeuralNetworks2020,
  title = {Artificial {{Neural Networks}} for {{Neuroscientists}}: {{A Primer}}},
  shorttitle = {Artificial {{Neural Networks}} for {{Neuroscientists}}},
  author = {Yang, Guangyu Robert and Wang, Xiao-Jing},
  date = {2020-09},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {107},
  number = {6},
  pages = {1048--1070},
  issn = {08966273},
  doi = {10.1016/j.neuron.2020.09.005},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627320307054},
  urldate = {2023-03-26},
  abstract = {Artificial neural networks (ANNs) are essential tools in machine learning that have drawn increasing attention in neuroscience. Besides offering powerful techniques for data analysis, ANNs provide a new approach for neuroscientists to build models for complex behaviors, heterogeneous neural activity, and circuit connectivity, as well as to explore optimization in neural systems, in ways that traditional models are not designed for. In this pedagogical Primer, we introduce ANNs and demonstrate how they have been fruitfully deployed to study neuroscientific questions. We first discuss basic concepts and methods of ANNs. Then, with a focus on bringing this mathematical framework closer to neurobiology, we detail how to customize the analysis, structure, and learning of ANNs to better address a wide range of challenges in brain research. To help readers garner hands-on experience, this Primer is accompanied with tutorial-style code in PyTorch and Jupyter Notebook, covering major topics.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Yang-2020-Artificial Neural Networks for Neuroscientists.pdf}
}

@article{yu2019,
  title = {A {{Review}} of {{Recurrent Neural Networks}}: {{LSTM Cells}} and {{Network Architectures}}},
  shorttitle = {A {{Review}} of {{Recurrent Neural Networks}}},
  author = {Yu, Yong and Si, Xiaosheng and Hu, Changhua and Zhang, Jianxun},
  date = {2019-07},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {31},
  number = {7},
  pages = {1235--1270},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco_a_01199},
  url = {https://direct.mit.edu/neco/article/31/7/1235-1270/8500},
  urldate = {2023-05-07},
  abstract = {Recurrent neural networks (RNNs) have been widely adopted in research areas concerned with sequential data, such as text, audio, and video. However, RNNs consisting of sigma cells or tanh cells are unable to learn the relevant information of input data when the input gap is large. By introducing gate functions into the cell structure, the long short-term memory (LSTM) could handle the problem of long-term dependencies well. Since its introduction, almost all the exciting results based on RNNs have been achieved by the LSTM. The LSTM has become the focus of deep learning. We review the LSTM cell and its variants to explore the learning capacity of the LSTM cell. Furthermore, the LSTM networks are divided into two broad categories: LSTM-dominated networks and integrated LSTM networks. In addition, their various applications are discussed. Finally, future research directions are presented for LSTM networks.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Yu-2019-A Review of Recurrent Neural Networks.pdf}
}

@article{zhangComprehensiveReviewStability2014,
  title = {A {{Comprehensive Review}} of {{Stability Analysis}} of {{Continuous-Time Recurrent Neural Networks}}},
  author = {Zhang, Huaguang and Wang, Zhanshan and Liu, Derong},
  date = {2014-07},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {25},
  number = {7},
  pages = {1229--1262},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2014.2317880},
  abstract = {Stability problems of continuous-time recurrent neural networks have been extensively studied, and many papers have been published in the literature. The purpose of this paper is to provide a comprehensive review of the research on stability of continuous-time recurrent neural networks, including Hopfield neural networks, Cohen-Grossberg neural networks, and related models. Since time delay is inevitable in practice, stability results of recurrent neural networks with different classes of time delays are reviewed in detail. For the case of delay-dependent stability, the results on how to deal with the constant/variable delay in recurrent neural networks are summarized. The relationship among stability results in different forms, such as algebraic inequality forms, M-matrix forms, linear matrix inequality forms, and Lyapunov diagonal stability forms, is discussed and compared. Some necessary and sufficient stability conditions for recurrent neural networks without time delays are also discussed. Concluding remarks and future directions of stability analysis of recurrent neural networks are given.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}} and {{Learning Systems}}},
  keywords = {\textbackslash (M\textbackslash ) -matrix,Biological neural networks,Cohen-Grossberg neural networks,Cohen–Grossberg neural networks,Delays,discrete delay,distributed delays,Hopfield neural networks,linear matrix inequality (LMI),Lyapunov diagonal stability (LDS),M-matrix,Neurons,recurrent neural networks,Recurrent neural networks,robust stability,stability,Stability criteria,stability.},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\LWMFYGAB\\Zhang et al. - 2014 - A Comprehensive Review of Stability Analysis of Co.pdf}
}

@article{zohdy1997,
  title = {A Recurrent Dynamic Neural Network for Noisy Signal Representation},
  author = {Zohdy, Mohamed A. and Karam, Marc and Abdel-Aty Zohdy, Hoda S.},
  date = {1997-10},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {17},
  number = {2},
  pages = {77--97},
  issn = {09252312},
  doi = {10.1016/S0925-2312(97)00043-X},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S092523129700043X},
  urldate = {2023-05-09},
  abstract = {This paper presents a new recurrent dynamic neural network approach to solve noisy signal representation and processing problems. Essentially, the neural network solves, in a systematic way, for the sets of representation coefficients required to model a given signal in terms of basis elementary signals. The network converges by seeking the minimum energy states. The perceived advantages over traditional approaches are in robustness of computation and ability to handle time-varying noisy signals.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Zohdy-1997-A recurrent dynamic neural network for noisy signal representation.pdf}
}

@article{zotero-1744,
  title = {Geometric Singular Perturbation Theory},
  langid = {english},
  file = {C\:\\Users\\abel_\\Documents\\Rotations\\CIT\\Papers\\Jones-Geometric Singular Perturbation Theory.pdf}
}
