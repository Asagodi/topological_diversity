\documentclass{article}
%\usepackage{arxiv}
\usepackage[numbers,sort&compress,super,comma]{natbib} % this must be before neurips_2024
\usepackage[preprint]{neurips_2024}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amssymb, amsmath, amsthm, amsfonts}
\usepackage{thmtools, mathtools, mathrsfs, dsfont}
\usepackage{bbm}
\usepackage{forloop}
\usepackage[pdftex]{graphicx}  %remove demo option in your document
\usepackage{sidecap}
\PassOptionsToPackage{dvipsnames}{xcolor}
\usepackage{xcolor}
\definecolor{ForestGreen}{rgb}{0.13, 0.55, 0.13}
\definecolor{MidnightBlue}{rgb}{0.1, 0.1, 0.44}
\definecolor{BurntOrange}{rgb}{0.8, 0.33, 0.0}
\definecolor{Plum}{rgb}{0.56, 0.27, 0.52}
\usepackage[colorlinks=true,linkcolor=MidnightBlue,citecolor=ForestGreen,filecolor=TealBlue,urlcolor=Plum]{hyperref}
\hypersetup{breaklinks=true}

\newcommand{\ptitle}[1]{\textbf{#1:}\xspace}
\definecolor{mpcolor}{rgb}{1, 0.1, 0.59}
\definecolor{ascolor}{rgb}{1, 0.5, 0.}
\newcommand{\mpcomment}[1]{\textcolor{mpcolor}{(#1)}}
\newcommand{\ascomment}[1]{\textcolor{ascolor}{(#1)}}

\graphicspath{{figures}}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}[theorem]
\theoremstyle{definition} \newtheorem{definition}{Definition}
\theoremstyle{remark} \newtheorem{remark}{Remark}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\manifold}{\mathcal{M}}
\newcommand{\cl}{\operatorname{cl}}
\newcommand{\relu}{\operatorname{ReLU}}
\newcommand{\vol}{\operatorname{vol}}
\newcommand{\boa}{\operatorname{BoA}}
\newcommand{\T}{\operatorname{T}}
\newcommand{\Hpert}{H^{\text{pert}}}
\newcommand{\inv}{\operatorname{Inv}}
\newcommand{\spec}{\operatorname{spec}}
\newcommand{\sign}{\operatorname{sign}}

\newcommand{\defvec}[1]{\expandafter\newcommand\csname v#1\endcsname{{\mathbf{#1}}}}
\newcommand{\dm}[1]{\ensuremath{\mathrm{d}{#1}}} % dx dy dz dmu
\newcounter{ct}
\forLoop{1}{26}{ct}{
    \edef\letter{\alph{ct}}
    \expandafter\defvec\letter
}
% captial \vA
\forLoop{1}{26}{ct}{
    \edef\letter{\Alph{ct}}
    \expandafter\defvec\letter
}

\title{Maintaining and learning continuous attractors: Plasticity rules for slow invariant manifolds} %maintaining and learning? if the inv man optimization works out.
\author{\'Abel S\'agodi and Il Memming Park}
\date{\today}



\begin{document}
\maketitle

\section*{Abstract}
Continuous attractors enable stable representation of continuous variables in neural networks but are notoriously fragile due to the fine-tuning problem: small perturbations in synaptic weights, such as those induced by synaptic noise, can disrupt their structure and degrade function.
We propose a synaptic plasticity rule that counteracts these perturbations by dynamically adjusting synaptic weights based on neural activity.
Through analytical and computational studies, we demonstrate that this rule preserves attractor continuity while preventing drift and degradation over time.




\section{Introduction}


Intrinsic dynamics should self-organize and self-sustain in the face of fluctuating inputs and biological noise, including synaptic turnover and cell death.

It is well-known that continuous attractors are fragile. 
Recently, it was established that continuous attractors are functionally robust \citep{Sagodi2024a}.
Nevertheless, their function as continuous attractors degrades as a function of the distance from one \citep{Sagodi2024a}.



\paragraph{Plasticity for learning}
learning is shaped by abrupt changes in neural engagement \citep{hennig2021learning}

CA1 \citep{bittner2017behavioral} 

\paragraph{Plasticity for WM}
\citep{kilpatrick2018wm}

\paragraph{Timescales}
the instantaneous value of the synaptic efficacy can reflect the history of pre-synaptic activation over tens of seconds (i.e., the time scale of augmentation) or even minutes (i.e., the time scale of potentiation) rather than just seconds (i.e., the time scale of facilitation)  \citep{thomson2000facilitation}.
timescales of learning\citep{miller2024timescales}
rapid \citep{li2023rapid} \citep{dan2024neural} 
short term \citep{fioravante2011short}
\citep{brennan2023attractor}



such a mechanism with a slow learning rate is ideal for extracting the statistical regularities of the environment \citep{panichello2019error}

\paragraph{Attractors for neural computation}
multistability in attractors \citep{braun2010attractors}
Attractor dynamics with activity-dependent plasticity capture human working memory across time scales \citep{brennan2023attractor}

attractor dynamics for decision making \citep{luo2023noncanonical}

persistent activity \citep{curtis2010beyond} \citep{bray2017persistence}
\citep{constantinidis2018persistent} persistent spiking activity underlies working memory

Computing with dynamic attractors in neural networks \citep{hirsch1995computing}

\paragraph{Noisy brain}
\citep{cheung2010noisybrain}
\citep{rolls2010noisybrain}
benefits\citep{mcdonnell2011benefits}
development of a noisy brain \citep{mcintosh2010development}


\subsection{Continuous attractors}
Express the system as 
\begin{equation}
\dot x = f(x,W),
\end{equation} where $x$ is the neural activity, and $W$ represents synaptic weights.

Functions: navigation/head-direction\citep{wilson2023navigation, skaggs1994model}, evidence integration\citep{mante2013context, esnaola2022flexible}

To maintain a continuous attractor, synaptic plasticity should preserve the network’s structured connectivity while allowing for slow adaptation.
\citep{cannon1983oculomotor}
\citep{samsonovich1997pathintegration}

connectome \citep{kakaria2017ra}

\paragraph{HD}
\citep{skaggs1994model, redish1996coupled, boucheny2005continuous}
\citep{stentiford2022spiking}
\citep{barak2021mapping}

\paragraph{Continuous attractors are fragile}
Describe the fine-tuning problem.
%\begin{figure}[tbhp]
%     \centering
%    \includegraphics[width=\textwidth]{}
%       \caption{
%       }\label{fig:}
%\end{figure}
bifurcation \citep{dercole2011dynamical}
deformation \citep{goodridge2000modeling}

\paragraph{Functional robustness}

Fenichel

NIHM

\citep{li2013normally}

Distance to Bifurcation \citep{dobson2003sensitivity}


\paragraph{Maintenance of dynamics}


\paragraph{Maintenance of memory}
Error-correcting dynamics in visual working memory (with discrete attractors) \citep{panichello2019error} (S-type noise)
Several neural mechanisms might account for the increase in attractor strength with load, including increased drive into the network\citep{wang2018flexible} or changes in f-I gain via neuromodulation\citep{servan1990network}.





\paragraph{Learning}
inhibitory mechanism \citep{couey2013recurrent}
Learning accurate path integration in ring attractor models of the head direction system \citep{vafidis2022hd}

\subsection{Biology of plasticity}

Theory has been developed to describe memory storage of an analog stimulus (such as spatial location or eye position), in terms of continuous ‘bump attractors' and ‘line attractors'.
The stability of a working memory network is recognized as a serious problem; stability can be achieved if reverberation is largely mediated by NMDA receptors at recurrent synapses \citep{wang2001synaptic}.

spatial working memory\citep{compte2000synaptic}

flexibility \citep{remington2018flexible}

Desiderata for normative models of synaptic plasticity \citep{bredenberg2024desiderata}

\subsubsection{Homeostatic} \citep{turrigiano2004homeostatic}

Homeostatic scaling of all synapses of a neuron, which is obtained by adjusting the number of postsynaptic receptors proportionally to the neuron’s average firing activity \citep{turrigiano1998activity,turrigiano1999homeostatic},
 could maintain neural activity within a dynamic range and, more generally, stabilize neural circuit function despite the positive feedback of Hebbian plasticity \citep{toyoizumi2014modeling}. 
 
 \citep{niemeyer2021homeostasis}
 \citep{oleary2018homeostasis}
 
 \paragraph{Stability}
 \citep{aljaberi2021global}
 
 \paragraph{Excitatory-inhibitory balance}
 
 excitability \citep{daoudal2003long}
 
 control of inhibition \citep{machens2005flexible}

 Dynamic tuning of neural stability for cognitive control \citep{xu2024dynamic}
 
 A dynamically stabilized recurrent neural network \citep{saab2022stabilized}
 
\paragraph{Hebbian vs. homeostatic synaptic plasticity}
\citep{pozo2010unraveling}
Hebbian forms of plasticity, such as LTP, rapidly modify the efficacy of individual synapses associatively in an input-specific manner, and they are thought to represent the cellular mechanisms for storing memories

How might neurons discriminate between non-uniform homeostatic adaptations and input-specific changes when these two opposing forms of plasticity are expressed in overlapping domains? \citep{rabinowitch2006interplay, rabinowitch2008two}
 
 
\subsubsection{Additional}
\paragraph{Astrocytes} 
astro\citep{depitta2016astrocytes}
\citep{koshkin2024astrocyte}

\paragraph{Balancing Hebbian with homeostatic}
\citep{zenke2017temporal}

\paragraph{Self-organizing}
\citep{federer2018self}
\citep{kohonen2012self, kohonen2013essentials}

%%%%%%%%
\subsection{Plasticity math/theory}
\citep{gerstner2002mathematical}
\citep{berner2023adaptive}
\citep{clark2024theory}
\citep{tyulmankov2024computational}

bio plausible learning\citep{miconi2017biologically}

\subsubsection{Hebbian plasticity}
\citep{caporale2008spike}
\begin{equation}
\dot W_{ij} = F(x_i, x_j, W_{ij})
\end{equation}

oscillatory\citep{winder2009oscillatory}

\subsection{Learning plasticity rules} %inference of 
\citep{pereira2018attractor}

\citep{shervani2023meta}
\citep{bell2024discovering}
\citep{jordan2021plasticity}

Identifying learning rules from neural network observables \citep{nayebi2020learning} (waterfall illusion?)
\citep{kepple2022curriculum}
\citep{mcmahan2021learning}
\citep{ashwood2020inferring}

\citep{tyulmankov2022meta}

\citep{kusmierz2017learning}

learning-to-learn \citep{bellec2018long}

\subsection{Stability of plasticity overview}
\citep{kuan1994convergence}

\subsection{Robustness overview}
\citep{revay2023recurrent, revay2020contracting}

\subsection{Implications}

\subsubsection{Heterogeneity in tuning curves}




\subsubsection{Representational drift}
\citep{wolff2020drifting}

\citep{masset2022drifting}

\citep{buschman2022wm}

Representational drift as a window into neural and behavioural plasticity \citep{micou2023representational}

Understanding representational drift in olfaction with dynamical  systems \citep{barwich2023drift}

continual learning \citep{driscoll2022representational}

Experiment: tuning curves over time.




\subsubsection{Dynamic coding}
Dynamic coding \citep{stroud2024computational}

Experiment: plasticity for input weights





\section{Plasticity/Homeostasis/Adaptation}
%%%%%%%%%%%%%
\citep{martin2000plasticity}
\citep{takeuchi2014plasticity}

\cite{bell2024discovering} \citep{shervani2023meta}
\citep{chadwick2023learning}

signatures of task learning \citep{gurnani2023task}

\subsection{Problem setting for maintaining a continuous attractor}

Consider 
\begin{equation}\label{eq:ode}
\tau_x\dot x = f(x) 
\end{equation}
implementing a normally hyperbolic continuous attractor, i.e., there is $\manifold_0\coloneqq \{x \in \reals^N | f(x) = 0 \text{ and } \spec_{i>1}(J(x)) <0\}$  (all eigenvalues except for one of the Jacobian at are negative). 

Now imagine that this system has been affected so that the system has an approximate continuous attractor
\begin{equation}\label{eq:ode_pert}
\tau_x\dot x = f(x)  + \epsilon p(x).
\end{equation}

Then the flow on the perturbed persistent invariant manifold $\manifold_\epsilon$ is
\begin{equation}\label{eq:ode_pert}
\tau_x\dot x = \epsilon p'(x).
\end{equation}
%how are p and p' related?
We can say that they are close to each other, related to $\mathcal{O}(\epsilon)$, and the tangent/derivative of the of f(x) on $\manifold_\epsilon$ being $C^1$ close the one on $\manifold_0$.
%how relevant is the difference between invariant sets ?




\subsection{Learning rule to minimize the speed}%Omri Barak's suggesiton
On the continuous attractor we want to minimize the change in the neural state (in the absence of input).

Equivalence of Hopfield and Wilson-Cowan\citep{miller2012mathematical} for invertible $W$.
For a Hopfield type network\footnote{Wilson-Cowan type and Hopfield type (as used in e.g. \citep{gort2024emergence}) and referred to as $v$-equation and $r$-equation in \citep{miller2012mathematical}.}, i.e., with dynamics 
\begin{equation}\label{sec:hopfieldode}
\dot x = W\phi(x) + b
\end{equation}
 the gradient of the speed is
%use factor 1/2?
\begin{equation}
\frac{\partial}{\partial W_{ij}}\|\dot x \|^2 = 2 \dot x^T \frac{\partial}{\partial W_{ij}}\dot x = 2 \dot x_i\phi(x_j).
\end{equation}

%\begin{equation}
%\frac{\partial}{\partial W_{ij}} \|\dot{x}\| = \frac{2 \dot{x}_i \cdot \text{ReLU}(x_j)}{\|\dot{x}\|}
%\end{equation}


\paragraph{General activation function}
For a Wilson-Cowan type network, i.e., with dynamics 
\begin{equation}
\dot x = \phi(Wx+b) 
\end{equation}
 the gradient of the speed is
\begin{equation}
\frac{\partial}{\partial W_{ij}} \|\dot{x}\|^2 = 2 \dot{x}^T \cdot \phi'(Wx+b) \cdot x_j.
\end{equation}



%consider bias pert? 
\subsection{Noorman ring attractor}
Let's analyze $f(x,W_0,t) = -x(t) + W_0\relu(x(t)) + b$ with $W_0\in\reals^{n\times n}$ and $b\in\reals^n$ so that the system implements a ring attractor \citep{noorman2024accurate}.

We consider the case where the connectivity has been affected: $W(t=0) \leftarrow W_0 + \epsilon V$.
In this case we simply have $g(x,W,t) = (W(t)-W_0)\relu(x(t))$.

$g_i(x,W,t) = \sum_{j}^N(W_{ij}(t)-W_{0,ij})\relu(x_j(t))$.


\paragraph{Easy form of perturbation}
If $p(x) = (V-W)\relu(x(t))$


Define the plasticity rule
\begin{equation}\label{eq:plasticity_heur}
\tau_W\dot W_{ij}(t) = - \dot x_i \phi_j(x) 
\end{equation}
so that compactly
\begin{equation}\label{eq:plasticity_heur_comp}
\tau_W\dot W(t) = - \dot x \otimes \phi(x) 
\end{equation}

Fast Hebbian plasticity \citep{lansner2023fast}

If the angular velocity $v_in(t)$ is nonzero we have that 
\begin{equation}
\dot x = W^{sym}\phi(x) + v(t)W^{asym} + \epsilon p(x)
\end{equation}
hence we can use
\begin{equation}\label{eq:plasticity_heur_comp_vin}
\tau_W\dot W(t) = - (\dot x - v(t)W^{asym}) \otimes \phi(x) 
\end{equation}

\subsubsection{Equilibria}
At equilibrium, we have:
\begin{equation}
    \tau_W \dot{W} = - \dot{x} \otimes \operatorname{ReLU}(x) = 0.
\end{equation}
Since \( \operatorname{ReLU}(x) \) is nonnegative, this implies that
\begin{equation}
    \dot{x} = 0
\end{equation}
at equilibrium, which is consistent with the previous condition.


\subsubsection{Stability with Jacobian}
\citep{wersing2001dynamical}
\paragraph{Linearizing $\dot x$}
Taking the first-order Taylor expansion around \( (x^*, W^*) \):

\begin{equation}
    \tau_x \delta \dot{x} = -\delta x + W D_{\operatorname{ReLU}} \delta x + \delta W \operatorname{ReLU}(x^*),
\end{equation}
where \( D_{\operatorname{ReLU}} \) is a diagonal matrix with entries:

\begin{equation}
    (D_{\operatorname{ReLU}})_{ii} =
    \begin{cases}
        1, & x^*_i > 0, \\
        0, & x^*_i \leq 0.
    \end{cases}
\end{equation}

Thus, the linearized equation for \( \dot{x} \) is:

\begin{equation}
    \tau_x \delta \dot{x} = (-I + W D_{\operatorname{ReLU}}) \delta x + \delta W \operatorname{ReLU}(x^*).
\end{equation}


\paragraph{Linearizing $\dot W$}

\begin{equation}
    \tau_W \dot{W} = - \dot{x} \otimes \operatorname{ReLU}(x).
\end{equation}

Substituting \( \delta \dot{x} \) from the previous step:

\begin{equation}
    \tau_W \delta \dot{W} = -\left[ (-I + W D_{\operatorname{ReLU}}) \delta x + \delta W \operatorname{ReLU}(x^*) \right] \otimes \operatorname{ReLU}(x^*).
\end{equation}

Rearranging:

\begin{equation}
    \tau_W \delta \dot{W} = - (-I + W D_{\operatorname{ReLU}}) \delta x \otimes \operatorname{ReLU}(x^*) 
    - \delta W \operatorname{ReLU}(x^*) \otimes \operatorname{ReLU}(x^*).
\end{equation}


\paragraph{Joint Jacobian}
\begin{equation}
\begin{bmatrix}
    \tau_x \delta \dot{x} \\
    \tau_W \delta \dot{W}
\end{bmatrix}
=
\begin{bmatrix}
    (-I + W D_{\operatorname{ReLU}}) & \operatorname{ReLU}(x^*) \\
    -(-I + W D_{\operatorname{ReLU}}) \otimes \operatorname{ReLU}(x^*) & -\operatorname{ReLU}(x^*) \otimes \operatorname{ReLU}(x^*)
\end{bmatrix}
\begin{bmatrix}
    \delta x \\
    \delta W
\end{bmatrix}.
\end{equation}

Dividing by \( \tau_x \) and \( \tau_W \) for each row respectively, we obtain the Jacobian:

\begin{equation}
J =
\begin{bmatrix}
    \frac{1}{\tau_x} (-I + W D_{\operatorname{ReLU}}) & \frac{1}{\tau_x} \operatorname{ReLU}(x^*) \\
    -\frac{1}{\tau_W} ((-I + W D_{\operatorname{ReLU}}) \otimes \operatorname{ReLU}(x^*)) & -\frac{1}{\tau_W} (\operatorname{ReLU}(x^*) \otimes \operatorname{ReLU}(x^*))
\end{bmatrix}.
\end{equation}

\subsubsection{Stability with Lyapunov}
The Lyapunov function should now reflect deviations from both manifolds. A candidate function could be:
\[
V(x, W) = \frac{1}{2} \| \delta x \|^2 + \frac{1}{2} \| \delta W \|^2,
\]
where 
\[
\delta x = x - x^*(W) \quad \text{and} \quad \delta W = W - W^*(x)
\]

are the deviations from the equilibrium manifolds of \(x\) and \(W\), respectively.

Since both \(x\) and \(W\) have manifolds of fixed points, this function ensures that deviations from the equilibrium in both \(x\) and \(W\) are captured.

The time derivative of the Lyapunov function is:
\[
\dot{V} = \langle \delta x, \dot{x} \rangle + \langle \delta W, \dot{W} \rangle.
\]

We then compute \(\dot{V}\) as follows:
\[
\dot{V} = \left\langle \delta x, \frac{1}{\tau_x} \left( -x + W \operatorname{ReLU}(x) + b \right) \right\rangle 
+ \left\langle \delta W, -\frac{1}{\tau_W} \dot{x} \otimes \operatorname{ReLU}(x) \right\rangle.
\]

\subsubsection{Stability with contractivity analysis}
%weight decay necessary for contractivity ananlysis?

Plasticity analysis through contractivity \citep{kozachkov2022matrix} \citep{centorrino2024modeling}.

\paragraph{Transverse contractivity}
Skew-Symmetric Feedback Interconnection \citep{manchester2014transverse}
Bounded Feedback Interconnections \citep{wang2005contraction} and \citep{manchester2014transverse}





\subsubsection{Numerical experiments}
\begin{equation}
dW_{ij,t} = \mu(X_t, W_{ij,t}, t)dt + \sigma(X_t, W_{ij,t}, t)dB_t
\end{equation}
 where $B$ denotes a Wiener process and $ \mu(X_t, W_{ij,t}, t) = -\dot x_t \otimes x_t$

Euler-Maruyama method
\[
W_{t+\Delta t} = W_t -( \dot x_t \otimes x_t) \Delta t + \sigma \sqrt{\Delta t} \, \xi
\]
where \(\xi \sim \mathcal{N}(0, \sigma_W\mathbb{I})\) is an i.i.d. Gaussian noise matrix:
\begin{equation}
\xi \sim \mathcal{N}(0,\sigma_W\mathbb{I}), \ \sigma_W=\tfrac{1}{100}.
\end{equation}


%\subsection{Control through input (bias-based optimization of slowness on manifold)}
\subsection{Minimizing globally across the invariant manifold}
Gradient view of plasticity \citep{richards2023plasticity}

Total velocity along original invariant set:
\begin{equation}
v_{\manifold_0}   = \int_{x\in\manifold_0}  \|\epsilon p(x)\|dx.
\end{equation}

Total velocity along perturbed invariant set:
\begin{equation}
v_{\manifold_\epsilon}  =\int_{x\in\manifold_\epsilon} \|f(x) + \epsilon p(x)\|dx.
\end{equation}


Let's analyze $f(x,W_0,t) = -x(t) + W_0\relu(x(t)) + b$ with $W_0\in\reals^{n\times n}$ and $b\in\reals^n$ so that the system implements a ring attractor \citep{noorman2024accurate}.


How can we change the inputs to the neurons so that the invariant manifold is slowest, i.e., velocity is minimized?

We calculate the gradient w.r.t. a new bias term $c$ that we add linearly as an input to each neuron:
\begin{equation}
\nabla_{c}v_{\manifold_0} = \frac{\partial}{\partial c}\int_{x\in\manifold_0}  \|\epsilon p(x) + c\|dx = \int_{x\in\manifold_0}  \frac{\epsilon p(x) + c}{\|\epsilon p(x) + c\|}dx.
\end{equation}

We calculate the gradient w.r.t. a weight change $W_{ij}$ on the original invariant manifold:
\begin{equation}
\nabla_{W_{ij}}v_{\manifold_0} = \frac{\partial}{\partial W_{ij}}\int_{x\in\manifold_0}  \|f(x) + \epsilon p(x)\|dx = \int_{x\in\manifold_0} 
\frac{(W \text{ReLU}(x) + \epsilon p(x))_i}{\| W \text{ReLU}(x) + \epsilon p(x) \|} \text{ReLU}(x)_j
dx.
\end{equation}



%can we calculate that for manifold_epsilon?
With the new invariant manifold:
\begin{equation}
\nabla_{W_{ij}}v_{\manifold_\epsilon} = \frac{\partial}{\partial W_{ij}}\int_{x\in\manifold_{\epsilon}}  \|f(x) + \epsilon p(x)\|dx = \int_{x\in\manifold_{\epsilon}}  \relu(x_j)\frac{f(x) + \epsilon p(x)}{\|f(x) +\epsilon p(x) \|}dx.
\end{equation}


\subsubsection{Assumptions to make this global rule work}

 
\paragraph{Repair over longer time scale}
 
\paragraph{Navigating the manifold sufficiently (through spontaneous diffusion or control)}


\subsection{Homeostatic synaptic scaling}
Similar (?) to \citep{renart2003robust}

\ascomment{$\theta$ (orientation) dependence? Can be ignored when averaging over long timescales?}
But maybe it's enough to just globally set mean activation targets?
This is only supposed to help with pushing the network away from local minima that are not an actually good slow invariant manifold.



Assume we are at a fixed point of Eq.~\ref{sec:hopfieldode}, i.e., $x^\star = \phi^{-1}(-W^{-1}b)$. %$\phi(x^\star) = -W^{-1}b$.
%better to use target for \phi(x^\star)?
If we want the system to have a stationary firing pattern $x_{\operatorname{target}}$, we can adjust the weights according to the gradient of the following loss function:
\begin{equation}\label{eq:synscaling_gradient}
\frac{\partial}{\partial W_{ij}} \|x^\star - x_{\operatorname{target}}\|^2 = 2(x^\star - x_{\operatorname{target}})^T\frac{\partial}{\partial W_{ij}} \phi^{-1}(-W^{-1}b)
\end{equation}

\[
 \frac{\partial}{\partial W_{ij}} \phi^{-1}(-W^{-1} b) =   \left( - \frac{\partial}{\partial W_{ij}} \left( \phi^{-1}(-W^{-1} b) \right) \right)
\]

Using the chain rule:

\[
=  \left( \phi'^{-1}(-W^{-1} b) \cdot \frac{\partial}{\partial W_{ij}} \left( -W^{-1} b \right) \right)
\]


The derivative of \( -W^{-1} b \) with respect to \( W_{ij} \) is:
\[
\frac{\partial}{\partial W_{ij}} \left( -W^{-1} b \right) = W^{-1} \left( e_i e_j^T \right) W^{-1}
\]

\paragraph{target for $\phi(x^\star)$}
\[\frac{\partial}{\partial W_{ij}} \|\phi(x^\star) - r_{\operatorname{target}}\|^2 = -2(\phi(x^\star) - r_{\operatorname{target}})^T\frac{\partial}{\partial W_{ij}}W^{-1}b = 2(\phi(x^\star) - r_{\operatorname{target}})^TW^{-1} E_{ij} W^{-1} \]
where
\[
\frac{\partial}{\partial W_{ij}} \left( W^{-1} \right) = - W^{-1} E_{ij} W^{-1}.
\]
where \( E_{ij} \) is the matrix with 1 in the \( (i, j) \)-th position and 0 elsewhere.


\paragraph{Adjusting the bias}
\[
\frac{\partial}{\partial b_i} \left( W^{-1} b \right) = \left( W^{-1} \right)_{\cdot i}
\]

\[\frac{\partial}{\partial b_{i}} \|\phi(x^\star) - r_{\operatorname{target}}\|^2 = -2(\phi(x^\star) - r_{\operatorname{target}})^T\frac{\partial}{\partial b_{i}}W^{-1}b = -2(\phi(x^\star) - r_{\operatorname{target}})^T \left( W^{-1} \right)_{\cdot i}\]


\paragraph{From integration}
\[
x(t) = \int_0^Tdt \dot x(t) = \int_0^Tdt W\phi(x(t)) + b
\]

\paragraph{From stationary distribution}
Make the stationary distribution have a peak that is consistent with rate target.
This seems to not be a sufficient constraint. 
Or not well-defined problem.


To find the stationary distribution for a stochastic differential equation, we usually rely on the Fokker-Planck equation, which describes the evolution of the probability density \( p(x,t) \) of the system. For an equation of the form:

\[
\dot{x} = f(x) + \eta(t),
\]

where \( f(x) \) represents the deterministic part and \( \eta(t) \) represents the noise, the Fokker-Planck equation for the stationary distribution \( p(x) \) is:

\[
0 = -\nabla \cdot ( f(x) p(x) ) + \frac{1}{2} \sigma^2 \nabla^2 p(x),
\]

where \( \sigma^2 \) is the variance of the noise (assuming the noise has zero mean and white noise characteristics).




\section{Optimization of the location of invariant manifolds} %or double optimization for location of NHIM and parameters
\citep{agrachev2009controllability,agrachev2022control}

RA: Diffeomorphism between $S^1$  and embedding

\citep{ocko2018emergent}

\subsection{Manifold learning}
\citep{huys2014functional}
Relevant \citep{chang2023novo}

Control view \citep{pisarchik2014control} of multistability
Controlling brain dynamics: Landscape and transition path for working memory \citep{ye2023wm}
\citep{badre2021dimensionality}
\citep{altan2023control}

Optimization view \citep{hennig2021learningoptimization}

\citep{kumar2023bundle}

probing learning \citep{marschall2023probing}

Perturbing low dimensional activity manifolds \citep{warnberg2019perturbing}

Existence of slow-fast dynamics in random networks \citep{schmalfuss2008invariant, shaham2017slow}
NHIM in random \citep{li2013normally}

? \citep{federer2018self}

invariant manfiolds \citep{guckenheimer2015invariant}

\subsubsection{How to account for change in invariant manifold?}
How does the invariant manifold $\manifold_{\epsilon,c}$ change as a function of $c$?

\begin{equation}
\dot x = g(x,c) = f(x) + \epsilon p(x) + c
\end{equation}

We have that 
\begin{equation}
\frac{d}{dt} \phi_c(y) =g(\phi_c(y), c) \quad \text{for all points} \quad x = \phi_c(y) \text{ on the manifold}.
\end{equation}

Total derivative
\begin{equation}
\frac{d}{dc} \frac{d}{dt} \phi_c(y) = D_x g(\phi_c(y), c) \frac{\partial}{\partial c} \phi_c(y) + \frac{\partial g}{\partial c}.
\end{equation}


For the manifold $\manifold_{\epsilon,c}$ remain invariant, the deformation of $\phi_c(y)$ must still satisfy the dynamics, meaning:
\begin{equation}
D_x g(\phi_c(y), c) \frac{\partial \phi_c(y)}{\partial c} + \frac{\partial g}{\partial c} = 0.
\end{equation}

If $D_x g$ is invertible, we can solve for $\frac{\partial \phi_c(y)}{\partial c}$, giving:
\begin{equation}
\frac{\partial \phi_c(y)}{\partial c} = - (D_x g)^{-1} \frac{\partial g}{\partial c}.
\end{equation}


\subsubsection{Weights}
For $\dot x= f(x) =W\phi(x)$ and invariant manifold $x=\psi_{W_{ij}}(y)$

We have that 
\begin{equation}
\frac{d}{dt} \psi(y) =f(\psi(y)) \quad \text{for all points} \quad x = \psi(y) \text{ on the manifold}.
\end{equation}

\begin{equation}
\frac{\partial}{\partial W_{ij}} \left( W \phi(\psi_{W_{ij}}(y)) \right) = \phi(\psi_{W_{ij}}(y)) e_{ij} + W \frac{d}{dW_{ij}} \phi(\psi_{W_{ij}}(y)),
\end{equation}

\begin{equation}
W \frac{\partial}{\partial W_{ij}}  \phi(\psi_{W_{ij}}(y)) = - \phi(\psi_{W_{ij}}(y)) e_{ij}.
\end{equation}


\begin{equation}
\frac{\partial}{\partial W_{ij}}  \phi(\psi_{W_{ij}}(y)) = \phi'(\psi_{W_{ij}}(y))\frac{d}{dW_{ij}}\psi_{W_{ij}}(y)
\end{equation}

We get 
\begin{equation}
\frac{\partial}{\partial W_{ij}} \psi_{W_{ij}}(y) = - \left(\phi'(\psi_{W_{ij}}(y))\right)^{-1} W^{-1} \phi(\psi_{W_{ij}}(y)) e_{ij}.
\end{equation}



\paragraph{Target location of invariant manifold}
$x = \psi_{target}(y)$

Metric
- Hausdorff (non-differentiable)
-- issue: Hausdorff distance is not differentiable, as it involves taking the supremum over pointwise distances, making it non-smooth at changes where the supremum switches between different points.


- Pointwise distance (irrelevant invariance)

- The soft nearest-neighbor distance:
\[
d(M_1, M_2) = \int_{M_1} \frac{\sum\limits_{y \in M_2} \|x - y\|^2 e^{-\alpha \|x - y\|^2}}{\sum\limits_{y \in M_2} e^{-\alpha \|x - y\|^2}} \, d\mu(x).
\]
The gradient is given by:
\[
\nabla_x f(x) = 2 \sum_{y \in M_2} (x - y) w(y),
\]
where the soft weighting function is:
\[
w(y) = \frac{e^{-\alpha \|x - y\|^2}}{\sum\limits_{y' \in M_2} e^{-\alpha \|x - y'\|^2}}.
\]


- The Optimal Transport Distance, also known as the Wasserstein Distance, is defined as follows.
If one defines probability measures \( \mu_1 \) and \( \mu_2 \) over the manifolds, then the Wasserstein distance is given by:
\[
W_p(\mu_1, \mu_2) = \inf_{\gamma \in \Pi(\mu_1, \mu_2)} \left( \int d(x, y)^p \, d\gamma(x, y) \right)^{\frac{1}{p}}.
\]





\section{Discussion}




\newpage
%\bibliographystyle{plain}
\bibliographystyle{unsrtnat_IMP_v1}
\bibliography{../../all_ref.bib, ../../catniplab.bib}


\newpage
\appendix

\section{Types of plasticity}
\subsection{Hebbian/Anti-Hebbian Plasticity}
\subsection{Dong \& Hopfield Synaptic Plasticity}
\subsection{Presynaptic Plasticity}
\citep{mongillo2008synaptic}
\citep{masse2019circuit}

\subsection{Synaptic scaling}
\citep{renart2003robust}

\subsection{Covariance Rule}
\citep{amit1989modeling}
Consider a covariance-based learning rule \citep{sejnowski1989hebb, gerstner2002mathematical}, with a leak term  
\begin{equation}
    \dot{W} = \nu \left( \phi(x) - \langle \phi \rangle \right) \left( \phi(x) - \langle \phi \rangle \right)^T - \gamma W
\end{equation}
where $\nu \geq 0$ is a learning rate and $\langle \phi \rangle$ denotes a running average over the firing rate  
\begin{equation}
    \langle \phi \rangle_i = \int_{t-\Delta}^{t} \phi(x(s))_i \, ds
\end{equation}
with some window size $\Delta > 0$.

\subsection{The Bayesian confidence propagation learning rule (BCPNN)}
See e.g. \citep{lansner2023fast}

\subsection{Metaplasticity}
 - Bienenstock-Cooper-Munro (BCM) model \cite{bienenstock1982theory}. The core component of this synaptic update rule is its non-monotonic, time-varying dependence on the postsynaptic activity:  
\begin{equation}
    \Delta w_i = \eta_w y (y - \theta) x_i
\end{equation}
where $\theta$ is a variable threshold. If the postsynaptic activity $y$ is below this threshold, the synapse will be depressed; above, it will be potentiated. Critically, the threshold itself depends on the postsynaptic neuron’s activity history, for example, a low-pass filtered version of it \cite[Ch.~8.2]{dayan2005theoretical}:  
\begin{equation}
    \Delta \theta = \eta_\theta (y^2 - \theta)
\end{equation}



\end{document}