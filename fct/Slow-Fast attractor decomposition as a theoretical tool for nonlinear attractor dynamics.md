### Introduction
Mathematically simplifying how the complex network of neurons processes information over time is a major issue in computational and theoretical neuroscience. There have been several successful approaches: (1) dimensionality reduction methods have shown that the neural dynamics can be captured on a lower-dimensional manifold, indicating general attractor dynamics, (2) linear dynamical systems have been widely used to approximate behaviors as a small set of recurrent features combined with an effectively feed-forward component, (3) fixed points and continuous attractors have been identified as the mechanisms for discrete and continuous-valued short-term memory, respectively. Each of these approaches craves out a particular aspect that can be simplified.

There are two sources of variability that could introduce a devastating change in the function of neural system. The first kind of noise (S-type noise) is in the neural activity (membrane potentials, action potentials, firing rates, and neuromodulations) that can push the internal states away from the optimal trajectory. Attractor dynamics defend against this type of noise, since small perturbations off the attractor manifold is mostly undone thanks to the attraction, except in the case of non-point attractors, where it can generate diffusion along the direction of the manifold. The second kind of variability (D-type noise) is of the parameters of the neural system (synaptic strength, connectivity, and membrane properties) where their change is typically long-lasting if not permanent, and can modify the geometry and topological structure of attractor dynamics. An important of the sources for the D-type noise is noise coming from the learning process where changes to the synaptic weights are driven by stimulus, internal states, and rewards all of which can be quite noisy.

Despite these potentially devastating changes, the ability of the neural system in generating meaningful behavior is remarkably resistant. This is in contrast to some of the theoretical brittleness of some of the presumed underlying mechanisms. For example, continuous attractors are known to be (structurally) unstable in the presence of D-type noise, making them practically non-existent in biological systems. If that's the case, in what sense can we say that the continuous attractor is a good approximation of the behavior of the network?

At the same time, not all relevant attractors are fixed points or continuous manifolds with associated persistent activity. We have shown that stable limit cycles that underlie nonlinear periodic oscillations also should be considered, as far as short-term memory and temporal learning is concerned (Park, Sagodi, Sokol 2023).
###