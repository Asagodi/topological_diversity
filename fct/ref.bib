@article{sussillo2014,
  title={Neural circuits as computational dynamical systems},
  author={Sussillo, David},
  journal={Current opinion in neurobiology},
  volume={25},
  pages={156--163},
  year={2014},
  publisher={Elsevier}
}

@article{barak2017recurrent,
  author    = {Barak, Omri},
  title     = {Recurrent Neural Networks as Versatile Tools of Neuroscience Research},
  journal   = {Current Opinion in Neurobiology},
  volume    = {46},
  pages     = {1-6},
  year      = {2017},
}

@article{vyas2020computation,
  title={Computation through neural population dynamics},
  author={Vyas, Saurabh and Golub, Matthew D and Sussillo, David and Shenoy, Krishna V},
  journal={Annual review of neuroscience},
  volume={43},
  pages={249--275},
  year={2020},
  publisher={Annual Reviews}
}


%reverse engineering
@article{marom2009,
  title={On the precarious path of reverse neuro-engineering},
  author={Marom, Shimon and Meir, Ron and Braun, Erez and Gal, Asaf and Kermany, Einat and Eytan, Danny},
  journal={Frontiers in Computational Neuroscience},
  volume={3},
  pages={495},
  year={2009},
  publisher={Frontiers}
}

@article{sussillo2013blackbox,
  title={Opening the black box: Low-dimensional dynamics in high-dimensional recurrent neural networks},
  author={Sussillo, David and Barak, Omri},
  journal={Neural computation},
  volume={25},
  number={3},
  pages={626--649},
  year={2013},
  publisher={MIT Press}
}

@article{maheswaranathan2019universality,
  title={Universality and individuality in neural dynamics across large populations of recurrent networks},
  author={Maheswaranathan, Niru and Williams, Alex and Golub, Matthew and Ganguli, Surya and Sussillo, David},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{beer2018,
  title={Dynamics of dynamics: Following the formation of a line attractor},
  author={Beer, Chen and Barak, Omri},
  journal={arXiv preprint arXiv:1805.09603},
  year={2018}
}

@article{smith2021reverse,
  title={Reverse engineering recurrent neural networks with {Jacobian} switching linear dynamical systems},
  author={Smith, Jimmy and Linderman, Scott and Sussillo, David},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={16700--16713},
  year={2021}
}

@article{beer2024,
  title={Revealing and reshaping attractor dynamics in large networks of cortical neurons},
  author={Beer, Chen and Barak, Omri},
  journal={PLOS Computational Biology},
  volume={20},
  number={1},
  pages={e1011784},
  year={2024},
  publisher={Public Library of Science San Francisco, CA USA}
}

@inproceedings{turner2023,
  title={The Simplicity Bias in Multi-Task {RNNs}: Shared Attractors, Reuse of Dynamics, and Geometric Representation},
  author={Turner, Elia and Barak, Omri},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}

@InProceedings{haviv2019,
  title = 	 {Understanding and Controlling Memory in Recurrent Neural Networks},
  author =       {Haviv, Doron and Rivkind, Alexander and Barak, Omri},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2663--2671},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/haviv19a/haviv19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/haviv19a.html}
}

@article{schaeffer2020,
  title={Reverse-engineering recurrent neural network solutions to a hierarchical inference task for mice},
  author={Schaeffer, Rylan and Khona, Mikail and Meshulam, Leenoy and International Brain Laboratory and Fiete, Ila Rani},
  journal={bioRxiv},
  pages={2020--06},
  year={2020},
  publisher={Cold Spring Harbor Laboratory}
}

@article{driscoll2022,
  title={Flexible multitask computation in recurrent networks utilizes shared dynamical motifs},
  author={Driscoll, Laura and Shenoy, Krishna and Sussillo, David},
  journal={bioRxiv},
  pages={2022--08},
  year={2022},
  publisher={Cold Spring Harbor Laboratory}
}

@article{pals2024,
  title={Trained recurrent neural networks develop phase-locked limit cycles in a working memory task},
  author={Pals, Matthijs and Macke, Jakob H and Barak, Omri},
  journal={PLOS Computational Biology},
  volume={20},
  number={2},
  pages={e1011852},
  year={2024},
  publisher={Public Library of Science San Francisco, CA USA}
}


%persistence
@article{barak2007,
  title={Persistent activity in neural networks with dynamic synapses},
  author={Barak, Omri and Tsodyks, Misha},
  journal={PLoS computational biology},
  volume={3},
  number={2},
  pages={e35},
  year={2007},
  publisher={Public Library of Science San Francisco, USA}
}

@article{barak2014,
  title={Working models of working memory},
  author={Barak, Omri and Tsodyks, Misha},
  journal={Current opinion in neurobiology},
  volume={25},
  pages={20--24},
  year={2014},
  publisher={Elsevier}
}


%stability
%S-type
@article{panichello2019,
  title={Error-correcting dynamics in visual working memory},
  author={Panichello, Matthew F and DePasquale, Brian and Pillow, Jonathan W and Buschman, Timothy J},
  journal={Nature communications},
  volume={10},
  number={1},
  pages={3366},
  year={2019},
  publisher={Nature Publishing Group UK London}
}


%D-type
@article{marom2023,
  title={A biophysical perspective on the resilience of neuronal excitability across timescales},
  author={Marom, Shimon and Marder, Eve},
  journal={Nature Reviews Neuroscience},
  volume={24},
  number={10},
  pages={640--652},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{seeholzer2019,
  title={Stability of working memory in continuous attractor networks under the control of short-term plasticity},
  author={Seeholzer, Alexander and Deger, Moritz and Gerstner, Wulfram},
  journal={PLoS computational biology},
  volume={15},
  number={4},
  pages={e1006928},
  year={2019},
  publisher={Public Library of Science San Francisco, CA USA}
}

%CAs
@article{stringer2002headdirection,
  title={Self-organizing continuous attractor networks and path integration: One-dimensional models of head direction cells},
  author={Stringer, SM and Trappenberg, TP and Rolls, ET and Araujo, IETd},
  journal={Network: Computation in Neural Systems},
  volume={13},
  number={2},
  pages={217--242},
  year={2002},
  publisher={Taylor \& Francis}
}

@article{stringer2002placecells,
  title={Self-organizing continuous attractor networks and path integration: Two-dimensional models of place cells},
  author={Stringer, SM and Rolls, ET and Trappenberg, TP and De Araujo, IET},
  journal={Network: computation in neural systems},
  volume={13},
  number={4},
  pages={429--446},
  year={2002},
  publisher={Taylor \& Francis}
}  

@article{stringer2003,
  title={Self-organizing continuous attractor networks and motor function},
  author={Stringer, Simon M and Rolls, Edmund T and Trappenberg, Thomas P and de Ara{\'u}jo, Ivan E Tavares},
  journal={Neural Networks},
  volume={16},
  number={2},
  pages={161--182},
  year={2003},
  publisher={Elsevier}
}

@article{darshan2022,
  title={Learning to represent continuous variables in heterogeneous neural networks},
  author={Darshan, Ron and Rivkind, Avi},
  journal={Cell Reports},
  volume={39},
  number={1},
  pages={110612},
  year={2022},
  publisher={Elsevier}
}

%%%neural integration

@article{cannon1983,
  title={A proposed neural network for the integrator of the oculomotor system},
  author={Cannon, Stephen C and Robinson, David A and Shamma, Shihab},
  journal={Biological cybernetics},
  volume={49},
  number={2},
  pages={127--136},
  year={1983},
  publisher={Springer}
}

@article{goncalves2014,
  title={Optogenetic perturbations reveal the dynamics of an oculomotor integrator},
  author={Gon{\c{c}}alves, Pedro J and Arrenberg, Aristides B and Hablitzel, Bastian and Baier, Herwig and Machens, Christian K},
  journal={Frontiers in neural circuits},
  volume={8},
  pages={10},
  year={2014},
  publisher={Frontiers Media SA}
}

@article{wolpert1995,
  title={An internal model for sensorimotor integration},
  author={Wolpert, Daniel M and Ghahramani, Zoubin and Jordan, Michael I},
  journal={Science},
  volume={269},
  number={5232},
  pages={1880--1882},
  year={1995},
  publisher={American Association for the Advancement of Science}
}

@article{koulakov2002,
  title={Model for a robust neural integrator},
  author={Koulakov, Alexei A and Raghavachari, Sridhar and Kepecs, Adam and Lisman, John E},
  journal={Nature neuroscience},
  volume={5},
  number={8},
  pages={775--782},
  year={2002},
  publisher={Nature Publishing Group US New York}
}

@article{renart2003,
  title    = "Robust spatial working memory through homeostatic synaptic
              scaling in heterogeneous cortical networks",
  author   = "Renart, Alfonso and Song, Pengcheng and Wang, Xiao-Jing",
  abstract = "The concept of bell-shaped persistent neural activity represents
              a cornerstone of the theory for the internal representation of
              analog quantities, such as spatial location or head direction.
              Previous models, however, relied on the unrealistic assumption of
              network homogeneity. We investigate this issue in a network model
              where fine tuning of parameters is destroyed by heterogeneities
              in cellular and synaptic properties. Heterogeneities result in
              the loss of stored spatial information in a few seconds. Accurate
              encoding is recovered when a homeostatic mechanism scales the
              excitatory synapses to each cell to compensate for the
              heterogeneity in cellular excitability and synaptic inputs.
              Moreover, the more realistic model produces a wide diversity of
              tuning curves, as commonly observed in recordings from prefrontal
              neurons. We conclude that recurrent attractor networks in
              conjunction with appropriate homeostatic mechanisms provide a
              robust, biologically plausible theoretical framework for
              understanding the neural circuit basis of spatial working memory.",
  journal  = "Neuron",
  volume   =  38,
  number   =  3,
  pages    = "473--485",
  month    =  may,
  year     =  2003,
  url      = "http://dx.doi.org/10.1016/s0896-6273(03)00255-1",
  language = "en",
  issn     = "0896-6273",
  pmid     = "12741993",
  doi      = "10.1016/s0896-6273(03)00255-1"
}

@article{wong2007,
  title={Neural circuit dynamics underlying accumulation of time-varying evidence during perceptual decision making},
  author={Wong, Kong-Fatt and Huk, Alexander C and Shadlen, Michael N and Wang, Xiao-Jing},
  journal={Frontiers in Computational Neuroscience},
  volume={1},
  pages={115},
  year={2007},
  publisher={Frontiers}
}

@article{burak2009,
  author = {Burak, Yoram and Fiete, Ila R.},
  title = {Accurate Path Integration in Continuous Attractor Network Models of Grid Cells},
  journal = {PLoS Computational Biology},
  volume = {5},
  number = {2},
  pages = {e1000291},
  year = {2009},
}

@article{khona2022,
  author = {Khona, Manan and Fiete, Ila R.},
  title = {Attractor and Integrator Networks in the Brain},
  journal = {Nature Reviews Neuroscience},
  volume = {23},
  number = {12},
  pages = {744-766},
  year = {2022},
}

@article{vafidis2022,
  title={Learning accurate path integration in ring attractor models of the head direction system},
  author={Vafidis, Pantelis and Owald, David and D'Albis, Tiziano and Kempter, Richard},
  journal={Elife},
  volume={11},
  pages={e69841},
  year={2022},
  publisher={eLife Sciences Publications Limited}
}

@article{aksay2007,
  author = {Aksay, Emel and Olasagasti, Inaki and Mensh, Brett D. and Baker, Robert and Goldman, Matthew S. and Tank, David W.},
  title = {Functional Dissection of Circuitry in a Neural Integrator},
  journal = {Nature Neuroscience},
  volume = {10},
  number = {4},
  pages = {494-504},
  year = {2007},
}

@inbook{goldman2007,
  author = {Goldman, Matthew and Compte, Albert and Wang, Xiao-Jing},
  title = {Neural Integrators: Recurrent Mechanisms and Models},
  booktitle = {New Encyclopedia of Neuroscience},
  pages = {1-26},
  year = {2007},
}

@article{hulse2020,
  author = {Hulse, Benjamin K. and Jayaraman, Vivek},
  title = {Mechanisms Underlying the Neural Computation of Head Direction},
  journal = {Annual Review of Neuroscience},
  volume = {43},
  pages = {31-54},
  year = {2020},
}

@article{noorman2022,
  title={Accurate angular integration with only a handful of neurons},
  author={Noorman, Marcella and Hulse, Brad K and Jayaraman, Vivek and Romani, Sandro and Hermundstad, Ann M},
  journal={bioRxiv},
  pages={2022--05},
  year={2022},
  publisher={Cold Spring Harbor Laboratory}
}

@article{turner2017,
  title={Angular velocity integration in a fly heading circuit},
  author={Turner-Evans, Daniel and Wegener, Stephanie and Rouault, Herve and Franconville, Romain and Wolff, Tanya and Seelig, Johannes D and Druckmann, Shaul and Jayaraman, Vivek},
  journal={Elife},
  volume={6},
  pages={e23496},
  year={2017},
  publisher={eLife Sciences Publications, Ltd}
}

@article{branco2016,
  title={Near-Perfect Synaptic Integration by {Nav1.7} in Hypothalamic Neurons Regulates Body Weight},
  author={Branco, Tiago and Tozer, Aaron and Magnus, Christopher J. and Sugino, Ken and Tanaka, Shinsuke and Lee, Albert K. and Wood, John N. and Sternson, Scott M.},
  journal={Cell},
  volume={165},
  number={7},
  pages={1749--1761},
  year={2016},
  publisher={Elsevier}
}


%localization
@article{yang2022,
  title={A brainstem integrator for self-location memory and positional homeostasis in zebrafish},
  author={Yang, Ekin and Zwart, Maarten F and James, Brielle and Rubinov, Mikail and Wei, Zhiyuan and Narayan, Sujay and Vladimirov, Nikita and Mensh, Brett D and Fitzgerald, James E and Ahrens, Misha B},
  journal={Cell},
  volume={185},
  number={26},
  pages={5011--5027.e20},
  year={2022},
  publisher={Elsevier}
}

@article{samsonovich1997,
  title={Path integration and cognitive mapping in a continuous attractor neural network model},
  author={Samsonovich, Alexei and McNaughton, Bruce L},
  journal={Journal of Neuroscience},
  volume={17},
  number={15},
  pages={5900--5920},
  year={1997},
  publisher={Soc Neuroscience}
}

%head direction%%HD
@article{kim2019generation,
  title={Generation of stable heading representations in diverse visual scenes},
  author={Kim, Sung Soo and Hermundstad, Ann M and Romani, Sandro and Abbott, LF and Jayaraman, Vivek},
  journal={Nature},
  volume={576},
  number={7785},
  pages={126--131},
  year={2019},
  publisher={Nature Publishing Group UK London}
}

@article{turner2020neuroanatomical,
  title={The neuroanatomical ultrastructure and function of a biological ring attractor},
  author={Turner-Evans, Daniel B and Jensen, Kristopher T and Ali, Saba and Paterson, Tyler and Sheridan, Arlo and Ray, Robert P and Wolff, Tanya and Lauritzen, J Scott and Rubin, Gerald M and Bock, Davi D and others},
  journal={Neuron},
  volume={108},
  number={1},
  pages={145--163},
  year={2020},
  publisher={Elsevier}
}

%decision making
@article{wong2008,
  title={Temporal dynamics underlying perceptual decision making: Insights from the interplay between an attractor model and parietal neurophysiology},
  author={Wong, Kong-Fatt and Huk, Alexander C},
  journal={Frontiers in neuroscience},
  volume={2},
  pages={383},
  year={2008},
  publisher={Frontiers}
}

%working memory
@article{wimmer2014,
  title={Bump attractor dynamics in prefrontal cortex explains behavioral precision in spatial working memory},
  author={Wimmer, Klaus and Nykamp, Duane Q and Constantinidis, Christos and Compte, Albert},
  journal={Nature neuroscience},
  volume={17},
  number={3},
  pages={431--439},
  year={2014},
  publisher={Nature Publishing Group US New York}
}

%interpretability
@article{lipton2018,
  title={The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery},
  author={Lipton, Zachary C.},
  journal={Queue},
  volume={16},
  number={3},
  pages={31--57},
  year={2018},
}

@article{erasmus2021,
  title={What is interpretability?},
  author={Erasmus, Adrian and Brunet, Tyler DP and Fisher, Eyal},
  journal={Philosophy \& Technology},
  volume={34},
  number={4},
  pages={833--862},
  year={2021},
  publisher={Springer}
}

@article{potochnik2021,
  title={Our World Isn't Organized into Levels},
  author={Potochnik, Angela},
  year={2021}
}

@article{potochnik2020,
  title={Idealization and many aims},
  author={Potochnik, Angela},
  journal={Philosophy of science},
  volume={87},
  number={5},
  pages={933--943},
  year={2020},
  publisher={Cambridge University Press}
}

@book{potochnik2017idealization,
  title={Idealization and the Aims of Science},
  author={Potochnik, Angela},
  year={2017},
  publisher={University of Chicago Press}
}

@article{potochnik2016,
  title={Scientific explanation: Putting communication first},
  author={Potochnik, Angela},
  journal={Philosophy of Science},
  volume={83},
  number={5},
  pages={721--732},
  year={2016},
  publisher={Cambridge University Press}
}



%training and generalization
@InProceedings{rame2022fishr,
  title = 	 {Fishr: Invariant Gradient Variances for Out-of-Distribution Generalization},
  author =       {Rame, Alexandre and Dancette, Corentin and Cord, Matthieu},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {18347--18377},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/rame22a/rame22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/rame22a.html},
  abstract = 	 {Learning robust models that generalize well under changes in the data distribution is critical for real-world applications. To this end, there has been a growing surge of interest to learn simultaneously from multiple training domains - while enforcing different types of invariance across those domains. Yet, all existing approaches fail to show systematic benefits under controlled evaluation protocols. In this paper, we introduce a new regularization - named Fishr - that enforces domain invariance in the space of the gradients of the loss: specifically, the domain-level variances of gradients are matched across training domains. Our approach is based on the close relations between the gradient covariance, the Fisher Information and the Hessian of the loss: in particular, we show that Fishr eventually aligns the domain-level loss landscapes locally around the final weights. Extensive experiments demonstrate the effectiveness of Fishr for out-of-distribution generalization. Notably, Fishr improves the state of the art on the DomainBed benchmark and performs consistently better than Empirical Risk Minimization. Our code is available at https://github.com/alexrame/fishr.}
}



%fitting
@InProceedings{zhao2016,
  author        = {Zhao, Yuan and Park, Il Memming},
  title         = {Interpretable Nonlinear Dynamic Modeling of Neural Trajectories},
  booktitle     = {Advances in Neural Information Processing Systems (NIPS)},
  year          = {2016},
  abstract      = {A central challenge in neuroscience is understanding how neural system
implements computation through its dynamics. We propose a nonlinear
time series model aimed at characterizing interpretable dynamics
from neural trajectories. Our model assumes low-dimensional continuous
dynamics in a finite volume. It incorporates a prior assumption about
globally contractional dynamics to avoid overly enthusiastic extrapolation
outside of the support of observed trajectories. We show that our
model can recover qualitative features of the phase portrait such
as attractors, slow points, and bifurcations, while also producing
reliable long-term future predictions in a variety of dynamical models
and in real neural data.},
  archiveprefix = {arXiv},
  eprint        = {1608.06546},
  keywords      = {autoregressive, bifurcation, chaos, continuous-attractor, dynamics, neural-dynamics, nips, oscillation, tensorflow},
  primaryclass  = {q-bio.QM},
  youtube	= {https://www.youtube.com/watch?v=7oWRZRpaq_I},
  url = {https://papers.nips.cc/paper/6543-interpretable-nonlinear-dynamic-modeling-of-neural-trajectories},
}


@article{zhao2017variational,
  title={Variational latent gaussian process for recovering single-trial dynamics from population spike trains},
  author={Zhao, Yuan and Park, Il Memming},
  journal={Neural computation},
  volume={29},
  number={5},
  pages={1293--1316},
  year={2017},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{pandarinath2018inferring,
  title={Inferring single-trial neural population dynamics using sequential auto-encoders},
  author={Pandarinath, Chethan and O’Shea, Daniel J and Collins, Jasmine and Jozefowicz, Rafal and Stavisky, Sergey D and Kao, Jonathan C and Trautmann, Eric M and Kaufman, Matthew T and Ryu, Stephen I and Hochberg, Leigh R and others},
  journal={Nature methods},
  volume={15},
  number={10},
  pages={805--815},
  year={2018},
  publisher={Nature Publishing Group US New York}
}

@article{pagan2022,
  title={A new theoretical framework jointly explains behavioral and neural variability across subjects performing flexible decision-making},
  author={Pagan, Marino and Tang, Vincent D and Aoi, Mikio C and Pillow, Jonathan W and Mante, Valerio and Sussillo, David and Brody, Carlos D},
  journal={bioRxiv},
  pages={2022--11},
  year={2022},
  publisher={Cold Spring Harbor Laboratory}
}

@inproceedings{zoltowski2020general,
  title={A general recurrent state space framework for modeling neural dynamics during decision-making},
  author={Zoltowski, David and Pillow, Jonathan and Linderman, Scott},
  booktitle={International Conference on Machine Learning},
  pages={11680--11691},
  year={2020},
  organization={PMLR}
}