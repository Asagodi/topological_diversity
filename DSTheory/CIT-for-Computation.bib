@online{2019ChaudhuriHD,
  title = {2019\_{{Chaudhuri}}\_{{HD}}\_{{NN}}.Pdf},
  url = {https://drive.google.com/file/d/1Oym57wzqrgHosZUsCZU2w-WXOLsFUPiF/view?usp=embed_facebook},
  urldate = {2023-03-25},
  organization = {{Google Docs}},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\XBZ26WD8\\view.html}
}

@article{bakerPhilosophicalUnderstandingRepresentation,
  title = {A {{Philosophical Understanding}} of {{Representation}} for {{Neuroscience}}},
  author = {Baker, Ben and Lansdell, Benjamin and Kording, Konrad},
  abstract = {Neuroscientists often describe neural activity as a representation of something, or claim to have found evidence for a neural representation. But what do these statements mean? The reasons to call some neural activity a representation and the assumptions that come with this term are not generally made clear from its common uses in neuroscience. Representation is a central concept in philosophy of mind, with a rich history going back to the ancient period. In order to clarify its usage in neuroscience, here we advance a link between the connotations of this term across these disciplines. We draw on a broad range of discourse in philosophy to distinguish three key aspects of representation: correspondence, functional role, and teleology. We argue that each of these aspects are implied by the explanatory role the term plays in neuroscience. However, evidence related to all three aspects is rarely presented or discussed in the course of individual studies that aim to identify representations. Overlooking the significance of all three aspects hinders communication in neuroscience, as it obscures the limitations of experimental paradigms and conceals gaps in our understanding of the phenomena of primary interest. Working from this three-part view, we discuss how to move toward clearer communication about representations in the brain.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\WUV99R9A\\Baker et al. - A Philosophical Understanding of Representation fo.pdf}
}

@article{barakRecurrentNeuralNetworks2017,
  title = {Recurrent Neural Networks as Versatile Tools of Neuroscience Research},
  author = {Barak, Omri},
  date = {2017-10},
  journaltitle = {Current Opinion in Neurobiology},
  shortjournal = {Current Opinion in Neurobiology},
  volume = {46},
  pages = {1--6},
  issn = {09594388},
  doi = {10.1016/j.conb.2017.06.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0959438817300429},
  urldate = {2023-03-26},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\2Z8NKNTW\\Barak - 2017 - Recurrent neural networks as versatile tools of ne.pdf}
}

@article{beerDynamicsSmallContinuousTime1995a,
  title = {On the {{Dynamics}} of {{Small Continuous-Time Recurrent Neural Networks}}},
  author = {Beer, Randall D.},
  date = {1995-03},
  journaltitle = {Adaptive Behavior},
  shortjournal = {Adaptive Behavior},
  volume = {3},
  number = {4},
  pages = {469--509},
  issn = {1059-7123, 1741-2633},
  doi = {10.1177/105971239500300405},
  url = {http://journals.sagepub.com/doi/10.1177/105971239500300405},
  urldate = {2022-06-18},
  abstract = {Dynamical neural networks are being increasingly employed in a variety of contexts, including as simple model nervous systems for autonomous agents. For this reason, there is a growing need for a comprehensive understanding of their dynamical properties. Using a combination of elementary analysis and numerical studies, this article begins a systematic examination of the dynamics of continuous-time recurrent neural networks. Specifically, a fairly complete description of the possible dynamical behavior and bifurcations of one- and two-neuron circuits is given, along with a few specific results for larger networks. This analysis provides both qualitative insight and, in many cases, quantitative formulas for predicting the dynamical behavior of particular circuits and how that behavior changes as network parameters are varied. These results demonstrate that even small circuits are capable of a rich variety of dynamical behavior (including chaotic dynamics). An approach to understanding the dynamics of circuits with time-varying inputs is also presented. Finally, based on this analysis, several strategies for focusing evolutionary searches into fruitful regions of network parameter space are suggested.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\HAERV46C\\Beer - 1995 - On the Dynamics of Small Continuous-Time Recurrent.pdf}
}

@article{beerInformationProcessingDynamics2015,
  title = {Information {{Processing}} and {{Dynamics}} in {{Minimally Cognitive Agents}}},
  author = {Beer, Randall D. and Williams, Paul L.},
  date = {2015-01},
  journaltitle = {Cognitive Science},
  shortjournal = {Cogn Sci},
  volume = {39},
  number = {1},
  pages = {1--38},
  issn = {03640213},
  doi = {10.1111/cogs.12142},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/cogs.12142},
  urldate = {2023-02-26},
  abstract = {There has been considerable debate in the literature about the relative merits of information processing versus dynamical approaches to understanding cognitive processes. In this article, we explore the relationship between these two styles of explanation using a model agent evolved to solve a relational categorization task. Specifically, we separately analyze the operation of this agent using the mathematical tools of information theory and dynamical systems theory. Information-theoretic analysis reveals how task-relevant information flows through the system to be combined into a categorization decision. Dynamical analysis reveals the key geometrical and temporal interrelationships underlying the categorization decision. Finally, we propose a framework for directly relating these two different styles of explanation and discuss the possible implications of our analysis for some of the ongoing debates in cognitive science.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\ABWIQCGT\\Beer and Williams - 2015 - Information Processing and Dynamics in Minimally C.pdf}
}

@article{beerParameterSpaceStructure2006,
  title = {Parameter {{Space Structure}} of {{Continuous-Time Recurrent Neural Networks}}},
  author = {Beer, Randall D.},
  date = {2006-12},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {18},
  number = {12},
  pages = {3009--3051},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.2006.18.12.3009},
  url = {https://direct.mit.edu/neco/article/18/12/3009-3051/7104},
  urldate = {2023-02-25},
  abstract = {A fundamental challenge for any general theory of neural circuits is how to characterize the structure of the space of all possible circuits over a given model neuron. As a first step in this direction, this letter begins a systematic study of the global parameter space structure of continuous-time recurrent neural networks (CTRNNs), a class of neural models that is simple but dynamically universal. First, we explicitly compute the local bifurcation manifolds of CTRNNs. We then visualize the structure of these manifolds in net input space for small circuits. These visualizations reveal a set of extremal saddle node bifurcation manifolds that divide CTRNN parameter space into regions of dynamics with different effective dimensionality. Next, we completely characterize the combinatorics and geometry of an asymptotically exact approximation to these regions for circuits of arbitrary size. Finally, we show how these regions can be used to calculate estimates of the probability of encountering different kinds of dynamics in CTRNN parameter space.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\DZU78KSU\\Beer - 2006 - Parameter Space Structure of Continuous-Time Recur.pdf}
}

@article{beiranShapingDynamicsMultiple2021,
  title = {Shaping Dynamics with Multiple Populations in Low-Rank Recurrent Networks},
  author = {Beiran, Manuel and Dubreuil, Alexis and Valente, Adrian and Mastrogiuseppe, Francesca and Ostojic, Srdjan},
  date = {2021-05-13},
  journaltitle = {Neural Computation},
  volume = {33},
  number = {6},
  eprint = {2007.02062},
  eprinttype = {arxiv},
  eprintclass = {q-bio},
  pages = {1572--1615},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco_a_01381},
  url = {http://arxiv.org/abs/2007.02062},
  urldate = {2023-02-26},
  abstract = {An emerging paradigm proposes that neural computations can be understood at the level of dynamical systems that govern low-dimensional trajectories of collective neural activity. How the connectivity structure of a network determines the emergent dynamical system however remains to be clarified. Here we consider a novel class of models, Gaussian-mixture low-rank recurrent networks, in which the rank of the connectivity matrix and the number of statistically-defined populations are independent hyper-parameters. We show that the resulting collective dynamics form a dynamical system, where the rank sets the dimensionality and the population structure shapes the dynamics. In particular, the collective dynamics can be described in terms of a simplified effective circuit of interacting latent variables. While having a single, global population strongly restricts the possible dynamics, we demonstrate that if the number of populations is large enough, a rank R network can approximate any R-dimensional dynamical system.},
  langid = {english},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\PAEMPM4D\\Beiran et al. - 2021 - Shaping dynamics with multiple populations in low-.pdf}
}

@article{bressloffSpatiotemporalDynamicsContinuum2012,
  title = {Spatiotemporal Dynamics of Continuum Neural Fields},
  author = {Bressloff, Paul C},
  date = {2012-01-27},
  journaltitle = {Journal of Physics A: Mathematical and Theoretical},
  shortjournal = {J. Phys. A: Math. Theor.},
  volume = {45},
  number = {3},
  pages = {033001},
  issn = {1751-8113, 1751-8121},
  doi = {10.1088/1751-8113/45/3/033001},
  url = {https://iopscience.iop.org/article/10.1088/1751-8113/45/3/033001},
  urldate = {2023-03-25},
  abstract = {We survey recent analytical approaches to studying the spatiotemporal dynamics of continuum neural fields. Neural fields model the large-scale dynamics of spatially structured biological neural networks in terms of nonlinear integrodifferential equations whose associated integral kernels represent the spatial distribution of neuronal synaptic connections. They provide an important example of spatially extended excitable systems with nonlocal interactions and exhibit a wide range of spatially coherent dynamics including traveling waves oscillations and Turing-like patterns.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\MBRDB9FL\\Bressloff - 2012 - Spatiotemporal dynamics of continuum neural fields.pdf}
}

@article{brittenAnalysisVisualMotion1992,
  title = {The Analysis of Visual Motion: A Comparison of Neuronal and Psychophysical Performance},
  shorttitle = {The Analysis of Visual Motion},
  author = {Britten, Kh and Shadlen, Mn and Newsome, Wt and Movshon, Ja},
  date = {1992-12-01},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {12},
  number = {12},
  pages = {4745--4765},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.12-12-04745.1992},
  url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.12-12-04745.1992},
  urldate = {2022-06-18},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\QBK63VHC\\Britten et al. - 1992 - The analysis of visual motion a comparison of neu.pdf}
}

@article{brodyBasicMechanismsGraded2003,
  title = {Basic Mechanisms for Graded Persistent Activity: Discrete Attractors, Continuous Attractors, and Dynamic Representations},
  shorttitle = {Basic Mechanisms for Graded Persistent Activity},
  author = {Brody, Carlos D and Romo, Ranulfo and Kepecs, Adam},
  date = {2003-04},
  journaltitle = {Current Opinion in Neurobiology},
  shortjournal = {Current Opinion in Neurobiology},
  volume = {13},
  number = {2},
  pages = {204--211},
  issn = {09594388},
  doi = {10.1016/S0959-4388(03)00050-3},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0959438803000503},
  urldate = {2023-03-25},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\XTHM2SDR\\Brody et al. - 2003 - Basic mechanisms for graded persistent activity d.pdf}
}

@article{brodyNeuralUnderpinningsEvidence2016,
  title = {Neural Underpinnings of the Evidence Accumulator},
  author = {Brody, Carlos D and Hanks, Timothy D},
  date = {2016-04},
  journaltitle = {Current Opinion in Neurobiology},
  shortjournal = {Current Opinion in Neurobiology},
  volume = {37},
  pages = {149--157},
  issn = {09594388},
  doi = {10.1016/j.conb.2016.01.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0959438816000040},
  urldate = {2022-06-18},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\NJ2FWK82\\Brody and Hanks - 2016 - Neural underpinnings of the evidence accumulator.pdf}
}

@article{bruntonRatsHumansCan2013,
  title = {Rats and {{Humans Can Optimally Accumulate Evidence}} for {{Decision-Making}}},
  author = {Brunton, Bingni W. and Botvinick, Matthew M. and Brody, Carlos D.},
  date = {2013-04-05},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {340},
  number = {6128},
  pages = {95--98},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1233912},
  url = {https://www.science.org/doi/10.1126/science.1233912},
  urldate = {2023-03-25},
  abstract = {How to Make Decisions                            Recently, a number of methods to probe internal properties of nonlinear neural systems have been developed. In these methods, highly variable stimuli are used to explore the input space of the system. Neural responses are then studied using models that take advantage of the known trial-by-trial stimulus information.                                Brunton                 et al.                              (p.               95               ) adapted this combined approach to decision-making. Both in rats and humans, the diffusion constant of the drift-diffusion model of decision-making was zero, implying that the noise is all in the processing of sensory input and not in the evidence accumulator. In addition, rats gradually accumulated evidence for decision-making, with strong effects of sensory adaptation on gradual accumulation of evidence.                        ,              A model of decision-making that is based on the accumulative processing of noisy information is described.           ,              The gradual and noisy accumulation of evidence is a fundamental component of decision-making, with noise playing a key role as the source of variability and errors. However, the origins of this noise have never been determined. We developed decision-making tasks in which sensory evidence is delivered in randomly timed pulses, and analyzed the resulting data with models that use the richly detailed information of each trial’s pulse timing to distinguish between different decision-making mechanisms. This analysis allowed measurement of the magnitude of noise in the accumulator’s memory, separately from noise associated with incoming sensory evidence. In our tasks, the accumulator’s memory was noiseless, for both rats and humans. In contrast, the addition of new sensory evidence was the primary source of variability. We suggest our task and modeling approach as a powerful method for revealing internal properties of decision-making processes.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\T6UUFQ8Q\\Brunton et al. - 2013 - Rats and Humans Can Optimally Accumulate Evidence .pdf}
}

@article{burakAccuratePathIntegration2009,
  title = {Accurate {{Path Integration}} in {{Continuous Attractor Network Models}} of {{Grid Cells}}},
  author = {Burak, Yoram and Fiete, Ila R.},
  editor = {Sporns, Olaf},
  date = {2009-02-20},
  journaltitle = {PLoS Computational Biology},
  shortjournal = {PLoS Comput Biol},
  volume = {5},
  number = {2},
  pages = {e1000291},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000291},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1000291},
  urldate = {2023-03-25},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\7AFQ9EG2\\Burak and Fiete - 2009 - Accurate Path Integration in Continuous Attractor .pdf}
}

@article{cunninghamDimensionalityReductionLargescale2014,
  title = {Dimensionality Reduction for Large-Scale Neural Recordings},
  author = {Cunningham, John P and Yu, Byron M},
  date = {2014-11},
  journaltitle = {Nature neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {17},
  number = {11},
  eprint = {25151264},
  eprinttype = {pmid},
  pages = {1500--1509},
  issn = {1097-6256},
  doi = {10.1038/nn.3776},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4433019/},
  urldate = {2023-03-22},
  abstract = {Most sensory, cognitive and motor functions depend on the interactions of many neurons. In recent years, there has been rapid development and increasing use of technologies for recording from large numbers of neurons, either sequentially or simultaneously. A key question is what scientific insight can be gained by studying a population of recorded neurons beyond studying each neuron individually. Here, we examine three important motivations for population studies: single-trial hypotheses requiring statistical power, hypotheses of population response structure and exploratory analyses of large data sets. Many recent studies have adopted dimensionality reduction to analyze these populations and to find features that are not apparent at the level of individual neurons. We describe the dimensionality reduction methods commonly applied to population activity and offer practical advice about selecting methods and interpreting their outputs. This review is intended for experimental and computational researchers who seek to understand the role dimensionality reduction has had and can have in systems neuroscience, and who seek to apply these methods to their own data.},
  pmcid = {PMC4433019},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\E9UCCR6L\\Cunningham and Yu - 2014 - Dimensionality reduction for large-scale neural re.pdf}
}

@article{driscollComputationCorticalDynamics2018,
  title = {Computation through {{Cortical Dynamics}}},
  author = {Driscoll, Laura N. and Golub, Matthew D. and Sussillo, David},
  date = {2018-06},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {98},
  number = {5},
  pages = {873--875},
  issn = {08966273},
  doi = {10.1016/j.neuron.2018.05.029},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627318304276},
  urldate = {2022-06-18},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\SGRIFRWM\\Driscoll et al. - 2018 - Computation through Cortical Dynamics.pdf}
}

@article{driscollDynamicReorganizationNeuronal2017,
  title = {Dynamic {{Reorganization}} of {{Neuronal Activity Patterns}} in {{Parietal Cortex}}},
  author = {Driscoll, Laura N. and Pettit, Noah L. and Minderer, Matthias and Chettih, Selmaan N. and Harvey, Christopher D.},
  date = {2017-08},
  journaltitle = {Cell},
  shortjournal = {Cell},
  volume = {170},
  number = {5},
  pages = {986-999.e16},
  issn = {00928674},
  doi = {10.1016/j.cell.2017.07.021},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0092867417308280},
  urldate = {2022-06-18},
  abstract = {Neuronal representations change as associations are learned between sensory stimuli and behavioral actions. However, it is poorly understood whether representations for learned associations stabilize in cortical association areas or continue to change following learning. We tracked the activity of posterior parietal cortex neurons for a month as mice stably performed a virtual-navigation task. The relationship between cells’ activity and task features was mostly stable on single days but underwent major reorganization over weeks. The neurons informative about task features (trial type and maze locations) changed across days. Despite changes in individual cells, the population activity had statistically similar properties each day and stable information for over a week. As mice learned additional associations, new activity patterns emerged in the neurons used for existing representations without greatly affecting the rate of change of these representations. We propose that dynamic neuronal activity patterns could balance plasticity for learning and stability for memory.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\P65PK82Z\\Driscoll et al. - 2017 - Dynamic Reorganization of Neuronal Activity Patter.pdf}
}

@article{driscollMultitaskComputationRNNs,
  title = {Multitask {{Computation}} in {{RNNs Utilizes Shared Dynamical Motifs}}},
  author = {Driscoll, Laura and Shenoy, Krishna and Sussillo, David},
  pages = {1},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\RW25SW3X\\Driscoll et al. - Multitask Computation in RNNs Utilizes Shared Dyna.pdf}
}

@article{dubreuilRolePopulationStructure2022,
  title = {The Role of Population Structure in Computations through Neural Dynamics},
  author = {Dubreuil, Alexis and Valente, Adrian and Beiran, Manuel and Mastrogiuseppe, Francesca and Ostojic, Srdjan},
  date = {2022-06},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {25},
  number = {6},
  pages = {783--794},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-022-01088-4},
  url = {https://www.nature.com/articles/s41593-022-01088-4},
  urldate = {2022-06-18},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\92AV2ZVQ\\Dubreuil et al. - 2022 - The role of population structure in computations t.pdf}
}

@article{dunckerOrganizingRecurrentNetwork,
  title = {Organizing Recurrent Network Dynamics by Task-Computation to Enable Continual Learning},
  author = {Duncker, Lea and Driscoll, Laura N and Shenoy, Krishna V and Sahani, Maneesh and Sussillo, David},
  pages = {11},
  abstract = {Biological systems face dynamic environments that require continual learning. It is not well understood how these systems balance the tension between flexibility for learning and robustness for memory of previous behaviors. Continual learning without catastrophic interference also remains a challenging problem in machine learning. Here, we develop a novel learning rule designed to minimize interference between sequentially learned tasks in recurrent networks. Our learning rule preserves network dynamics within activity-defined subspaces used for previously learned tasks. It encourages dynamics associated with new tasks that might otherwise interfere to instead explore orthogonal subspaces, and it allows for reuse of previously established dynamical motifs where possible. Employing a set of tasks used in neuroscience, we demonstrate that our approach successfully eliminates catastrophic interference and offers a substantial improvement over previous continual learning algorithms. Using dynamical systems analysis, we show that networks trained using our approach can reuse similar dynamical structures across similar tasks. This possibility for shared computation allows for faster learning during sequential training. Finally, we identify organizational differences that emerge when training tasks sequentially versus simultaneously.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\KA4FHI4B\\Duncker et al. - Organizing recurrent network dynamics by task-comp.pdf}
}

@article{ehrlichPsychRNNAccessibleFlexible2021,
  title = {{{PsychRNN}}: {{An Accessible}} and {{Flexible Python Package}} for {{Training Recurrent Neural Network Models}} on {{Cognitive Tasks}}},
  shorttitle = {{{PsychRNN}}},
  author = {Ehrlich, Daniel B. and Stone, Jasmine T. and Brandfonbrener, David and Atanasov, Alexander and Murray, John D.},
  date = {2021-01},
  journaltitle = {eneuro},
  shortjournal = {eNeuro},
  volume = {8},
  number = {1},
  pages = {ENEURO.0427-20.2020},
  issn = {2373-2822},
  doi = {10.1523/ENEURO.0427-20.2020},
  url = {https://www.eneuro.org/lookup/doi/10.1523/ENEURO.0427-20.2020},
  urldate = {2022-06-18},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\CJQ7IKWU\\Ehrlich et al. - 2021 - PsychRNN An Accessible and Flexible Python Packag.pdf}
}

@article{favelaDynamicalRenaissanceNeuroscience2021,
  title = {The Dynamical Renaissance in Neuroscience},
  author = {Favela, Luis H.},
  date = {2021-12},
  journaltitle = {Synthese},
  shortjournal = {Synthese},
  volume = {199},
  number = {1-2},
  pages = {2103--2127},
  issn = {0039-7857, 1573-0964},
  doi = {10.1007/s11229-020-02874-y},
  url = {https://link.springer.com/10.1007/s11229-020-02874-y},
  urldate = {2023-03-23},
  abstract = {Although there is a substantial philosophical literature on dynamical systems theory in the cognitive sciences, the same is not the case for neuroscience. This paper attempts to motivate increased discussion via a set of overlapping issues. The first aim is primarily historical and is to demonstrate that dynamical systems theory is currently experiencing a renaissance in neuroscience. Although dynamical concepts and methods are becoming increasingly popular in contemporary neuroscience, the general approach should not be viewed as something entirely new to neuroscience. Instead, it is more appropriate to view the current developments as making central again approaches that facilitated some of neuroscience’s most significant early achievements, namely, the Hodgkin–Huxley and FitzHugh–Nagumo models. The second aim is primarily critical and defends a version of the “dynamical hypothesis” in neuroscience. Whereas the original version centered on defending a noncomputational and nonrepresentational account of cognition, the version I have in mind is broader and includes both cognition and the neural systems that realize it as well. In view of that, I discuss research on motor control as a paradigmatic example demonstrating that the concepts and methods of dynamical systems theory are increasingly and successfully being applied to neural systems in contemporary neuroscience. More significantly, such applications are motivating a stronger metaphysical claim, that is, understanding neural systems as being dynamical systems, which includes not requiring appeal to representations to explain or understand those phenomena. Taken together, the historical claim and the critical claim demonstrate that the dynamical hypothesis is undergoing a renaissance in contemporary neuroscience.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\9BCX74B9\\Favela - 2021 - The dynamical renaissance in neuroscience.pdf}
}

@article{feudelHomoclinicBifurcationHodgkin2000,
  title = {Homoclinic Bifurcation in a {{Hodgkin}}–{{Huxley}} Model of Thermally Sensitive Neurons},
  author = {Feudel, Ulrike and Neiman, Alexander and Pei, Xing and Wojtenek, Winfried and Braun, Hans and Huber, Martin and Moss, Frank},
  date = {2000-03},
  journaltitle = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  shortjournal = {Chaos},
  volume = {10},
  number = {1},
  pages = {231--239},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.166488},
  url = {http://aip.scitation.org/doi/10.1063/1.166488},
  urldate = {2023-03-26},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\PYIK9EJW\\Feudel et al. - 2000 - Homoclinic bifurcation in a Hodgkin–Huxley model o.pdf}
}

@online{FlexibleSensorimotorComputations,
  title = {Flexible {{Sensorimotor Computations}} through {{Rapid Reconfiguration}} of {{Cortical Dynamics}} | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.neuron.2018.05.020},
  url = {https://reader.elsevier.com/reader/sd/pii/S0896627318304185?token=5FA3AE0889BEA0A615026098CD694D65AE3DF0C7E4A66936B829635DA46C94BBB4947421033194AE29A30737D727A6A1&originRegion=eu-west-1&originCreation=20230331134150},
  urldate = {2023-03-31},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\NG67R6BX\\Flexible Sensorimotor Computations through Rapid R.pdf}
}

@article{frankleLOTTERYTICKETHYPOTHESIS2019,
  title = {{{THE LOTTERY TICKET HYPOTHESIS}}: {{FINDING SPARSE}}, {{TRAINABLE NEURAL NETWORKS}}},
  author = {Frankle, Jonathan and Carbin, Michael},
  date = {2019},
  pages = {42},
  abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\QJ9D5299\\Frankle and Carbin - 2019 - THE LOTTERY TICKET HYPOTHESIS FINDING SPARSE, TRA.pdf}
}

@article{ganguliOneDimensionalDynamicsAttention2008,
  title = {One-{{Dimensional Dynamics}} of {{Attention}} and {{Decision Making}} in {{LIP}}},
  author = {Ganguli, Surya and Bisley, James W. and Roitman, Jamie D. and Shadlen, Michael N. and Goldberg, Michael E. and Miller, Kenneth D.},
  date = {2008-04},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {58},
  number = {1},
  pages = {15--25},
  issn = {08966273},
  doi = {10.1016/j.neuron.2008.01.038},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627308001682},
  urldate = {2022-06-18},
  abstract = {Where we allocate our visual spatial attention depends upon a continual competition between internally generated goals and external distractions. Recently it was shown that single neurons in the macaque lateral intraparietal area (LIP) can predict the amount of time a distractor can shift the locus of spatial attention away from a goal. We propose that this remarkable dynamical correspondence between single neurons and attention can be explained by a network model in which generically high-dimensional firing-rate vectors rapidly decay to a single mode. We find direct experimental evidence for this model, not only in the original attentional task, but also in a very different task involving perceptual decision making. These results confirm a theoretical prediction that slowly varying activity patterns are proportional to spontaneous activity, pose constraints on models of persistent activity, and suggest a network mechanism for the emergence of robust behavioral timing from heterogeneous neuronal populations.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\F8ISII2H\\Ganguli et al. - 2008 - One-Dimensional Dynamics of Attention and Decision.pdf}
}

@article{gaoSimplicityComplexityBrave2015,
  title = {On Simplicity and Complexity in the Brave New World of Large-Scale Neuroscience},
  author = {Gao, Peiran and Ganguli, Surya},
  date = {2015-06},
  journaltitle = {Current Opinion in Neurobiology},
  shortjournal = {Current Opinion in Neurobiology},
  volume = {32},
  pages = {148--155},
  issn = {09594388},
  doi = {10.1016/j.conb.2015.04.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0959438815000768},
  urldate = {2023-02-26},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\HQCI9M25\\Gao and Ganguli - 2015 - On simplicity and complexity in the brave new worl.pdf}
}

@article{glorotUnderstandingDifficultyTraining,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  author = {Glorot, Xavier and Bengio, Yoshua},
  pages = {8},
  abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\43CMX8FI\\Glorot and Bengio - Understanding the difﬁculty of training deep feedf.pdf}
}

@article{goldNeuralBasisDecision2007,
  title = {The {{Neural Basis}} of {{Decision Making}}},
  author = {Gold, Joshua I. and Shadlen, Michael N.},
  date = {2007-07-01},
  journaltitle = {Annual Review of Neuroscience},
  shortjournal = {Annu. Rev. Neurosci.},
  volume = {30},
  number = {1},
  pages = {535--574},
  issn = {0147-006X, 1545-4126},
  doi = {10.1146/annurev.neuro.29.051605.113038},
  url = {https://www.annualreviews.org/doi/10.1146/annurev.neuro.29.051605.113038},
  urldate = {2022-06-18},
  abstract = {The study of decision making spans such varied fields as neuroscience, psychology, economics, statistics, political science, and computer science. Despite this diversity of applications, most decisions share common elements including deliberation and commitment. Here we evaluate recent progress in understanding how these basic elements of decision formation are implemented in the brain. We focus on simple decisions that can be studied in the laboratory but emphasize general principles likely to extend to other settings.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\9HRZEBIN\\Gold and Shadlen - 2007 - The Neural Basis of Decision Making.pdf}
}

@article{golubFixedPointFinderTensorflowToolbox2018,
  title = {{{FixedPointFinder}}: {{A Tensorflow}} Toolbox for Identifying and Characterizing Fixed Points in Recurrent Neural Networks},
  shorttitle = {{{FixedPointFinder}}},
  author = {Golub, Matthew and Sussillo, David},
  date = {2018-11-01},
  journaltitle = {Journal of Open Source Software},
  shortjournal = {JOSS},
  volume = {3},
  number = {31},
  pages = {1003},
  issn = {2475-9066},
  doi = {10.21105/joss.01003},
  url = {http://joss.theoj.org/papers/10.21105/joss.01003},
  urldate = {2022-06-18},
  abstract = {Recurrent neural networks (RNNs) are powerful function approximators that can be designed or trained to solve a variety of computational tasks. Such tasks require the transformation of a set of time-varying input signals into a set of time-varying output signals. Neuroscientists are increasingly interested in using RNNs to explain complex relationships present in recorded neural activity (Pandarinath et al., 2018) and to propose dynamical mechanisms through which a population of neurons might implement a computation (Mante, Sussillo, Shenoy, \& Newsome, 2013; Remington, Narain, Hosseini, \& Jazayeri, 2018). Once fit to neural recordings or trained to solve a task of interest, an RNN can be reverse-engineered to understand how a computation is implemented in a high-dimensional recurrent neural system, which can suggest hypotheses for how the task might be solved by the brain.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\GSVREJNA\\Golub and Sussillo - 2018 - FixedPointFinder A Tensorflow toolbox for identif.pdf}
}

@article{guestLogicalInferenceBrains,
  title = {On Logical Inference over Brains, Behaviour, and Artificial Neural Networks},
  author = {Guest, Olivia and Martin, Andrea E},
  abstract = {In the cognitive, computational, and neuro- sciences, we often reason about what models (viz., formal and/or computational) represent, learn, or “know”, as well as what algorithm they instantiate. The putative goal of such reasoning is to generalize claims about the model in question to claims about the mind and brain. This reasoning process typically presents as inference about the representations, processes, or algorithms the human mind and brain instantiate. Such inference is often based on a model’s performance on a task, and whether that performance approximates human behaviour or brain activity. The model in question is often an artificial neural network (ANN) model, though the problems we discuss are generalizable to all reasoning over models. Arguments typically take the form “the brain does what the ANN does because the ANN reproduced the pattern seen in brain activity” or “cognition works this way because the ANN learned to approximate task performance.” Then, the argument concludes that models achieve this outcome by doing what people do or having the capacities people have. At first blush, this might appear as a form of modus ponens, a valid deductive logical inference rule. However, as we explain in this article, this is not the case, and thus, this form of argument eventually results in affirming the consequent — a logical or inferential fallacy. We discuss what this means broadly for research in cognitive science, neuroscience, and psychology; what it means for models when they lose the ability to mediate between theory and data in a meaningful way; and what this means for the logic, the metatheoretical calculus, our fields deploy in high-level scientific inference.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\VCJ45HR3\\Guest and Martin - On logical inference over brains, behaviour, and a.pdf}
}

@article{hanksDistinctRelationshipsParietal2015,
  title = {Distinct Relationships of Parietal and Prefrontal Cortices to Evidence Accumulation},
  author = {Hanks, Timothy D. and Kopec, Charles D. and Brunton, Bingni W. and Duan, Chunyu A. and Erlich, Jeffrey C. and Brody, Carlos D.},
  date = {2015-04},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {520},
  number = {7546},
  pages = {220--223},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14066},
  url = {http://www.nature.com/articles/nature14066},
  urldate = {2022-06-18},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\HKUKRDDU\\Hanks et al. - 2015 - Distinct relationships of parietal and prefrontal .pdf}
}

@article{havivUnderstandingControllingMemory,
  title = {Understanding and {{Controlling Memory}} in {{Recurrent Neural Networks}}},
  author = {Haviv, Doron and Rivkind, Alexnader and Barak, Omri},
  abstract = {To be effective in sequential data processing, Recurrent Neural Networks (RNNs) are required to keep track of past events by creating memories. While the relation between memories and the network’s hidden state dynamics was established over the last decade, previous works in this direction were of a predominantly descriptive nature focusing mainly on locating the dynamical objects of interest. In particular, it remained unclear how dynamical observables affect the performance, how they form and whether they can be manipulated. Here, we utilize different training protocols, datasets and architectures to obtain a range of networks solving a delayed classification task with similar performance, alongside substantial differences in their ability to extrapolate for longer delays. We analyze the dynamics of the network’s hidden state, and uncover the reasons for this difference. Each memory is found to be associated with a nearly steady state of the dynamics which we refer to as a ’slow point’. Slow point speeds predict extrapolation performance across all datasets, protocols and architectures tested. Furthermore, by tracking the formation of the slow points we are able to understand the origin of differences between training protocols. Finally, we propose a novel regularization technique that is based on the relation between hidden state speeds and memory longevity. Our technique manipulates these speeds, thereby leading to a dramatic improvement in memory robustness over time, and could pave the way for a new class of regularization methods.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\4SE6IU2I\\Haviv et al. - Understanding and Controlling Memory in Recurrent .pdf}
}

@article{hopfieldPatternRecognitionComputation1995,
  title = {Pattern Recognition Computation Using Action Potential Timing for Stimulus Representation},
  author = {Hopfield, J. J.},
  date = {1995-07},
  journaltitle = {Nature},
  volume = {376},
  number = {6535},
  pages = {33--36},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/376033a0},
  url = {https://www.nature.com/articles/376033a0},
  urldate = {2023-03-25},
  abstract = {A computational model is described in which the sizes of variables are represented by the explicit times at which action potentials occur, rather than by the more usual 'firing rate' of neurons. The comparison of patterns over sets of analogue variables is done by a network using different delays for different information paths. This mode of computation explains how one scheme of neuroarchitecture can be used for very different sensory modalities and seemingly different computations. The oscillations and anatomy of the mammalian olfactory systems have a simple interpretation in terms of this representation, and relate to processing in the auditory system. Single-electrode recording would not detect such neural computing. Recognition 'units' in this style respond more like radial basis function units than elementary sigmoid units.},
  issue = {6535},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science}
}

@article{ijspeertCentralPatternGenerators2008,
  title = {Central Pattern Generators for Locomotion Control in Animals and Robots: {{A}} Review},
  shorttitle = {Central Pattern Generators for Locomotion Control in Animals and Robots},
  author = {Ijspeert, Auke Jan},
  date = {2008-05},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {21},
  number = {4},
  pages = {642--653},
  issn = {08936080},
  doi = {10.1016/j.neunet.2008.03.014},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608008000804},
  urldate = {2023-03-26},
  abstract = {The problem of controlling locomotion is an area in which neuroscience and robotics can fruitfully interact. In this article, I will review research carried out on locomotor central pattern generators (CPGs), i.e. neural circuits capable of producing coordinated patterns of high-dimensional rhythmic output signals while receiving only simple, low-dimensional, input signals. The review will first cover neurobiological observations concerning locomotor CPGs and their numerical modelling, with a special focus on vertebrates. It will then cover how CPG models implemented as neural networks or systems of coupled oscillators can be used in robotics for controlling the locomotion of articulated robots. The review also presents how robots can be used as scientific tools to obtain a better understanding of the functioning of biological CPGs. Finally, various methods for designing CPGs to control specific modes of locomotion will be briefly reviewed. In this process, I will discuss different types of CPG models, the pros and cons of using CPGs with robots, and the pros and cons of using robots as scientific tools. Open research topics both in biology and in robotics will also be discussed.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\A6QXKMFC\\Ijspeert - 2008 - Central pattern generators for locomotion control .pdf}
}

@article{jazayeriTemporalContextCalibrates2010,
  title = {Temporal Context Calibrates Interval Timing},
  author = {Jazayeri, Mehrdad and Shadlen, Michael N.},
  date = {2010-08},
  journaltitle = {Nature neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {13},
  number = {8},
  eprint = {20581842},
  eprinttype = {pmid},
  pages = {1020--1026},
  issn = {1097-6256},
  doi = {10.1038/nn.2590},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2916084/},
  urldate = {2023-03-30},
  abstract = {We use our sense of time to identify temporal relationships between events and to anticipate actions. How well we can exploit temporal contingencies depends on the variability of our measurements of time. We asked humans to reproduce time intervals drawn from different underlying distributions. As expected, production times were more variable for longer intervals. Surprisingly however, production times exhibited a systematic regression towards the mean. Consequently, estimates for a sample interval differed depending on the distribution from which it was drawn. A performance-optimizing Bayesian model that takes the underlying distribution of samples into account provided an accurate description of subjects’ performance, variability and bias. This finding suggests that the central nervous system incorporates knowledge about temporal uncertainty to adapt internal timing mechanisms to the temporal statistics of the environment.},
  pmcid = {PMC2916084},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\4LHDV872\\Jazayeri and Shadlen - 2010 - Temporal context calibrates interval timing.pdf}
}

@article{jordanGatedRecurrentUnits2021,
  title = {Gated {{Recurrent Units Viewed Through}} the {{Lens}} of {{Continuous Time Dynamical Systems}}},
  author = {Jordan, Ian D. and Sokół, Piotr Aleksander and Park, Il Memming},
  date = {2021-07-22},
  journaltitle = {Frontiers in Computational Neuroscience},
  shortjournal = {Front. Comput. Neurosci.},
  volume = {15},
  pages = {678158},
  issn = {1662-5188},
  doi = {10.3389/fncom.2021.678158},
  url = {https://www.frontiersin.org/articles/10.3389/fncom.2021.678158/full},
  urldate = {2022-06-18},
  abstract = {Gated recurrent units (GRUs) are specialized memory elements for building recurrent neural networks. Despite their incredible success on various tasks, including extracting dynamics underlying neural data, little is understood about the specific dynamics representable in a GRU network. As a result, it is both difficult to know a priori how successful a GRU network will perform on a given task, and also their capacity to mimic the underlying behavior of their biological counterparts. Using a continuous time analysis, we gain intuition on the inner workings of GRU networks. We restrict our presentation to low dimensions, allowing for a comprehensive visualization. We found a surprisingly rich repertoire of dynamical features that includes stable limit cycles (nonlinear oscillations), multi-stable dynamics with various topologies, and homoclinic bifurcations. At the same time we were unable to train GRU networks to produce continuous attractors, which are hypothesized to exist in biological neural networks. We contextualize the usefulness of different kinds of observed dynamics and support our claims experimentally.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\99THCC9U\\Jordan et al. - 2021 - Gated Recurrent Units Viewed Through the Lens of C.pdf}
}

@book{kaczynskiComputationalHomology2004,
  title = {Computational Homology},
  author = {Kaczynski, Tomasz and Mischaikow, Konstantin Michael and Mrozek, Marian},
  date = {2004},
  series = {Applied Mathematical Sciences},
  number = {v. 157},
  publisher = {{Springer}},
  location = {{New York}},
  isbn = {978-0-387-40853-8},
  langid = {english},
  pagetotal = {480},
  keywords = {Homology theory},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\A8GLIPM7\\Kaczynski et al. - 2004 - Computational homology.pdf}
}

@article{khona2022,
  title = {Attractor and Integrator Networks in the Brain},
  author = {Khona, Mikail and Fiete, Ila R.},
  date = {2022-12},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {23},
  number = {12},
  pages = {744--766},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/s41583-022-00642-0},
  url = {https://www.nature.com/articles/s41583-022-00642-0},
  urldate = {2023-03-25},
  abstract = {In this review, we describe the singular success of attractor neural network models in describing how the brain maintains persistent activity states for working memory, error-corrects, and integrates noisy cues. We consider the mechanisms by which simple and forgetful units can organize to collectively generate dynamics on the long time-scales required for such computations. We discuss the myriad potential uses of attractor dynamics for computation in the brain, and showcase notable examples of brain systems in which inherently low-dimensional continuous attractor dynamics have been concretely and rigorously identified. Thus, it is now possible to conclusively state that the brain constructs and uses such systems for computation. Finally, we look ahead by highlighting recent theoretical advances in understanding how the fundamental tradeoffs between robustness and capacity and between structure and flexibility can be overcome by reusing and recombining the same set of modular attractors for multiple functions, so they together produce representations that are structurally constrained and robust but exhibit high capacity and are flexible.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\4U2V2QSS\\Khona and Fiete - 2022 - Attractor and integrator networks in the brain.pdf}
}

@article{kimRingAttractorDynamics2017,
  title = {Ring Attractor Dynamics in the {{{\emph{Drosophila}}}} Central Brain},
  author = {Kim, Sung Soo and Rouault, Hervé and Druckmann, Shaul and Jayaraman, Vivek},
  date = {2017-05-26},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {356},
  number = {6340},
  pages = {849--853},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aal4835},
  url = {https://www.science.org/doi/10.1126/science.aal4835},
  urldate = {2023-03-28},
  abstract = {Representing direction in the fly                            A population of cells called compass neurons represents a fruitfly's heading direction. Kim               et al.               used imaging and optogenetics in behaving flies to elucidate the functional architecture of the underlying neuronal network. They observed local excitation and global inhibition between the compass neurons. The features of the network were best explained by a ring attractor network model. Until now, this hypothesized network structure has been difficult to demonstrate in a real brain.                                         Science               , this issue p.               849                        ,              A neuronal network in the fly brain uses global inhibition and local excitation to enforce an internal representation of heading direction.           ,              Ring attractors are a class of recurrent networks hypothesized to underlie the representation of heading direction. Such network structures, schematized as a ring of neurons whose connectivity depends on their heading preferences, can sustain a bump-like activity pattern whose location can be updated by continuous shifts along either turn direction. We recently reported that a population of fly neurons represents the animal’s heading via bump-like activity dynamics. We combined two-photon calcium imaging in head-fixed flying flies with optogenetics to overwrite the existing population representation with an artificial one, which was then maintained by the circuit with naturalistic dynamics. A network with local excitation and global inhibition enforces this unique and persistent heading representation. Ring attractor networks have long been invoked in theoretical work; our study provides physiological evidence of their existence and functional architecture.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\JTDZ5G6Z\\Kim et al. - 2017 - Ring attractor dynamics in the Drosophila c.pdf}
}

@book{kochLargescaleNeuronalTheories1994,
  title = {Large-Scale Neuronal Theories of the Brain},
  editor = {Koch, Christof and Davis, Joel L.},
  date = {1994},
  series = {Computational Neuroscience},
  publisher = {{MIT Press}},
  location = {{Cambridge, Mass}},
  isbn = {978-0-262-11183-6},
  langid = {english},
  pagetotal = {343},
  keywords = {Brain,Cerebral cortex,Cognitive neuroscience,Neural circuitry,Neurons},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\RGTWGQIS\\Koch and Davis - 1994 - Large-scale neuronal theories of the brain.pdf}
}

@book{kochLargescaleNeuronalTheories1994a,
  title = {Large-Scale Neuronal Theories of the Brain},
  editor = {Koch, Christof and Davis, Joel L.},
  date = {1994},
  series = {Computational Neuroscience},
  publisher = {{MIT Press}},
  location = {{Cambridge, Mass}},
  isbn = {978-0-262-11183-6},
  langid = {english},
  pagetotal = {343},
  keywords = {Brain,Cerebral cortex,Cognitive neuroscience,Neural circuitry,Neurons},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\CMTIFGVC\\Koch and Davis - 1994 - Large-scale neuronal theories of the brain.pdf}
}

@article{koulakovModelRobustNeural2002,
  title = {Model for a Robust Neural Integrator},
  author = {Koulakov, Alexei A. and Raghavachari, Sridhar and Kepecs, Adam and Lisman, John E.},
  date = {2002-08},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {5},
  number = {8},
  pages = {775--782},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn893},
  url = {http://www.nature.com/articles/nn893},
  urldate = {2022-06-18},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\8HWGPDJE\\Koulakov et al. - 2002 - Model for a robust neural integrator.pdf}
}

@article{langeConfirmationBiasPerceptual2021,
  title = {A Confirmation Bias in Perceptual Decision-Making Due to Hierarchical Approximate Inference},
  author = {Lange, Richard D. and Chattoraj, Ankani and Beck, Jeffrey M. and Yates, Jacob L. and Haefner, Ralf M.},
  editor = {Peters, Megan A. K.},
  date = {2021-11-29},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLoS Comput Biol},
  volume = {17},
  number = {11},
  pages = {e1009517},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1009517},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1009517},
  urldate = {2023-02-24},
  abstract = {Making good decisions requires updating beliefs according to new evidence. This is a dynamical process that is prone to biases: in some cases, beliefs become entrenched and resistant to new evidence (leading to primacy effects), while in other cases, beliefs fade over time and rely primarily on later evidence (leading to recency effects). How and why either type of bias dominates in a given context is an important open question. Here, we study this question in classic perceptual decision-making tasks, where, puzzlingly, previous empirical studies differ in the kinds of biases they observe, ranging from primacy to recency, despite seemingly equivalent tasks. We present a new model, based on hierarchical approximate inference and derived from normative principles, that not only explains both primacy and recency effects in existing studies, but also predicts how the type of bias should depend on the statistics of stimuli in a given task. We verify this prediction in a novel visual discrimination task with human observers, finding that each observer’s temporal bias changed as the result of changing the key stimulus statistics identified by our model. The key dynamic that leads to a primacy bias in our model is an overweighting of new sensory information that agrees with the observer’s existing belief—a type of ‘confirmation bias’. By fitting an extended drift-diffusion model to our data we rule out an alternative explanation for primacy effects due to bounded integration. Taken together, our results resolve a major discrepancy among existing perceptual decision-making studies, and suggest that a key source of bias in human decision-making is approximate hierarchical inference.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\N6P6Q9SZ\\Lange et al. - 2021 - A confirmation bias in perceptual decision-making .pdf}
}

@article{maheswaranathanUniversalityIndividualityNeural,
  title = {Universality and Individuality in Neural Dynamics across Large Populations of Recurrent Networks},
  author = {Maheswaranathan, Niru and Williams, Alex and Golub, Matthew and Ganguli, Surya and Sussillo, David},
  pages = {13},
  abstract = {Task-based modeling with recurrent neural networks (RNNs) has emerged as a popular way to infer the computational function of different brain regions. These models are quantitatively assessed by comparing the low-dimensional neural representations of the model with the brain, for example using canonical correlation analysis (CCA). However, the nature of the detailed neurobiological inferences one can draw from such efforts remains elusive. For example, to what extent does training neural networks to solve common tasks uniquely determine the network dynamics, independent of modeling architectural choices? Or alternatively, are the learned dynamics highly sensitive to different model choices? Knowing the answer to these questions has strong implications for whether and how we should use task-based RNN modeling to understand brain dynamics. To address these foundational questions, we study populations of thousands of networks, with commonly used RNN architectures, trained to solve neuroscientifically motivated tasks and characterize their nonlinear dynamics. We find the geometry of the RNN representations can be highly sensitive to different network architectures, yielding a cautionary tale for measures of similarity that rely on representational geometry, such as CCA. Moreover, we find that while the geometry of neural dynamics can vary greatly across architectures, the underlying computational scaffold—the topological structure of fixed points, transitions between them, limit cycles, and linearized dynamics—often appears universal across all architectures.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\S9IR2HXA\\Maheswaranathan et al. - Universality and individuality in neural dynamics .pdf}
}

@article{maheswaranathanUniversalityIndividualityNeurala,
  title = {Universality and Individuality in Neural Dynamics across Large Populations of Recurrent Networks},
  author = {Maheswaranathan, Niru and Williams, Alex and Golub, Matthew and Ganguli, Surya and Sussillo, David},
  pages = {13},
  abstract = {Task-based modeling with recurrent neural networks (RNNs) has emerged as a popular way to infer the computational function of different brain regions. These models are quantitatively assessed by comparing the low-dimensional neural representations of the model with the brain, for example using canonical correlation analysis (CCA). However, the nature of the detailed neurobiological inferences one can draw from such efforts remains elusive. For example, to what extent does training neural networks to solve common tasks uniquely determine the network dynamics, independent of modeling architectural choices? Or alternatively, are the learned dynamics highly sensitive to different model choices? Knowing the answer to these questions has strong implications for whether and how we should use task-based RNN modeling to understand brain dynamics. To address these foundational questions, we study populations of thousands of networks, with commonly used RNN architectures, trained to solve neuroscientifically motivated tasks and characterize their nonlinear dynamics. We find the geometry of the RNN representations can be highly sensitive to different network architectures, yielding a cautionary tale for measures of similarity that rely on representational geometry, such as CCA. Moreover, we find that while the geometry of neural dynamics can vary greatly across architectures, the underlying computational scaffold—the topological structure of fixed points, transitions between them, limit cycles, and linearized dynamics—often appears universal across all architectures.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\SBBSK4DX\\Maheswaranathan et al. - Universality and individuality in neural dynamics .pdf}
}

@article{manteContextdependentComputationRecurrent2013,
  title = {Context-Dependent Computation by Recurrent Dynamics in Prefrontal Cortex},
  author = {Mante, Valerio and Sussillo, David and Shenoy, Krishna V. and Newsome, William T.},
  date = {2013-11},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {503},
  number = {7474},
  pages = {78--84},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature12742},
  url = {http://www.nature.com/articles/nature12742},
  urldate = {2022-06-18},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\S9NP3PHB\\Mante et al. - 2013 - Context-dependent computation by recurrent dynamic.pdf}
}

@online{MechanisticModelsMachine,
  title = {Mechanistic Models versus Machine Learning, a Fight Worth Fighting for the Biological Community?},
  doi = {10.1098/rsbl.2017.0660},
  url = {https://royalsocietypublishing.org/doi/epdf/10.1098/rsbl.2017.0660},
  urldate = {2023-03-23},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\JVA6ZFYH\\Mechanistic models versus machine learning, a figh.pdf;C\:\\Users\\abel_\\Zotero\\storage\\7U6PVUVX\\rsbl.2017.html}
}

@article{meunierTheoryAlgorithmsImplementations,
  title = {A Theory of Algorithms and Implementations and Their Relevance to Cognitive Science},
  author = {Meunier, Anja and Markham, Alex and Grosse-Wentrup, Moritz},
  abstract = {The question of how algorithms in general and cognitive skills in particular are implemented by our nervous system is at the core of cognitive science. The notions of what it means for a physical system (such as our nervous system) to implement an algorithm, however, are surprisingly vague. We argue that a rigorous theory is needed to formulate and evaluate precise hypotheses about the brain’s cognitive functions and propose a definition of the term algorithm as a chain of functions. Subsequently, we define the term implementation via a sequence of projections from a dynamical system, represented by a Markov process, to the algorithm. We furthermore show the practical applicability of this approach in a simulated example. We believe that the theory proposed here contributes to bridging the gap between the algorithmic and the implementational level by rendering the task at hand theoretically precise.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\3IXMF5ZX\\Meunier et al. - A theory of algorithms and implementations and the.pdf}
}

@inproceedings{nassarLearningStructuredNeural2018,
  title = {Learning {{Structured Neural Dynamics From Single Trial Population Recording}}},
  booktitle = {2018 52nd {{Asilomar Conference}} on {{Signals}}, {{Systems}}, and {{Computers}}},
  author = {Nassar, Josue and Linderman, Scott W. and Zhao, Yuan and Bugallo, Monica and Park, Il Memming},
  date = {2018-10},
  pages = {666--670},
  publisher = {{IEEE}},
  location = {{Pacific Grove, CA, USA}},
  doi = {10.1109/ACSSC.2018.8645122},
  url = {https://ieeexplore.ieee.org/document/8645122/},
  urldate = {2023-03-29},
  abstract = {To understand the complex nonlinear dynamics of neural circuits, we fit a structured state-space model called tree-structured recurrent switching linear dynamical system (TrSLDS) to noisy high-dimensional neural time series. TrSLDS is a multi-scale hierarchical generative model for the state-space dynamics where each node of the latent tree captures locally linear dynamics. TrSLDS can be learned efficiently and in a fully Bayesian manner using Gibbs sampling. We showcase TrSLDS’ potential of inferring lowdimensional interpretable dynamical systems on a variety of examples.},
  eventtitle = {2018 52nd {{Asilomar Conference}} on {{Signals}}, {{Systems}}, and {{Computers}}},
  isbn = {978-1-5386-9218-9},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\2FHXE756\\Nassar et al. - 2018 - Learning Structured Neural Dynamics From Single Tr.pdf}
}

@online{NeuralPopulationGeometry,
  title = {Neural Population Geometry: {{An}} Approach for Understanding Biological and Artificial Neural Networks | {{Elsevier Enhanced Reader}}},
  shorttitle = {Neural Population Geometry},
  doi = {10.1016/j.conb.2021.10.010},
  url = {https://reader.elsevier.com/reader/sd/pii/S0959438821001227?token=FC6FF22F05125BE50AFEE8196F27F77EAD6394160B862BE8C3AFDE936641854E6B3A05597B3368C9494C41AC7531EC03&originRegion=eu-west-1&originCreation=20230306174946},
  urldate = {2023-03-06},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\7ZZBXTVG\\Neural population geometry An approach for unders.pdf}
}

@online{NeuroanatomicalUltrastructureFunction,
  title = {The {{Neuroanatomical Ultrastructure}} and {{Function}} of a {{Biological Ring Attractor}} | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.neuron.2020.08.006},
  url = {https://reader.elsevier.com/reader/sd/pii/S0896627320306139?token=D28A44ACE9B2FB0ABAD916372D64DA122D3DCA1BB621230B240F926AA89613CDA7C880DFB1E1DD59A559726FBA7A2076&originRegion=eu-west-1&originCreation=20230326101737},
  urldate = {2023-03-26},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\CF9692KZ\\The Neuroanatomical Ultrastructure and Function of.pdf}
}

@online{NeuroanatomicalUltrastructureFunctiona,
  title = {The {{Neuroanatomical Ultrastructure}} and {{Function}} of a {{Biological Ring Attractor}} | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.neuron.2020.08.006},
  url = {https://reader.elsevier.com/reader/sd/pii/S0896627320306139?token=D28A44ACE9B2FB0ABAD916372D64DA122D3DCA1BB621230B240F926AA89613CDA7C880DFB1E1DD59A559726FBA7A2076&originRegion=eu-west-1&originCreation=20230326101737},
  urldate = {2023-03-26},
  langid = {english}
}

@online{PIIS0896627303,
  title = {{{PII}}: {{S0896-6273}}(03)00255-1 | {{Elsevier Enhanced Reader}}},
  shorttitle = {{{PII}}},
  doi = {10.1016/S0896-6273(03)00255-1},
  url = {https://reader.elsevier.com/reader/sd/pii/S0896627303002551?token=59DB6371C4B5E61DDACEF8FED34216AC45757B50104ED7B1FB58292347D90BCEC7BD8526B13BFEA3A39D93377CAECCAE&originRegion=eu-west-1&originCreation=20230324113355},
  urldate = {2023-03-24},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\5A9S4NWI\\PII S0896-6273(03)00255-1  Elsevier Enhanced Rea.pdf}
}

@article{pollockEngineeringRecurrentNeural2020,
  title = {Engineering Recurrent Neural Networks from Task-Relevant Manifolds and Dynamics},
  author = {Pollock, Eli and Jazayeri, Mehrdad},
  editor = {Soltani, Alireza},
  date = {2020-08-12},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLoS Comput Biol},
  volume = {16},
  number = {8},
  pages = {e1008128},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008128},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1008128},
  urldate = {2023-02-26},
  abstract = {Many cognitive processes involve transformations of distributed representations in neural populations, creating a need for population-level models. Recurrent neural network models fulfill this need, but there are many open questions about how their connectivity gives rise to dynamics that solve a task. Here, we present a method for finding the connectivity of networks for which the dynamics are specified to solve a task in an interpretable way. We apply our method to a working memory task by synthesizing a network that implements a drift-diffusion process over a ring-shaped manifold. We also use our method to demonstrate how inputs can be used to control network dynamics for cognitive flexibility and explore the relationship between representation geometry and network capacity. Our work fits within the broader context of understanding neural computations as dynamics over relatively lowdimensional manifolds formed by correlated patterns of neurons.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\DK2JV8UB\\Pollock and Jazayeri - 2020 - Engineering recurrent neural networks from task-re.pdf}
}

@article{rabinovichGenerationReshapingSequences2006,
  title = {Generation and Reshaping of Sequences in Neural Systems},
  author = {Rabinovich, Mikhail I. and Huerta, Ramón and Varona, Pablo and Afraimovich, Valentin S.},
  date = {2006-12},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol Cybern},
  volume = {95},
  number = {6},
  pages = {519},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-006-0121-5},
  url = {https://link.springer.com/10.1007/s00422-006-0121-5},
  urldate = {2022-06-18},
  abstract = {The generation of informational sequences and their reorganization or reshaping is one of the most intriguing subjects for both neuroscience and the theory of autonomous intelligent systems. In spite of the diversity of sequential activities of sensory, motor, and cognitive neural systems, they have many similarities from the dynamical point of view. In this review we discus the ideas, models, and mathematical image of sequence generation and reshaping on different levels of the neural hierarchy, i.e., the role of a sensory network dynamics in the generation of a motor program (hunting swimming of marine mollusk Clione), olfactory dynamical coding, and sequential learning and decision making. Analysis of these phenomena is based on the winnerless competition principle. The considered models can be a basis for the design of biologically inspired autonomous intelligent systems.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\TT29U5EJ\\Rabinovich et al. - 2006 - Generation and reshaping of sequences in neural sy.pdf}
}

@article{rabinovichTransientCognitiveDynamics2008,
  title = {Transient {{Cognitive Dynamics}}, {{Metastability}}, and {{Decision Making}}},
  author = {Rabinovich, Mikhail I. and Huerta, Ramón and Varona, Pablo and Afraimovich, Valentin S.},
  editor = {Friston, Karl J.},
  date = {2008-05-02},
  journaltitle = {PLoS Computational Biology},
  shortjournal = {PLoS Comput Biol},
  volume = {4},
  number = {5},
  pages = {e1000072},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000072},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1000072},
  urldate = {2022-06-18},
  abstract = {The idea that cognitive activity can be understood using nonlinear dynamics has been intensively discussed at length for the last 15 years. One of the popular points of view is that metastable states play a key role in the execution of cognitive functions. Experimental and modeling studies suggest that most of these functions are the result of transient activity of large-scale brain networks in the presence of noise. Such transients may consist of a sequential switching between different metastable cognitive states. The main problem faced when using dynamical theory to describe transient cognitive processes is the fundamental contradiction between reproducibility and flexibility of transient behavior. In this paper, we propose a theoretical description of transient cognitive dynamics based on the interaction of functionally dependent metastable cognitive states. The mathematical image of such transient activity is a stable heteroclinic channel, i.e., a set of trajectories in the vicinity of a heteroclinic skeleton that consists of saddles and unstable separatrices that connect their surroundings. We suggest a basic mathematical model, a strongly dissipative dynamical system, and formulate the conditions for the robustness and reproducibility of cognitive transients that satisfy the competing requirements for stability and flexibility. Based on this approach, we describe here an effective solution for the problem of sequential decision making, represented as a fixed time game: a player takes sequential actions in a changing noisy environment so as to maximize a cumulative reward. As we predict and verify in computer simulations, noise plays an important role in optimizing the gain.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\CZPZV297\\Rabinovich et al. - 2008 - Transient Cognitive Dynamics, Metastability, and D.pdf}
}

@article{rajanRecurrentNetworkModels2016,
  title = {Recurrent {{Network Models}} of {{Sequence Generation}} and {{Memory}}},
  author = {Rajan, Kanaka and Harvey, Christopher~D. and Tank, David~W.},
  date = {2016-04},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {90},
  number = {1},
  pages = {128--142},
  issn = {08966273},
  doi = {10.1016/j.neuron.2016.02.009},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627316001021},
  urldate = {2022-06-18},
  abstract = {Sequential activation of neurons is a common feature of network activity during a variety of behaviors, including working memory and decision making. Previous network models for sequences and memory emphasized specialized architectures in which a principled mechanism is pre-wired into their connectivity. Here we demonstrate that, starting from random connectivity and modifying a small fraction of connections, a largely disordered recurrent network can produce sequences and implement working memory efficiently. We use this process, called Partial In-Network Training (PINning), to model and match cellular resolution imaging data from the posterior parietal cortex during a virtual memoryguided two-alternative forced-choice task. Analysis of the connectivity reveals that sequences propagate by the cooperation between recurrent synaptic interactions and external inputs, rather than through feedforward or asymmetric connections. Together our results suggest that neural sequences may emerge through learning from largely unstructured network architectures.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\CH69II4H\\Rajan et al. - 2016 - Recurrent Network Models of Sequence Generation an.pdf}
}

@article{ratcliffModeling2alternativeForcedchoice2018,
  title = {Modeling 2-Alternative Forced-Choice Tasks: {{Accounting}} for Both Magnitude and Difference Effects},
  shorttitle = {Modeling 2-Alternative Forced-Choice Tasks},
  author = {Ratcliff, Roger and Voskuilen, Chelsea and Teodorescu, Andrei},
  date = {2018-06},
  journaltitle = {Cognitive Psychology},
  shortjournal = {Cognitive Psychology},
  volume = {103},
  pages = {1--22},
  issn = {00100285},
  doi = {10.1016/j.cogpsych.2018.02.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S001002851830032X},
  urldate = {2022-06-18},
  abstract = {We present a model-based analysis of two-alternative choice tasks in which two stimuli are presented side by side and subjects must make a comparative judgment (e.g., which stimulus is brighter). Stimuli can vary on two dimensions, the difference in strength of the two stimuli and the magnitude of each stimulus. Differences between the two stimuli produce typical RT and accuracy effects (i.e., subjects respond more quickly and more accurately when there is a larger difference between the two). However, the overall magnitude of the pair of stimuli also affects RT and accuracy. In the more common two-choice task, a single stimulus is presented and the stimulus varies on only one dimension. In this two-stimulus task, if the standard diffusion decision model is fit to the data with only drift rate (evidence accumulation rate) differing among conditions, the model cannot fit the data. However, if either of one of two variability parameters is allowed to change with stimulus magnitude, the model can fit the data. This results in two models that are extremely constrained with about one tenth of the number of parameters than there are data points while at the same time the models account for accuracy and correct and error RT distributions. While both of these versions of the diffusion model can account for the observed data, the model that allows across-trial variability in drift to vary might be preferred for theoretical reasons. The diffusion model fits are compared to the leaky competing accumulator model which did not perform as well.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\DID8KTCF\\Ratcliff et al. - 2018 - Modeling 2-alternative forced-choice tasks Accoun.pdf}
}

@online{RepresentationalGeometryIntegrating,
  title = {Representational Geometry: Integrating Cognition, Computation, and the Brain | {{Elsevier Enhanced Reader}}},
  shorttitle = {Representational Geometry},
  doi = {10.1016/j.tics.2013.06.007},
  url = {https://reader.elsevier.com/reader/sd/pii/S1364661313001277?token=2B29750225A029CE9334CA1A16AC31A88B5B5B6718718D7EE9A9B5BD964FAA6B325022AD55C85F29AA31E346161635B0&originRegion=eu-west-1&originCreation=20230324195003},
  urldate = {2023-03-24},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\5W4R7QH4\\Representational geometry integrating cognition, .pdf}
}

@article{richmondHowComputationExplains,
  title = {How {{Computation Explains}}},
  author = {Richmond, Andrew},
  abstract = {I discuss the monumental shift in our understanding of the brain triggered by the project of computational cognitive science: the use of tools, concepts, and strategies from the computer sciences to investigate the brain. Philosophers have typically understood this project, and the computational explanations it provides, to assume that the brain is a computer, in a sense to be specified by the metaphysics of computation. That metaphysics, by revealing what exactly we attribute to the brain when we say it computes, is supposed to show how and why computational explanations work, and in doing so to provide a philosophical foundation for them. In contrast, I give an account of computational explanation that focuses on the resources computational explanations bring to bear on the study of the brain. I argue that computational explanations help cognitive scientists build perspicuous models that capture precisely the kinds of causal structures they seek, and that no metaphysics of computation is required to understand how they do this.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\GR9CPN3P\\Richmond - How Computation Explains.pdf}
}

@article{rigottiImportanceMixedSelectivity2013,
  title = {The Importance of Mixed Selectivity in Complex Cognitive Tasks},
  author = {Rigotti, Mattia and Barak, Omri and Warden, Melissa R. and Wang, Xiao-Jing and Daw, Nathaniel D. and Miller, Earl K. and Fusi, Stefano},
  date = {2013-05},
  journaltitle = {Nature},
  volume = {497},
  number = {7451},
  pages = {585--590},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature12160},
  url = {https://www.nature.com/articles/nature12160},
  urldate = {2022-08-31},
  abstract = {Single-neuron activity in the prefrontal cortex (PFC) is tuned to mixtures of multiple task-related aspects. Such mixed selectivity is highly heterogeneous, seemingly disordered and therefore difficult to interpret. We analysed the neural activity recorded in monkeys during an object sequence memory task to identify a role of mixed selectivity in subserving the cognitive functions ascribed to the PFC. We show that mixed selectivity neurons encode distributed information about all task-relevant aspects. Each aspect can be decoded from the population of neurons even when single-cell selectivity to that aspect is eliminated. Moreover, mixed selectivity offers a significant computational advantage over specialized responses in terms of the repertoire of input–output functions implementable by readout neurons. This advantage originates from the highly diverse nonlinear selectivity to mixtures of task-relevant variables, a signature of high-dimensional neural representations. Crucially, this dimensionality is predictive of animal behaviour as it collapses in error trials. Our findings recommend a shift of focus for future studies from neurons that have easily interpretable response tuning to the widely observed, but rarely analysed, mixed selectivity neurons.},
  issue = {7451},
  langid = {english},
  keywords = {Cognitive control,Neural encoding},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\7V74CTT6\\Rigotti et al. - 2013 - The importance of mixed selectivity in complex cog.pdf;C\:\\Users\\abel_\\Zotero\\storage\\IFZTFT7B\\nature12160.html}
}

@article{samsonovichPathIntegrationCognitive1997,
  title = {Path {{Integration}} and {{Cognitive Mapping}} in a {{Continuous Attractor Neural Network Model}}},
  author = {Samsonovich, Alexei and McNaughton, Bruce L.},
  date = {1997-08-01},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {17},
  number = {15},
  pages = {5900--5920},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.17-15-05900.1997},
  url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.17-15-05900.1997},
  urldate = {2023-03-27},
  abstract = {A minimal synaptic architecture is proposed for how the brain might perform path integration by computing the next internal representation of self-location from the current representation and from the perceived velocity of motion. In the model, a place-cell assembly called a “chart” contains a two-dimensional attractor set called an “attractor map” that can be used to represent coordinates in any arbitrary environment, once associative binding has occurred between chart locations and sensory inputs. In hippocampus, there are different spatial relations among place fields in different environments and behavioral contexts. Thus, the same units may participate in many charts, and it is shown that the number of uncorrelated charts that can be encoded in the same recurrent network is potentially quite large. According to this theory, the firing of a given place cell is primarily a cooperative effect of the activity of its neighbors on the currently active chart. Therefore, it is not particularly useful to think of place cells as encoding any particular external object or event. Because of its recurrent connections, hippocampal field CA3 is proposed as a possible location for this “multichart” architecture; however, other implementations in anatomy would not invalidate the main concepts. The model is implemented numerically both as a network of integrate-and-fire units and as a “macroscopic” (with respect to the space of states) description of the system, based on a continuous approximation defined by a system of stochastic differential equations. It provides an explanation for a number of hitherto perplexing observations on hippocampal place fields, including doubling, vanishing, reshaping in distorted environments, acquiring directionality in a two-goal shuttling task, rapid formation in a novel environment, and slow rotation after disorientation. The model makes several new predictions about the expected properties of hippocampal place cells and other cells of the proposed network.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\2GNULZHJ\\Samsonovich and McNaughton - 1997 - Path Integration and Cognitive Mapping in a Contin.pdf}
}

@article{seeholzerStabilityWorkingMemory2019,
  title = {Stability of Working Memory in Continuous Attractor Networks under the Control of Short-Term Plasticity},
  author = {Seeholzer, Alexander and Deger, Moritz and Gerstner, Wulfram},
  editor = {Burak, Yoram},
  date = {2019-04-19},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLoS Comput Biol},
  volume = {15},
  number = {4},
  pages = {e1006928},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006928},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1006928},
  urldate = {2023-03-06},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\UNU8EWK9\\Seeholzer et al. - 2019 - Stability of working memory in continuous attracto.pdf}
}

@article{seung1996,
  title = {How the Brain Keeps the Eyes Still},
  author = {Seung, H. S.},
  date = {1996-11-12},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {93},
  number = {23},
  pages = {13339--13344},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.93.23.13339},
  url = {https://pnas.org/doi/full/10.1073/pnas.93.23.13339},
  urldate = {2023-03-06},
  abstract = {The brain can hold the eyes still because it stores a memory of eye position. The brain’s memory of horizontal eye position appears to be represented by persistent neural activity in a network known as the neural integrator, which is localized in the brainstem and cerebellum. Existing experimental data are reinterpreted as evidence for an ‘‘attractor hypothesis’’ that the persistent patterns of activity observed in this network form an attractive line of fixed points in its state space. Line attractor dynamics can be produced in linear or nonlinear neural networks by learning mechanisms that precisely tune positive feedback.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\YAQ8MLQ7\\Seung - 1996 - How the brain keeps the eyes still.pdf}
}

@article{seung1998,
  title = {Continuous Attractors and Oculomotor Control},
  author = {Sebastian Seung, H.},
  date = {1998-10},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {11},
  number = {7-8},
  pages = {1253--1258},
  issn = {08936080},
  doi = {10.1016/S0893-6080(98)00064-1},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608098000641},
  urldate = {2023-03-06},
  abstract = {A recurrent neural network can possess multiple stable states, a property that many brain theories have implicated in learning and memory. There is good evidence for such multistability in the brainstem neural network that controls eye position. Because the stable states are arranged in a continuous dynamical attractor, the network can store a memory of eye position with analog neural encoding. Continuous attractors in model networks depend on precisely tuned positive feedback, and their robust maintenance requires mechanisms of synaptic plasticity. These ideas may have wider scope than just the oculomotor system. More generally, the internal models postulated by theories of biological motor control may be recurrent networks with continuous attractors. ᭧ 1998 Published by Elsevier Science Ltd. All rights reserved.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\JBLU5X5N\\Sebastian Seung - 1998 - Continuous attractors and oculomotor control.pdf}
}

@article{shadlenMotionPerceptionSeeing1996,
  title = {Motion Perception: Seeing and Deciding.},
  shorttitle = {Motion Perception},
  author = {Shadlen, M N and Newsome, W T},
  date = {1996-01-23},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {93},
  number = {2},
  pages = {628--633},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.93.2.628},
  url = {https://pnas.org/doi/full/10.1073/pnas.93.2.628},
  urldate = {2022-06-18},
  abstract = {The primate visual system offers unprecedented opportunities for investigating the neural basis of cognition. Even the simplest visual discrimination task requires processing of sensory signals, formation of a decision, and orchestration of a motor response. With our extensive knowledge of the primate visual and oculomotor systems as a base, it is now possible to investigate the neural basis of simple visual decisions that link sensation to action. Here we describe an initial study of neural responses in the lateral intraparietal area (LIP) of the cerebral cortex while alert monkeys discriminated the direction ofmotion in a visual display. A subset of LIP neurons carried high-level signals that may comprise a neural correlate of the decision process in our task. These signals are neither sensory nor motor in the strictest sense; rather they appear to reflect integration of sensory signals toward a decision appropriate for guiding movement. If this ultimately proves to be the case, several fascinating issues in cognitive neuroscience will be brought under rigorous physiological scrutiny.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\EWT5KIWI\\Shadlen and Newsome - 1996 - Motion perception seeing and deciding..pdf}
}

@article{shagrirMarrComputationalLevelTheories2010,
  title = {Marr on {{Computational-Level Theories}}},
  author = {Shagrir, Oron},
  date = {2010-10},
  journaltitle = {Philosophy of Science},
  shortjournal = {Philos. of Sci.},
  volume = {77},
  number = {4},
  pages = {477--500},
  issn = {0031-8248, 1539-767X},
  doi = {10.1086/656005},
  url = {https://www.cambridge.org/core/product/identifier/S0031824800013052/type/journal_article},
  urldate = {2023-03-23},
  abstract = {According to Marr, a computational-level theory consists of two elements, the               what               and the               why               . This article highlights the distinct role of the Why element in the computational analysis of vision. Three theses are advanced: (               a               ) that the Why element plays an explanatory role in computational-level theories, (               b               ) that its goal is to explain why the computed function (specified by the What element) is appropriate for a given visual task, and (               c               ) that the explanation consists in showing that the functional relations between the representing cells are similar to the “external” mathematical relations between the entities that these cells represent.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\G8N5SNHF\\Shagrir - 2010 - Marr on Computational-Level Theories.pdf}
}

@article{sussilloOpeningBlackBox2013,
  title = {Opening the {{Black Box}}: {{Low-Dimensional Dynamics}} in {{High-Dimensional Recurrent Neural Networks}}},
  shorttitle = {Opening the {{Black Box}}},
  author = {Sussillo, David and Barak, Omri},
  date = {2013-03},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {25},
  number = {3},
  pages = {626--649},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00409},
  url = {https://direct.mit.edu/neco/article/25/3/626-649/7854},
  urldate = {2022-06-18},
  abstract = {Recurrent neural networks (RNNs) are useful tools for learning nonlinear relationships between time-varying inputs and outputs with complex temporal dependencies. Recently developed algorithms have been successful at training RNNs to perform a wide variety of tasks, but the resulting networks have been treated as black boxes: their mechanism of operation remains unknown. Here we explore the hypothesis that fixed points, both stable and unstable, and the linearized dynamics around them, can reveal crucial aspects of how RNNs implement their computations. Further, we explore the utility of linearization in areas of phase space that are not true fixed points but merely points of very slow movement. We present a simple optimization technique that is applied to trained RNNs to find the fixed and slow points of their dynamics. Linearization around these slow regions can be used to explore, or reverse-engineer, the behavior of the RNN. We describe the technique, illustrate it using simple examples, and finally showcase it on three high-dimensional RNN examples: a 3-bit flip-flop device, an input-dependent sine wave generator, and a two-point moving average. In all cases, the mechanisms of trained networks could be inferred from the sets of fixed and slow points and the linearized dynamics around them.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\ASTVQ7FI\\Sussillo and Barak - 2013 - Opening the Black Box Low-Dimensional Dynamics in.pdf}
}

@article{tsodyksAttractorNeuralNetwork1999,
  title = {Attractor Neural Network Models of Spatial Maps in Hippocampus},
  author = {Tsodyks, Misha},
  date = {1999},
  journaltitle = {Hippocampus},
  shortjournal = {Hippocampus},
  volume = {9},
  number = {4},
  pages = {481--489},
  issn = {1050-9631, 1098-1063},
  doi = {10.1002/(SICI)1098-1063(1999)9:4<481::AID-HIPO14>3.0.CO;2-S},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/(SICI)1098-1063(1999)9:4<481::AID-HIPO14>3.0.CO;2-S},
  urldate = {2023-03-25},
  langid = {english}
}

@article{valenteProbingRelationshipLatent2022,
  title = {Probing the {{Relationship Between Latent Linear Dynamical Systems}} and {{Low-Rank Recurrent Neural Network Models}}},
  author = {Valente, Adrian and Ostojic, Srdjan and Pillow, Jonathan W.},
  date = {2022-08-16},
  journaltitle = {Neural Computation},
  volume = {34},
  number = {9},
  pages = {1871--1892},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco_a_01522},
  url = {https://direct.mit.edu/neco/article/34/9/1871/112382/Probing-the-Relationship-Between-Latent-Linear},
  urldate = {2022-09-19},
  abstract = {Abstract             A large body of work has suggested that neural populations exhibit low-dimensional dynamics during behavior. However, there are a variety of different approaches for modeling low-dimensional neural population activity. One approach involves latent linear dynamical system (LDS) models, in which population activity is described by a projection of low-dimensional latent variables with linear dynamics. A second approach involves low-rank recurrent neural networks (RNNs), in which population activity arises directly from a low-dimensional projection of past activity. Although these two modeling approaches have strong similarities, they arise in different contexts and tend to have different domains of application. Here we examine the precise relationship between latent LDS models and linear low-rank RNNs. When can one model class be converted to the other, and vice versa? We show that latent LDS models can only be converted to RNNs in specific limit cases, due to the non-Markovian property of latent LDS models. Conversely, we show that linear RNNs can be mapped onto LDS models, with latent dimensionality at most twice the rank of the RNN. A surprising consequence of our results is that a partially observed RNN is better represented by an LDS model than by an RNN consisting of only observed units.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\BNLC3Z6W\\Valente-2022-Probing the Relationship Between Latent Linear Dynamical Systems and Low-rank Recurrent Neural Network Models.pdf}
}

@article{vyasComputationNeuralPopulation2020,
  title = {Computation {{Through Neural Population Dynamics}}},
  author = {Vyas, Saurabh and Golub, Matthew D. and Sussillo, David and Shenoy, Krishna V.},
  date = {2020-07-08},
  journaltitle = {Annual Review of Neuroscience},
  shortjournal = {Annu. Rev. Neurosci.},
  volume = {43},
  number = {1},
  pages = {249--275},
  issn = {0147-006X, 1545-4126},
  doi = {10.1146/annurev-neuro-092619-094115},
  url = {https://www.annualreviews.org/doi/10.1146/annurev-neuro-092619-094115},
  urldate = {2022-06-18},
  abstract = {Significant experimental, computational, and theoretical work has identified rich structure within the coordinated activity of interconnected neural populations. An emerging challenge now is to uncover the nature of the associated computations, how they are implemented, and what role they play in driving behavior. We term this computation through neural population dynamics. If successful, this framework will reveal general motifs of neural population activity and quantitatively describe how neural population dynamics implement computations necessary for driving goal-directed behavior. Here, we start with a mathematical primer on dynamical systems theory and analytical tools necessary to apply this perspective to experimental data. Next, we highlight some recent discoveries resulting from successful application of dynamical systems. We focus on studies spanning motor control, timing, decision-making, and working memory. Finally, we briefly discuss promising recent lines of investigation and future directions for the computation through neural population dynamics framework.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\KM55JPFM\\Vyas et al. - 2020 - Computation Through Neural Population Dynamics.pdf}
}

@article{wangDecisionMakingRecurrent2008,
  title = {Decision {{Making}} in {{Recurrent Neuronal Circuits}}},
  author = {Wang, Xiao-Jing},
  date = {2008-10},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {60},
  number = {2},
  pages = {215--234},
  issn = {08966273},
  doi = {10.1016/j.neuron.2008.09.034},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627308008362},
  urldate = {2022-06-18},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\FERB7VJA\\Wang - 2008 - Decision Making in Recurrent Neuronal Circuits.pdf}
}

@article{weinbergerStaticDynamicHybridityDynamical2022,
  title = {Static-{{Dynamic Hybridity}} in {{Dynamical Models}} of {{Cognition}}},
  author = {Weinberger, Naftali and Allen, Colin},
  date = {2022-04},
  journaltitle = {Philosophy of Science},
  shortjournal = {Philos. sci.},
  volume = {89},
  number = {2},
  pages = {283--301},
  issn = {0031-8248, 1539-767X},
  doi = {10.1017/psa.2021.27},
  url = {https://www.cambridge.org/core/product/identifier/S0031824821000271/type/journal_article},
  urldate = {2023-03-23},
  abstract = {Dynamical models of cognition have played a central role in recent cognitive science. In this paper, we consider a common strategy by which dynamical models describe their target systems neither as purely static nor as purely dynamic, but rather using a hybrid approach. This hybridity reveals how dynamical models involve representational choices that are important for understanding the relationship between dynamical and non-dynamical representations of a system.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\EGSQE5CW\\Weinberger and Allen - 2022 - Static-Dynamic Hybridity in Dynamical Models of Co.pdf}
}

@online{whittingtonDisentanglingBiologicalConstraints2022,
  title = {Disentangling with {{Biological Constraints}}: {{A Theory}} of {{Functional Cell Types}}},
  shorttitle = {Disentangling with {{Biological Constraints}}},
  author = {Whittington, James C. R. and Dorrell, Will and Ganguli, Surya and Behrens, Timothy E. J.},
  date = {2022-09-30},
  eprint = {2210.01768},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  url = {http://arxiv.org/abs/2210.01768},
  urldate = {2023-03-22},
  abstract = {Neurons in the brain are often finely tuned for specific task variables. Moreover, such disentangled representations are highly sought after in machine learning. Here we mathematically prove that simple biological constraints on neurons, namely nonnegativity and energy efficiency in both activity and weights, promote such sought after disentangled representations by enforcing neurons to become selective for single factors of task variation. We demonstrate these constraints lead to disentangling in a variety of tasks and architectures, including variational autoencoders. We also use this theory to explain why the brain partitions its cells into distinct cell types such as grid and object-vector cells, and also explain when the brain instead entangles representations in response to entangled task factors. Overall, this work provides a mathematical understanding of why, when, and how neurons represent factors in both brains and machines, and is a first step towards understanding of how task demands structure neural representations.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\SFXYW39C\\Whittington et al. - 2022 - Disentangling with Biological Constraints A Theor.pdf;C\:\\Users\\abel_\\Zotero\\storage\\YAM7RFAH\\2210.html}
}

@article{wimmerBumpAttractorDynamics2014,
  title = {Bump Attractor Dynamics in Prefrontal Cortex Explains Behavioral Precision in Spatial Working Memory},
  author = {Wimmer, Klaus and Nykamp, Duane Q and Constantinidis, Christos and Compte, Albert},
  date = {2014-03},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {17},
  number = {3},
  pages = {431--439},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3645},
  url = {http://www.nature.com/articles/nn.3645},
  urldate = {2023-03-25},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\JW3WSEW2\\Wimmer et al. - 2014 - Bump attractor dynamics in prefrontal cortex expla.pdf}
}

@article{wongNeuralCircuitDynamics2007,
  title = {Neural Circuit Dynamics Underlying Accumulation of Time-Varying Evidence during Perceptual Decision Making},
  author = {Wong, Kong-Fatt},
  date = {2007},
  journaltitle = {Frontiers in Computational Neuroscience},
  shortjournal = {Front. Comput. Neurosci.},
  volume = {1},
  issn = {16625188},
  doi = {10.3389/neuro.10.006.2007},
  url = {http://journal.frontiersin.org/article/10.3389/neuro.10.006.2007/abstract},
  urldate = {2022-06-18},
  abstract = {How do neurons in a decision circuit integrate time-varying signals, in favor of or against alternative choice options? To address this question, we used a recurrent neural circuit model to simulate an experiment in which monkeys performed a direction-discrimination task on a visual motion stimulus. In a recent study, it was found that brief pulses of motion perturbed neural activity in the lateral intraparietal area (LIP), and exerted corresponding effects on the monkey’s choices and response times. Our model reproduces the behavioral observations and replicates LIP activity which, depending on whether the direction of the pulse is the same or opposite to that of a preferred motion stimulus, increases or decreases persistently over a few hundred milliseconds. Furthermore, our model accounts for the observation that the pulse exerts a weaker influence on LIP neuronal responses when the pulse is late relative to motion stimulus onset. We show that this violation of time-shift invariance (TSI) is consistent with a recurrent circuit mechanism of time integration. We further examine time integration using two consecutive pulses of the same or opposite motion directions. The induced changes in the performance are not additive, and the second of the paired pulses is less effective than its standalone impact, a prediction that is experimentally testable. Taken together, these findings lend further support for an attractor network model of time integration in perceptual decision making.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\EHWIXBVQ\\Wong - 2007 - Neural circuit dynamics underlying accumulation of.pdf}
}

@article{wongNeuralCircuitDynamics2007a,
  title = {Neural Circuit Dynamics Underlying Accumulation of Time-Varying Evidence during Perceptual Decision Making},
  author = {Wong, Kong-Fatt},
  date = {2007},
  journaltitle = {Frontiers in Computational Neuroscience},
  shortjournal = {Front. Comput. Neurosci.},
  volume = {1},
  issn = {16625188},
  doi = {10.3389/neuro.10.006.2007},
  url = {http://journal.frontiersin.org/article/10.3389/neuro.10.006.2007/abstract},
  urldate = {2022-06-18},
  abstract = {How do neurons in a decision circuit integrate time-varying signals, in favor of or against alternative choice options? To address this question, we used a recurrent neural circuit model to simulate an experiment in which monkeys performed a direction-discrimination task on a visual motion stimulus. In a recent study, it was found that brief pulses of motion perturbed neural activity in the lateral intraparietal area (LIP), and exerted corresponding effects on the monkey’s choices and response times. Our model reproduces the behavioral observations and replicates LIP activity which, depending on whether the direction of the pulse is the same or opposite to that of a preferred motion stimulus, increases or decreases persistently over a few hundred milliseconds. Furthermore, our model accounts for the observation that the pulse exerts a weaker influence on LIP neuronal responses when the pulse is late relative to motion stimulus onset. We show that this violation of time-shift invariance (TSI) is consistent with a recurrent circuit mechanism of time integration. We further examine time integration using two consecutive pulses of the same or opposite motion directions. The induced changes in the performance are not additive, and the second of the paired pulses is less effective than its standalone impact, a prediction that is experimentally testable. Taken together, these findings lend further support for an attractor network model of time integration in perceptual decision making.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\NMMNTAZ3\\Wong - 2007 - Neural circuit dynamics underlying accumulation of.pdf}
}

@article{wongRecurrentNetworkMechanism2006,
  title = {A {{Recurrent Network Mechanism}} of {{Time Integration}} in {{Perceptual Decisions}}},
  author = {Wong, Kong-Fatt and Wang, Xiao-Jing},
  date = {2006-01-25},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {26},
  number = {4},
  pages = {1314--1328},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3733-05.2006},
  url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.3733-05.2006},
  urldate = {2023-03-28},
  abstract = {Recent physiological studies using behaving monkeys revealed that, in a two-alternative forced-choice visual motion discrimination task, reaction time was correlated with ramping of spike activity of lateral intraparietal cortical neurons. The ramping activity appears to reflect temporal accumulation, on a timescale of hundreds of milliseconds, of sensory evidence before a decision is reached. To elucidate the cellular and circuit basis of such integration times, we developed and investigated a simplified two-variable version of a biophysically realistic cortical network model of decision making. In this model, slow time integration can be achieved robustly if excitatory reverberation is primarily mediated by NMDA receptors; our model with only fast AMPA receptors at recurrent synapses produces decision times that are not comparable with experimental observations. Moreover, we found two distinct modes of network behavior, in which decision computation by winner-take-all competition is instantiated with or without attractor states for working memory. Decision process is closely linked to the local dynamics, in the “decision space” of the system, in the vicinity of an unstable saddle steady state that separates the basins of attraction for the two alternative choices. This picture provides a rigorous and quantitative explanation for the dependence of performance and response time on the degree of task difficulty, and the reason for which reaction times are longer in error trials than in correct trials as observed in the monkey experiment. Our reduced two-variable neural model offers a simple yet biophysically plausible framework for studying perceptual decision making in general.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\9TVLNBN6\\Wong and Wang - 2006 - A Recurrent Network Mechanism of Time Integration .pdf}
}

@article{wongTemporalDynamicsUnderlying2008,
  title = {Temporal Dynamics Underlying Perceptual Decision Making: {{Insights}} from the Interplay between an Attractor Model and Parietal Neurophysiology},
  shorttitle = {Temporal Dynamics Underlying Perceptual Decision Making},
  author = {Wong, Kong-Fatt},
  date = {2008-12-15},
  journaltitle = {frontiers in Neuroscience},
  shortjournal = {Front. Neurosci.},
  volume = {2},
  number = {2},
  pages = {245--254},
  issn = {16624548},
  doi = {10.3389/neuro.01.028.2008},
  url = {http://journal.frontiersin.org/article/10.3389/neuro.01.028.2008/abstract},
  urldate = {2022-06-18},
  abstract = {Recent neurophysiological studies in awake, behaving primates have revealed that neurons in certain brain areas appear to integrate sensory evidence over time during the performance of perceptual decision-making tasks. Neurons in the lateral intraparietal area (LIP) of rhesus monkeys exhibit such decision-related signals while the animals view and judge the direction of a visual motion display. Further investigation of this temporal integration process using brief perturbations of the sensory evidence has suggested that LIP neurons do not integrate evidence in a perfect, linear manner. We describe how a biophysically-plausible attractor network model can account for many aspects of the temporal dynamics of neural activity during these perceptual decisions. We also review a larger set of models and explain how the dynamics during and after temporal integration can help to distinguish the underlying neural mechanisms. Finally, we propose some crucial theoretically-motivated experiments that are needed to test among models.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\PPZDP4EU\\Wong - 2008 - Temporal dynamics underlying perceptual decision m.pdf}
}

@article{wuComputingContinuousAttractors2005,
  title = {Computing with {{Continuous Attractors}}: {{Stability}} and {{Online Aspects}}},
  shorttitle = {Computing with {{Continuous Attractors}}},
  author = {Wu, Si and Amari, Shun-ichi},
  date = {2005-10-01},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {17},
  number = {10},
  pages = {2215--2239},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/0899766054615626},
  url = {https://direct.mit.edu/neco/article/17/10/2215-2239/6959},
  urldate = {2023-03-25},
  abstract = {Two issues concerning the application of continuous attractors in neural systems are investigated: the computational robustness of continuous attractors with respect to input noises and the implementation of Bayesian online decoding. In a perfect mathematical model for continuous attractors, decoding results for stimuli are highly sensitive to input noises, and this sensitivity is the inevitable consequence of the system's neutral stability. To overcome this shortcoming, we modify the conventional network model by including extra dynamical interactions between neurons. These interactions vary according to the biologically plausible Hebbian learning rule and have the computational role of memorizing and propagating stimulus information accumulated with time. As a result, the new network model responds to the history of external inputs over a period of time, and hence becomes insensitive to short-term fluctuations. Also, since dynamical interactions provide a mechanism to convey the prior knowledge of stimulus, that is, the information of the stimulus presented previously, the network effectively implements online Bayesian inference. This study also reveals some interesting behavior in neural population coding, such as the trade-off between decoding stability and the speed of tracking time-varying stimuli, and the relationship between neural tuning width and the tracking speed.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\U7ETNZW5\\Wu and Amari - 2005 - Computing with Continuous Attractors Stability an.pdf}
}

@article{wuContinuousAttractorNeural2016,
  title = {Continuous {{Attractor Neural Networks}}: {{Candidate}} of a {{Canonical Model}} for {{Neural Information Representation}}},
  shorttitle = {Continuous {{Attractor Neural Networks}}},
  author = {Wu, Si and Wong, K Y Michael and Fung, C C Alan and Mi, Yuanyuan and Zhang, Wenhao},
  date = {2016-02-10},
  journaltitle = {F1000Research},
  shortjournal = {F1000Res},
  volume = {5},
  eprint = {26937278},
  eprinttype = {pmid},
  pages = {F1000 Faculty Rev-156},
  issn = {2046-1402},
  doi = {10.12688/f1000research.7387.1},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4752021/},
  urldate = {2023-03-25},
  abstract = {Owing to its many computationally desirable properties, the model of continuous attractor neural networks (CANNs) has been successfully applied to describe the encoding of simple continuous features in neural systems, such as orientation, moving direction, head direction, and spatial location of objects. Recent experimental and computational studies revealed that complex features of external inputs may also be encoded by low-dimensional CANNs embedded in the high-dimensional space of neural population activity. The new experimental data also confirmed the existence of the M-shaped correlation between neuronal responses, which is a correlation structure associated with the unique dynamics of CANNs. This body of evidence, which is reviewed in this report, suggests that CANNs may serve as a canonical model for neural information representation.},
  pmcid = {PMC4752021},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\GHHRVBZ7\\Wu et al. - 2016 - Continuous Attractor Neural Networks Candidate of.pdf}
}

@article{zhaoInterpretableNonlinearDynamic,
  title = {Interpretable {{Nonlinear Dynamic Modeling}} of {{Neural Trajectories}}},
  author = {Zhao, Yuan and Park, Il Memming},
  pages = {9},
  abstract = {A central challenge in neuroscience is understanding how neural system implements computation through its dynamics. We propose a nonlinear time series model aimed at characterizing interpretable dynamics from neural trajectories. Our model assumes low-dimensional continuous dynamics in a finite volume. It incorporates a prior assumption about globally contractional dynamics to avoid overly enthusiastic extrapolation outside of the support of observed trajectories. We show that our model can recover qualitative features of the phase portrait such as attractors, slow points, and bifurcations, while also producing reliable longterm future predictions in a variety of dynamical models and in real neural data.},
  langid = {english},
  file = {C\:\\Users\\abel_\\Zotero\\storage\\RRNFK5FA\\Zhao and Park - Interpretable Nonlinear Dynamic Modeling of Neural.pdf}
}
