%\documentclass{article}
\documentclass{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage{amssymb, amsmath, amsthm}
\usepackage{thmtools, mathtools, mathrsfs}
\usepackage{amsfonts}
\usepackage[sort&compress,numbers]{natbib}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{float}
\usepackage{bm}
\usepackage{tikz}
\usetikzlibrary{positioning,matrix,arrows,decorations.pathmorphing}
\usepackage{tikz-cd} 
\definecolor{processblue}{cmyk}{0.8,0,0,0}
\definecolor{mpcolor}{rgb}{1, 0.1, 0.59}
\newcommand{\mpcomment}[1]{(\textbf{MP:\ }\textcolor{mpcolor}{#1})}

\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\DeclareMathOperator{\Inv}{Inv}
\DeclareMathOperator{\innt}{int}
\newcommand{\probP}{\text{I\kern-0.15em P}}

\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\theoremstyle{remark}
\newtheorem{remark}{Remark}

\newcommand{\reals}{\mathbb{R}}

 \usepackage{thmtools, thm-restate} \newtheorem{conjecture}[theorem]{Conjecture}
 \newtheorem{corollary}[theorem]{Corollary}

\title{A framework for Infinite Horizon Neural Computation on Compact Domains}
\author{\'Abel S\'agodi}
\date{September 1, 2023}

\begin{document}
\maketitle


\section*{Goal}
The goal is to give a framework to be able to characterize and describe all possible computations that can be implemented in neural networks with asymptotic dynamics and robustly in the language of dynamical systems (at a level that is understandable for humans).

For asymptotic dynamics one can formulate a surjective mapping to finite-state machines (FSMs) if MSs are fixed points or one restricts what effects the input can have.
If no continua are allowed (because they are not robust) there is a surjective mapping to nondeterministic FSMs. 
But this framework is more expressive because it can deal with infinite states as long as they are on a continuum.
See Sec.\ref{sec:fsm}

\section*{Abstract}
This document presents a comprehensive framework for characterizing and describing computations realizable within the asymptotic dynamics of neural networks on compact domains in the language of dynamical systems,  leveraging Conley's decomposition theorem to elucidate the nature of flows in compact phase portraits. The scope of the possible computations corresponds to infinite horizon computations with respect to decoding time. We state theorems to describe the indispensable reliance on chain-recurrent sets in asymptotic neural computations on compact domains. Memory states are identified as the Morse sets of the system. The interplay between meaningful memory states and neural computation with linear decoders is analyzed, emphasizing the requirement of linear separability for sets of trajectories grouped by the preimage of the input. 
Additionally, we describe the syntax of neural computation, providing insights into input-driven dynamics under diverse modeling conditions. 

\newpage



\section{Introduction to Infinite horizon computation with ODEs}	
Computation is generally equated with information-processing, and this is why the notion of information is crucial in models of computation for the account of implementation: a computational process is one that transforms the stream of information it has as input into a stream of information for output \citep{milkowski2014}.
Flexibility: Both computers and brains are general‑purpose devices: they can operate in a variety of ways, which makes them adaptable to their environment, and this, in turn, can explain intelligence. \citep{milkowski2018computermetaphor}
cognition requires not only flexibility, which in turn requires universality, but also that semantic interpretability requires universality.
 In other words, \citep{pylyshyn1984} has claimed that finite‑state machines (FSM) cannot be semantically interpretable
%This is exactly why Newell and Simon made \emph{universal computation} part of their hypothesis.
We define the implemented (neural) computation as an input-output mapping for times $t\in T\subset\reals$ defined by the function $\varphi:\reals^T\rightarrow\reals^K$:\footnote{We discuss some restrictions on the type of mapping we consider in Sec.~\ref{sec:jar}.}
\begin{equation}
o(t) = \varphi(\{u(t)\}_{t\in T})
\end{equation}
where $o(t)$ is the target output for each time point $t\in T$ and the input  $u:\reals_{+}\to \reals^{m}$  is a Lebesgue measurable essentially bounded external input.

Internal dynamics is a non-autonomous ODE, driven by input $u(t)$:
\begin{align}
\dot x(t) &= f(x(t),u(t))\label{eq:ode}\\
y(t) &= g(x(t))	\label{eq:output}
\end{align}
where $f:\reals^N\rightarrow\reals^N$, is a Lipschitz continuous function w.r.t. the first argument and uniformly w.r.t. the second one. This ensures that there exists a unique absolutely continuous solution of the system in Eq.~\ref{eq:ode}.
while the output is given a simple (linear, monotonic) mapping $g:\reals^N\rightarrow\reals^K$.

\begin{definition}
Infinite horizon: agnostic to decoding time
\end{definition}

\begin{remark}
The output can only change if there is a change in the input. 
So cued output is allowed, but timing tasks are excluded.
\end{remark}

\subsection{Justifications and restrictions}\label{sec:jar}
\subsubsection{Asymptotic}
We justify the focus on asymptotic dynamics by considering systems where the asymptotic dynamics dominates due to a significant time scale difference between stimulus statistics and the asymptotic dynamics.

\subsubsection{Compact domain}
Brain activity is compact

\subsubsection{Robustness}
Brain is noisy, see also Rem.~\ref{rem:error_noise}

\paragraph{Robustness definition(s)}

\begin{definition}[Robust computation]\footnote{Based on Definition 2.2.1.in \citep{kuehn2015}}
 Let $\varphi$ and $\varphi'$ be two input-output mappings.
  Then we say that $\varphi$ is $\epsilon$-close to $\varphi'$ if
\begin{equation}\label{eq:error_def}
\sup_{u\in \mathcal{I}}\|\int_{t\in T}\varphi(t) - \int_{t\in T}\varphi'(t)\|  \leq \epsilon,
\end{equation}
\end{definition}


Minimal noise model: 
A single random delta function is applied to the asymptotic behavior
Definition:
Asymptotic dynamics is given by $x(t)$
For a random $t \in \reals$ we reinitilize the system as $x'(t)=x(t) + \theta$ where $\theta\sim \mathcal{U}(\mathcal{N}_\epsilon(M))$ (the uniform distribution over the $\epsilon$-neighbourhood of the Morse set $M=\{x(t)\colon t\in \reals\}$.

Def 1: Robustness in the light of this definition for a FSM equivalent DS can be formulated as that the error as defined in Eq.~\ref{eq:error_def} is zero.

Def 2: Robustness in the light of this definition for a FSM equivalent DS can be formulated as that the error as defined in Eq.~\ref{eq:error_def} is a continuous function of $\epsilon$.

Later: implications to what can be a robust, persistent memory state
E.g.  FSM equivalent DS: only stable fixed points as memory states (because of equivalence)
but also CAs are not allowed (because of error/perturbation effect)


\section{Theorems on infinite horizon neural computation on a compact domain}\label{sec:theorems}


\subsection{Memory states}%the words in the language
Q: What form can the states in the finite-state machine take?
A: They are the Morse sets.

Young’s account of basic memory unitsmnemons in his terminology—is not entirely loose, as the mnemon is supposed to “record one bit of information” \citep{young1978}. %p.87
Memory Units in Cephalopods (Sec.4.1 in \citep{milkowski2018})
Gifford Lectures (Young 1978), instead of framing memory in terms of static models or representations, he uses the term “program.”: expresses a concept of hierarchical and dynamical control

“programs of the brain” are related to cyclical operations of the nervous system, as driven by physiological rhythms

Young framed the computational principles of two memory systems of the octopus in terms of distributed processing in “serial networks, with recurrent circuits” (Young 1991, p. 200).

\begin{theorem}
Asymptotic neural computation that is bounded (that happens on a compact domain) has to rely on Morse sets to represent (input, internal and output states).
\end{theorem}

\begin{proof}
The asymptotic dynamics converges to the Morse sets for a compact dynamical system \cite{conley1978}, see Sec.~\ref{sec:dst}.
\end{proof}


\begin{theorem}	
Infinite horizon neural computation that is bounded has to rely on the sets that have a Morse set as their $\omega$-limit set.
\end{theorem}	

\begin{proof}
Invariance to time decoding implies that trajectories (after the input is presented) that get decoded into a certain output need to retain that decoding.
%mathematical formulation

Each trajectory converges to a Morse set asymptotically.

\begin{itemize}
\item For constant output (after last input): Each of these trajectories needs to be mapped onto the same value as the $\omega$-limit set.
\item For dynamic output: Each state along these trajectories needs to be mapped onto the same value as the $\omega$-limit set corresponding to a value along the dynamic output. %is the notion of inertial manifold useful here
\end{itemize}
\end{proof}


%\subsubsection{Meaningful memory states (for the behavior) are charaterized by the output}
\subsection{Characterization of memory states}

\begin{theorem}
For neural computation with a linear decoder, the sets of trajectories grouped by input (i.e. preimages of the decoder $g^{-1}(y)$ for $y\in Y$ the output space) must be linearly separable.
\end{theorem}

\begin{remark}
If the assumption of a compact domain is relaxed, then the computation can involve an inertial manifold (as long as it is aligned with the nullspace of the decoder).
\end{remark}

We distinguish between two classes of Morse sets.
The first one can only hold a single memory state. 
The Morse sets in this class either have a periodic or quasiperiodic behavior or they contain a wandering set.
All points inside such a set need to be mapped onto the same point, i.e. for some $y\in Y$ for all $x\in M$ we have that $g(x)=y$.

The second one can hold a continuum of memory states. %These are continuous attractor-like sets.
Such sets to be used as a neural integrator we state the following constraint on the input:
There is an input value that pushes the internal state along $M$.

On the other hand if $M$ is  mapped onto a single output, then the integrated information is either not used or used only internally in the computation.

\subsection{Syntax}%how words and morphemes combine to form larger units
%how information is integrated
%how input interacts with the represented information/the internal state

\subsubsection{Input driven dynamics}
Two cases to model it:
\begin{enumerate}
\item Input is modelled as a delta function, input reinitializes the system
\item Input is continuous 
\begin{itemize}
\item If input is constant, study it as a bifurcation parameter
\item Otherwise: non-autonomous dynamics (perhaps inertial manifold can be useful, they can provide a condition when dynamics is equivalent to the dynamics of an autonomous system)
\item Consider timescale of input effect much shorter, then consider only asymptotic of input driven dynamics (but this would make neural integration of a continuous signal impossible)
\end{itemize}
\end{enumerate}


\section{Appendix}
\subsection{Dynamical systems theory}\label{sec:dst}

Conley's decomposition theorem states that every flow of a dynamical system with compact phase portrait admits a decomposition into a chain-recurrent part and a gradient-like flow part.


Given a compact invariant set $S$ is there a finest collection of subsets of $S$ off of which one can define a Lyapunov function: the chain recurrent set. Let us first introduce the notion of chain recurrence.

\begin{definition}[$(\epsilon,\tau)$ chain]
An $(\epsilon,\tau)$ chain from $x$ to $y$ is a finite sequence 
\[f(x_i, t_i) \subset  X \times  [\tau,\infty); i = 1, \dots, n\]
such that $x = x_1, t_i\geq \tau, \mu(\varphi(t_i, x_i), x_{i+1})\leq \varepsilon$   and $\mu(\varphi(t_n, x_n), y)\leq \epsilon$.
 If there exists an $(\epsilon,\tau)$ chain from $x$ to $y$, then we write $x \succeq_{(\epsilon,\tau)}y$. If  $x \succeq_{(\epsilon,\tau)}y$ for all $(\epsilon,\tau)$, then  $x \succeq y$.
\end{definition}


\begin{definition}[Chain-recurrent set]
The chain recurrent set of $X$ under the flow $\varphi$ is defined by 
\[\mathcal{R}(X,\varphi) = \{x\in X|x\succeq x\}.\]
\end{definition}

The flow is called \emph{strongly gradient-like} if the chain recurrent set is totally disconnected (and consequently equal to the rest point set). 

The recurrent set captures all the recurrent dynamics as stated and proven in \citep[Chapter I.8.2]{conley1978}.

The only type of dynamics that is outside of the recurrent set is strongly gradient-like:

Statement. Every flow on a compact space is uniquely represented as the extension of a chain recu"ent flow by a strongly gradient-like flow; that is the flow admits a unique subflow which is chain recurrent and such that the quotient flow is strongly gradient-like.

\begin{remark}\label{rem:error_noise}
%error->noise
The significance of the statement is explained by contrasting the two types (strongly gradient-like and chain recurrent) of flow, and the contrast is best seen if (arbitrarily) small errors are allowed in following solutions. For strongly gradient-like flows the following is true: if $U$ is any neighborhood of the rest point set, there exists a positive $\epsilon$ such that if the (persistent) error made in following solutions for time 1 is no larger than $\epsilon$ then all solutions eventually enter and subsequently stay in $U$.

Chain recurrent flows are just the opposite: if $\dots,x_{-1},x_0, x_{1},\dots$ is any bi-infinite sequence of points all of which lie in the same component of the space, then no matter how small the allowed (persistent) error, there is an approximate solution which runs through these points in sequence. Thus all knowledge of the asymptotic behavior is lost when (arbitrarily small) persistent errors are allowed.
\end{remark}

\begin{definition}
Let $\{M_1, \dots, M_n\}$ be any finite ordered collection of invariant sets with the following property: the sets are disjoint and on collapsing them to distinct points there is obtained a gradient-like quotient flow whose rest point set consists of the collapse points, and such that the ordering of the sets corresponds to that of the points induced by the Lyapounov function.
 Each of the sets $M_i$ is called a Morse set and the ordered collection is called a Morse decomposition.
\end{definition}


\section{Finite state machine equivalencies}\label{sec:fsm}
\begin{theorem}
If the sinks (final nodes in the directed acyclic graph (DAG) representation of the Morse Decomposition) of finest Morse decomposition is composed of fixed points, then there exists a surjective mapping to deterministic finite state machines.
\end{theorem}

\begin{proof}

\end{proof}

\begin{corollary}
Equivalency to Moore machines
\end{corollary}

\begin{corollary}
Morse sets can be non-stationary as long as they are the final states of the FSM.
\end{corollary}

%\begin{figure}[H]
% \centering
% \includegraphics[width=0.99\textwidth]{figs/.pdf}
% \caption{}
% \label{fig:tanh_dydot}
%\end{figure}



 \newpage
 \addcontentsline{toc}{section}{References}
 \bibliographystyle{plain}
 \bibliography{ref.bib}

\end{document}
