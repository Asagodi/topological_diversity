%\documentclass{article}
\documentclass{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage{amssymb, amsmath, amsthm}
\usepackage{thmtools, mathtools, mathrsfs}
\usepackage{amsfonts}
\usepackage[sort&compress,numbers]{natbib}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{float}
\usepackage{bm}
\usepackage{tikz}
\usetikzlibrary{positioning,matrix,arrows,decorations.pathmorphing}
\usepackage{tikz-cd} 
\definecolor{processblue}{cmyk}{0.8,0,0,0}
\definecolor{mpcolor}{rgb}{1, 0.1, 0.59}
\newcommand{\mpcomment}[1]{(\textbf{MP:\ }\textcolor{mpcolor}{#1})}

\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\DeclareMathOperator{\Inv}{Inv}
\DeclareMathOperator{\innt}{int}
\newcommand{\probP}{\text{I\kern-0.15em P}}

\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\theoremstyle{remark}
\newtheorem{remark}{Remark}

 \usepackage{thmtools, thm-restate} \newtheorem{conjecture}[theorem]{Conjecture}

\title{A framework for Infinite Horizon Neural Computation on Compact Domains}
\author{\'Abel S\'agodi}
\date{September 1, 2023}

\begin{document}
\maketitle


\section{Goal}
The goal is to give a framework to be able to characterize and describe all possible computations that can be implemented in neural networks with asymptotic dynamics and robustly in the language of dynamical systems (at a level that is understandable for humans).


\section{Abstract}
This document presents a comprehensive framework for characterizing and describing computations realizable within the asymptotic dynamics of neural networks on compact domains in the language of dynamical systems,  leveraging Conley's decomposition theorem to elucidate the nature of flows in compact phase portraits. The scope of the possible computations corresponds to infinite horizon computations with respect to decoding time. We state theorems to describe the indispensable reliance on chain-recurrent sets in asymptotic neural computations on compact domains. Memory states, crucial to neural behavior, are dissected, revealing their characterization in terms of sets with chain-recurrent sets as $\omega$-limit. The interplay between meaningful memory states and neural computation with linear decoders is analyzed, emphasizing the requirement of linear separability for sets of trajectories grouped by input. Additionally, we describe the syntax of neural computation, providing insights into input-driven dynamics under diverse modeling conditions. 


\section{Justifications}
\subsection{Asymptotic}
We justify the focus on asymptotic dynamics by considering systems where the asymptotic dynamics dominates due to a significant time scale difference between stimulus statistics and the asymptotic dynamics.

\subsection{Compact domain}
Brain activity is compact

\subsection{Robustness requirement}
Brain is noisy

\subsubsection{Robustness definition}



\section{Infinite horizon computation}	
Neural Computation: IO 
\begin{definition}
Infinite horizon: agnostic to decoding time
\end{definition}

\begin{remark}
The output can only change if there is a change in the input. 
So cued output is allowed, but timing tasks are excluded.
\end{remark}

\section{Theorems on infinite horizon neural computation on a compact domain}


\subsection{Memory states}%the words in the language

\begin{theorem}
Asymptotic neural computation that is bounded (that happens on a compact domain) has to rely on Morse sets.
\end{theorem}

\begin{theorem}	
Infinite horizon neural computation that is bounded has to rely on the sets that have a chain-recurrent set as their $\omega$-limit set.
\end{theorem}	

\begin{proof}
Invariance to time decoding implies that trajectories (after the input is presented) that get decoded into a certain output need to retain that decoding.

Convergence to the non-wandering set for a compact dynamical system \citep{conley1978}

Each of these trajectories will be in the set that has the same $\omega$-limit set.
\end{proof}


\subsubsection{Meaningful memory states (for the behavior) are charaterized by the output}
\begin{theorem}
For neural computation with a linear decoder, the sets of trajectories grouped by input (i.e. preimages of the decoder $f^{-1}(y)$ for $y\in Y$ the output space) must be linearly separable.
\end{theorem}

\begin{remark}
If the assumption of a compact domain is relaxed, then the computation can involve an inertial manifold (as long as it is aligned with the nullspace of the decoder).
\end{remark}


\subsection{Syntax}%how words and morphemes combine to form larger units
%how information is integrated
%how input interacts with the represented information/the internal state

\subsubsection{Input driven dynamics}
Two cases to model it:
\begin{enumerate}
\item Input is modelled as a delta function, input reinitializes the system
\item Input is continuous 
\begin{itemize}
\item If input is constant, study it as a bifurcation parameter
\item Otherwise: non-autonomous dynamics (perhaps inertial manifold can be useful, they can provide a condition when dynamics is equivalent to the dynamics of an autonomous system)
\item Consider timescale of input effect much shorter, then consider only asymptotic of input driven dynamics (but this would make neural integration of a continuous signal impossible)
\end{itemize}
\end{enumerate}


\section*{Appendix}
\subsection{Dynamical systems theory}

Conley's decomposition theorem states that every flow of a dynamical system with compact phase portrait admits a decomposition into a chain-recurrent part and a gradient-like flow part.


Given a compact invariant set $S$ is there a finest collection of subsets of $S$ off of which one can define a Lyapunov function: the chain recurrent set. Let us first introduce the notion of chain recurrence.

\begin{definition}[$(\epsilon,\tau)$ chain]
An $(\epsilon,\tau)$ chain from $x$ to $y$ is a finite sequence 
\[f(x_i, t_i) \subset  X \times  [\tau,\infty); i = 1, \dots, n\]
such that $x = x_1, t_i\geq \tau, \mu(\varphi(t_i, x_i), x_{i+1})\leq \varepsilon$   and $\mu(\varphi(t_n, x_n), y)\leq \epsilon$.
 If there exists an $(\epsilon,\tau)$ chain from $x$ to $y$, then we write $x \succeq_{(\epsilon,\tau)}y$. If  $x \succeq_{(\epsilon,\tau)}y$ for all $(\epsilon,\tau)$, then  $x \succeq y$.
\end{definition}


\begin{definition}[Chain-recurrent set]
The chain recurrent set of $X$ under the flow $\varphi$ is defined by 
\[\mathcal{R}(X,\varphi) = \{x\in X|x\succeq x\}.\]
\end{definition}

The flow is called \emph{strongly gradient-like} if the chain recurrent set is totally disconnected (and consequently equal to the rest point set). 

The recurrent set captures all the recurrent dynamics as stated and proven in \cite[Chapter I.8.2]{conley1978}.

The only type of dynamics that is outside of the recurrent set is strongly gradient-like:

Statement. Every flow on a compact space is uniquely represented as the extension of a chain recu"ent flow by a strongly gradient-like flow; that is the flow admits a unique subflow which is chain recurrent and such that the quotient flow is strongly gradient-like.



%\begin{figure}[H]
% \centering
% \includegraphics[width=0.99\textwidth]{figs/.pdf}
% \caption{}
% \label{fig:tanh_dydot}
%\end{figure}



% \newpage
% \addcontentsline{toc}{section}{References}
% \bibliographystyle{plain}
% \bibliography{CIT-for-Computation.bib, cit.bib, CITCOD.bib}

\end{document}
