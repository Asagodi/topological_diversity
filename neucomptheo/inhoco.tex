%\documentclass{article}
\documentclass{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage{amssymb, amsmath, amsthm}
\usepackage{thmtools, mathtools, mathrsfs}
\usepackage{amsfonts}
\usepackage[sort&compress,numbers]{natbib}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{float}
\usepackage{bm}
\usepackage{tikz}
\usetikzlibrary{positioning,matrix,arrows,decorations.pathmorphing}
\usepackage{tikz-cd} 
\definecolor{processblue}{cmyk}{0.8,0,0,0}
\definecolor{mpcolor}{rgb}{1, 0.1, 0.59}
\newcommand{\mpcomment}[1]{(\textbf{MP:\ }\textcolor{mpcolor}{#1})}

\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\DeclareMathOperator{\Inv}{Inv}
\DeclareMathOperator{\innt}{int}
\newcommand{\probP}{\text{I\kern-0.15em P}}

\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\theoremstyle{remark}
\newtheorem{remark}{Remark}

 \usepackage{thmtools, thm-restate} \newtheorem{conjecture}[theorem]{Conjecture}

\title{A framework for Infinite Horizon Neural Computation on Compact Domains}
\author{\'Abel S\'agodi}
\date{September 1, 2023}

\begin{document}
\maketitle


\section{Goal}
The goal is to give a framework to be able to characterize and describe all possible computations that can be implemented in neural networks with asymptotic dynamics and robustly in the language of dynamical systems (at a level that is understandable for humans).

For asymptotic dynamics one can formulate a 1-to-1 mapping to finite-state machines.


\section{Abstract}
This document presents a comprehensive framework for characterizing and describing computations realizable within the asymptotic dynamics of neural networks on compact domains in the language of dynamical systems,  leveraging Conley's decomposition theorem to elucidate the nature of flows in compact phase portraits. The scope of the possible computations corresponds to infinite horizon computations with respect to decoding time. We state theorems to describe the indispensable reliance on chain-recurrent sets in asymptotic neural computations on compact domains. Memory states, crucial to neural behavior, are dissected, revealing their characterization in terms of sets with chain-recurrent sets as $\omega$-limit. The interplay between meaningful memory states and neural computation with linear decoders is analyzed, emphasizing the requirement of linear separability for sets of trajectories grouped by input. Additionally, we describe the syntax of neural computation, providing insights into input-driven dynamics under diverse modeling conditions. 


\section{Justifications}
\subsection{Asymptotic}
We justify the focus on asymptotic dynamics by considering systems where the asymptotic dynamics dominates due to a significant time scale difference between stimulus statistics and the asymptotic dynamics.

\subsection{Compact domain}
Brain activity is compact

\subsection{Robustness requirement}
Brain is noisy

\subsubsection{Robustness definition}



\section{Infinite horizon computation}	
Neural Computation: I/O mapping for times $t\in T$
\begin{equation}
o(t) = \varphi(\{u(t)\}_{t\in T})
\end{equation}


\begin{align}
\begin{cases}
\dot x(t) &= f(x(t),u(t))\\
y(t) &= g(x(t))
\end{cases}
\end{align}


\begin{definition}
Infinite horizon: agnostic to decoding time
\end{definition}

\begin{remark}
The output can only change if there is a change in the input. 
So cued output is allowed, but timing tasks are excluded.
\end{remark}

\section{Theorems on infinite horizon neural computation on a compact domain}


\subsection{Memory states}%the words in the language

\begin{theorem}
Asymptotic neural computation that is bounded (that happens on a compact domain) has to rely on Morse sets to represent (input, internal and output states).
\end{theorem}

\begin{proof}
The asymptotic dynamics converges to the Morse sets for a compact dynamical system \citep{conley1978}, see Sec.~\ref{sec:dst}.
\end{proof}


\begin{theorem}	
Infinite horizon neural computation that is bounded has to rely on the sets that have a Morse set as their $\omega$-limit set.
\end{theorem}	

\begin{proof}
Invariance to time decoding implies that trajectories (after the input is presented) that get decoded into a certain output need to retain that decoding.
%mathematical formulation

Each trajectory converges to a Morse set asymptotically.

\begin{itemize}
\item For constant output (after last input): Each of these trajectories needs to be mapped onto the same value as the $\omega$-limit set.
\item For dynamic output: Each state along these trajectories needs to be mapped onto the same value as the $\omega$-limit set corresponding to a value along the dynamic output. %is the notion of inertial manifold useful here
\end{itemize}â€¢
\end{proof}


\subsubsection{Meaningful memory states (for the behavior) are charaterized by the output}
\begin{theorem}
For neural computation with a linear decoder, the sets of trajectories grouped by input (i.e. preimages of the decoder $g^{-1}(y)$ for $y\in Y$ the output space) must be linearly separable.
\end{theorem}

\begin{remark}
If the assumption of a compact domain is relaxed, then the computation can involve an inertial manifold (as long as it is aligned with the nullspace of the decoder).
\end{remark}


\subsection{Syntax}%how words and morphemes combine to form larger units
%how information is integrated
%how input interacts with the represented information/the internal state

\subsubsection{Input driven dynamics}
Two cases to model it:
\begin{enumerate}
\item Input is modelled as a delta function, input reinitializes the system
\item Input is continuous 
\begin{itemize}
\item If input is constant, study it as a bifurcation parameter
\item Otherwise: non-autonomous dynamics (perhaps inertial manifold can be useful, they can provide a condition when dynamics is equivalent to the dynamics of an autonomous system)
\item Consider timescale of input effect much shorter, then consider only asymptotic of input driven dynamics (but this would make neural integration of a continuous signal impossible)
\end{itemize}
\end{enumerate}


\section*{Appendix}
\subsection{Dynamical systems theory}\label{sec:dst}

Conley's decomposition theorem states that every flow of a dynamical system with compact phase portrait admits a decomposition into a chain-recurrent part and a gradient-like flow part.


Given a compact invariant set $S$ is there a finest collection of subsets of $S$ off of which one can define a Lyapunov function: the chain recurrent set. Let us first introduce the notion of chain recurrence.

\begin{definition}[$(\epsilon,\tau)$ chain]
An $(\epsilon,\tau)$ chain from $x$ to $y$ is a finite sequence 
\[f(x_i, t_i) \subset  X \times  [\tau,\infty); i = 1, \dots, n\]
such that $x = x_1, t_i\geq \tau, \mu(\varphi(t_i, x_i), x_{i+1})\leq \varepsilon$   and $\mu(\varphi(t_n, x_n), y)\leq \epsilon$.
 If there exists an $(\epsilon,\tau)$ chain from $x$ to $y$, then we write $x \succeq_{(\epsilon,\tau)}y$. If  $x \succeq_{(\epsilon,\tau)}y$ for all $(\epsilon,\tau)$, then  $x \succeq y$.
\end{definition}


\begin{definition}[Chain-recurrent set]
The chain recurrent set of $X$ under the flow $\varphi$ is defined by 
\[\mathcal{R}(X,\varphi) = \{x\in X|x\succeq x\}.\]
\end{definition}

The flow is called \emph{strongly gradient-like} if the chain recurrent set is totally disconnected (and consequently equal to the rest point set). 

The recurrent set captures all the recurrent dynamics as stated and proven in \cite[Chapter I.8.2]{conley1978}.

The only type of dynamics that is outside of the recurrent set is strongly gradient-like:

Statement. Every flow on a compact space is uniquely represented as the extension of a chain recu"ent flow by a strongly gradient-like flow; that is the flow admits a unique subflow which is chain recurrent and such that the quotient flow is strongly gradient-like.

\begin{remark}
error->noise
The significance of the statement is explained by contrasting the two types (strongly gradient-like and chain recurrent) of flow, and the contrast is best seen if (arbitrarily) small errors are allowed in following solutions. For strongly gradient-like flows the following is true: if $U$ is any neighborhood of the rest point set, there exists a positive $\epsilon$ such that if the (persistent) error made in following solutions for time 1 is no larger than $\epsilon$ then all solutions eventually enter and subsequently stay in $U$.

Chain recurrent flows are just the opposite: if $\dots,x_{-1},x_0, x{1},\dots$ is any bi-infinite sequence of points all of which lie in the same component of the space, then no matter how small the allowed (persistent) error, there is an approximate solution which runs through these points in sequence. Thus all knowledge of the asymptotic behavior is lost when (arbitrarily small) persistent errors are allowed.
\end{remark}

\begin{definition}
Let $\{M_1, \dots, M_n\}$ be any finite ordered collection of invariant sets with the following property: the sets are disjoint and on collapsing them to distinct points there is obtained a gradient-like quotient flow whose rest point set consists of the collapse points, and such that the ordering of the sets corresponds to that of the points induced by the Lyapounov function.
 Each of the sets $M_i$ is called a Morse set and the ordered collection is called a Morse decomposition.
\end{definition}

%\begin{figure}[H]
% \centering
% \includegraphics[width=0.99\textwidth]{figs/.pdf}
% \caption{}
% \label{fig:tanh_dydot}
%\end{figure}



% \newpage
% \addcontentsline{toc}{section}{References}
% \bibliographystyle{plain}
% \bibliography{CIT-for-Computation.bib, cit.bib, CITCOD.bib}

\end{document}
