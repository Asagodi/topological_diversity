@InProceedings{Filipe2025a,
  author    = {Carolina Filipe and Mahmoud Elmakki and Guilherme Costa-Ferreira and Il Memming Park},
  booktitle = {Computational and Systems Neuroscience ({COSYNE})},
  title     = {Conditional Diffusion Framework for Analyzing Neural Dynamics Across Multiple Contexts},
  year      = {2025},
}

@InProceedings{Vermani2025a,
  author    = {Ayesha Vermani and Josue Nassar and Hyungju Jeon and Matthew Dowling and Il Memming Park},
  booktitle = {Computational and Systems Neuroscience ({COSYNE})},
  title     = {Meta-Dynamical State Space Models for Integrative Neural Data Analysis},
  year      = {2025},
}

@InProceedings{Sagodi2025a,
  author = "S\'agodi, \'Abel and Mart\'in-S\'anchez, Guillermo and Sok{\'o}{\l}, Piotr
                   and Park, Il Memming",
  booktitle = {Computational and Systems Neuroscience ({COSYNE})},
  title     = {Approximate continuous attractor theory},
  year      = {2025},
}

@INPROCEEDINGS{Sagodi2024b,
  author = "S\'agodi, \'Abel and Mart\'in-S\'anchez, Guillermo and Sok{\'o}{\l}, Piotr
                   and Park, Il Memming",
  title = "Slow Manifold Dynamics for Working Memory are near Continuous Attractors",
  booktitle = {Bernstein Conference},
  doi = "10.12751/nncn.bc2024.076",
  url = {https://abstracts.g-node.org/conference/BC24/abstracts#/uuid/5964bd61-91e8-4d1a-a9ca-e6d137f456b4},
  year = 2024,
  poster_no = "I 59",
}

@INPROCEEDINGS{Filipe2024a,
  author        = "Filipe, A. Carolina and Park, Il Memming",
  title = "{NeuroTask}: A Benchmark Dataset for Multi-Task Neural Analysis",
  booktitle = {Bernstein Conference},
  doi = "10.12751/nncn.bc2024.020",
  url = {https://abstracts.g-node.org/conference/BC24/abstracts#/uuid/ccb5dd62-403b-4da0-a839-2652ab70ac28},
  code = {https://github.com/catniplab/NeuroTask/tree/nwb},
  year = 2024,
  poster_no = "I 1",
}

@InProceedings{Vermani2024b,
  title         = "Meta-dynamical state space models for integrative neural data
                   analysis",
  author        = "Vermani, Ayesha and Nassar, Josue and Jeon, Hyungju and
                   Dowling, Matthew and Park, Il Memming",
  abstract      = "Learning shared structure across environments facilitates
                   rapid learning and adaptive behavior in neural systems. This
                   has been widely demonstrated and applied in machine learning
                   to train models that are capable of generalizing to novel
                   settings. However, there has been limited work exploiting the
                   shared structure in neural activity during similar tasks for
                   learning latent dynamics from neural recordings. Existing
                   approaches are designed to infer dynamics from a single
                   dataset and cannot be readily adapted to account for
                   statistical heterogeneities across recordings. In this work,
                   we hypothesize that similar tasks admit a corresponding
                   family of related solutions and propose a novel approach for
                   meta-learning this solution space from task-related neural
                   activity of trained animals. Specifically, we capture the
                   variabilities across recordings on a low-dimensional manifold
                   which concisely parametrizes this family of dynamics, thereby
                   facilitating rapid learning of latent dynamics given new
                   recordings. We demonstrate the efficacy of our approach on
                   few-shot reconstruction and forecasting of synthetic
                   dynamical systems, and neural recordings from the motor
                   cortex during different arm reaching tasks.",
  booktitle = {International Conference on Learning Representations (ICLR)},
  month     = apr,
  year          =  2025,
  url           = "http://arxiv.org/abs/2410.05454",
  url		= "https://openreview.net/forum?id=SRpq5OBpED",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "2410.05454",
}

@InProceedings{Vermani2024a,
  author    = {Ayesha Vermani and Matthew Dowling and Hyungju Jeon and Ian Jordan and Josue Nassar and Yves Bernaerts and Yuan Zhao and Steven Van Vaerenbergh and Il Memming Park},
  booktitle = {European Signal Processing Conference},
  title     = {Real-time machine learning strategies for a new kind of neuroscience experiments},
  year      = {2024},
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "2409.01280",
}

@InProceedings{Vermani2023b,
  author    = {Ayesha Vermani and Il Memming Park and Josue Nassar},
  booktitle = {International Conference on Learning Representations (ICLR)},
  title     = {Leveraging Generative Models for Unsupervised Alignment of Neural Time Series Data},
  year      = {2024},
  month     = may,
  abstract  = {Large scale inference models are widely used in neuroscience to extract latent representations from high-dimensional neural recordings. Due to the statistical heterogeneities between sessions and animals, a new model is trained from scratch to infer the underlying dynamics for each new dataset. This is computationally expensive and does not fully leverage all the available data. Moreover, as these models get more complex, they can be challenging to train. In parallel, it is becoming common to use pre-trained models in the machine learning community for few shot and transfer learning. One major hurdle that prevents the re-use of generative models in neuroscience is the complex spatio-temporal structure of neural dynamics within and across animals. Interestingly, the underlying dynamics identified from different datasets on the same task are qualitatively similar. In this work, we exploit this observation and propose a source-free and unsupervised alignment approach that utilizes the learnt dynamics and enables the re-use of trained generative models. We validate our approach on simulations and show the efficacy of the alignment on neural recordings from the motor cortex obtained during a reaching task.},
  url       = {https://openreview.net/forum?id=9zhHVyLY4K},
}

@InProceedings{Sagodi2024a,
  title         = "Back to the Continuous Attractor",
  author        = "S\'agodi, \'Abel and Mart\'in-S\'anchez, Guillermo and Sok{\'o}{\l}, Piotr
                   and Park, Il Memming",
  booktitle     = {Advances in Neural Information Processing Systems (NeurIPS)},
  abstract      = "Continuous attractors offer a unique class of solutions for
                   storing continuous-valued variables in recurrent system
                   states for indefinitely long time intervals. Unfortunately,
                   continuous attractors suffer from severe structural
                   instability in general--they are destroyed by most
                   infinitesimal changes of the dynamical law that defines them.
                   This fragility limits their utility especially in biological
                   systems as their recurrent dynamics are subject to constant
                   perturbations. We observe that the bifurcations from
                   continuous attractors in theoretical neuroscience models
                   display various structurally stable forms. Although their
                   asymptotic behaviors to maintain memory are categorically
                   distinct, their finite-time behaviors are similar. We build
                   on the persistent manifold theory to explain the
                   commonalities between bifurcations from and approximations of
                   continuous attractors. Fast-slow decomposition analysis
                   uncovers the persistent manifold that survives the seemingly
                   destructive bifurcation. Moreover, recurrent neural networks
                   trained on analog memory tasks display approximate continuous
                   attractors with predicted slow manifold structures.
                   Therefore, continuous attractors are functionally robust and
                   remain useful as a universal analogy for understanding analog
                   memory.",
  month         =  dec,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "q-bio.NC",
  arxivid       = "2408.00109",
  eprint        = "2408.00109",
  url		= "https://openreview.net/forum?id=fvG6ZHrH0B",
}

@InProceedings{Park2024a,
  author    = {Il Memming Park},
  booktitle = {Computational and Systems Neuroscience ({COSYNE})},
  title     = {Persistent activity bump on a ring without a continuous ring attractor},
  year      = {2024},
  abstract  = {We propose a robust recurrent memory mechanism for integrating and maintaining head direction without a ring attractor. The ring attractor is a popular theoretical mechanism for maintaining a continuous internal representation of a periodic variable. For instance, they are used to describe the neural dynamics underlying head direction cells, grid cells, and visual working memory. At the same time, it is well-known that mathematically, continuous attractors are prone to fine tuning, where small changes in the connectivity or biophysical parameters lead to the destruction of the mechanism. We describe an alternative, robust mechanism for working memory of a periodic variable using the theory of stable limit cycles. In particular, we detail a biophysical implementation that can explain the bump of activity observed over the ellipsoid body of insects. The content of working memory is stored in the relative phase difference between two independent oscillators, whose periods are constrained by conduction delay. A phase independent readout is achieved through a delay line and nonlinear coincidence detectors that can be mapped onto the readout system, i.e., the ellipsoid body. Our theory makes several strong predictions: the bump of activity on the ellipsoid body is tightly periodic on a short time scale, is robust to partial lesions, and exhibits small, consistent systematic drifts due to misadjustments within each animal. Furthermore, we investigate the impact of diffusion in information encoding and memory due to various noise sources.},
}

@InProceedings{Jeon2024b,
  author    = {Hyungju Jeon and Il Memming Park},
  booktitle = {European Signal Processing Conference},
  title     = {Quantifying signal-to-noise ratio in neural latent trajectories via {F}isher information},
  month         =  aug,
  year      = {2024},
  archivePrefix = "arXiv",
  primaryClass  = "q-bio.NC",
  eprint        = "2408.08752"
}

@Conference{Jeon2024a,
  author    = {Hyungju Jeon and Matthew Dowling and Il Memming Park},
  booktitle = {European Conference on Brain Stimulation},
  title     = {Closed-loop active sensing for nonlinear system identification in attractor dynamics disorders},
  year      = {2024},
  note      = {(oral presentation)},
  abstract  = {Neurological disorders pose multifaceted challenges for intervention due to their diverse root causes, ranging from cellular, pathogenic, or metabolic dysfunctions to more recently discovered disruptions in complex neuronal dynamics. While conventional treatment methods have demonstrated efficacy in conditions with known cellular or metabolic dysfunctions, they fall short in disorders arising from system-level dynamics disruptions, such as movement disorders, and psychiatric conditions like schizophrenia. However, identifying the underlying mechanisms of such disturbances in system-level dynamics is often hindered by modern data-driven challenges, particularly regarding the size of datasets. The sheer volume of neural recordings measuring the synchronous activity of thousands of neurons simultaneously can impede timely and efficient identification of underlying mechanisms.

To address these challenges, we propose a real-time closed-loop active sensing for nonlinear system identification. Our approach aims to expedite the throughput of current experimental design paradigms and bridge the gap between traditional treatments and the complexities of brain-system dynamics. In particular, we employ Bayesian experimental design to accelerate inference of underlying neural dynamics and their changes by introducing structured perturbation to the system. We consider a synthetic test-bed mimicking neurological disorders characterized by the emergence of disease state attractors; then, we show how statistically informed interventions in the form of carefully chosen perturbations can be used to identify the dynamics of the attractor and restore regularity of the population level neural dynamics by driving the neural state away from disease state attractors.},
}

@InProceedings{Dowling2024b,
  title         = "{eXponential} {FAmily} Dynamical Systems ({XFADS}):
                   Large-scale nonlinear Gaussian state-space modeling",
  author        = "Dowling, Matthew and Zhao, Yuan and Park, Il Memming",
  booktitle     = {Advances in Neural Information Processing Systems (NeurIPS)},
  abstract      = "State-space graphical models and the variational autoencoder
                   framework provide a principled apparatus for learning
                   dynamical systems from data. State-of-the-art probabilistic
                   approaches are often able to scale to large problems at the
                   cost of flexibility of the variational posterior or
                   expressivity of the dynamics model. However, those
                   consolidations can be detrimental if the ultimate goal is to
                   learn a generative model capable of explaining the
                   spatiotemporal structure of the data and making accurate
                   forecasts. We introduce a low-rank structured variational
                   autoencoding framework for nonlinear Gaussian state-space
                   graphical models capable of capturing dense covariance
                   structures that are important for learning dynamical systems
                   with predictive capabilities. Our inference algorithm
                   exploits the covariance structures that arise naturally from
                   sample based approximate Gaussian message passing and
                   low-rank amortized posterior updates -- effectively
                   performing approximate variational smoothing with time
                   complexity scaling linearly in the state dimensionality. In
                   comparisons with other deep state-space model architectures
                   our approach consistently demonstrates the ability to learn a
                   more predictive generative model. Furthermore, when applied
                   to neural physiological recordings, our approach is able to
                   learn a dynamical system capable of forecasting population
                   spiking and behavioral correlates from a small portion of
                   single trials.",
  month         =  dec,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "2403.01371",
  arxivid       = "2403.01371",
  url           = "https://openreview.net/forum?id=Ln8ogihZ2S",
}

@InProceedings{Dowling2024a,
  author    = {Matthew Dowling and Yuan Zhao and Il Memming Park},
  booktitle = {Computational and Systems Neuroscience ({COSYNE})},
  title     = {{XFADS}: Predicting single-trial cued behavior solely from preparatory activity},
  year      = {2024},
  abstract  = {Consider a classic neuroscience dataset --- comprising recordings from monkey's motor cortex during a center-out reaching task. A prevailing hypothesis elucidating the coordinated neural population dynamics in this task suggests that the initial condition of a dynamical system, steering muscle movement, is set during the preparatory phase of the trial. The motor cortex, released from the initial condition and governed by the dynamical system, controls the muscles orchestrating the monkey's movement. However, existing cutting-edge tools often rely on extraneous-input-driven dynamical systems, which tend to learn control inputs anti-causally or lack a dynamic systems model entirely. This led us to explore the feasibility of learning the hypothesized autonomous dynamical system, exclusively from neural data, that is capable of causally predicting, initiated before movement onset, monkey's reaching. Consequently, we developed a novel Bayesian inference framework, eXponential FAmily Dynamical System (XFADS), aimed at learning highly expressive dynamical systems from neural data. Remarkably, we confirm the long standing autonomous nature of motor cortex activity associated with the planned reach behavior through single-trial causal forecasting. Specifically, our inferred stochastic dynamics revealed that the causally-inferred latent states, derived without any prior knowledge of the target or future behavior, were nearly as effective in predicting the monkey's arm velocity as the latent states utilizing the entire trial data.},
}

@INPROCEEDINGS{Vermani2023a,
  author        = "Vermani, Ayesha and Nassar, Josue and Park, Il Memming",
  title = "Aligning high-dimensional neural recordings for data efficient inference of dynamics and prediction",
  booktitle = {Bernstein Conference},
  doi = "10.12751/nncn.bc2023.291",
  url = {https://abstracts.g-node.org/conference/BC23/abstracts#/uuid/9c78d90e-d9e2-4ac6-9c41-e00328c57426},
  year = 2023,
  poster_no = "IV 111",
}

@INPROCEEDINGS{Stone2023c,
  author        = "Stone, Iris and Sagiv, Yotam and Park, Il Memming and Pillow, Jonathan",
  title = "Spectral learning of {B}ernoulli linear dynamical systems models",
  booktitle = {Bernstein Conference},
  doi = "10.12751/nncn.bc2023.288",
  url = {https://abstracts.g-node.org/conference/BC23/abstracts#/uuid/9067fd8a-f2c4-4535-a686-8fe99c653492},
  year = 2023,
  poster_no = "IV 108",
}

@article{Stone2023b,
  title         = "Spectral learning of {B}ernoulli linear dynamical systems
                   models for decision-making",
  author        = "Stone, Iris R and Sagiv, Yotam and Park, Il Memming and
                   Pillow, Jonathan W",
  abstract      = "Latent linear dynamical systems with Bernoulli observations
                   provide a powerful modeling framework for identifying the
                   temporal dynamics underlying binary time series data, which
                   arise in a variety of contexts such as binary
                   decision-making and discrete stochastic processes such as
                   binned neural spike trains. Here, we develop a spectral
                   learning method for fast, efficient fitting of Bernoulli
                   latent linear dynamical system (LDS) models. Our approach
                   extends traditional subspace identification methods to the
                   Bernoulli setting via a transformation of the first and
                   second sample moments. This results in a robust, fixed-cost
                   estimator that avoids the hazards of local optima and the
                   long computation time of iterative fitting procedures like
                   the expectation-maximization (EM) algorithm. In regimes
                   where data is limited or assumptions about the statistical
                   structure of the data are not met, we demonstrate that the
                   spectral estimate provides a good initialization for
                   Laplace-EM fitting. Finally, we show that the estimator
                   provides substantial benefits to real world settings by
                   analyzing data from mice performing a sensory
                   decision-making task.",
  journal       = "Transactions on Machine Learning Research",
  code          = "https://github.com/irisstone/bestLDS",
  issn          = {2835-8856},
  month         = Jul,
  year          = 2023,
  archivePrefix = "arXiv",
  eprint        = "2303.02060",
  primaryClass  = "stat.ML",
  url           = {https://openreview.net/forum?id=giw2vcAhiH},
  arxivid       = "2303.02060"
}

@InProceedings{Stone2023a,
    author    = {Iris Stone and Yotam Sagiv and Il Memming Park and Jonathan W. Pillow},
    title     = {Spectral learning of {B}ernoulli latent dynamical system models for decision-making},
    booktitle = {Computational and Systems Neuroscience ({COSYNE})},
    year      = {2023},
}

@PhdThesis{Sokol2023b,
  author   = {Piotr Sok{\'o}{\l}},
  school   = {Stony Brook University},
  title    = {Geometry of learning and representation in neural networks},
  year     = {2023},
  month    = may,
  abstract = {Advances in artificial neural networks and computational neuroscience highlight perti- nent properties of latent representations, such as their dimensionality and topology, and how the two relate to overall performance on behavioral tasks, learning, and generalization.
We develop mathematical approaches that focus on learnability and robustness to adversarial perturbations as essential properties of latent representations. In Chapters 1 and 2, we propose a theory that specifies the conditions that allow neural models to learn through backpropagation. Dually, it has significant implications that shed light on representations in the brain. Interestingly, by requiring that backpropagating gradients neither explode nor vanish, we can recover equivalent conditions under which neural representations can faithfully represent continuous variables.
In Chapter 3 we analyze the relationship between the dimensionality of neural repre- sentations and their robustness to adversarial perturbations. This highlights an interesting tradeoff between efficacy, dimensionality, and robustness. This tradeoff is equally salient for models of biological computation and artificial neural networks.},
}

@InProceedings{Sokol2023a,
    author    = {Sok{\'o}{\l}, Piotr and Park, Il Memming},
    title     = {Only two types of attractors support representation of continuous variables, and learning over long time-spans},
    booktitle = {Computational and Systems Neuroscience ({COSYNE})},
    year      = {2023},
}

@INPROCEEDINGS{Park2023b,
  author        = "Park, Il Memming and S{\'a}godi, {\'A}bel and Sok{\'o}{\l},
                   Piotr Aleksander",
  title = "Persistent learning signals without continuous attractors",
  booktitle = {Bernstein Conference},
  doi = "10.12751/nncn.bc2023.348",
  url = {https://abstracts.g-node.org/conference/BC23/abstracts#/uuid/ecbf6440-d119-4cbc-8324-665e7cefe8d6},
  year = 2023,
  poster_no = "IV 302",
}

@UNPUBLISHED{Park2023a,
  title         = "Persistent learning signals and working memory without
                   continuous attractors",
  author        = "Park, Il Memming and S{\'a}godi, {\'A}bel and Sok{\'o}{\l},
                   Piotr Aleksander",
  abstract      = "Neural dynamical systems with stable attractor structures,
                   such as point attractors and continuous attractors, are
                   hypothesized to underlie meaningful temporal behavior that
                   requires working memory. However, working memory may not
                   support useful learning signals necessary to adapt to
                   changes in the temporal structure of the environment. We
                   show that in addition to the continuous attractors that are
                   widely implicated, periodic and quasi-periodic attractors
                   can also support learning arbitrarily long temporal
                   relationships. Unlike the continuous attractors that suffer
                   from the fine-tuning problem, the less explored
                   quasi-periodic attractors are uniquely qualified for
                   learning to produce temporally structured behavior. Our
                   theory has broad implications for the design of artificial
                   learning systems and makes predictions about observable
                   signatures of biological neural dynamics that can support
                   temporal dependence learning and working memory. Based on
                   our theory, we developed a new initialization scheme for
                   artificial recurrent neural networks that outperforms
                   standard methods for tasks that require learning temporal
                   dynamics. Moreover, we propose a robust recurrent memory
                   mechanism for integrating and maintaining head direction
                   without a ring attractor.",
  month         =  aug,
  year          =  2023,
  archivePrefix = "arXiv",
  eprint        = "2308.12585",
  primaryClass  = "q-bio.NC",
  arxivid       = "2308.12585"
}

@Article{Levi2021a,
  author   = {Levi, Aaron J and Zhao, Yuan and Park, Il Memming and Huk, Alexander C},
  journal  = {Journal of Neuroscience},
  title    = {Sensory and choice responses in {MT} distinct from motion encoding},
  year     = {2023},
  month    = mar,
  note     = {(Levi and Zhao are co-first authors)},
  number   = {12},
  pages    = {2090--2103},
  volume   = {43},
  abstract = {Macaque area MT is well known for its visual motion selectivity
              and relevance to motion perception, but the possibility of it
              also reflecting non-sensory functions has largely been ignored.
              Manipulating subjects' temporal evidence weighting revealed
              multiple components of MT responses that were, surprisingly, not
              interpretable as behaviorally-relevant modulations of motion
              encoding, nor as consequences of readout of motion direction.
              MT's time-varying motion-driven responses were starkly changed by
              our strategic manipulation, but with timecourses opposite the
              subjects' temporal weighting strategies. Furthermore, large
              choice-correlated signals were represented in population activity
              distinctly from motion responses (even after the stimulus) with
              multiple phases that both lagged psychophysical readout and
              preceded motor responses. These results reveal multiple cognitive
              contributions to MT responses that are task-related but not
              functionally relevant to encoding or decoding of motion for
              psychophysical direction discrimination, calling into question
              its nature as a simple sensory area. \#\#\# Competing Interest
              Statement The authors have declared no competing interest.},
  doi      = {10.1523/JNEUROSCI.0267-22.2023},
  url      = {https://www.jneurosci.org/content/43/12/2090},
  PMID     = {36781221},
  PMCID    = {10042117},
}

@InProceedings{Dowling2023c,
  author    = {Matthew Dowling and Yuan Zhao and Il Memming Park},
  booktitle = {International Conference on Machine Learning (ICML)},
  title     = {Linear time {GP}s for inferring latent trajectories from neural spike trains},
  year      = {2023},
  month     = Jul,
  arxivid       = "2306.01802",
  archiveprefix = {arXiv},
  eprint        = {2306.01802},
  primaryclass  = {q-bio.NC},
  abstract  = {Latent Gaussian process (GP) models are widely used in neuroscience to uncover hidden state evolutions from sequential observations, mainly in neural activity recordings. While latent GP models provide a principled and powerful solution in theory, the intractable posterior in non-conjugate settings necessitates approximate inference schemes, which may lack scalability. In this work, we propose cvHM, a general inference framework for latent GP models leveraging Hida-Mat\'ern kernels and conjugate computation variational inference (CVI). With cvHM, we are able to perform variational inference of latent neural trajectories with linear time complexity for arbitrary likelihoods. The reparameterization of stationary kernels using Hida-Matérn GPs helps us connect the latent variable models that encode prior assumptions through dynamical systems to those that encode trajectory assumptions through GPs. In contrast to previous work, we use bidirectional information filtering, leading to a more concise implementation. Furthermore, we employ the Whittle approximate likelihood to achieve highly efficient hyperparameter learning.},
  url       = {https://openreview.net/forum?id=1hWB5XEUMa},
}

@InProceedings{Dowling2023b,
  author    = {Matthew Dowling and Yuan Zhao and Il Memming Park},
  booktitle = {Computational and Systems Neuroscience ({COSYNE})},
  title     = {The Exponential Family Variational Kalman Filter for Real-time Neural Dynamics},
  year      = {2023},
}

@InProceedings{Dowling2023a,
  author        = {Matthew Dowling and Yuan Zhao and Il Memming Park},
  booktitle     = {International Conference on Learning Representations (ICLR)},
  title         = {Real-time variational method for learning neural trajectory and its dynamics},
  year          = {2023},
  month         = feb,
  note          = {(top 25\%)},
  abstract      = {Latent variable models have become instrumental in computational neuroscience for reasoning about neural computation.  This has fostered the development of powerful offline algorithms for extracting latent neural trajectories from neural recordings.  However, despite the potential of real-time alternatives to give immediate feedback to experimentalists, and enhance experimental design, they have received markedly less attention.  In this work, we introduce the exponential family variational Kalman filter (eVKF), an online recursive Bayesian method aimed at inferring latent trajectories while simultaneously learning the dynamical system generating them.  eVKF works for arbitrary likelihoods and utilizes the constant base measure exponential family to model the latent state stochasticity. We derive a closed-form variational analog to the predict step of the Kalman filter which leads to a provably tighter bound on the ELBO compared to another online variational method. We validate our method on synthetic and real-world data, and, notably, show that it achieves competitive performance.},
  archiveprefix = {arXiv},
  arxivid       = {2305.11278},
  eprint        = {2305.11278},
  primaryclass  = {stat.ML},
  url           = {https://openreview.net/forum?id=M_MvkWgQSt},
  youtube       = {https://iclr.cc/virtual/2023/poster/10849},
}

@MastersThesis{Arora2023a,
  author   = {Tushar Arora},
  school   = {Stony Brook University},
  title    = {Exploring the expressive power of latent variable models},
  year     = {2023},
  month    = may,
  abstract = {Advancements in neural recording technologies has allowed us to collect high di- mensional observations by recording thousands of neurons at the same time. How- ever, experimental evidence has shown that high dimensional neural activity may be confined to a manifold of much lower dimension [16, 26, 11, 51, 61]. This phenomenon has been observed for example in context dependent decision making tasks [42, 37], and delay modulated tasks [11]. Latent variable models (LVMs) are powerful models used to recover the low dimensional dynamics. Some popular variants of LVMs are factor analysis, state space models (SSM), principal component analysis (static SSM), etc. We will be concentrating on non-linear SSMs which are expressive enough to capture complex dynamics while remaining interpretable.
Inferring dynamics from temporal data is integral to many applications in machine learning and neuroscience, but the complexity and expressive power of the dynamics model can vary. In Machine Learning applications, sequence models for language and video use large recurrent networks where the complexity of the dynamics model is tied to the dimensionality of the states and the number of the recurrent units. In these models increasing the state space and number of units helps increase the expressive power. Meanwhile for neuroscience applications, increasing the dimensionality of state space and recurrent units is not desired as the end goal is to understand the dynamics and the topological properties. Given the postulate that computations underlying high dimensional observations live on a low dimensional manifold, it is detrimental to have a large state space dimension. When using these methods, ideally we should be able to keep the dimensionality of the latent variables/hidden variables/factors low.
In this work we show that (1) prediction measures for goodness of learned dynamics model is more informative quantification and should be the standard measure for testing generative models; (2) using dynamics model that decouple expressive power from state space dimensionality makes model selection easier, produce better long term predictions, and we claim it makes the model more interpretable.},
}

@Article{Zhao2019a,
  author        = {Zhao, Yuan and Nassar, Josue and Jordan, Ian and Bugallo, M{\'o}nica and Park, Il Memming},
  title         = {Streaming Variational {M}onte {C}arlo},
  year          = {2022},
  journal	= {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  abstract      = {Nonlinear state-space models are powerful tools to describe
                   dynamical structures in complex time series. In a streaming
                   setting where data are processed one sample at a time,
                   simultaneously inferring the state and their nonlinear
                   dynamics has posed significant challenges in practice. We
                   develop a novel online learning framework, leveraging
                   variational inference and sequential Monte Carlo, which
                   enables flexible and accurate Bayesian joint filtering. Our
                   method provides a filtering posterior arbitrarily close to
                   the true filtering distribution for a wide class of dynamics
                   models and observation models. Specifically, the proposed
                   framework can efficiently infer a posterior over the
                   dynamics using sparse Gaussian processes. Constant time
                   complexity per sample makes our approach amenable to online
                   learning scenarios and suitable for real-time applications.},
  archiveprefix = {arXiv},
  eprint        = {1906.01549},
  primaryclass  = {stat.ML},
  journal       = "IEEE transactions on pattern analysis and machine intelligence",
  volume        =  45,
  number        =  1,
  pages         = "1150--1161",
  month         =  feb,
  year          =  2022,
  url           = "http://dx.doi.org/10.1109/TPAMI.2022.3153225",
  issn          = "0162-8828",
  PMID          = "35201981",
  PMCID         = "10082974",
  arxivid       = "1906.01549",
  doi           = "10.1109/TPAMI.2022.3153225",
  code		= {https://github.com/catniplab/svmc},
}

@InProceedings{Vermani2022a,
    author    = {Ayesha Vermani and Ke Chen and Joshua Kogan and Alfredo Fontanini and Il Memming Park},
    title     = {Can time dependent and invariant decoders co-exist?},
    booktitle = {Computational and Systems Neuroscience ({COSYNE})},
    year      = {2022},
}

@Article{Neophytou2021b,
  author        = {Neophytou, Demetrios and Arribas, Diego and Levy, Robert and Arora, Tushar and Park, Il Memming and Oviedo, Hysell V},
  journal       = {PLoS Biology},
  title         = {Differences in temporal processing speeds between the right and left auditory cortex reflect the strength of recurrent synaptic connectivity},
  year          = {2022},
  month         = oct,
  number        = {10},
  pages         = {e3001803},
  volume        = {20},
  abstract      = {Brain asymmetry in the sensitivity to spectrotemporal modulation is an established functional feature that underlies the perception of speech and music. The left Auditory Cortex (ACx) is believed to specialize in processing fast temporal components of speech sounds, and the right ACx slower components. However, the circuit features and neural computations behind these lateralized spectrotemporal processes are poorly understood. To answer these mechanistic questions we use mice, an animal model that captures some relevant features of human communication systems. In this study, we screened for circuit features that could subserve temporal integration differences between the left and right ACx. We mapped excitatory input to principal neurons in all cortical layers and found significantly stronger recurrent connections in the superficial layers of the right ACx compared to the left. We hypothesized that the underlying recurrent neural dynamics would exhibit differential characteristic timescales corresponding to their hemispheric specialization. To investigate, we recorded spike trains from awake mice and estimated the network time constants using a statistical method to combine evidence from multiple weak signal-to-noise ratio neurons. We found longer temporal integration windows in the superficial layers of the right ACx compared to the left as predicted by stronger recurrent excitation. Our study shows substantial evidence linking stronger recurrent synaptic connections to longer network timescales. These findings support speech processing theories that purport asymmetry in temporal integration is a crucial feature of lateralization in auditory processing.},
  archiveprefix = {bioRxiv},
  doi           = {10.1371/journal.pbio.3001803},
  doi_biorxiv   = {10.1101/2021.04.14.439872},
  language      = {en},
  url           = {https://www.biorxiv.org/content/10.1101/2021.04.14.439872},
  PMID = {36269764},
  PMCID = {9629599},
}

@PhdThesis{Nassar2022a,
  author = {Josue Nassar},
  school = {Stony Brook University},
  title  = {Bayesian Machine Learning for Analyzing and Controlling Neural Populations},
  year   = {2022},
}

@PhdThesis{Jordan2022a,
  author = {Ian D. Jordan},
  school = {Stony Brook Univeersity},
  title  = {Metastable Dynamics Underlying Neural Computation},
  year   = {2022},
}

@InProceedings{Dowling2022a,
    title     = "{Hida-Mat{\'e}rn} Gaussian Processes",
    author    = "Dowling, Matthew and Sok{\'o}{\l}, Piotr and Park, Il Memming",
    booktitle = {Computational and Systems Neuroscience ({COSYNE})},
    year      = {2022},
}

@Article{Brinkman2021a,
  author        = {Brinkman, Braden A W and Yan, Han and Maffei, Arianna and Park, Il Memming and Fontanini, Alfredo and Wang, Jin and La Camera, Giancarlo},
  journal       = {Applied Physics Reviews},
  title         = {Metastable dynamics of neural circuits and networks},
  year          = {2022},
  abstract      = {Cortical neurons emit seemingly erratic trains of action
                   potentials, or ``spikes'', and neural network dynamics
                   emerge from the coordinated spiking activity within neural
                   circuits. These rich dynamics manifest themselves in a
                   variety of patterns which emerge spontaneously or in
                   response to incoming activity produced by sensory inputs. In
                   this review, we focus on neural dynamics that is best
                   understood as a sequence of repeated activations of a number
                   of discrete hidden states. These transiently occupied states
                   are termed ``metastable'' and have been linked to important
                   sensory and cognitive functions. In the rodent gustatory
                   cortex, for instance, metastable dynamics have been
                   associated with stimulus coding, with states of expectation,
                   and with decision making. In frontal, parietal and motor
                   areas of macaques, metastable activity has been related to
                   behavioral performance, choice behavior, task difficulty,
                   and attention. In this article, we review the experimental
                   evidence for neural metastable dynamics together with
                   theoretical approaches to the study of metastable activity
                   in neural circuits. These approaches include: (i) a
                   theoretical framework based on non-equilibrium statistical
                   physics for network dynamics; (ii) statistical approaches to
                   extract information about metastable states from a variety
                   of neural signals, and (iii) recent neural network
                   approaches, informed by experimental results, to model the
                   emergence of metastable dynamics. By discussing these
                   topics, we aim to provide a cohesive view of how transitions
                   between different states of activity may provide the neural
                   underpinnings for essential functions such as perception,
                   memory, expectation or decision making, and more generally,
                   how the study of metastable neural activity may advance our
                   understanding of neural circuit function in health and
                   disease.},
  archiveprefix = {arXiv},
  PMID = {35284030},
  PMCID         = {8900181},
  arxivid       = {2110.03025},
  eprint        = {2110.03025},
  primaryclass  = {q-bio.NC},
  doi           = {10.1063/5.0062603},
  publisher = "American Institute of Physics",
  volume    =  9,
  number    =  1,
  pages     = "011313",
  month     =  mar,
  year      =  2022,
}

@InProceedings{Pei2021b,
  author        = {Pei, Felix and Ye, Joel and Zoltowski, David and Wu, Anqi and Chowdhury, Raeed H and Sohn, Hansem and O'Doherty, Joseph E and Shenoy, Krishna V and Kaufman, Matthew T and Churchland, Mark and Jazayeri, Mehrdad and Miller, Lee E and Pillow, Jonathan and Park, Il Memming and Dyer, Eva L and Pandarinath, Chethan},
  booktitle     = {Advances in Neural Information Processing Systems (NeurIPS)},
  title         = {Neural Latents Benchmark '21: Evaluating latent variable models of neural population activity},
  year          = {2021},
  month         = sep,
  note          = {(PF and YU are co-first authors)},
  abstract      = {Advances in neural recording present increasing
                   opportunities to study neural activity in unprecedented
                   detail. Latent variable models (LVMs) are promising tools
                   for analyzing this rich activity across diverse neural
                   systems and behaviors, as LVMs do not depend on known
                   relationships between the activity and external experimental
                   variables. However, progress in latent variable modeling is
                   currently impeded by a lack of standardization, resulting in
                   methods being developed and compared in an ad hoc manner. To
                   coordinate these modeling efforts, we introduce a benchmark
                   suite for latent variable modeling of neural population
                   activity. We curate four datasets of neural spiking activity
                   from cognitive, sensory, and motor areas to promote models
                   that apply to the wide variety of activity seen across these
                   areas. We identify unsupervised evaluation as a common
                   framework for evaluating models across datasets, and apply
                   several baselines that demonstrate benchmark diversity. We
                   release this benchmark through EvalAI.
                   http://neurallatents.github.io},
  archiveprefix = {arXiv},
  arxivid       = {2109.04463},
  eprint        = {2109.04463},
  primaryclass  = {cs.LG},
  url           = {https://openreview.net/forum?id=KVMS3fl4Rsv},
}

@INPROCEEDINGS{Pei2021a,
    author = {Felix Pei and Joel Ye and Andrew R. Sedler and David Zoltowski and Anqi Wu and Raeed H. Chowdhury and Hansem Sohn and Joseph E. O'Doherty and Krishna V. Shenoy and Matthew T. Kaufman and Mark Churchland and Mehrdad Jazayeri and Lee E. Miller and Il Memming Park and Eva Dyer and Jonathan Pillow and Chethan Pandarinath},
    title = {Advancing the investigation of neural population structure with the Neural Latents Benchmark},
    booktitle = {Society for Neuroscience},
    year = {2021},
}

@InProceedings{Neophytou2021a,
  author    = {Demetrios Neophytou and Diego Arribas and Hysell Oviedo and Il Memming Park},
  title     = {{Quasi-Bayesian} estimation of time constants supports lateralized auditory computation},
  booktitle = {Computational and Systems Neuroscience ({COSYNE})},
  year      = {2021},
}

@Conference{Jordan2021a,
  author    = {Ian Jordan and Piotr Sokol and Il Memming Park},
  title     = {Mechanisms Underlying Sequence-to-Sequence Working Memory},
  booktitle = {DeepMath},
  year      = {2021},
  url       = {https://www.deepmath-conference.com},
}

@Article{Jordan2019a,
  author        = {Jordan, Ian D and Sokol, Piotr Aleksander and Park, Il Memming},
  journal       = {Frontiers in Computational Neuroscience},
  title         = {Gated recurrent units viewed through the lens of continuous time dynamical systems},
  year          = {2021},
  abstract      = {Gated recurrent units (GRUs) are specialized memory elements for building recurrent neural networks. Despite their incredible success on various tasks, including extracting dynamics underlying neural data, little is understood about the specific dynamics representable in a GRU network. As a result, it is both difficult to know a priori how successful a GRU network will perform on a given task, and also their capacity to mimic the underlying behavior of their biological counterparts. Using a continuous time analysis, we gain intuition on the inner workings of GRU networks. We restrict our presentation to low dimensions, allowing for a comprehensive visualization. We found a surprisingly rich repertoire of dynamical features that includes stable limit cycles (nonlinear oscillations), multi-stable dynamics with various topologies, and homoclinic bifurcations. At the same time we were unable to train GRU networks to produce continuous attractors, which are hypothesized to exist in biological neural networks. We contextualize the usefulness of different kinds of observed dynamics and support our claims experimentally.},
  archiveprefix = {arXiv},
  eprint        = {1906.01005},
  primaryclass  = {cs.LG},
  PMCID         = {8339926},
  PMID          = {34366817},
  doi           = "10.3389/fncom.2021.678158",
}

@Unpublished{Dowling2021b,
  title         = "{Hida-Mat{\'e}rn} Kernel",
  author        = "Dowling, Matthew and Sok{\'o}{\l}, Piotr and Park, Il
                   Memming",
  abstract      = "We present the class of Hida-Mat\textbackslash'ern kernels,
                   which is the canonical family of covariance functions over
                   the entire space of stationary Gauss-Markov Processes. It
                   extends upon Mat\textbackslash'ern kernels, by allowing for
                   flexible construction of priors over processes with
                   oscillatory components. Any stationary kernel, including the
                   widely used squared-exponential and spectral mixture
                   kernels, are either directly within this class or are
                   appropriate asymptotic limits, demonstrating the generality
                   of this class. Taking advantage of its Markovian nature we
                   show how to represent such processes as state space models
                   using only the kernel and its derivatives. In turn this
                   allows us to perform Gaussian Process inference more
                   efficiently and side step the usual computational burdens.
                   We also show how exploiting special properties of the state
                   space representation enables improved numerical stability in
                   addition to further reductions of computational complexity.",
  month         =  jul,
  year          =  2021,
  url           = "http://arxiv.org/abs/2107.07098",
  archivePrefix = "arXiv",
  eprint        = "2107.07098",
  primaryClass  = "stat.ML",
  arxivid       = "2107.07098"
}

@InProceedings{Dowling2021a,
  author    = {Matthew Dowling and Yuan Zhao and Memming Park},
  title     = {{NP-GLM}: Nonparametric {GLM}},
  booktitle = {Computational and Systems Neuroscience ({COSYNE})},
  year      = {2021},
}

@InProceedings{Arribas2021a,
  author    = {Diego Arribas and Yuan Zhao and Memming Park},
  title     = {Framework to generate more realistic GLM spike trains},
  booktitle = {Computational and Systems Neuroscience ({COSYNE})},
  year      = {2021},
}

@InProceedings{Zhao2020a,
  author    = {Yuan Zhao and Josue Nassar and Il Memming Park},
  title     = {Real-time discovery of effective dynamics from streaming noisy neural observations},
  booktitle = {Computational and Systems Neuroscience ({COSYNE})},
  year      = {2020},
}

@Article{Zhao2019b,
  author         = {Yuan Zhao and Jacob L. Yates and Aaron Levi and Alex Huk and Il Memming Park},
  journal        = {{PLOS} Computational Biology},
  title          = {Stimulus-choice (mis)alignment in primate area {MT}},
  year           = {2020},
  month          = may,
  _archiveprefix = {bioRxiv},
  _pages         = {884189+},
  _publisher     = {Cold Spring Harbor Labs Journals},
  abstract       = {For stimuli near perceptual threshold, the trial-by-trial activity of single neurons in many sensory areas is correlated with the animal's perceptual report. This phenomenon has often been attributed to feedforward readout of the neural activity by the downstream decision-making circuits. The interpretation of choice-correlated activity is quite ambiguous, but its meaning can be better understood in the light of population-wide correlations among sensory neurons. Using a statistical nonlinear dimensionality reduction technique on single-trial ensemble recordings from the middle temporal area during perceptual-decision-making, we extracted low-dimensional neural trajectories that captured the population-wide fluctuations. We dissected the particular contributions of sensory-driven versus choice-correlated activity in the low-dimensional population code. We found that the neural trajectories strongly encoded the direction of the stimulus in single dimension with a temporal signature similar to that of single MT neurons. If the downstream circuit were optimally utilizing this information, choice-correlated signals should be aligned with this stimulus encoding dimension. Surprisingly, we found that a large component of the choice information resides in the subspace orthogonal to the stimulus representation inconsistent with the optimal readout view. This misaligned choice information allows the feedforward sensory information to coexist with the decision-making process. The time course of these signals suggest that this misaligned contribution likely is feedback from the downstream areas. We hypothesize that this non-corrupting choice-correlated feedback might be related to learning or reinforcing sensory-motor relations in the sensory population.},
  data           = {https://doi.org/10.6084/m9.figshare.11413182.v1},
  doi            = {10.1371/journal.pcbi.1007614},
  youtube        = {https://youtu.be/qMDYBgxIxdU},
  url            = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007614},
  PMID = {32421716},
  PMCID = {7259805},
}

@Article{Zhao2017c,
  title         = {Variational online learning of neural dynamics},
  author        = "Zhao, Yuan and Park, Il Memming",
  abstract      = "State space models provide an interpretable framework for
                   complex time series by combining an intuitive dynamical
                   system model with a probabilistic observation model. We
                   developed a flexible online learning framework for latent
                   nonlinear state dynamics and filtered latent states. Our
                   method utilizes the stochastic gradient variational Bayes
                   method to jointly optimize the parameters of the nonlinear
                   dynamics, observation model, and the black-box recognition
                   model. Unlike previous approaches, our framework can
                   incorporate non-trivial observation noise models and has
                   potential of inferring in real-time. We test our method on
                   point process and Gaussian observations driven by continuous
                   attractor dynamics and nonstationary systems, demonstrating
                   its ability to recover the phase portrait, filtered
                   trajectory, and produce long-term predictions for online
                   machine learning.",
  year          =  2020,
  month         = oct,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1707.09049",
  PMCID         = {7591751},
  PMID          = {33154718},
  doi           = {10.3389/fncom.2020.00071},
  journal       = {Frontiers in Computational Neuroscience},
  code          = {https://github.com/catniplab/vjf.git}
}

@InProceedings{Sokol2018a,
  author        = {Piotr Sokol and Il Memming Park},
  title         = {Information geometry of orthogonal initializations and training},
  booktitle     = {International Conference on Learning Representations (ICLR)},
  year          = {2020},
  abstract      = {Recently mean field theory has been successfully used to analyze properties of
wide, random neural networks. It gave rise to a prescriptive theory for initializing
feed-forward neural networks with orthogonal weights, which ensures that both the
forward propagated activations and the backpropagated gradients are near `2 isometries
and as a consequence training is orders of magnitude faster. Despite strong
empirical performance, the mechanisms by which critical initializations confer an
advantage in the optimization of deep neural networks are poorly understood. Here
we show a novel connection between the maximum curvature of the optimization
landscape (gradient smoothness) as measured by the Fisher information matrix
(FIM) and the spectral radius of the input-output Jacobian, which partially explains
why more isometric networks can train much faster. Furthermore, given that orthogonal
weights are necessary to ensure that gradient norms are approximately
preserved at initialization, we experimentally investigate the benefits of maintaining
orthogonality throughout training, and we conclude that manifold optimization of
weights performs well regardless of the smoothness of the gradients. Moreover,
we observe a surprising yet robust behavior of highly isometric initializations-
even though such networks have a lower FIM condition number at initialization,
and therefore by analogy to convex functions should be easier to optimize, experimentally
they prove to be much harder to train with stochastic gradient descent.
We propose an explanation for this phenomenon by exploting connections between
Fisher geometry and the recently introduced Neural Tangent Kernel.},
  archiveprefix = {arXiv},
  day           = {9},
  eprint        = {1810.03785},
  keywords      = {deep-learning, machine-learning, mean-field},
  posted-at     = {2018-10-10 15:14:09},
  url           = {https://openreview.net/forum?id=rkg1ngrFPr},
}

@Unpublished{Park2017a,
    author = {Park, Il Memming and Pillow, Jonathan W.},
    day = {22},
    doi = {10.1101/178418},
    archiveprefix = {bioRxiv},
    journal = {bioRxiv},
    keywords = {bayesian-efficient-coding, preprint, tuning-curve},
    month = jul,
    pages = {178418+},
    publisher = {Cold Spring Harbor Labs Journals},
    title = {{B}ayesian Efficient Coding},
    url = {https://www.biorxiv.org/content/10.1101/178418v3},
    year = {2020},
    data = {https://doi.org/10.5281/zenodo.3970842},
    note = {(under review)}
}

@Conference{Nassar2020d,
  author    = {Josue Nassar and Piotr Sokol and Sue Yeon Chung and Kenneth Harris and Il Memming Park},
  title     = {On ${1/n}$ neural representation and robustness},
  booktitle = {DeepMath},
  year      = {2020},
  url       = {https://www.deepmath-conference.com},
}

@InProceedings{Nassar2020c,
  author    = {Josue Nassar and Piotr Sokol and SueYeon Chang and Kenneth Harris and Il Memming Park},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  title     = {On $1/n$ neural representation and robustness},
  year      = {2020},
  note      = {(JN and PS are co-first authors)},
  abstract  = {Understanding the nature of representation in neural networks is a goal shared by neuroscience and machine learning. It is therefore exciting that both fields converge not only on shared questions but also on similar approaches. A pressing question in these areas is understanding how the structure of the representation used by neural networks affects both their generalization, and robustness to perturbations. In this work, we investigate the latter by juxtaposing experimental results regarding the covariance spectrum of neural representations in the mouse V1 (Stringer et al) with artificial neural networks. We use adversarial robustness to probe Stringer et al's theory regarding the causal role of a 1/n covariance spectrum. We empirically investigate the benefits such a neural code confers in neural networks, and illuminate its role in multi-layer architectures. Our results show that imposing the experimentally observed structure on artificial neural networks makes them more robust to adversarial attacks. Moreover, our findings complement the existing theory relating wide neural networks to kernel methods, by showing the role of intermediate representations.},
  month         =  oct,
  archivePrefix = "arXiv",
  eprint        = "2012.04729",
  primaryClass  = "cs.LG",
  arxivid   = {2012.04729},
  url       = {https://papers.nips.cc/paper/2020/hash/44bf89b63173d40fb39f9842e308b3f9-Abstract.html},
}

@InProceedings{Nassar2020a,
  author    = {Josue Nassar and Piotr Sokol and SueYeon Chung and Kenneth Harris and Il Memming Park},
  title     = {Spectral regularization in biological and artificial neural networks},
  booktitle = {Computational and Systems Neuroscience ({COSYNE})},
  year      = {2020},
}

@Article{Michaelis2020a,
  author   = {Brenden Michaelis and Kyle Leathers and Barry Ache and Yuriy Bobkov and Jose Principe and Raheleh Baharloo and Il Memming Park and Matthew Reidenbach},
  journal  = {Scientific Reports},
  title    = {Odor tracking in marine organisms: the importance of temporal and spatial intermittency of the odor signal},
  year     = {2020},
  issn     = {2045-2322},
  month    = may,
  pages    = {7961},
  volume   = {10},
  abstract = {In aquatic and terrestrial environments, odorants are dispersed by currents that create concentration distributions that are spatially and temporally complex. Animals navigating in a plume must therefore rely upon intermittent, and time-varying information to find the source. Navigation has typically been studied as a spatial information problem, with the aim of movement towards higher mean concentrations. However, this spatial information alone, without information of the temporal dynamics of the plume, is insufficient to explain the accuracy and speed of many animals tracking odors. Recent studies have identified a subpopulation of olfactory receptor neurons (ORNs) that consist of intrinsically rhythmically active `bursting' ORNs (bORNs) in the lobster, Panulirus argus. As a population, bORNs provide a neural mechanism dedicated to encoding the time between odor encounters. Using a numerical simulation of a large-scale plume, the lobster is used as a framework to construct a computer model to examine the utility of intermittency for orienting within a plume. Results show that plume intermittency is reliably detectable when sampling simulated odorants on the order of seconds, and provides the most information when animals search along the plume edge. Both the temporal and spatial variation in intermittency is predictably structured on scales relevant for a searching animal that encodes olfactory information utilizing bORNs, and therefore is suitable and useful as a navigational cue.},
  doi      = {10.1038/s41598-020-64766-y},
  PMID = {32409665},
  PMCID = {7224200},
}


@InProceedings{Kepple2020a,
    author = {Daniel R. Kepple and Daewon Lee and Colin Prepscius and Volkan Isler and Il Memming Park and Daniel L. Lee},
    title = {Jointly learning visual motion and confidence from local patches in event cameras},
    year = {2020},
    booktitle = {16th {E}uropean conference on computer vision ({ECCV}2020)},
    note = {spotlight},
    doi = {10.1007/978-3-030-58539-6_30}
}

@Article{Jordan2019b,
  author        = {Jordan, Ian D. and Park, Il Memming},
  journal       = {Entropy},
  title         = {Birhythmic analog circuit maze: A nonlinear neurostimulation testbed},
  year          = {2020},
  month         = may,
  number        = {5},
  pages         = {537},
  volume        = {22},
  abstract      = {Brain dynamics can exhibit narrow-band nonlinear oscillations and multistability. For a subset of disorders of consciousness and motor control, we hypothesize that some symptoms originate from the inability to spontaneously transition from one attractor to another. Using external perturbations, such as electrical pulses delivered by deep brain stimulation devices, it may be possible to induce such transition out of the pathological attractors. However, the induction of transition may be non-trivial, rendering the current open-loop stimulation strategies insufficient. In order to develop next-generation neural stimulators that can intelligently learn to induce attractor transitions, we require a platform to test the efficacy of such systems. To this end, we designed an analog circuit as a model for the multistable brain dynamics. The circuit spontaneously oscillates stably on two periods as an instantiation of a 3-dimensional continuous-time gated recurrent neural network. To discourage simple perturbation strategies such as constant or random stimulation patterns from easily inducing transition between the stable limit cycles, we designed a state-dependent nonlinear circuit interface for external perturbation. We demonstrate the existence of nontrivial solutions to the transition problem in our circuit implementation.},
  archiveprefix = {arXiv},
  arxivid       = {2004.10658},
  doi           = {10.3390/e22050537},
  eprint        = {2004.10658},
  language      = {en},
  primaryclass  = {q-bio.NC},
  PMCID         = {7517031},
  PMID          = {33286310},
  publisher     = {Multidisciplinary Digital Publishing Institute},
  url           = {https://www.mdpi.com/1099-4300/22/5/537},
}

@Unpublished{Dowling2020a,
  author        = {Dowling, Matthew and Zhao, Yuan and Park, Il Memming},
  title         = {Non-parametric generalized linear model},
  month         = sep,
  year          = {2020},
  abstract      = {A fundamental problem in statistical neuroscience is to
                   model how neurons encode information by analyzing
                   electrophysiological recordings. A popular and widely-used
                   approach is to fit the spike trains with an autoregressive
                   point process model. These models are characterized by a set
                   of convolutional temporal filters, whose subsequent analysis
                   can help reveal how neurons encode stimuli, interact with
                   each other, and process information. In practice a
                   sufficiently rich but small ensemble of temporal basis
                   functions needs to be chosen to parameterize the filters.
                   However, obtaining a satisfactory fit often requires
                   burdensome model selection and fine tuning the form of the
                   basis functions and their temporal span. In this paper we
                   propose a nonparametric approach for jointly inferring the
                   filters and hyperparameters using the Gaussian process
                   framework. Our method is computationally efficient taking
                   advantage of the sparse variational approximation while
                   being flexible and rich enough to characterize arbitrary
                   filters in continuous time lag. Moreover, our method
                   automatically learns the temporal span of the filter. For
                   the particular application in neuroscience, we designed
                   priors for stimulus and history filters useful for the spike
                   trains. We compare and validate our method on simulated and
                   real neural spike train data.},
  archiveprefix = {arXiv},
  arxivid       = {2009.01362},
  eprint        = {2009.01362},
  primaryclass  = {stat.ML},
}

@Article{Dikecligil2019a,
  author = {Gulce Nazli Dikecligil and Dustin Graham and Il Memming Park and Alfredo Fontanini},
  title = {Layer and cell type specific response properties of gustatory cortex neurons in awake mice},
  journal = {Journal of Neuroscience},
  doi = {10.1523/JNEUROSCI.1579-19.2020},
  year = {2020}
}

@InProceedings{Arribas2020a,
  author    = {Diego M. Arribas and Yuan Zhao and Il Memming Park},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  title     = {Rescuing neural spike train models from bad {MLE}},
  abstract      = "The standard approach to fitting an autoregressive spike
                   train model is to maximize the likelihood for one-step
                   prediction. This maximum likelihood estimation (MLE) often
                   leads to models that perform poorly when generating samples
                   recursively for more than one time step. Moreover, the
                   generated spike trains can fail to capture important
                   features of the data and even show diverging firing rates.
                   To alleviate this, we propose to directly minimize the
                   divergence between neural recorded and model generated spike
                   trains using spike train kernels. We develop a method that
                   stochastically optimizes the maximum mean discrepancy
                   induced by the kernel. Experiments performed on both real
                   and synthetic neural data validate the proposed approach,
                   showing that it leads to well-behaving models. Using
                   different combinations of spike train kernels, we show that
                   we can control the trade-off between different features
                   which is critical for dealing with model-mismatch.",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  eprint        = "2010.12362",
  primaryClass  = "stat.ML",
  arxivid       = "2010.12362",
  code		= {https://github.com/catniplab/mmd-glm},
  url		= {https://papers.nips.cc/paper/2020/hash/186b690e29892f137b4c34cfa40a3a4d-Abstract.html},
}

@Conference{Sokol2019c,
  author    = {Piotr Sokol and Ian Jordan and Il Memming Park},
  title     = {Information geometry at initialization and beyond},
  booktitle = {DeepMath},
  year      = {2019},
  url       = {https://www.deepmath-conference.com},
}

@Conference{Sokol2019b,
  author    = {Piotr Sokol and Il Memming Park},
  title     = {Limit Cycle Neural Networks Have Infinite Memory},
  booktitle = {DeepMath},
  year      = {2019},
  url       = {https://www.deepmath-conference.com},
}

@Conference{Sokol2020d,
  author    = {Piotr A. Sokol and Ian Jordan and Eben Kadile and Il Memming Park},
  title = "Unexpected benefits of learning with neural oscillation: stable backpropagation with limit cycles",
  booktitle = "From Neuroscience to Artificially Intelligent Systems (NAISys)",
  year = 2020,
  poster_no = "168",
}

@InProceedings{Sokol2019a,
  author    = {Piotr Sokol and Ian Jordan and Eben Kadile and Il Memming Park},
  booktitle = {53rd Asilomar Conference on Signals, Systems and Computers},
  title     = {Adjoint dynamics of stable limit cycle neural networks},
  year      = {2019},
  abstract  = {Exploding and vanishing gradient are both major problems often faced when an artificial neural network is trained with gradient descent. Inspired by the ubiquity and robustness of nonlinear oscillations in biological neural systems, we investigate the properties of their artificial counterpart, the stable limit cycle neural networks. Using a continuous time dynamical system interpretation of neural networks and backpropagation, we show that stable limit cycle neural networks have non-exploding gradients, and at least one effective non-vanishing gradient dimension. We conjecture that limit cycles can support the learning of long temporal dependence in both biological and artificial neural networks.},
  doi       = {10.1109/IEEECONF44664.2019.9049080},
  keywords  = {limit cycle; backpropagation},
  pdf       = {Sokol2019a.pdf},
}

@InProceedings{Nassar2019a,
  author    = {Josue Nassar and Scott Linderman and Il Memming Park and Monica Bugallo},
  title     = {Tree-structured locally linear dynamics model to uproot {B}ayesian neural data analysis},
  booktitle = {Computational and Systems Neuroscience ({COSYNE})},
  year      = {2019},
}

@InProceedings{Nassar2018b,
  author        = {Josue Nassar and Scott W. Linderman and Monica Bugallo and Il Memming Park},
  title         = {Tree-Structured Recurrent Switching Linear Dynamical Systems for Multi-Scale Modeling},
  booktitle     = {International Conference on Learning Representations (ICLR)},
  year          = {2019},
  month         = nov,
  abstract      = {Many real-world systems studied are governed by complex, nonlinear dynamics. By modeling these dynamics, we can gain insight into how these systems work, make predictions about how they will behave, and develop strategies for controlling them. While there are many methods for modeling nonlinear dynamical systems, existing techniques face a trade off between offering interpretable descriptions and making accurate predictions. Here, we develop a class of models that aims to achieve both simultaneously, smoothly interpolating between simple descriptions and more complex, yet also more accurate models. Our probabilistic model achieves this multi-scale property through a hierarchy of locally linear dynamics that jointly approximate global nonlinear dynamics. We call it the tree-structured recurrent switching linear dynamical system. To fit this model, we present a fully-Bayesian sampling procedure using Polya-Gamma data augmentation to allow for fast and conjugate Gibbs sampling. Through a variety of synthetic and real examples, we show how these models outperform existing methods in both interpretability and predictive capability.},
  archiveprefix = {arXiv},
  day           = {30},
  eprint        = {1811.12386},
  keywords      = {neural-dynamics, statistical-neuroscience},
  url           = {https://openreview.net/forum?id=HkzRQhR9YX},
  code          = {https://github.com/catniplab/tree_structured_rslds},
}

@InProceedings{Mofakham2019b,
  author    = {Sima Mofakham and Adam Fry and Joseph Adachi and Nathan Winans and Justine Liang and Bradley Ashcroft and Himanshu Sharma and Susan Fiore and Il Memming Park and Charles Mikell},
  title     = {Electrophysiological prognostication of functional cortical integrity after traumatic brain injury},
  booktitle = {American Association of Neurological Surgeons ({AANS})},
  year      = {2019},
  abstract  = {Whether and how consciousness returns after severe traumatic brain injury (TBI) is not clear, mainly due to our lack of understanding of how consciousness arises. Previous studies revealed that correlated activity across a distributed network of cortical cells, the frontoparietal network (FPN), is required for return of consciousness. But data on the transition from coma to wakefulness are sparse. To address this gap, we performed depth electrode recordings of the anterior cingulate gyrus (ACC) and dorsolateral prefrontal cortex (DLPFC) along with simultaneous scalp EEG recording in patients recovering from coma after TBI. We sought to extract predictive electrophysiological biomarkers of regaining consciousness after traumatic brain injury. Methods: In this study one healthy control (epilepsy) and five comatose traumatic brain injury patients with GCS < 8 and with no major brain structural abnormalities were enrolled. Upon obtaining informed consent by a legally authorized representative, we implanted a seizure monitoring stereotactic (10-contact) depth electrode spanning from DLPFC to ACC. To probe the functional integrity of cortical webs we administered repetitive single-pulse stimulation to ACC and DLPFC (100 $\mu$S, 1 Hz stimulation, 100 reps). Cortico-cortical evoked potentials (CCEPs) were recorded both via the implanted depth electrode and scalp contacts. Results: We found that recovery of consciousness is associated with increasing amplitude and complexity of CCEPs in DLFPC. While all patients had increased amplitude over time, patients who recovered developed polyphasic, complex waveforms, as measured by zero crossing and permutation entropy (n = 5). The patients who did not recover (n = 2) had large-amplitude, but simple-waveform CCEP responses. Conclusion: The characteristics of the evoked responses to electrical stimulation may be a useful surrogate measurement of the integrity of cortical networks required for consciousness. CCEPs potentially be used as an indicator of integrity of cortex to predict outcome after TBI.},
  owner     = {memming},
}

@InProceedings{Mofakham2019a,
  author    = {Sima Mofakham and Adam Fry and Joseph Adachi and Bradley Ashcroft and Nathan Winans and Justine Liang and Himanshu Sharma and Susan Fiore and Il Memming Park and Charles Mikell},
  title     = {Recovery of consciousness after traumatic brain injury: Biomarkers and a mechanistic model},
  booktitle = {Computational and Systems Neuroscience ({COSYNE})},
  year      = {2019},
}

@Article{Hocker2019a,
  author    = {David Hocker and Il Memming Park},
  journal   = {{PLOS} Computational Biology},
  title     = {Myopic control of neural dynamics},
  year      = {2019},
  month     = mar,
  abstract  = {Manipulating the dynamics of neural systems through targeted stimulation is a frontier of research and clinical neuroscience; however, the control schemes considered for neural systems are mismatched for the unique needs of manipulating neural dynamics. An appropriate control method should respect the variability in neural systems, incorporating moment to moment ``input'' to the neural dynamics and behaving based on the current neural state, irrespective of the past trajectory. We propose such a controller under a nonlinear state-space feedback framework that steers one dynamical system to function as through it were another dynamical system entirely. This ``myopic'' controller is formulated through a novel variant of a model reference control cost that manipulates dynamics in a step-wise manner, omitting the need to pre-calculate a rigid and computationally costly neural feedback control solution. To demonstrate the breadth of this control's utility, two examples with distinctly different applications in neuroscience are studied. First an unhealthy motor-like system containing an unwanted beta-oscillation spiral attractor is controlled to function as a healthy motor system, a relevant clinical example for neurological disorders. Second, we show the myopic control's utility to probe the causal dynamics of cognitive processes by transforming a winner-take-all decision-making system to operate as a robust neural integrator of evidence.},
  doi       = {10.1371/journal.pcbi.1006854},
  owner     = {memming},
  timestamp = {2017.12.31},
  PMID = {30856171},
  PMCID = {6428347},
  url       = {https://www.biorxiv.org/content/early/2017/12/30/241299},
}

@InProceedings{Bobkov2019a,
  author    = {Yuriy Bobkov and Il Park and Brenden T. Michaelis and Tom Matthews and Matthew A. Reidenbach and Jos\'e C. Pr\'incipe and Barry Ache},
  title     = {Coding spatiotemporal characteristics of odor signals},
  booktitle = {Association for Chemoreception (AChemS) Annual Meeting},
  year      = {2019},
  abstract  = {Odor signals are generally assumed to be processed based on input from canonical tonically active primary olfactory receptor neurons (ORNs). Recent evidence suggests, however, that a subpopulation of ORNs consists of intrinsically rhythmically active, `bursting' ORNs (bORNS) with unique functional properties. (1) They encode olfactory information by having their rhythmicity entrained by the odor stimulus; they don't discharge phaso-tonically as do canonical ORNs.  (2) As a population they can accurately and instantaneously encode the interval since the last odor encounter up to 10s of seconds using information inherent in the olfactory modality.  (3) Particularly interesting is that they can do this without the need to implicate a memory mechanism.  Given the structure inherent in turbulent odor plumes, the ability of bORNs to encode the time between `whiffs' would provide a novel way of navigating turbulent plumes. Indeed, recent computational modeling shows an `animat' can navigate odor plumes with considerable more efficiency using odor time than odor concentration.   To further support and expand this computational model, we are now using combined physiological and morphological analysis to characterize the molecular receptive range of bORNs, map their central projection, and begin to identify the strategy for synaptic processing of bORN-derived information at the first olfactory relay.  While more work clearly needs to be done, and hopefully our work will encourage others to do that, our findings suggest encoding odor time is a fundamental feature of olfaction that potentially can be used to navigate odor plumes in animals as diverse as crustaceans and mammals.},
  owner     = {memming},
}

@InProceedings{Zhao2018a,
  author    = {Yuan Zhao and Il Memming Park},
  title     = {Accessing neural states in real time: recursive variational {B}ayesian dual estimation},
  booktitle = {Computational and Systems Neuroscience ({COSYNE})},
  year      = {2018},
}

@InProceedings{Nassar2018a,
  author    = {Nassar, Josue and Linderman, Scott and Zhao, Yuan and Bugallo, M\'onica and Park, Il Memming},
  title     = {Learning structured neural dynamics from single trial population recording},
  booktitle = {52nd Asilomar Conference on Signals, Systems and Computers},
  year      = {2018},
  abstract  = {To understand the complex nonlinear dynamics of neural circuits, we fit a structured state-space model called \textit{tree-structured recurrent switching linear dynamical system} (TrSLDS) to noisy high-dimensional neural time series.
TrSLDS is a multi-scale hierarchical generative model for the state-space dynamics where each node of the latent tree captures locally linear dynamics.
TrSLDS can be learned efficiently and in a  fully Bayesian manner using Gibbs sampling.
We showcase TrSLDS' potential of inferring low-dimensional interpretable dynamical systems on a variety of examples.},
  url       = {https://ieeexplore.ieee.org/document/8645122},
  pdf       = {Nassar2018a.pdf},
  owner     = {memming},
  timestamp = {2018.07.18},
}

@Article{Kirschen2018a,
  author   = {Kirschen, Gregory W. and Ge, Shaoyu and Park, Il Memming},
  journal  = {Neuroscience Letters},
  title    = {Probability of viral labeling of neural stem cells {\it in vivo}},
  year     = {2018},
  month    = may,
  abstract = {In the neuroscience field over the past several decades, viral vectors have become powerful gene delivery systems to study neural populations of interest. For neural stem cell (NSC) biology, such viruses are often used to birth-date and track NSCs over developmental time in lineage tracing experiments. Yet, the probability of successful infection of a given stem cell in vivo remains unknown. This information would be helpful to inform investigators interested in titrating their viruses to selectively target sparsely-populated clusters of cells in the nervous system. Here, we describe a novel approach to calculate the probability of successful viral infection of NSCs using experimentally-derived cell cluster data from our newly-developed method to sparsely label adult NSCs, and a simple statistical derivation. Others interested in precisely defining their viral infection efficiency can use this method for a variety of basic and translational studies.},
  doi      = {10.1016/j.neulet.2018.05.016},
  keywords = {neural-stem-cells, probability-estimation},
}

@InProceedings{Hocker2018a,
  author    = {David Hocker and Il Memming Park},
  title     = {Myopic Control: A New Control Objective for Neural Population Dynamics},
  booktitle = {Computational and Systems Neuroscience ({COSYNE})},
  year      = {2018},
}

@Article{Esfahany2018b,
  author        = {Esfahany, Kathleen and Siergiej, Isabel and Zhao, Yuan and Park, Il Memming},
  journal       = {eNeuro},
  title         = {Organization of neural population code in mouse visual system},
  year          = {2018},
  month         = jul,
  pages         = {0414--17},
  abstract      = {The mammalian visual system consists of several anatomically distinct areas, layers, and cell types. To understand the role of these subpopulations in visual information processing, we analyzed neural signals recorded from excitatory neurons from various anatomical and functional structures. For each of 186 mice, one of six genetically tagged cell-types and one of six visual areas were targeted while the mouse was passively viewing various visual stimuli. We trained linear classifiers to decode one of six visual stimulus categories with distinct spatiotemporal structures from the population neural activity. We found that neurons in both the primary visual cortex and secondary visual areas show varying degrees of stimulus-specific decodability, and neurons in superficial layers tend to be more informative about the stimulus categories. Additional decoding analyses of directional motion were consistent with these findings. We observed synergy in the population code of direction in several visual areas suggesting area-specific organization of information representation across neurons. These differences in decoding capacities shed light on the specialized organization of neural information processing across anatomically distinct subpopulations, and further establish the mouse as a model for understanding visual perception.},
  archiveprefix = {bioRxiv},
  day           = {6},
  doi           = {10.1523/ENEURO.0414-17.2018},
  publisher     = {Society for Neuroscience},
  url           = {https://www.biorxiv.org/content/early/2018/03/25/220558},
}

@InProceedings{Esfahany2018a,
  author    = {Kathleen Esfahany and Isabel Siergiej and Yuan Zhao and Il Memming Park},
  title     = {Organization of neural population code in mouse visual system},
  booktitle = {Computational and Systems Neuroscience ({COSYNE})},
  year      = {2018},
}

@InProceedings{Bobkov2018a,
  author    = {Yuriy Bobkov and Il Memming Park and Brenden T. Michaelis and Tom Matthews and Matthew A. Reidenbach and Jos\'e C. Pr\'incipe and Barry Ache},
  booktitle = {European Chemoreception Research Organization (ECRO)},
  title     = {Rhythmically discharging olfactory receptor neurons can encode the spatiotemporal characteristics of odor signals within complexfluid environments},
  year      = {2018},
  address   = {W\"urzburg, Germany},
  month     = sep,
  abstract  = {Sensory signals, including olfactory signals, are typically encoded by tonic receptor neurons. A significant portion of the olfactory receptor neurons in some species is intrinsically rhythmically active or `bursting' (bORNS). Rather than phaso-tonically discharging to the odor onset as characteristic of tonic olfactory receptor neurons, in bORNs the frequency of their inherent burst is entrained by the intermittency inherent in turbulent odor plumes. Each bORN responds to a relatively narrow range of stimulus frequencies (intermittency) based on their inherent rate of bursting discharge and the phase dependency of their response to odor stimulation. Using computational and analytical approaches we have shown that heterogeneous populations of such uncoupled oscillatory neurons have the capacity to reliably encode the temporal properties of intermittent odor signals as long as seconds to many tens of seconds that characterize natural odor plumes. In the present study, we expand our current understanding bORN-based encoding by characterizing the molecular receptive range of bORNs, mapping their central projection, and beginning to identify the strategy for the synaptic processing of bORN-derived information at the first olfactory relay.},
  comment   = {Poster session: Theme II - Vertebrate taste and olfaction: periphery (Th-P-054)},
  owner     = {memming},
  timestamp = {2019.01.09},
  url       = {https://coms.events/ECRO2018/data/abstracts/en/abstract_0089.html},
}

@InProceedings{Zhao2017b,
  author    = {Yuan Zhao and Jacob Yates and Il Memming Park},
  title     = {Low-dimensional state-space trajectory of choice at the population level in area {MT}},
  booktitle = {Computational and Systems Neuroscience ({COSYNE})},
  year      = {2017},
  owner     = {memming},
  timestamp = {2017.02.11},
}

@InProceedings{Zhao2017a,
  author    = {Yuan Zhao and Il Memming Park},
  title     = {Gotta infer{'}em all: dynamical features from neural trajectories},
  booktitle = {Computational and Systems Neuroscience ({COSYNE})},
  year      = {2017},
  owner     = {memming},
  timestamp = {2017.02.11},
}

@Article{Zhao2016a,
  author               = {Zhao, Yuan and Park, Il Memming},
  title                = {Variational Latent {G}aussian Process for Recovering Single-Trial Dynamics from Population Spike Trains},
  journal              = {Neural Computation},
  year                 = {2017},
  volume               = {29},
  number               = {5},
  month                = may,
  abstract             = {When governed by underlying low-dimensional dynamics, the interdependence of simultaneously recorded populations of neurons can be explained by a small number of shared factors, or a low-dimensional trajectory. Recovering these latent trajectories, particularly from single-trial population recordings, may help us understand the dynamics that drive neural computation. However, due to the biophysical constraints and noise in the spike trains, inferring trajectories from data is a challenging statistical problem in general. Here, we propose a practical and efficient inference method, the variational latent gaussian process (vLGP). The vLGP combines a generative model with a history-dependent point process observation, together with a smoothness prior on the latent trajectories. The vLGP improves on earlier methods for recovering latent trajectories, which assume either observation models inappropriate for point processes or linear dynamics. We compare and validate vLGP on both simulated data sets and population recordings from the primary visual cortex. In the V1 data set, we find that vLGP achieves substantially higher performance than previous methods for predicting omitted spike trains, as well as capturing both the toroidal topology of visual stimuli space and the noise correlation. These results show that vLGP is a robust method with the potential to reveal hidden neural dynamics from large-scale neural recordings.},
  archiveprefix        = {arXiv},
  citeulike-article-id = {14025142},
  citeulike-linkout-0  = {http://arxiv.org/abs/1604.03053},
  day                  = {22},
  doi                  = {10.1162/NECO_a_00953},
  eprint               = {1604.03053},
  keywords             = {bayesian, computational-neuroscience, gaussian-process, glm, latent-dynamics, latent-variable, spike-train, statistical-neuroscience, statistics, torus, variational-bayes},
  posted-at            = {2016-04-30 16:02:05},
  primaryclass         = {stat.ML},
  youtube              = {https://www.youtube.com/watch?v=CrY5AfNH1ik},
  code                 = {https://github.com/catniplab/vlgp},
}

@Article{Yates2017a,
  author               = {Yates, Jacob L. and Park, Il Memming and Katz, Leor N. and Pillow, Jonathan W. and Huk, Alexander C.},
  title                = {Functional dissection of signal and noise in {MT} and {LIP} during decision-making},
  journal              = {Nature Neuroscience},
  year                 = {2017},
  month                = jul,
  issn                 = {1097-6256},
  volume               = {20},
  pages                = {1285--1292},
  citeulike-article-id = {14398857},
  citeulike-linkout-0  = {http://dx.doi.org/10.1038/nn.4611},
  day                  = {24},
  doi                  = {10.1038/nn.4611},
  keywords             = {functional-connectivity, glm, lip, mt, noise-correlation, psychophysics},
  posted-at            = {2017-07-24 16:43:28},
}

@InProceedings{Hocker2017b,
  author    = {David Hocker and Il Memming Park},
  title     = {Instability of the generalized linear model for spike trains},
  booktitle = {Computational and Systems Neuroscience ({COSYNE})},
  year      = {2017},
  owner     = {memming},
  timestamp = {2017.02.11},
}

@InProceedings{Hocker2017a,
  author    = {David Hocker and Il Memming Park},
  title     = {Multistep inference for generalized linear spiking models curbs runaway excitation},
  booktitle = {8th International IEEE EMBS Conference On Neural Engineering},
  year      = {2017},
  pages     = {613--616},
  month     = may,
  publisher = {IEEE},
  doi       = {10.1109/ner.2017.8008426},
  pdf       = {Hocker2017a.pdf},
  isbn      = {978-1-5090-4603-4},
  keywords  = {autoregressive, glm, multi-step-prediction, spike-train, time-series},
  location  = {Shanghai, China},
  owner     = {memming},
  timestamp = {2017.02.11},
}

@PhdThesis{Zhao2016e,
    author = {Yuan Zhao},
    school = {Stony Brook University},
    title = {Log-linear Model Based Tree and Latent Variable Model for Count Data},
    year = {2016},
}

@InProceedings{Zhao2016d,
  author        = {Zhao, Yuan and Park, Il Memming},
  title         = {Interpretable Nonlinear Dynamic Modeling of Neural Trajectories},
  booktitle     = {Advances in Neural Information Processing Systems (NIPS)},
  year          = {2016},
  abstract      = {A central challenge in neuroscience is understanding how neural system
implements computation through its dynamics. We propose a nonlinear
time series model aimed at characterizing interpretable dynamics
from neural trajectories. Our model assumes low-dimensional continuous
dynamics in a finite volume. It incorporates a prior assumption about
globally contractional dynamics to avoid overly enthusiastic extrapolation
outside of the support of observed trajectories. We show that our
model can recover qualitative features of the phase portrait such
as attractors, slow points, and bifurcations, while also producing
reliable long-term future predictions in a variety of dynamical models
and in real neural data.},
  archiveprefix = {arXiv},
  eprint        = {1608.06546},
  keywords      = {autoregressive, bifurcation, chaos, continuous-attractor, dynamics, neural-dynamics, nips, oscillation, tensorflow},
  primaryclass  = {q-bio.QM},
  youtube	= {https://www.youtube.com/watch?v=7oWRZRpaq_I},
  url = {https://papers.nips.cc/paper/6543-interpretable-nonlinear-dynamic-modeling-of-neural-trajectories},
}

@INPROCEEDINGS{Zhao2016c,
  author = {Yuan Zhao and Il Memming Park},
  title = {Variational inference of latent {G}aussian neural dynamics},
  booktitle = {International Conference on Machine Learning (ICML) Workshop on Computational
	Biology},
  year = {2016}
}

@INPROCEEDINGS{Zhao2016b,
  author = {Yuan Zhao and Il Memming Park},
  title = {Inferring low-dimensional network dynamics with variational latent
	{G}aussian process},
  booktitle = {Organization for Computational Neuroscience (CNS)},
  year = {2016}
}

@INPROCEEDINGS{Dikecligil2016,
  author = {Gulce Nazli Dikecligil and Dustin Graham and Il Memming Park and
	Alfredo Fontanini},
  title = {Layer Specific Sensorimotor Activity in the Gustatory Cortex of Licking
	Mice},
  booktitle = {Society for Neuroscience},
  year = {2016},
  owner = {memming}
}

@INPROCEEDINGS{Yates2015a,
  author = {Jacob Yates and Evan Archer and Alexander C. Huk and Il Memming Park},
  title = {Canonical correlations reveal co-variability between spike trains
	and local field potentials in area {MT}},
  booktitle = {Organization for Computational Neuroscience (CNS)},
  year = {2015}
}

@InProceedings{Wu2015b,
  author    = {Anqi Wu and Il Memming Park and Jonathan Pillow},
  title     = {Convolutional spike-triggered covariance analysis for neural subunit models},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year      = {2015},
  owner     = {memming},
  url       = {https://papers.nips.cc/paper/5962-convolutional-spike-triggered-covariance-analysis-for-neural-subunit-models},
}

@InProceedings{Wu2015a,
  author    = {Anqi Wu and Il Memming Park and Jonathan Pillow},
  title     = {Convolutional spike-triggered covariance analysis for estimating subunit models},
  booktitle = {Frontiers in Systems Neuroscience. Conference Abstract: Computational and Systems Neuroscience ({COSYNE})},
  year      = {2015},
  owner     = {memming},
}

@InProceedings{Park2015a,
  author    = {Il Memming Park and Jacob Yates and Alex Huk and Jonathan Pillow},
  title     = {Dynamic correlations between visual and decision areas during perceptual decision-making},
  booktitle = {Frontiers in Systems Neuroscience. Conference Abstract: Computational and Systems Neuroscience ({COSYNE})},
  year      = {2015},
  owner     = {memming},
}

@UNPUBLISHED{Archer2015a,
  author = {Archer, Evan and Park, Il Memming and Buesing, Lars and Cunningham,
	John and Paninski, Liam},
  title = {Black box variational inference for state space models},
  journal = {ArXiv e-prints},
  year = {2015},
  month = nov,
  abstract = {Latent variable time-series models are among the most heavily used
	tools from machine learning and applied statistics. These models
	have the advantage of learning latent structure both from noisy observations
	and from the temporal ordering in the data, where it is assumed that
	meaningful correlation structure exists across time. A few highly-structured
	models, such as the linear dynamical system with {linear-Gaussian}
	observations, have closed-form inference procedures (e.g. the Kalman
	Filter), but this case is an exception to the general rule that exact
	posterior inference in more complex generative models is intractable.
	Consequently, much work in time-series modeling focuses on approximate
	inference procedures for one particular class of models. Here, we
	extend recent developments in stochastic variational inference to
	develop a `black-box' approximate inference technique for latent
	variable models with latent dynamical structure. We propose a structured
	Gaussian variational approximate posterior that carries the same
	intuition as the standard Kalman filter-smoother but, importantly,
	permits us to use the same inference approach to approximate the
	posterior of much more general, nonlinear latent variable generative
	models. We show that our approach recovers accurate estimates in
	the case of basic models with closed-form posteriors, and more interestingly
	performs well in comparison to variational approaches that were designed
	in a bespoke fashion for specific non-conjugate models.},
  archiveprefix = {arXiv},
  citeulike-article-id = {13850684},
  citeulike-linkout-0 = {http://arxiv.org/abs/1511.07367},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1511.07367},
  day = {23},
  eprint = {1511.07367},
  keywords = {deep-learning, machine-learning, stochastic-gradient-descent-algorithm,
	time-series, variational-bayes},
  posted-at = {2016-02-23 15:24:40},
  primaryclass = {stat.ML},
  priority = {2},
}

@INPROCEEDINGS{Yates2014a,
  author = {Jacob L. Yates and Leor N. Katz and Il Memming Park and Jonathan
	W. Pillow and Alexander C. Huk},
  title = {Correlations and choice probabilities in simultaneously recorded
	{MT} and {LIP} neurons},
  booktitle = {Society for Neuroscience},
  year = {2014}
}

@Article{Park2014d,
  author               = {Park, Il Memming and Meister, Miriam L. R. and Huk, Alexander C. and Pillow, Jonathan W.},
  title                = {Encoding and decoding in parietal cortex during sensorimotor decision-making},
  journal              = {Nature Neuroscience},
  year                 = {2014},
  volume               = {17},
  number               = {10},
  pages                = {1395--1403},
  month                = oct,
  issn                 = {1097-6256},
  citeulike-article-id = {13342234},
  citeulike-linkout-0  = {http://dx.doi.org/10.1038/nn.3800},
  doi                  = {10.1038/nn.3800},
  pdf       = {Park2014d_typofixed.pdf},
  keywords             = {computational-neuroscience, decision-making, glm, lip, monkey, neural-code, neural-decoding},
  posted-at            = {2014-08-31 22:37:11},
}

@INPROCEEDINGS{Park2014c,
  author = {Park, Il Memming and Seth, Sohan and Van Vaerenbergh, Steven},
  title = {Probabilistic Kernel Least Mean Squares Algorithms},
  booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing
	(ICASSP)},
  year = {2014},
  doi = {10.1109/ICASSP.2014.6855214},
  url = {http://gtas.unican.es/pub/393},
  publisher = {IEEE}
}

@InProceedings{Park2014b,
  author    = {Il Memming Park and Evan Archer and Kenneth Latimer and Jonathan Pillow},
  title     = {Scalable nonparametric models for binary spike patterns},
  booktitle = {Frontiers in Systems Neuroscience. Conference Abstract: Computational and Systems Neuroscience ({COSYNE})},
  year      = {2014},
}

@ARTICLE{Park2014a,
  author = {Il Memming Park and Yuriy V. Bobkov and Barry W. Ache and Jos\'e
	C. Pr\'incipe},
  title = {Intermittency coding in the primary olfactory system: A neural substrate
	for olfactory scene analysis},
  journal = {The Journal of Neuroscience},
  year = {2014},
  volume = {34},
  pages = {941--952},
  number = {3},
  month = jan,
  abstract = {The spatial and temporal characteristics of the visual and acoustic
	sensory input are indispensable attributes for animals to perform
	scene analysis. In contrast, research in olfaction has focused almost
	exclusively on how the nervous system analyzes the quality and quantity
	of the sensory signal and largely ignored the spatiotemporal dimension
	especially in longer time scales. Yet, detailed analyses of the turbulent,
	intermittent structure of water- and air-borne odor plumes strongly
	suggest that spatio-temporal information in longer time scales can
	provide major cues for olfactory scene analysis for animals. We show
	that a bursting subset of primary olfactory receptor neurons ({bORNs})
	in lobster has the unexpected capacity to encode the temporal properties
	of intermittent odor signals. Each {bORN} is tuned to a specific
	range of stimulus intervals, and collectively {bORNs} can instantaneously
	encode a wide spectrum of intermittencies. Our theory argues for
	the existence of a novel peripheral mechanism for encoding the temporal
	pattern of odor that potentially serves as a neural substrate for
	olfactory scene analysis.},
  day = {15},
  doi = {10.1523/jneurosci.2204-13.2014},
  issn = {1529-2401},
  keywords = {neural-code, odor-plume, olfactory, uncoupled-oscillator},
  publisher = {Society for Neuroscience}
}

@InProceedings{Huk2014a,
  author    = {Alexander Huk and Jacob Yates and Leor Katz and Il Memming Park and Jonathan Pillow},
  title     = {Dissociated functional significance of choice-related activity across the primate dorsal stream},
  booktitle = {Frontiers in Systems Neuroscience. Conference Abstract: Computational and Systems Neuroscience ({COSYNE})},
  year      = {2014},
}

@Article{Archer2014a,
  author               = {Archer, Evan and Park, Il Memming and Pillow, Jonathan},
  journal              = {Journal of Machine Learning Research},
  title                = {{Bayes}ian Entropy Estimation for Countable Discrete Distributions},
  year                 = {2014},
  month                = oct,
  pages                = {2833--2868},
  volume               = {15},
  abstract             = {We consider the problem of estimating Shannon's entropy $H$ from discrete data, in cases where the number of possible symbols is unknown or even countably infinite. The {Pitman-Yor} process, a generalization of Dirichlet process, provides a tractable prior distribution over the space of countably infinite discrete distributions, and has found major applications in Bayesian non-parametric statistics and machine learning. Here we show that it also provides a natural family of priors for Bayesian entropy estimation, due to the fact that moments of the induced posterior distribution over $H$ can be computed analytically. We derive formulas for the posterior mean (Bayes' least squares estimate) and variance under Dirichlet and {Pitman-Yor} process priors. Moreover, we show that a fixed Dirichlet or {Pitman-Yor} process prior implies a narrow prior distribution over $H$, meaning the prior strongly determines the entropy estimate in the under-sampled regime. We derive a family of continuous mixing measures such that the resulting mixture of {Pitman-Yor} processes produces an approximately flat prior over $H$. We show that the resulting {Pitman-Yor} Mixture ({PYM}) entropy estimator is consistent for a large class of distributions. We explore the theoretical properties of the resulting estimator, and show that it performs well both in simulation and in application to real data.},
  archiveprefix        = {arXiv},
  citeulike-article-id = {12071222},
  citeulike-linkout-0  = {http://arxiv.org/abs/1302.0328},
  day                  = {2},
  eprint               = {1302.0328},
  keywords             = {bayesian, entropy-estimation, nonparametric-bayes, pitman-yor-process},
  primaryclass         = {cs.IT},
  url                  = {http://jmlr.org/papers/v15/archer14a.html},
}

@INPROCEEDINGS{Yates2013b,
  author = {Jacob Yates and Il Memming Park and Lawrence Cormack and Jonathan
	Pillow and Alexander Huk},
  title = {Precise characterization of dorsal stream neural activity during
	decision making},
  booktitle = {Society for Neuroscience},
  year = {2013}
}

@InProceedings{Yates2013a,
  author    = {Jacob Yates and Il Memming Park and Lawrence Cormack and Jonathan Pillow and Alexander Huk},
  title     = {Precise characterization of multiple {LIP} neurons in relation to stimulus and behavior},
  booktitle = {Frontiers in Systems Neuroscience. Conference Abstract: Computational and Systems Neuroscience ({COSYNE})},
  year      = {2013},
}

@InProceedings{Pillow2013a,
  author    = {Jonathan Pillow and Il Memming Park},
  title     = {Beyond {B}arlow: A {Bayes}ian theory of efficient neural coding},
  booktitle = {Frontiers in Systems Neuroscience. Conference Abstract: Computational and Systems Neuroscience ({COSYNE})},
  year      = {2013},
}

@Unpublished{Park2013g,
  author               = {Park, Il Memming and Seth, Sohan and Van Vaerenbergh, Steven},
  title                = {{Bayes}ian Extensions of Kernel Least Mean Squares},
  month                = oct,
  year                 = {2013},
  abstract             = {The kernel least mean squares ({KLMS}) algorithm is a computationally
	efficient nonlinear adaptive filtering method that "kernelizes" the
	celebrated (linear) least mean squares algorithm. We demonstrate
	that the least mean squares algorithm is closely related to the Kalman
	filtering, and thus, the {KLMS} can be interpreted as an approximate
	Bayesian filtering method. This allows us to systematically develop
	extensions of the {KLMS} by modifying the underlying state-space
	and observation models. The resulting extensions introduce many desirable
	properties such as "forgetting", and the ability to learn from discrete
	data, while retaining the computational simplicity and time complexity
	of the original algorithm.},
  archiveprefix        = {arXiv},
  citeulike-article-id = {12732257},
  citeulike-linkout-0  = {http://arxiv.org/abs/1310.5347},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1310.5347},
  day                  = {20},
  eprint               = {1310.5347},
  journal              = {ArXiv e-prints},
  keywords             = {adaptive-filter, bayesian, kernel-method, klms, online-algorithm, poisson-observation},
  posted-at            = {2013-10-23 12:18:03},
  primaryclass         = {st.ML},
}

@INPROCEEDINGS{Park2013f,
  author = {Il Memming Park and Evan Archer and Nicholas Priebe and Jonathan
	W. Pillow},
  title = {Spectral methods for neural characterization using generalized quadratic
	models},
  url = {http://papers.nips.cc/paper/4993-spectral-methods-for-neural-characterization-using-generalized-quadratic-models},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year = {2013},
  owner = {memming}
}

@INPROCEEDINGS{Park2013e,
  author = {Il Memming Park and Evan Archer and Kenneth Latimer and Jonathan
	W. Pillow},
  title = {Universal models for binary spike patterns using centered {D}irichlet
	processes},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  url = {http://papers.nips.cc/paper/5050-universal-models-for-binary-spike-patterns-using-centered-dirichlet-processes},
  year = {2013},
  owner = {memming}
}

@INPROCEEDINGS{Park2013c,
  author = {Il Memming Park and Evan Archer and Jonathan Pillow},
  title = {{B}ayesian entropy estimators for spike trains},
  booktitle = {Computational Neuroscience (CNS)},
  year = {2013}
}

@InProceedings{Park2013b,
  author    = {Il Memming Park and Evan Archer and Nicholas Priebe and Jonathan Pillow},
  title     = {Got a moment or two? {N}eural models and linear dimensionality reduction},
  booktitle = {Frontiers in Systems Neuroscience. Conference Abstract: Computational and Systems Neuroscience ({COSYNE})},
  year      = {2013},
}

@ARTICLE{Park2013a,
  author = {Park, Il Memming and Seth, Sohan and Paiva, Antonio R. C. and Li,
	Lin and Principe, Jose C.},
  title = {Kernel methods on spike train space for neuroscience: a tutorial},
  journal = {IEEE Signal Processing Magazine},
  year = {2013},
  volume = {30},
  pages = {149--160},
  number = {4},
  month = jul,
  abstract = {Over the last decade several positive definite kernels have been proposed
	to treat spike trains as objects in Hilbert space. However, for the
	most part, such attempts still remain a mere curiosity for both computational
	neuroscientists and signal processing experts. This tutorial illustrates
	why kernel methods can, and have already started to, change the way
	spike trains are analyzed and processed. The presentation incorporates
	simple mathematical analogies and convincing practical examples in
	an attempt to show the yet unexplored potential of positive definite
	functions to quantify point processes. It also provides a detailed
	overview of the current state of the art and future challenges with
	the hope of engaging the readers in active participation.},
  day = {24},
  doi = {10.1109/msp.2013.2251072},
  eprint = {1302.5964},
  issn = {1053-5888}
}

@INBOOK{Paiva2011,
  title = {Instantaneous cross-correlation analysis of neural ensembles with
	high temporal resolution},
  publisher = {Wiley},
  year = {2013},
  editor = {Dario Farina and Winnie Jensen and Metin Akay},
  author = {Ant\'onio R. C. Paiva and Il Park and Jos\'e C. Pr\'incipe and Justin
	Sanchez},
  booktitle = {Neural engineering applied to neurorehabilitation}
}

@INPROCEEDINGS{Archer2013d,
  author = {Evan Archer and Il Memming Park and Jonathan W. Pillow},
  title = {{Bayes}ian entropy estimation for binary spike train data using parametric
	prior knowledge},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  url = {http://papers.nips.cc/paper/4873-bayesian-entropy-estimation-for-binary-spike-train-data-using-parametric-prior-knowledge},
  year = {2013},
  owner = {memming}
}

@Article{Archer2013c,
  author               = {Archer, Evan and Park, Il Memming and Pillow, Jonathan},
  title                = {{Bayes}ian and Quasi-{Bayes}ian Estimators for Mutual Information from Discrete Data},
  journal              = {Entropy},
  year                 = {2013},
  volume               = {15},
  number               = {5},
  pages                = {1738--1755},
  month                = may,
  abstract             = {Mutual information ({MI}) quantifies the statistical dependency between
	a pair of random variables, and plays a central role in the analysis
	of engineering and biological systems. Estimation of {MI} is difficult
	due to its dependence on an entire joint distribution, which is difficult
	to estimate from samples. Here we discuss several regularized estimators
	for {MI} that employ priors based on the Dirichlet distribution.
	First, we discuss three {quasi-Bayesian} estimators that result
	from linear combinations of Bayesian estimates for conditional and
	marginal entropies. We show that these estimators are not in fact
	Bayesian, and do not arise from a well-defined posterior distribution
	and may in fact be negative. Second, we show that a fully Bayesian
	{MI} estimator proposed by Hutter (2002), which relies on a fixed
	Dirichlet prior, exhibits strong prior dependence and has large bias
	for small datasets. Third, we formulate a novel Bayesian estimator
	using a {mixture-of-Dirichlets} prior, with mixing weights designed
	to produce an approximately flat prior over {MI}. We examine the
	performance of these estimators with a variety of simulated datasets
	and show that, surprisingly, {quasi-Bayesian} estimators generally
	outperform our Bayesian estimator. We discuss outstanding challenges
	for {MI} estimation and suggest promising avenues for future research.},
  citeulike-article-id = {12335521},
  citeulike-linkout-0  = {http://dx.doi.org/10.3390/e15051738},
  citeulike-linkout-1  = {http://www.mdpi.com/1099-4300/15/5/1738},
  citeulike-linkout-2  = {http://www.mdpi.com/1099-4300/15/5/1738/pdf},
  day                  = {10},
  doi                  = {10.3390/e15051738},
  keywords             = {bayesian, entropy-estimation, estimation, information-theory, mutual-information},
  posted-at            = {2013-05-10 23:03:57},
}

@InProceedings{Archer2013b,
  author    = {Evan Archer and Il Memming Park and Jonathan Pillow},
  title     = {Semi-parametric {Bayes}ian entropy estimation for binary spike trains},
  booktitle = {Frontiers in Systems Neuroscience. Conference Abstract: Computational and Systems Neuroscience ({COSYNE})},
  year      = {2013},
}

@INPROCEEDINGS{Park2012e,
  author = {Il Memming Park and Miriam L. R. Meister and Alexander C. Huk and
	Jonathan W. Pillow},
  title = {Deciphering the code for sensorimotor decision-making at the level
	of single neurons in parietal cortex},
  booktitle = {Society for Neuroscience},
  year = {2012},
  note = {(oral presentation)},
  owner = {memming}
}

@Unpublished{Park2012c,
  author        = {Il Memming Park and Marcel Nassar and Mijung Park},
  title         = {Active {Bayes}ian Optimization: Minimizing Minimizer Entropy},
  month         = feb,
  year          = {2012},
  abstract      = {The ultimate goal of optimization is to find the minimizer of a target
	{function.However}, typical criteria for active optimization often
	ignore the uncertainty about the minimizer. We propose a novel criterion
	for global optimization and an associated sequential active learning
	strategy using Gaussian {processes.Our} criterion is the reduction
	of uncertainty in the posterior distribution of the function minimizer.
	It can also flexibly incorporate multiple global minimizers. We implement
	a tractable approximation of the criterion and demonstrate that it
	obtains the global minimizer accurately compared to conventional
	Bayesian optimization criteria.},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {http://adsabs.harvard.edu/abs/2012arXiv1202.2143M},
  archiveprefix = {arXiv},
  eprint        = {1202.2143},
  journal       = {ArXiv e-prints},
  keywords      = {Statistics - Methodology, Computer Science - Learning, Statistics - Machine Learning},
  primaryclass  = {stat.ME},
  code		= {https://github.com/memming/gpao},
}

@InProceedings{Park2012b,
  author    = {Il Memming Park and Jonathan Pillow},
  title     = {{Bayes}ian spike-triggered covariance and the elliptical {LNP} model},
  booktitle = {Frontiers in Systems Neuroscience. Conference Abstract: Computational and Systems Neuroscience ({COSYNE})},
  year      = {2012},
  owner     = {memming},
}

@Article{Park2012a,
  author    = {Il Memming Park and Sohan Seth and Murali Rao and Jos\'e C. Pr\'incipe},
  journal   = {Neural Computation},
  title     = {Strictly positive definite spike train kernels for point process divergences},
  year      = {2012},
  month     = aug,
  number    = {8},
  pages     = {2223--2250},
  volume    = {24},
  abstract  = {Exploratory tools that are sensitive to arbitrary statistical variations in spike train observations open up the possibility of novel neuroscientific discoveries. Developing such tools, however, is difficult due to the lack of Euclidean structure of the spike train space, and an experimenter usually prefers simpler tools that capture only limited statistical features of the spike train, such as mean spike count or mean firing rate. We explore strictly positive-definite kernels on the space of spike trains to offer both a structural representation of this space and a platform for developing statistical measures that explore features beyond count or rate. We apply these kernels to construct measures of divergence between two point processes and use them for hypothesis testing, that is, to observe if two sets of spike trains originate from the same underlying probability law. Although there exist positive-definite spike train kernels in the literature, we establish that these kernels are not strictly definite and thus do not induce measures of divergence. We discuss the properties of both of these existing nonstrict kernels and the novel strict kernels in terms of their computational complexity, choice of free parameters, and performance on both synthetic and real data through kernel principal component analysis and hypothesis testing.},
  doi       = {10.1162/NECO_a_00309},
  issue     = {8},
  pdf       = {Park2012a_spd_spike_train_kernel_divergence_color.pdf},
  owner     = {memming},
  timestamp = {2010.02.20},
}

@Article{Li2012a,
  author   = {Lin Li and Il Memming Park and Austin Brockmeier and Badong Chen and Sohan Seth and Joseph T. Francis and Justin C. Sanchez and Jos\'e C. Pr\'incipe},
  journal  = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
  title    = {Adaptive Inverse Control of Neural Spatiotemporal Spike Patterns with a Reproducing Kernel {H}ilbert Space ({RKHS}) Framework},
  year     = {2012},
  issn     = {1534-4320},
  month    = aug,
  number   = {4},
  pages    = {532--543},
  volume   = {21},
  doi      = {10.1109/TNSRE.2012.2200300},
  keywords = {Hilbert spaces;MIMO systems;adaptive control;adaptive signal processing;bioelectric phenomena;decoding;medical control systems;medical signal processing;multidimensional signal processing;neurophysiology;nonlinear control systems;signal representation;spatiotemporal phenomena;MIMO adaptive inverse control scheme;Schoenberg kernel maps;adaptive inverse control;control procedure;decoding accuracy;electrical stimulation;elicited system response;generalized linear model;linear algorithm;multidimensional time-varying signal representation;multiple-input multiple-output adaptive inverse control scheme;neural perturbations;neural signal representation;neural spatiotemporal spike patterns;neural stimulation;neural system plasticity;neurons;nonlinear neural system control;population spiking activity;realistic synthetic neural circuit;reproducing kernel Hilbert space framework;spike train RKHS;spikernel model;spiking responses;standard control methodology;system control problem;target system response;time-invariant homogeneous model;Adaptation models;Decoding;Integrated circuit modeling;Kernel;MIMO;Timing;Vectors;Adaptive inverse control;Schoenberg kernel;neural stimulation;spike timing representations},
}

@ARTICLE{Li2011a,
  author = {Lin Li and Il Park and Sohan Seth and Justin C. Sanchez and Jos\'e
	C. Pr\'incipe},
  title = {Functional Connectivity Dynamics Among Cortical Neurons: A Dependence
	Analysis},
  journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
  year = {2012},
  volume = {20},
  pages = {18--30},
  number = {1},
  month = jan,
  doi = {10.1109/TNSRE.2011.2176749},
  issn = {1534-4320},
  keywords = {behavior task;cortical neurons;cross correlation;dependence analysis;firing
	rates;food reaching task;functional connectivity dynamics;mean square
	contingency;monkey cortex;movement states;mutual information;neural
	assembly functional connectivity;neural ensemble recordings;pairwise
	functional connectivity;phase synchronization;phase-based metrics;robust
	estimators;statistical analysis;temporal resolutions;time 100 ms
	to 1000 ms;bioelectric potentials;biomechanics;brain;correlation
	methods;estimation theory;medical signal processing;neurophysiology;statistical
	analysis;synchronisation;},
  owner = {memmingpark},
  timestamp = {2010.12.30}
}

@ARTICLE{Bobkov2012a,
  author = {Yuriy Bobkov and Il Park and Kirill Ukhanov and Jos\'e C. Pr\'incipe
	and Barry W. Ache},
  title = {Cellular basis for response diversity in the olfactory periphery},
  journal = {PLoS One},
  year = {2012},
  volume = {7},
  pages = {e34843+},
  number = {4},
  month = apr,
  abstract = {An emerging idea in olfaction is that temporal coding of odor specificity
	can be intrinsic to the primary olfactory receptor neurons ({ORNs}).
	As a first step towards understanding whether lobster {ORNs} are
	capable of generating odor-specific temporal activity and what mechanisms
	underlie any such heterogeneity in discharge pattern, we characterized
	different patterns of activity in lobster {ORNs} individually and
	ensemble using patch-clamp recording and calcium imaging. We demonstrate
	that lobster {ORNs} show tonic excitation, tonic inhibition, phaso-tonic
	excitation, and bursting, and that these patterns are faithfully
	reflected in the calcium signal. We then demonstrate that the various
	dynamic patterns of response are inherent in the cells, and that
	this inherent heterogeneity is largely determined by heterogeneity
	in the underlying intrinsic conductances.},
  day = {13},
  doi = {10.1371/journal.pone.0034843}
}

@INPROCEEDINGS{Archer2012b,
  author = {Evan Archer and Il Memming Park and Jonathan W. Pillow},
  title = {{Bayes}ian estimation of discrete entropy with mixtures of stick
	breaking priors},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year = {2012},
  url = {https://papers.nips.cc/paper/4521-bayesian-estimation-of-discrete-entropy-with-mixtures-of-stick-breaking-priors},
  owner = {memming}
}

@InProceedings{Archer2012a,
  author    = {Evan Archer and Il Memming Park and Jonathan Pillow},
  title     = {{Bayes}ian entropy estimation for infinite neural alphabets},
  booktitle = {Frontiers in Systems Neuroscience. Conference Abstract: Computational and Systems Neuroscience ({COSYNE})},
  url = {http://papers.nips.cc/paper/4521-bayesian-estimation-of-discrete-entropy-with-mixtures-of-stick-breaking-priors},
  year      = {2012},
  owner     = {memming},
}

@ARTICLE{Seth2011a,
  author = {Sohan Seth and Murali Rao and Il Park and Jos\'e C. Pr\'incipe},
  title = {A Unified Framework for Quadratic Measures of Independence},
  journal = {IEEE Transactions on Signal Processing},
  year = {2011},
  volume = {59},
  pages = {3624--3635},
  month = aug,
  issue = {8},
  doi = {10.1109/TSP.2011.2153197},
  owner = {memming},
  timestamp = {2010.07.27}
}

@InProceedings{Park2011d,
  author               = {Park, Il and Seth, Sohan and Rao, Murali and Principe, Jos\'e C.},
  title                = {Estimation of symmetric chi-square divergence for point processes},
  booktitle            = {IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
  year                 = {2011},
  pages                = {2016--2019},
  month                = may,
  publisher            = {IEEE},
  citeulike-article-id = {11334584},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/ICASSP.2011.5946907},
  citeulike-linkout-1  = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5946907},
  doi                  = {10.1109/ICASSP.2011.5946907},
  isbn                 = {978-1-4577-0538-0},
  issn                 = {1520-6149},
  keywords             = {divergence, hypothesis-test, point-process},
  posted-at            = {2012-09-27 14:09:53},
}

@INPROCEEDINGS{Park2011c,
  author = {Il Memming Park and Jonathan W. Pillow},
  title = {{Bayes}ian Spike Triggered Covariance Analysis},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  url = {http://papers.nips.cc/paper/4411-bayesian-spike-triggered-covariance-analysis},
  year = {2011},
  owner = {memming},
  timestamp = {2011.10.24}
}

@CONFERENCE{Park2011b,
  author = {Il Memming Park and Sohan Seth and Jos\'e C. Pr\'incipe},
  title = {Spike Train Kernel Methods for Neuroscience},
  booktitle = {Joint Statistical Meeting},
  year = {2011},
  abstract = {Positive definite kernels has been widely used in the context of machine
	learning by the, so called, kernel machines such as the support vector
	machine and the kernel principal component analysis. An attractive
	property of a kernel machine is that it can be applied to arbitrary
	spaces as long as appropriate kernel is provided. We have developed
	spike train kernels and analyzed their properties in the context
	of two-sample problem, probability embedding as well as regression
	and classification. We discuss strictly positive definite kernels
	that provide theoretical foundation for its power.},
  owner = {memming},
  timestamp = {2011.07.14}
}

@InProceedings{Park2011a,
  author    = {Il Memming Park and Miriam Meister and Alexander Huk and Jonathan W Pillow},
  title     = {Detailed encoding and decoding of choice-related information from {LIP} spike trains},
  booktitle = {Frontiers in Systems Neuroscience. Conference Abstract: Computational and Systems Neuroscience ({COSYNE})},
  year      = {2011},
  owner     = {memming},
}

@CONFERENCE{Li2011b,
  author = {Lin Li and Il Memming Park and Sohan Seth and John Choi and Joseph
	T. Francis and Justin C. Sanchez and Jos\'e C. Pr\'incipe},
  title = {An adaptive decoder from spike trains to micro-stimulation using
	kernel least-mean-square ({KLMS}) algorithm},
  booktitle = {IEEE International Workshop on Machine Learning for Signal Processing (MLSP)},
  doi = {10.1109/MLSP.2011.6064603},
  year = {2011}
}

@INPROCEEDINGS{Seth2010a,
  author = {Sohan Seth and Il Park and Austin J. Brockmeier and Mulugeta Semework
	and John Choi and Joe Francis and Jos\'e C. Pr\'incipe},
  title = {A novel family of non-parametric cumulative based divergences for
	point processes},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  url = {http://papers.nips.cc/paper/4126-a-novel-family-of-non-parametric-cumulative-based-divergences-for-point-processes},
  year = {2010},
  pages = {2119--2127},
  owner = {memming},
  timestamp = {2010.02.18}
}

@BOOK{Principe2010,
  title = {Information Theoretic Learning},
  publisher = {Springer},
  year = {2010},
  author = {Jos\'e C. Pr\'incipe},
  isbn = {1441915699},
  url = {http://www.springer.com/us/book/9781441915696},
  owner = {memming},
  timestamp = {2009.05.19}
}

@INPROCEEDINGS{Park2010b,
  author = {Il Park and Jos\'e C. Pr\'incipe},
  title = {Quantification of Inter-trial Non-stationarity in Spike Trains from
	Periodically Stimulated Neural Cultures},
  booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing
	(ICASSP)},
  year = {2010},
  pages = {5442--5445},
  note = {Special session on Multivariate Analysis of Brain Signals: Methods
	and Applications},
  doi = {10.1109/ICASSP.2010.5494920},
  owner = {memming},
  timestamp = {2009.12.15}
}

@PhdThesis{Park2010a,
  author    = {Il Memming Park},
  title     = {Capturing spike train similarity structure: A point process divergence approach},
  school    = {The University of Florida},
  year      = {2010},
  month     = {Aug},
  abstract  = {Neurons mostly communicate via stereotypical events called action
	potentials (or spikes for short) giving rise to a time series called
	the neural spike train. Spike trains are random in nature, and hence
	one needs to deal with the probability law over the spike trains
	space (point process). Rich statistical descriptors are a prerequisite
	for statistical learning in the spike train domain; this provides
	necessary analysis tools for neural decoding, change detection, and
	neuron model fitting. The first and second order statistics prevalently
	used in neuroscience -- such as mean firing rate function, and correlation
	function -- do not fully describe the randomness, thus are partial
	statistics. However, restricting the study to these basic statistics
	implicitly limit what can be discovered. We propose three families
	of statistical divergences that enable non-Poisson, and more over,
	distribution-free spike train analysis. We extend the Kolmogorov-Smirnov
	test, $\phi$-divergence, and kernel based divergence to point processes.
	This is possible through the development of novel mathematical foundations
	for point process representations.
	
	Compared to the similarity or distance measures for spike trains that
	assumes predefined stochasticity and hence are not flexible, divergences
	applied to sets of spike trains capture the underlying probability
	law and measures the statistical similarity. Therefore, divergences
	are more robust, and assumption free. We apply the methodology on
	real data from neuronal cultures as well as anesthetized animals
	for neuroscience and neuroengineering applications posed as statistical
	inferences to evaluate their usefulness.},
  owner     = {memming},
  timestamp = {2016.10.24},
}

@CONFERENCE{Paiva2010c,
  author = {Ant\'onio R. C. Paiva and Il Park},
  title = {Which measure should we use for unsupervised spike train learning?},
  booktitle = {Statistical Analysis of Neuronal Data (SAND5)},
  year = {2010},
  owner = {memming},
  timestamp = {2010.05.24}
}

@INBOOK{Paiva2010b,
  title = {Inner Products for Representation and Learning in the Spike Train
	Domain},
  publisher = {Academic Press},
  year = {2010},
  editor = {Karim G. Oweiss},
  author = {Ant\'onio R. C. Paiva and Il Park and Jos\'e C. Pr\'incipe},
  booktitle = {Statistical Signal Processing for Neuroscience},
  isbn = {978-0123750273},
  owner = {memming},
  timestamp = {2010.01.15}
}

ARTICLE{Paiva2008d,
  author = {Ant\'onio R. C. Paiva and Il Park and Jos\'e C. Pr\'incipe},
  title = {Optimization in Reproducing Kernel {H}ilbert Spaces of Spike Trains},
  journal = {(book chapter in press)},
  owner = {memming},
  timestamp = {2008.10.08}
}

@Article{Paiva2009a,
  author    = {Ant\'onio R. C. Paiva and Il Park and Jos\'e C. Pr\'incipe},
  journal   = {Neural Computing \& Applications},
  title     = {A Comparison of Binless Spike Train Measures},
  year      = {2010},
  month     = sep,
  pages     = {405--419},
  volume    = {19},
  doi       = {10.1007/s00521-009-0307-6},
  issue     = {3},
  owner     = {memming},
  timestamp = {2009.12.15},
}

@INPROCEEDINGS{Li2010a,
  author = {Lin Li and Il Park and Sohan Seth and Justin C. Sanchez and Jos\'e
	C. Pr\'incipe},
  title = {Neuronal Functional Connectivity Dynamics in Cortex: An {MSC}-based
	Analysis},
  booktitle = {Annual International Conference of the IEEE Engineering in Medicine
	and Biology Society (EMBS)},
  year = {2010},
  owner = {memming},
  timestamp = {2010.06.29}
}

@INPROCEEDINGS{Brockmeier2010a,
  author = {Austin J. Brockmeier and Il Park and Babak Mahmoudi and Justin C.
	Sanchez and Jos\'e C. Pr\'incipe},
  title = {Spatio-Temporal Clustering of Firing Rates for Neural State Estimation},
  booktitle = {Annual International Conference of the IEEE Engineering in Medicine
	and Biology Society (EMBS)},
  year = {2010},
  owner = {memming},
  timestamp = {2010.06.29}
}

@INBOOK{bookch2010a,
  chapter = {9},
  title = {A Reproducing Kernel {H}ilbert Space Framework for Information-Theoretic
	Learning},
  publisher = {Springer},
  year = {2010},
  editor = {Jos\'e C. Pr\'incipe},
  author = {Jos\'e C. Pr\'incipe and Jian Wu Xu and Robert Jenssen and Antonio
	Paiva and Il Park},
  isbn = {978-1441915696},
  owner = {memming},
  timestamp = {2009.12.16}
}

@INPROCEEDINGS{Bobkov2010a,
  author = {Yuriy Bobkov and Kirill Ukhanov and Il Park and Jos\'e C. Pr\'incipe
	and Barry Ache},
  title = {Measuring Ensemble Activity in Lobster {ORN}s through Calcium Imaging},
  booktitle = {Association for Chemoreception (AChemS) Annual Meeting},
  year = {2010},
  abstract = {Lobster ORNs can be imaged in the olfactory organ in situ, thereby
	maintaining the normal polarity of the cells and the ionic environment
	of the olfactory cilia. The preparation gives simultaneous access
	to hundreds of ORNs that are viable for hours, thereby allowing rigorous
	characterization of their steadystate and dynamic properties. Odorants
	change the level of cytoplasmic Ca2+ in a dose-dependent manner in
	ORNs loaded with Ca2+-sensitive indicator either through bath application
	or via a patch electrode. The kinetics and amplitude of the odorantevoked
	Ca2+ signal correlate with the excitatory inward current, the degree
	of membrane depolarization, and the number of evoked action potentials,
	thereby establishing the physiological relevance of the Ca2+ signal.
	Spontaneous periodic Ca2+ transients in many ORNs correlate with
	spontaneous bursts of action potentials measured in single cells
	in the same cluster. We are using signal processing algorithms to
	analyze the level of correlated activity between these ORNs and the
	extent to which periodic calcium oscillations in different ORNs are
	synchronized by common intermittent excitatory input to test the
	predictions of our computational model for ensemble burst coding
	in these cells and the potential relevance of bursting input to olfactory
	scene analysis.},
  owner = {memming},
  timestamp = {2010.07.15}
}

@InProceedings{Seth2009a,
  author    = {Sohan Seth and Il Park and Jos\'e C. Pr\'incipe},
  title     = {A new nonparametric measure of conditional independence},
  booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
  year      = {2009},
  doi       = {10.1109/ICASSP.2009.4960250},
  owner     = {memming},
  timestamp = {2009.03.06},
}

@INPROCEEDINGS{Park2009c,
  author = {Il Park and Jos\'e C. Pr\'incipe},
  title = {Significance test for spike trains based on finite point process
	estimation},
  booktitle = {Society for Neuroscience},
  year = {2009},
  owner = {memming},
  timestamp = {2010.01.15}
}

@INPROCEEDINGS{Park2009b,
  author = {Il Park and Yuriy Bobkov and Kirill Ukhanov and Barry W. Ache and
	Jos\'e C. Pr\'incipe},
  title = {Input Driven Synchrony of Oscillating Olfactory Receptor Neurons:
	A Computational Modeling Study},
  booktitle = {Association for Chemoreception (AChemS) Annual Meeting},
  year = {2009},
  owner = {memming},
  timestamp = {2010.01.15}
}

@INPROCEEDINGS{Park2009a,
  author = {Il Park and Murali Rao and Thomas B. DeMarse and Jos\'e C. Pr\'incipe},
  title = {Point Process Model for Precisely Timed Spike Trains.},
  booktitle = {Frontiers in Systems Neuroscience. Conference Abstract: Computational
	and Systems Neuroscience (COSYNE)},
  year = {2009},
  doi = {doi: 10.3389/conf.neuro.06.2009.03.227},
  owner = {memming},
  timestamp = {2009.03.06}
}

@ARTICLE{Paiva2008b,
  author = {Ant\'onio R. C. Paiva and Il Park and Jos\'e C. Pr\'incipe},
  title = {A Reproducing Kernel {H}ilbert Space framework for Spike Trains},
  journal = {Neural Computation},
  year = {2009},
  volume = {21},
  pages = {424--449},
  number = {2},
  month = feb,
  doi = {10.1162/neco.2008.09-07-614},
  owner = {memming},
  timestamp = {2008.10.08}
}

@ARTICLE{Liu2009b,
  author = {Weifeng Liu and Il Park and Jos\'e C. Pr\'incipe},
  title = {An Information Theoretic Approach of Designing Sparse Kernel Adaptive
	Filters},
  journal = {IEEE Transactions on Neural Network},
  year = {2009},
  volume = {20},
  pages = {1950--1961},
  number = {12},
  month = dec,
  doi = {10.1109/TNN.2009.2033676},
  owner = {memming},
  timestamp = {2009.03.22}
}

@ARTICLE{Liu2009a,
  author = {Weifeng Liu and Il Park and Yiwen Wang and Jos\'e C. Pr\'incipe},
  title = {Extended Kernel Recursive Least Squares Algorithm},
  journal = {IEEE Transactions on Signal Processing},
  year = {2009},
  volume = {57},
  pages = {3801--3814},
  number = {10},
  month = oct,
  doi = {10.1109/TSP.2009.2022007},
  owner = {memming},
  timestamp = {2009.03.22}
}

@INPROCEEDINGS{Li2009,
  author = {Lin Li and Sohan Seth and Il Park and Justin C. Sanchez and Jos\'e
	C. Pr\'incipe},
  title = {Estimation and Visualization of Neuronal Functional Connectivity
	in Motor Tasks},
  booktitle = {Annual International Conference of the IEEE Engineering in Medicine
	and Biology Society (EMBS)},
  year = {2009},
  abstract = {In brain-machine interface (BMI) modeling, the firing patterns of
	hundreds of neurons are used to reconstruct a variety of kinematic
	variables. The large number of neurons produces an explosion in the
	number of free parameters, which affects model generalization. This
	paper proposes a model-free measure of pairwise neural dependence
	to rank the importance of neurons in neural to motor mapping. Compared
	to a model-dependent approach such as sensitivity analysis, sixty
	percent of the neurons with the strongest dependence coincide with
	the top 10 most sensitive neurons trained through the model. Using
	this data-driven approach that operates on the input data alone,
	it is possible to perform neuron selection in a more efficient way
	that is not subject to assumptions about decoding models. To further
	understand the functional dependencies that influence neural to motor
	mapping, we use an open source available graph visualization toolkit
	called Prefuse to visualize the neural dependency graph and quantify
	the functional connectivity in motor cortex. This tool when adapted
	to the analysis of neuronal recordings has the potential to easily
	display the relationships in data of large dimension.},
  doi = {10.1109/IEMBS.2009.5333991},
  owner = {memming},
  timestamp = {2009.12.15}
}

@ARTICLE{Dockendorf2008,
  author = {Karl Dockendorf and Il Park and Ping He and Jos\'e C. Pr\'incipe
	and Thomas B. DeMarse},
  title = {Liquid State Machines and Cultured Cortical Networks: The Separation
	Property},
  journal = {Biosystems},
  year = {2009},
  volume = {95},
  pages = {90--97},
  number = {2},
  month = feb,
  doi = {10.1016/j.biosystems.2008.08.001},
  owner = {memming},
  timestamp = {2008.04.15}
}

@INPROCEEDINGS{Bobkov2009a,
  author = {Yuriy Bobkov and Il Park and Kirill Ukhanov and Jos\'e C. Pr\'incipe
	and Barry W. Ache},
  title = {Population coding within an ensemble of rhythmically active primary
	olfactory receptor},
  booktitle = {Society for Neuroscience},
  year = {2009},
  owner = {memming},
  timestamp = {2010.01.15}
}

@Article{Xu2008,
  author               = {Xu, Jian Wu and Paiva, Ant\'onio R. C. and Park, Il and Pr\'incipe, Jos\'e C.},
  journal              = {IEEE Transactions on Signal Processing},
  title                = {A Reproducing Kernel {H}ilbert Space Framework for Information-Theoretic Learning},
  year                 = {2008},
  month                = aug,
  number               = {12},
  pages                = {5891--5902},
  volume               = {56},
  abstract             = {This paper provides a functional analysis perspective of information-theoretic
	learning (ITL) by defining bottom-up a reproducing kernel Hilbert
	space (RKHS) uniquely determined by the symmetric nonnegative definite
	kernel function known as the cross-information potential (CIP). The
	CIP as an integral of the product of two probability density functions
	characterizes similarity between two stochastic functions. We prove
	the existence of a one-to-one congruence mapping between the ITL
	RKHS and the Hilbert space spanned by square integrable probability
	density functions. Therefore, all the statistical descriptors in
	the original information-theoretic learning formulation can be rewritten
	as algebraic computations on deterministic functional vectors in
	the ITL RKHS, instead of limiting the functional view to the estimators
	as is commonly done in kernel methods. A connection between the ITL
	RKHS and kernel approaches interested in quantifying the statistics
	of the projected data is also established.},
  citeulike-article-id = {3744103},
  doi                  = {10.1109/TSP.2008.2005085},
  keywords             = {cnel, itl, rkhs},
  posted-at            = {2008-12-03 23:09:57},
}

@INPROCEEDINGS{Park2008b,
  author = {Il Park and Jos\'e C. Pr\'incipe},
  title = {Correntropy based Granger causality},
  booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing
	(ICASSP)},
  year = {2008},
  doi = {10.1109/ICASSP.2008.4518432},
  owner = {memming},
  timestamp = {2009.03.06}
}

@ARTICLE{Park2008a,
  author = {Il Park and Ant\'onio R. C. Paiva and Thomas B. DeMarse and Jos\'e
	C. Pr\'incipe},
  title = {An efficient algorithm for continuous-time cross correlogram of spike
	trains},
  journal = {Journal of Neuroscience Methods},
  year = {2008},
  volume = {168},
  pages = {514--523},
  number = {2},
  month = mar,
  abstract = {We propose an efficient algorithm to compute the smoothed correlogram
	for the detection of temporal relationship between two spike trains.
	Unlike the conventional histogram-based correlogram estimations,
	the proposed algorithm operates on continuous time and does not bin
	either the spike train nor the correlogram. Hence it can be more
	precise in detecting the effective delay between two recording sites.
	Moreover, it can take advantage of the higher temporal resolution
	of the spike times provided by the current recording methods. The
	Laplacian kernel for smoothing enables efficient computation of the
	algorithm. We also provide the basic statistics of the estimator
	and a guideline for choosing the kernel size. This new technique
	is demonstrated by estimating the effective delays in a neuronal
	network from synthetic data and recordings of dissociated cortical
	tissue.},
  doi = {10.1016/j.jneumeth.2007.10.005},
  owner = {memming},
  timestamp = {2007.10.21}
}

@inproceedings{Paiva2008c,
  author = {Ant\'onio R. C. Paiva and Il Park and Justin Sanchez and Jos\'e C.
	Pr\'incipe},
  title = {Peri-event Cross-Correlation over Time for Analysis of Interactions
	in Neuronal Firing},
  journal = {International Conference of the IEEE Engineering in Medicine and
	Biology Society (EMBC)},
  year = {2008},
  owner = {memming},
  timestamp = {2008.10.08}
}

@INPROCEEDINGS{Paiva2008a,
  author = {Ant\'onio R. C. Paiva and Il Park and Jos\'e C. Pr\'incipe},
  title = {Reproducing Kernel {H}ilbert Spaces for Spike Train Analysis},
  booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing
	(ICASSP)},
  year = {2008},
  doi = {10.1109/ICASSP.2008.4518834},
  owner = {memming},
  timestamp = {2008.04.15}
}

@MastersThesis{ParkMSThesis2007,
  author    = {Park, Il},
  title     = {Continuous time correlation analysis techniques for spike trains},
  school    = {The University of Florida},
  year      = {2007},
  owner     = {memming},
  timestamp = {2009.04.24},
}

@INPROCEEDINGS{Park2007b,
  author = {Il Park and Ant\'onio R. C. Paiva and Jos\'e C. Pr\'incipe and Thomas
	B. DeMarse},
  title = {An Efficient Computation of Continuous-time Correlogram of Spike
	Trains},
  booktitle = {Frontiers in Systems Neuroscience. Conference Abstract: Computational
	and Systems Neuroscience (COSYNE)},
  year = {2007},
  owner = {memming},
  timestamp = {2010.01.20}
}

@INPROCEEDINGS{Park2007,
  author = {Il Park and Ant\'onio R. C. Paiva and Thomas B. DeMarse and Jose
	C. Pr\'incipe and John Harris},
  title = {A Closed Form Solution for Multiple-Input Spike Based Adaptive Filters},
  booktitle = {IEEE International Joint Conference on Neural Networks (IJCNN)},
  year = {2007},
  owner = {memming},
  timestamp = {2008.04.15}
}

@InProceedings{Paiva2007b,
  author    = {Ant\'onio R. C. Paiva and Il Park and Jos\'e C. Pr\'incipe},
  title     = {Innovating Signal Processing for Spike Train Data},
  booktitle = {International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
  year      = {2007},
  address   = {Lyon, France},
  owner     = {memming},
  timestamp = {2009.12.15},
}

@InProceedings{Paiva2007a,
  title     = "Spectral Clustering of Synchronous Spike Trains",
  booktitle = {{IEEE} International Joint Conference on Neural Networks ({IJCNN})},
  author    = {Ant\'onio R. C. Paiva and Sudhir Rao and Il Park and Jos\'e C. Pr\'incipe},
  abstract  = "In this paper a clustering algorithm that learns the groups of
               synchronized spike trains directly from data is proposed.
               Clustering of spike trains based on the presence of synchronous
               neural activity is of high relevance in neurophys-iological
               studies. In this context such activity is thought to be
               associated with functional structures in the brain. In addition,
               clustering has the potential to analyze large volumes of data.
               The algorithm couples a distance between two spike trains
               recently proposed in the literature with spectral clustering.
               Finally, the algorithm is illustrated in sets of computer
               generated spike trains and analyzed for the dependence on its
               parameters and accuracy with respect to features of interest.",
  pages     = "1831--1835",
  month     =  aug,
  year      =  2007,
  url       = "http://dx.doi.org/10.1109/IJCNN.2007.4371236",
  keywords  = "Neurons;Clustering algorithms;Neuroscience;Data analysis;Random
               processes;Clustering methods;Algorithm design and
               analysis;Design methodology;Unsupervised learning;Machine
               learning",
  issn      = "2161-4407",
  doi       = "10.1109/IJCNN.2007.4371236",
  owner     = {memming},
  address   = {Orlando, FL, USA},
  timestamp = {2008.04.15},
}

@INPROCEEDINGS{Park2006,
  author = {Il Park and Dongming Xu and Thomas B. DeMarse and Jos\'e C. Pr\'incipe},
  title = {Modeling of Synchronized Burst in Dissociated Cortical Tissue: An
	Exploration of Parameter Space},
  booktitle = {IEEE International Joint Conference on Neural Networks (IJCNN)},
  year = {2006},
  doi = {10.1109/IJCNN.2006.246734},
  owner = {memming},
  timestamp = {2008.04.15}
}

@InProceedings{Park2005,
  author    = {Il Park and Jong C. Park},
  title     = {Modeling Causality in Biological Pathways for Logical Identification of Drug Targets},
  booktitle = {Bioinfo},
  year      = {2005},
  address   = {Busan, Korea},
  month     = sep,
  owner     = {memming},
  timestamp = {2008.04.15},
}

INPROCEEDINGS{Paiva2008e,
  author = {Ant\'onio R. C. Paiva and Il Park and Jos{\'e} C. Pr{\'i}ncipe},
  title = {Reproducing Kernel {H}ilbert Spaces for Spike Train Analysis},
  booktitle = {Conference on Computational Neuroscience},
  year = {2008},
  owner = {memming},
  timestamp = {2010.01.20}
}

@InBook{Paiva2010a,
  title     = {Optimization in Reproducing Kernel {H}ilbert Spaces of Spike Trains},
  publisher = {Springer},
  year      = {?},
  author    = {Ant\'onio R. C. Paiva and Il Park and Jos\'e C. Pr\'incipe},
  editor    = {W. Art Chaovalitwongse and Panos Pardalos and Petros Xanthopoulos,},
  note      = {(in press)},
  booktitle = {Computational Neuroscience},
  owner     = {memming},
  timestamp = {2010.01.15},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: saveOrderConfig:specified;year;true;citationkey;true;author;true;}
