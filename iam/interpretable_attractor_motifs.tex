\documentclass{article}
%\usepackage{arxiv}
\usepackage[numbers,sort&compress,super,comma]{natbib} % this must be before neurips_2024
\usepackage[preprint]{neurips_2024}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amssymb, amsmath, amsthm, amsfonts}
\usepackage{thmtools, mathtools, mathrsfs, dsfont}
\usepackage{bbm}
%\usepackage{bbold} %for mathbb{1}
\usepackage{forloop}
\usepackage[pdftex]{graphicx}  %remove demo option in your document
\usepackage{sidecap}
\PassOptionsToPackage{dvipsnames}{xcolor}
\usepackage{xcolor}
\definecolor{ForestGreen}{rgb}{0.13, 0.55, 0.13}
\definecolor{MidnightBlue}{rgb}{0.1, 0.1, 0.44}
\definecolor{BurntOrange}{rgb}{0.8, 0.33, 0.0}
\definecolor{Plum}{rgb}{0.56, 0.27, 0.52}
\usepackage[colorlinks=true,linkcolor=MidnightBlue,citecolor=ForestGreen,filecolor=TealBlue,urlcolor=Plum]{hyperref}
\hypersetup{breaklinks=true}

%for comments
\newcommand{\ptitle}[1]{\textbf{#1:}\xspace}
\definecolor{mpcolor}{rgb}{1, 0.1, 0.59}
\definecolor{ascolor}{rgb}{1, 0.5, 0.}
\newcommand{\mpcomment}[1]{\textcolor{mpcolor}{(#1)}}
\newcommand{\ascomment}[1]{\textcolor{ascolor}{(#1)}}

\graphicspath{{figures}}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}[theorem]
\theoremstyle{definition} \newtheorem{definition}{Definition}
\theoremstyle{remark} \newtheorem{remark}{Remark}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\sep}{\operatorname{sep}}
\newcommand{\cl}{\operatorname{cl}}
\newcommand{\vol}{\operatorname{vol}}
\newcommand{\boa}{\operatorname{BoA}}
\newcommand{\mcN}{\mathcal{N}}
\newcommand{\mcK}{\mathcal{K}}
\newcommand{\mcU}{\mathcal{U}}
\newcommand{\mcX}{\mathcal{X}}
\newcommand{\T}{\operatorname{T}}
\newcommand{\TR}{\T\!\mathbb{R}}
\newcommand{\TM}{\T\!M}
\newcommand{\TpM}{\T_p\!M}
\newcommand{\Diff}{\operatorname{Diff}}
\newcommand{\Hpert}{H^{\text{pert}}}
\newcommand{\inv}{\operatorname{Inv}}

\newcommand{\defvec}[1]{\expandafter\newcommand\csname v#1\endcsname{{\mathbf{#1}}}}
\newcommand{\dm}[1]{\ensuremath{\mathrm{d}{#1}}} % dx dy dz dmu
\newcounter{ct}
\forLoop{1}{26}{ct}{
    \edef\letter{\alph{ct}}
    \expandafter\defvec\letter
}

% captial \vA
\forLoop{1}{26}{ct}{
    \edef\letter{\Alph{ct}}
    \expandafter\defvec\letter
}


%\title{Interpretable attractor motifs for a new language for neural computation}
\title{Attractor motifs for an interpretable language for neural computation}
\author{\'Abel S\'agodi and Il Memming Park}
\date{\today}



\begin{document}
\maketitle

\section*{Abstract}




\section{Introduction}\label{sec:intro}
Understanding the relationship between neural activity and behavior is a fundamental question in neuroscience.
 Recent research has demonstrated that despite the complexity of neuronal networks, their activity often resides in a low-dimensional manifold, capturing only a fraction of the potential complexity of the system \citep{duncker2021dynamics}.
 This observation suggests that neural activity is shaped by strong underlying constraints, possibly imposed by behaviorally relevant computations and external environmental demands.

Behavior, in many cases, is surprisingly simple. Even in seemingly complex tasks, animals and humans exhibit structured, stereotyped actions that can often be described using a small number of latent variables.
 From motor control to decision-making, behavioral trajectories unfold in a manner that is both predictable and constrained, hinting at a fundamental simplicity underlying observed actions.
 This raises an important question: if neural activity underlies behavior, should it not also exhibit similar simplicity?

If neural dynamics are indeed tuned to drive behavior, we should expect a correspondence between the low-dimensional structure of neural activity and the inherent simplicity of behavior.
 This suggests that much of the apparent complexity in neural recordings may stem from redundant or task-irrelevant variability rather than meaningful high-dimensional computation.
  In this work, we explore the hypothesis that neural representations of behavior are fundamentally simple, reflecting a constrained, low-dimensional organization that enables efficient motor control, decision-making, and cognitive processing.
 Through computational modeling and empirical analyses, we demonstrate how low-dimensional neural manifolds can emerge and how their structure aligns with behavioral simplicity, providing insights into the fundamental principles governing neural computation.

\subsection{Our contribution}\label{sec:contribution}




\section{Background and related work}\label{sec:background}
A theory of (neural) computation\citep{jaeger2021theory, jaeger2023timescales, jaeger2023theory}
- Dynamical system theory as the source for the syntax for the language of neural computation.


\subsection{Existing methods for dynamics}
\subsubsection{Reconstructing dynamics}

Learning interpretable continuous-time models of latent stochastic dynamical systems \citep{duncker2019learning}

Phase2vec \citep{ricci2022phase2vec}

MARBLE \citep{gosztolai2025marble}



\subsubsection{Methods to compare/classify dynamics}\label{sec:compare}
%Comparison of dynamical systems
Concepts:
\begin{itemize}
\item equivalence of systems: topological conjugacy (Def.~\ref{def:top_conj}).%/equivalence
\item asymptotic behavior: $\omega$-limit sets
\end{itemize}

Taking apart topological conjugacy (diffeomorphism from motif):
\begin{itemize}
\item On asymptotic / $\omega$-limit set / invariant set
\item On transients
\end{itemize}


\paragraph{SOTA}
assess the limit sets
Fixed-point based \citep{sussillo2013blackbox,katz2017fibers,golub2018fixedpointfinder}

Also limit cycles 

slow invariant manifolds\citep{Sagodi2024a}

TWA\citep{moriel2024timewarpattend}

SVCCA\citep{raghu2017svcca}

Dynamical Similarity Analysis\citep{ostrow2024beyond}

Diffeomorphic vector field alignment for comparing dynamics across learned models\citep{chen2024dform}
- orbital similarity loss (Eq.5)



\subsubsection{Decomposition of the dynamics}\label{sec:decomposition}
into basins of attraction through Conley’s Fundamental Theorem for Dynamical Systems \citep{conley1978morse, norton1995fundamental,mischaikow1999cit}

FSM-based approaches\citep{pollack1991induction, casey1996dynamics, jacobsson2005ruleextraction, ashwin2021excitable, oliva2019fsm, cotteret2024fsm}
Behaviorism is based on an engineering approach, treating the mind as a control system for the organism. This corresponds to an approximation of the recurrent neural dynamics (brain states) by finite state automata (behavioral states). Another approximations to neural dynamics is described, leading to a Platonic-like model of mind based on psychological spaces. \citep{duch1998platonic}

For different type of dynamcis (ergodic) but that separates the state space into coherent/metastable regions between which the Perron–Frobenius operator describes transitions.

Main shortcoming: difficulty with dealing with complicated attractors.
When is decomposition of attractor into motifs possible?

On the lifting and reconstruction of nonlinear systems with multiple invariant sets \citep{pan2024lifting}




\subsubsection{Reduction}\label{sec:reduction} %simplification of dynamics
In each basin of attraction we have one attractor.
We can simplify it's dynamics by looking at its normal form.

\begin{itemize}
\item approximation based (model order reduction): schilders2008model
\item topology based Delay Embedding and Takens' Theorem and Morse Decomposition
\item \citep{zemlianova2024dynamical}
\item motifs: normal forms \citep{full1999templates} \citep{nayfeh2011normalforms} and \citep{bonilla2012discriminative} (also referred to as prototypes in literature)
\item  \citep{mezic2005spectral}
\item libraries \citep{brunton2014compressive}
\item  primitives (more for behavioral) \citep{ijspeert2013dynamical}
\item fast-slow \citep{dsilva2016data} \citep{haller2017exact}
\item averaging methods
\item FSMs: \citep{giles1991extracting, casey1996dynamics, giles1999equivalence, oliva2019fsm, ceni2020excitable, cotteret2024fsm, aichernig2024learning} + weighted\citep{wei2024weighted}
\end{itemize}


\subsection{Interpretability}
What is interpretability? \citep{erasmus2021interpretability}
\citep{kar2022interpretability}
\citep{whiteway2019interpretable}
levels \citep{hochstein2022levels}
extrinsic and intrinsic dimensionality\citep{jazayeri2021interpreting}

 machine learning: important yet slippery\citep{lipton2018mythos}	
 Building Blocks of Interpretability \citep{olah2018interpretability}
\citep{beisbart2022interpretability}
simple isn't easy \citep{raz2024ml}
Manipulating and measuring model interpretability\citep{poursabzi2021manipulating}

\subsubsection{Simplicity}
\citep{gao2015simplicity}
\citep{dyer2023simplest}
\citep{quinn2022information}


\subsubsection{Interpretability attempts}
SVCCA\citep{raghu2017svcca}
MinimalRNN\citep{chen2017minimalrnn}
Towards interpreting recurrent neural networks through probabilistic abstraction \citep{dong2020towards}
Expressive architectures enhance interpretability of dynamics-based neural population models \citep{sedler2023expressive}
Interpretable Latent Factors with Sparse Component Analysis \citep{zimnik2024identifying}
Almost-Linear RNNs Yield Highly Interpretable Symbolic Codes in Dynamical Systems Reconstruction \citep{brenner2024almost}
\citep{he2024multilevel}



\section{The model}
\subsection{Description}
\subsubsection{Single global attractor}
We consider a target dynamical system $\dot{x} = f(x)$ which has a single global attractor.
 We assume that this system is topologically equivalent to one of a family of attractor motifs defined by dynamical systems $\dot{y} = g_i(y)$ where each $g_i$ represents a different canonical attractor in its normal form.
  This means that there exists a homeomorphism $\Phi$ mapping solutions of $g_i$ to solutions of $f$.
  Assuming smoothness, we model $\Phi$ as a diffeomorphism.
Our goal is to determine both the correct attractor motif $g_i$ and the diffeomorphism $\Phi$ given only observed trajectories $\{x_j(t_k)\}$ sampled from $f$.

\paragraph{Trajectory based loss function}
Since we do not have direct access to the vector field but only to sampled trajectories, we approximate the system using a learned diffeomorphism \( \hat{\Phi}_\theta \), parameterized by a normalizing flow\citep{kobyzev2020normalizing,papamakarios2021normalizing}, an invertible ResNet\citep{he2016deep}, or a Neural ODE (NODE) \citep{chen2018neural}.
 We seek to minimize the discrepancy between the transformed trajectory and the corresponding trajectory in one of the candidate motifs.

Define \( y(t) = \hat{\Phi}_\theta^{-1}(x(t)) \) as the estimated transformation of the observed trajectory into the motif's normal form. If the correct motif \( g_i \) is selected, then there should exist an approximate reparametrization of time \( \tau(t) \) such that
\[
y(\tau(t)) \approx y_{\text{model}}(t),
\]
where \( y_{\text{model}}(t) \) is a trajectory obtained by integrating \( \dot{y} = g_i(y) \) from an initial condition close to \( y(0) \).

A loss function capturing this trajectory discrepancy is given by:
\[
\mathcal{L}(\theta, i) = \sum_{t} \Big\| \hat{\Phi}_\theta^{-1}(x(t)) - y_{\text{model}}(t) \Big\|^2,
\]
where \( y_{\text{model}}(t) \) is computed via numerical integration of \( g_i \) from an initial condition inferred from the transformed data.

The final optimization objective is:
\[
\min_{\theta, i} \mathcal{L}(\theta, i) % + \lambda \mathcal{R}(\theta), %regularization? NODE\citep{finlay2020trainnode}
\]
where \( \lambda \) is a regularization coefficient.

This formulation allows for the joint identification of both the correct attractor motif \( g_i \) and the transformation \( \Phi \) that maps the observed system into its canonical form while only requiring topological equivalence rather than explicit vector


\subsection{Application on simple example}

\subsection{Comparison to other methods}



\section{Applications}



\section{Discussion}




\newpage
\bibliographystyle{unsrtnat_IMP_v1}
\bibliography{../all_ref.bib,../catniplab.bib}
\newpage
\appendix



%%%%%TOP
\section{Topology}\label{sec:topology}


\subsection{Diffeomorphisms in practice}\label{sec:diffeomorphisms}

a simple sufficient condition for a family of flows on a smooth compact manifold $M$ to generate the group $\Diff0_(M)$ of all diffeomorphisms of M that are isotopic to the identity \citep{caponigro2010families}.

Stability of diffeomorphisms implies that $\Diff(M)\subset C^1(M,M)$  is an open subset.

Normalizing flows\citep{kobyzev2020normalizing}

\paragraph{ResNets}
the layers of a ResNet can be considered as Euler-discretization of the integration of a flow of a diffeomorphism\citep{rousseau2020residual}


\paragraph{Flow-based}
NODE\citep{finlay2020trainnode}

Conjugate Mappings \citep{bramburger2021conjugate}

%any homeomorphism on a $p$-dimensional Euclidean space can be approximated by a Neural ODE or an i-ResNet operating on a $2p$-dimensional Euclidean space \citep{zhang2020approximation}
Any homeomorphism of $\mathbb {R} ^{n}$ can be approximated by a neural ODE operating on $\mathbb {R} ^{2n+1}$, proved by combining Whitney embedding theorem for manifolds and the universal approximation theorem for neural networks  \citep{zhang2020approximation}

diffeomorphisms from gradient information of desired costs \citep{lai2021parallelised}




\subsubsection{Decomposing diffeomorphisms}\label{sec:diff_dec}
\paragraph{Lie}
Since diffeomorphisms form a Lie group under composition, a diffeomorphism 
\( f \) can be written formally as:
\[
f = \exp(X),
\]
where \( X \) is a vector field (i.e., an element of the Lie algebra of the diffeomorphism group). Expanding \( X \) in a Fourier basis:
\[
X(x) = \sum_k \hat{X}(k) e^{i k \cdot x}
\]
provides a Fourier-like decomposition of the infinitesimal generator.

\subsubsection{Implementations}
Freia\citep{freia} %https://github.com/vislearn/FrEIA

\citep{dinh2016density} % https://github.com/AxelNathanson/pytorch-normalizing-flows?tab=readme-ov-file

\citep{stimper2023normflows} % https://github.com/VincentStimper/normalizing-flows



\section{Geometry}
Geometric Learning on Manifolds\citep{mostowsky2024geometrickernels}


\paragraph{Standard Matérn Kernel (Based on Geodesic Distance)}
Replace the Euclidean distance  $\| x - x' \|$  in the Matérn kernel with the distance to the manifold  $ d_{\text{manifold}}(x, x')$. 
This distance is now the measure of how far two points are from each other, but considering the geometry of the manifold.
\begin{equation}
k(x, x') = \frac{1}{\Gamma(\nu)} \left( \frac{\sqrt{2\nu} d_{\text{manifold}}(x, x')}{\ell} \right)^\nu K_\nu\left( \frac{\sqrt{2\nu} d_{\text{manifold}}(x, x')}{\ell} \right).
\end{equation}


\subsection{Implementations}
\citep{miolane2020geomstats}
%https://github.com/geomstats/geomstats?tab=readme-ov-file





%%%%%DS
\section{Dynamical systems background}

\subsection{Comparing dynamics}
\subsubsection{Exact equivalence: Topological conjugacy}\label{sec:top_conj}
\begin{definition}\label{def:top_conj}
Let \( f: X \to X \) and \( g: Y \to Y \) be two continuous dynamical systems, where \( X \) and \( Y \) are topological spaces.
 The systems are said to be \emph{topologically conjugate} if there exists a homeomorphism \( h: X \to Y \) such that the following conjugacy condition holds:
\[
h \circ f = g \circ h.
\]
That is, the following diagram commutes:
\[
\begin{array}{ccc}
X & \xrightarrow{f} & X \\
\downarrow h &  & \downarrow h \\
Y & \xrightarrow{g} & Y
\end{array}
\]
\end{definition}



\subsubsection{Defective equivalence: Mostly Conjugate Dynamical Systems}
\citep{skufca2007relaxing, skufca2008mostlyconjugate, bollt2010comparing}




\subsection{Normal forms}

Examples
Saddle ring
\begin{equation}
\begin{aligned}
\dot{x} &= (\sqrt{x^2 + y^2} - 1) x, \\
\dot{y} &= (\sqrt{x^2 + y^2} - 1) y.
\end{aligned}
\end{equation}


\subsubsection{Pushforward of the vector field: flow under a diffeomorphism}
See e.g., Eq.1.2 in \citep{agrachev2013control}.

Given a diffeomorphism \( \Phi: M \to M \), we consider the vector field \( V \in \text{Vec}(M) \), governing the dynamical system:
\[
\dot{q} = V(q).
\]

The diffeomorphism \( \Phi \) induces a pushforward of the vector field \( V \), given by:
\[
(\Phi_* V)(p) = D\Phi(\Phi^{-1}(p)) V(\Phi^{-1}(p)),
\]
where \( D\Phi \) is the differential (Jacobian) of \( \Phi \).  

\paragraph{Transformed Dynamical System}
In the new coordinates \( x = \Phi(q) \), differentiating with respect to time gives:
\[
\dot{x} = \frac{d}{dt} \Phi(q) = D\Phi(q) \dot{q}.
\]

Substituting \( \dot{q} = V(q) \), we obtain:
\[
\dot{x} = D\Phi(q) V(q).
\]

Rewriting in terms of \( x \), where \( q = \Phi^{-1}(x) \), the transformed vector field in the new coordinates is:
\[
\dot{x} = (D\Phi \circ \Phi^{-1}) V(\Phi^{-1}(x)).
\]

Thus, the transformed vector field is:
\[
V' = \Phi_* V = (D\Phi \circ \Phi^{-1}) \cdot (V \circ \Phi^{-1}).
\]
This describes how a diffeomorphism \( \Phi \) acts on the vector field governing the dynamics.





\subsection{Coherence in behavior}
\subsubsection{Perfect coherence: Invariant manifolds}%+forward invariance


\subsubsection{Asymptotic behavior}\label{sec:asymptotic}
Infinite time horizon perfect coherence





\subsubsection{Imperfect coherence}%+forward invariance
\paragraph{Almost invariant manifolds}
 Almost-invariant regions are identified via eigenvectors of a transfer operator and are ranked by the corresponding eigenvalues in the order of the sets' invariance or ``leakiness'' \citep{froyland2009almost}.
    
    
    
\paragraph{Metastable behavior}\label{sec:metastable}
    
    
    
\paragraph{Coherent regions}
Regions that remain coherent and relatively nondispersive over finite periods of time \citep{fackeldey2019metastable}.



\subsection{Reconstructing dynamics}\label{sec:rec_dyn}
\subsubsection{NODE}
\citep{finlay2020trainnode}





\end{document}

