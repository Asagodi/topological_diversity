\documentclass{article}
%\usepackage{arxiv}
\usepackage[numbers,sort&compress,super,comma]{natbib} % this must be before neurips_2024
\usepackage[preprint]{neurips_2024}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amssymb, amsmath, amsthm, amsfonts}
\usepackage{thmtools, mathtools, mathrsfs, dsfont}
\usepackage{bbm}
%\usepackage{bbold} %for mathbb{1}
\usepackage{forloop}
\usepackage[pdftex]{graphicx}  %remove demo option in your document
\usepackage{sidecap}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem} %easy customization of itemize/enumerate lists
\PassOptionsToPackage{dvipsnames}{xcolor}
\usepackage{xcolor}
\definecolor{ForestGreen}{rgb}{0.13, 0.55, 0.13}
\definecolor{MidnightBlue}{rgb}{0.1, 0.1, 0.44}
\definecolor{BurntOrange}{rgb}{0.8, 0.33, 0.0}
\definecolor{Plum}{rgb}{0.56, 0.27, 0.52}
\usepackage[colorlinks=true,linkcolor=MidnightBlue,citecolor=ForestGreen,filecolor=TealBlue,urlcolor=Plum]{hyperref}
\hypersetup{breaklinks=true}

%for comments
\newcommand{\ptitle}[1]{\textbf{#1:}\xspace}
\definecolor{mpcolor}{rgb}{1, 0.1, 0.59}
\definecolor{ascolor}{rgb}{1, 0.5, 0.}
\newcommand{\mpcomment}[1]{\textcolor{mpcolor}{(#1)}}
\newcommand{\ascomment}[1]{\textcolor{ascolor}{(#1)}}

\graphicspath{{figures}}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}[theorem]
\theoremstyle{definition} \newtheorem{definition}{Definition}
\theoremstyle{remark} \newtheorem{remark}{Remark}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\sep}{\operatorname{sep}}
\newcommand{\cl}{\operatorname{cl}}
\newcommand{\vol}{\operatorname{vol}}
\newcommand{\boa}{\operatorname{BoA}}
\newcommand{\mcN}{\mathcal{N}}
\newcommand{\mcK}{\mathcal{K}}
\newcommand{\mcU}{\mathcal{U}}
\newcommand{\mcX}{\mathcal{X}}
\newcommand{\T}{\operatorname{T}}
\newcommand{\TR}{\T\!\mathbb{R}}
\newcommand{\TM}{\T\!M}
\newcommand{\TpM}{\T_p\!M}
\newcommand{\Diff}{\operatorname{Diff}}
\newcommand{\Hpert}{H^{\text{pert}}}
\newcommand{\inv}{\operatorname{Inv}}

\newcommand{\defvec}[1]{\expandafter\newcommand\csname v#1\endcsname{{\mathbf{#1}}}}
\newcommand{\dm}[1]{\ensuremath{\mathrm{d}{#1}}} % dx dy dz dmu
\newcounter{ct}
\forLoop{1}{26}{ct}{
    \edef\letter{\alph{ct}}
    \expandafter\defvec\letter
}

% captial \vA
\forLoop{1}{26}{ct}{
    \edef\letter{\Alph{ct}}
    \expandafter\defvec\letter
}


%\title{Interpretable attractor motifs for a new language for neural computation}
\title{Attractor motifs for an interpretable language for neural computation}
\author{\'Abel S\'agodi and Il Memming Park}
\date{\today}



\begin{document}
\maketitle

\section*{Abstract}




\section{Introduction}\label{sec:intro}
Understanding the relationship between neural activity and behavior is a fundamental question in neuroscience.
 Recent research has demonstrated that despite the complexity of neuronal networks, their activity often resides in a low-dimensional manifold, capturing only a fraction of the potential complexity of the system \citep{duncker2021dynamics}.
 This observation suggests that neural activity is shaped by strong underlying constraints, possibly imposed by behaviorally relevant computations and external environmental demands.

Behavior, in many cases, is surprisingly simple. Even in seemingly complex tasks, animals and humans exhibit structured, stereotyped actions that can often be described using a small number of latent variables.
 From motor control to decision-making, behavioral trajectories unfold in a manner that is both predictable and constrained, hinting at a fundamental simplicity underlying observed actions.

justify assumption: if neural activity underlies behavior, should it not also exhibit similar simplicity

If neural dynamics are indeed tuned to drive behavior, we should expect a correspondence between the low-dimensional structure of neural activity and the inherent simplicity of behavior.
 This suggests that much of the apparent complexity in neural recordings may stem from redundant or task-irrelevant variability rather than meaningful high-dimensional computation.
  In this work, we explore the hypothesis that neural representations of behavior are fundamentally simple, reflecting a constrained, low-dimensional organization that enables efficient motor control, decision-making, and cognitive processing.
 Through computational modeling and empirical analyses, we demonstrate how low-dimensional neural manifolds can emerge and how their structure aligns with behavioral simplicity, providing insights into the fundamental principles governing neural computation.

\subsection{Our contribution}\label{sec:contribution}




\section{Background and related work}\label{sec:background}
A theory of (neural) computation\citep{jaeger2021theory, jaeger2023timescales, jaeger2023theory}
- Dynamical system theory as the source for the syntax for the language of neural computation.

Similar idea: DYNAMO \citep{cotler2023analyzing}

a language describing the axes of the space of possible dynamical solutions and their characteristics \citep{pagan2022dm}

\subsection{Computation through dynamics}
\citep{mante2013context}
\citep{sussillo2014neural}
\citep{vyas2020ctd}
\citep{versteeg2023expressive, sedler2023expressive}
\citep{dinc2025latentcomputing}

latent state space modeling \citep{zoltowski2020general} 

CtD benchmark \citep{versteeg2025computation}

RNNs for neural computation \citep{chaisangmongkon2017transience}
Black box \citep{sussillo2013blackbox}
Reverse-engineering \citep{maheswaranathan2019reverse} \citep{golub2018fixedpointfinder} \citep{smith2021reverse}
Diversity of solutions \citep{maheswaranathan2019universality}\citep{jarne2023initialization} \citep{turner2021charting} \citep{nayebi2021heterogeneity} \citep{zhong2023mechanistic}


%\subsubsection{Theoretical neuroscience}
%\citep{thompson2021explanation}
%\citep{levenstein2023theory}

\subsubsection{Canonical models for neural computation}
\citep{chirimuuta2014minimal}

\subsection{Existing methods for dynamics}
\subsubsection{Reconstructing dynamics}
Learning interpretable continuous-time models of latent stochastic dynamical systems \citep{duncker2019learning}

Phase2vec \citep{ricci2022phase2vec}
\citep{moriel2024timewarpattend}
Smooth Prototype Equivalences \citep{friedman2025characterizing}

MARBLE \citep{gosztolai2025marble}


\subsubsection{Methods to compare/classify dynamics}\label{sec:compare}
%Comparison of dynamical systems
Concepts:
\begin{itemize}
\item equivalence of systems: topological conjugacy (Def.~\ref{def:top_conj}).%/equivalence
\item asymptotic behavior: $\omega$-limit sets
\end{itemize}

Taking apart topological conjugacy (diffeomorphism from motif):
\begin{itemize}
\item On asymptotic / $\omega$-limit set / invariant set
\item On transients
\end{itemize}


\paragraph{SOTA}
assess the limit sets
Fixed-point based \citep{sussillo2013blackbox,katz2017fibers,golub2018fixedpointfinder}

Also limit cycles 

slow invariant manifolds\citep{Sagodi2024a}

TWA\citep{moriel2024timewarpattend}

SVCCA\citep{raghu2017svcca}

Dynamical Similarity Analysis\citep{ostrow2024beyond} \citep{kamiya2024koopman}\\
What we lose (because of the truncation): limit cycles, multistability
But we can capture some slow manifold behavior!

Diffeomorphic vector field alignment for comparing dynamics across learned models\citep{chen2024dform}
- orbital similarity loss (Eq.~5)



\subsubsection{Decomposition of the dynamics}\label{sec:decomposition}
into basins of attraction through Conley’s Fundamental Theorem for Dynamical Systems \citep{conley1978morse, norton1995fundamental,mischaikow1999cit}

FSM-based approaches\citep{pollack1991induction, casey1996dynamics, jacobsson2005ruleextraction, ashwin2021excitable, oliva2019fsm, cotteret2024fsm}
Behaviorism is based on an engineering approach, treating the mind as a control system for the organism. This corresponds to an approximation of the recurrent neural dynamics (brain states) by finite state automata (behavioral states). Another approximations to neural dynamics is described, leading to a Platonic-like model of mind based on psychological spaces. \citep{duch1998platonic}

neural automata\citep{goles2013neural, uria2024invariants}

For different type of dynamcis (ergodic) but that separates the state space into coherent/metastable regions between which the Perron–Frobenius operator describes transitions.

Main shortcoming: difficulty with dealing with complicated attractors.
When is decomposition of attractor into motifs possible?

On the lifting and reconstruction of nonlinear systems with multiple invariant sets \citep{pan2024lifting}

\citep{driscoll2024flexible}

compositional tasks \citep{tafazoli2024building}



\paragraph{Neural ensembles}
\citep{yuste2024ensembles}



\subsubsection{Reduction}\label{sec:reduction} %simplification of dynamics
In each basin of attraction we have one attractor.
We can simplify it's dynamics by looking at its normal form.

\begin{itemize}
\item approximation based (model order reduction): \citep{schilders2008model}
\item topology based Delay Embedding and Takens' Theorem  and Morse Decomposition 
\item low-dimensional reduced model \citep{zemlianova2024dynamical}
\item motifs: normal forms \citep{full1999templates} \citep{nayfeh2011normalforms} and \citep{bonilla2012discriminative} (also referred to as prototypes in literature)
\item  Spectral properties of dynamical systems, model reduction and decompositions \citep{mezic2005spectral}
\item libraries \citep{brunton2014compressive}, SINDy  one starts with a large library of candidate functions (or dynamical motifs) that could potentially describe the dynamics of a system. The method then uses sparse regression techniques to select a few terms that best capture the observed time-series data \citep{brunton2016discovering, brunton2016sparse, fasel2022ensemble}
\item  primitives: linear \citep{kaul2020linear} (more for behavioral \citep{ijspeert2013dynamical})
\item fast-slow \citep{dsilva2016data} \citep{haller2017exact}
\item averaging methods \citep{sanders2007averaging} (invariant tori\citep{novaes2024invariant}), Theorem 9.4 in \citep{hoppensteadt2012weakly}
\item FSMs: \citep{giles1991extracting, casey1996dynamics, giles1999equivalence, oliva2019fsm, ceni2020excitable, cotteret2024fsm, aichernig2024learning} + weighted\citep{wei2024weighted}
\item effective dynamics \citep{menier2025interpretable}
\end{itemize}


\subsection{Interpretability}
\ascomment{Should this come before Reduction?}
%phos
What is interpretability? \citep{erasmus2021interpretability}
\citep{madsen2024interpretability}
levels \citep{hochstein2022levels}

%neuro
\citep{whiteway2019interpretable}
\citep{kar2022interpretability}
extrinsic and intrinsic dimensionality\citep{jazayeri2021interpreting}

 machine learning: important yet slippery\citep{lipton2018mythos}	
 Building Blocks of Interpretability \citep{olah2018interpretability}
\citep{beisbart2022interpretability}
simple isn't easy \citep{raz2024ml}
Manipulating and measuring model interpretability\citep{poursabzi2021manipulating}: \ascomment{A way to compare?}
\citep{he2024multilevel}

\subsubsection{Simplicity}
\citep{gao2015simplicity}
\citep{dyer2023simplest}
\citep{quinn2022information}

\subsection{Minimal}
\citep{beer1996toward}
\citep{chirimuuta2014minimal}
\citep{batterman2014minimal}
\citep{brancazio2023minimal}


\paragraph{Reduction/idealization/simplification/abstraction}
\citep{marr1976computation, marr2010vision}

\citep{chirimuuta2018mmm} %chirimuuta2022artifacts, chirimuuta2024analogies

\citep{potochnik2021levels,
potochnik2020idealization,
potochnik2017idealization}

\citep{stinson2020idealized}
\citep{chirimuuta2024brain} 

\subsubsection{Understanding}
\citep{deregt2017understanding}
\citep{potochnik2017idealization}
\citep{guest2023logical}

Understanding brains \citep{marder2015understanding} \citep{lindsay2023testing} \citep{barman2024towards} \citep{dowling2018understanding}
prediction vs understanding \citep{chirimuuta2021prediction}


\subsubsection{Works mentioning interpretability}
\citep{whiteway2019interpretable}
\citep{kar2022interpretability}
 Making hippocampal manifolds physiologically interpretable \citep{esparza2023interpretable}
black box to mixture of interpretable models \citep{ghosh2023blackbox}
\citep{schneider2023learnable}
\citep{brenner2024almost}
\citep{menier2025interpretable}

\subsubsection{Our levels of interperetability}
Proposal: guiding principles for a hierarchy of interpretability


Message: poset of interpretable models

Message: reducing complexity, introducing hierarchy 
\citep{Vermani2025b}
\citep{geadah2024parsing, geadah2025modeling}


\subsubsection{Interpretability in RNNs}
SVCCA\citep{raghu2017svcca}
%MinimalRNN\citep{chen2017minimalrnn} more about simplifying gates
Tiny RNN\citep{jian2023tinyrnn}
Towards interpreting recurrent neural networks through probabilistic abstraction \citep{dong2020towards}
Expressive architectures enhance interpretability of dynamics-based neural population models \citep{sedler2023expressive}
Interpretable Latent Factors with Sparse Component Analysis \citep{zimnik2024identifying}
Almost-Linear RNNs Yield Highly Interpretable Symbolic Codes in Dynamical Systems Reconstruction \citep{brenner2024almost}
\citep{he2024multilevel}
\citep{schaeffer2020reverseengineering} Representations and Dynamics Distillations (RADD)
\citep{turner2023simplicity}
FSM: \citep{oliva2019fsm}, \citep{cotteret2024fsm},  ENA \citep{ceni2020interpreting}
low-rank RNNs \citep{beiran2021shaping, valente2022extracting, valente2022probing}

Understanding and controlling the geometry of memory organization in RNNs \citep{haputhanthri2025understanding}

\subsubsection{In neuroscience}
\paragraph{Motor primitives}
In motor cortex, neural trajectories during reaching movements often evolve along a low-dimensional cyclic structure, making it possible to interpret them as dynamical primitives for movement \citep{ijspeert2013dynamical}.

\citep{marton2021efficient}



\section{Matching dynamical motifs}
\subsection{An interpretable motif-centered metric for dynamical systems}
Consider our target dynamical system to be defined on a compact subset of $\reals^n$.

Let $f\in C^1$ define the vector field of our target system
and let $g_i$ be a dynamical motif.
Then, there exists a perturbation $p\in C^1$ 
 and a homeomorphism $\Phi:\reals^n\rightarrow\reals^n$ such that 
$\|\psi_{f+p} - \Phi(\psi_{g_i})\|_\infty = 0$.

Therefore, we can define the distance w.r.t. each motif as $d(f,g_i) = \inf \|p\|$ with the above constraint.

Projection of $f$ onto library

\ascomment{Convergence during training of RNNs: for task that relies on analog memory to low distance to dynamical motif that is a continuous attractor diffeomorphic to the memory manifold.}



\subsubsection{Recognizable geometric-topological features}
fixed points, limit cycles

continuous attractors

invariant manifolds
manifolds neuro/RNN\citep{langdon2023unifying, can2021emergence,
cueva2021continuous,
gort2024emergence,
mishra2021continual,
chaudhuri2019attractor, ghazizadeh2021slowmanifold, duncker2021dynamics, pezon2024linking}
neuro \citep{fortunato2024nonlinear}

manifold alginment \citep{kuoch2024probing}

approximate continuous attractors as slow invariant manifolds\citep{Sagodi2024a}


\subsection{Motifs: The building blocks}
\subsubsection{Guiding principles}
Attractors reduce behavior to "basins" in phase space

human-compatible descriptions

motifs should be all topologically distinct (not topologically conjugate)

%\subsubsection{The base case} % \subsubsection{Constructing}


\paragraph{Enumerating attractors dimension-wise}
In dynamical systems, attractors can be systematically classified according to their topological dimension, with the simplest topological spaces serving as the fundamental building blocks at each level. This perspective provides a principled way to enumerate attractors that can arise in ordinary differential equations (ODEs).
\begin{enumerate}[start=0,label={\bfseries Dim \arabic*:}]
\item  Stable fixed point
\item  Bounded line attractor ($[0,1]\subset\reals$), ring attractor ($S_0^1$), stable limit cycle ($S_1^1$)
\item Bounded square attractor ($[0,1]^2$), Torus attractor ($(S_0^1)^2$), LC-torus attractor ($S_0^1\times S_1^1$), QPTA ($(S_1^1)^2$), Sphere attractor $S^2$
\item ...
\end{enumerate}

This pattern continues, with higher-dimensional attractors constructed from fundamental topological spaces such as $S^n$ or their Cartesian and toroidal products.

\ascomment{do we need to account for the possible existence of an attractor of the form $[0,1]^2\setminus D_{1/2}$?}




\subsection{Dynamical motif matching}
\subsubsection{Single global attractor}
We consider a target dynamical system $\dot{x} = f(x)$ which has a single global attractor.
 We assume that this system is topologically equivalent to one of a family of attractor motifs defined by dynamical systems $\dot{y} = g_i(y)$ where each $g_i$ represents a different canonical attractor in its normal form.
  This means that there exists a homeomorphism $\Phi$ mapping solutions of $g_i$ to solutions of $f$.
  Assuming smoothness, we model $\Phi$ as a diffeomorphism.
Our goal is to determine both the correct (best fitting/most explanatory) attractor motif $g_i$ and the diffeomorphism $\Phi$ given only observed trajectories $\{x_j(t_k)\}$ sampled from $f$.

\paragraph{Trajectory based loss function}
Since we do not have direct access to the vector field but only to sampled trajectories, we approximate the system using a learned diffeomorphism \( \hat{\Phi}_\theta \), parameterized by a normalizing flow\citep{kobyzev2020normalizing,papamakarios2021normalizing}, an invertible ResNet\citep{he2016deep}, or a Neural ODE (NODE) \citep{chen2018neural}.
 We seek to minimize the discrepancy between the transformed trajectory and the corresponding trajectory in one of the candidate motifs.

%time repraram
 If the correct motif \( g_i \) is selected, then there should exist an approximate reparametrization of time \( \tau(t) \) such that
\[
y(\tau(t)) \approx x(t),
\]
where \( y(t) \) is a trajectory obtained by integrating \( \dot{y} = g_i(y) \) from an initial condition \( y(0) \). % (close to y(0))


A loss function capturing this trajectory discrepancy is given by:
\[
\mathcal{L}(\theta, i) = \sum_{t} \Big\| (x(t)) - \hat{\Phi}_\theta^{-1}(y(t)) \Big\|^2,
\]
where \( y_{\text{model}}(t) \) is computed via numerical integration of \( g_i \) from an initial condition inferred from the transformed data.

The final optimization objective is:
\[
\min_{\theta, i} \mathcal{L}(\theta, i). % + \lambda \mathcal{R}(\theta), %regularization? NODE\citep{finlay2020trainnode}
\]
%where \( \lambda \) is a regularization coefficient.

This formulation allows for the joint identification of both the correct attractor motif \( g_i \) and the transformation \( \Phi \) that maps the observed system into its canonical/normal form.


\subsection{Application on simple example}

\subsection{Comparison to other methods}



\section{Applications}



\section{Discussion}




\newpage
\bibliographystyle{unsrtnat_IMP_v1}
\bibliography{../all_ref.bib,../catniplab.bib}
\newpage
\appendix



%%%%%TOP
\section{Topology}\label{sec:topology}


\subsection{Diffeomorphisms in practice}\label{sec:diffeomorphisms}

a simple sufficient condition for a family of flows on a smooth compact manifold $M$ to generate the group $\Diff0_(M)$ of all diffeomorphisms of M that are isotopic to the identity \citep{caponigro2010families}.

Stability of diffeomorphisms implies that $\Diff(M)\subset C^1(M,M)$  is an open subset.

Normalizing flows\citep{kobyzev2020normalizing}

\paragraph{ResNets}
the layers of a ResNet can be considered as Euler-discretization of the integration of a flow of a diffeomorphism\citep{rousseau2020residual}


\paragraph{Flow-based}
NODE\citep{finlay2020trainnode}

Conjugate Mappings \citep{bramburger2021conjugate}

%any homeomorphism on a $p$-dimensional Euclidean space can be approximated by a Neural ODE or an i-ResNet operating on a $2p$-dimensional Euclidean space \citep{zhang2020approximation}
Any homeomorphism of $\mathbb {R} ^{n}$ can be approximated by a neural ODE operating on $\mathbb {R} ^{2n+1}$, proved by combining Whitney embedding theorem for manifolds and the universal approximation theorem for neural networks  \citep{zhang2020approximation}

diffeomorphisms from gradient information of desired costs \citep{lai2021parallelised}




\subsubsection{Decomposing diffeomorphisms}\label{sec:diff_dec}
\paragraph{Lie}
Since diffeomorphisms form a Lie group under composition, a diffeomorphism 
\( f \) can be written formally as:
\[
f = \exp(X),
\]
where \( X \) is a vector field (i.e., an element of the Lie algebra of the diffeomorphism group). Expanding \( X \) in a Fourier basis:
\[
X(x) = \sum_k \hat{X}(k) e^{i k \cdot x}
\]
provides a Fourier-like decomposition of the infinitesimal generator.

\subsubsection{Implementations}
Freia\citep{freia} %https://github.com/vislearn/FrEIA

\citep{dinh2016density} % https://github.com/AxelNathanson/pytorch-normalizing-flows?tab=readme-ov-file

\citep{stimper2023normflows} % https://github.com/VincentStimper/normalizing-flows



\section{Geometry}
Geometric Learning on Manifolds\citep{mostowsky2024geometrickernels}


\paragraph{Standard Matérn Kernel (Based on Geodesic Distance)}
Replace the Euclidean distance  $\| x - x' \|$  in the Matérn kernel with the distance to the manifold  $ d_{\text{manifold}}(x, x')$. 
This distance is now the measure of how far two points are from each other, but considering the geometry of the manifold.
\begin{equation}
k(x, x') = \frac{1}{\Gamma(\nu)} \left( \frac{\sqrt{2\nu} d_{\text{manifold}}(x, x')}{\ell} \right)^\nu K_\nu\left( \frac{\sqrt{2\nu} d_{\text{manifold}}(x, x')}{\ell} \right).
\end{equation}


\subsection{Implementations}
\citep{miolane2020geomstats}
%https://github.com/geomstats/geomstats?tab=readme-ov-file





%%%%%DS
\section{Dynamical systems background}

\subsection{Comparing dynamics}
\subsubsection{Exact equivalence: Topological conjugacy}\label{sec:top_conj}
\begin{definition}\label{def:top_conj}
Let \( f: X \to X \) and \( g: Y \to Y \) be two continuous dynamical systems, where \( X \) and \( Y \) are topological spaces.
 The systems are said to be \emph{topologically conjugate} if there exists a homeomorphism \( h: X \to Y \) such that the following conjugacy condition holds:
\[
h \circ f = g \circ h.
\]
That is, the following diagram commutes:
\[
\begin{array}{ccc}
X & \xrightarrow{f} & X \\
\downarrow h &  & \downarrow h \\
Y & \xrightarrow{g} & Y
\end{array}
\]
\end{definition}

\paragraph{Topological conjugacy of time series}
\citep{dlotko2024topconj}

\subsubsection{Defective equivalence: Mostly Conjugate Dynamical Systems}
\citep{skufca2007relaxing, skufca2008mostlyconjugate, bollt2010comparing}




\subsection{Normal forms}

\subsubsection{Normal forms for autonomous dynamics}
Examples
Saddle ring
\begin{equation}
\begin{aligned}
\dot{x} &= (\sqrt{x^2 + y^2} - 1) x, \\
\dot{y} &= (\sqrt{x^2 + y^2} - 1) y.
\end{aligned}
\end{equation}


\subsubsection{Pushforward of the vector field: flow under a diffeomorphism}
See e.g., Eq.1.2 in \citep{agrachev2013control}.

Given a diffeomorphism \( \Phi: M \to M \), we consider the vector field \( V \in \text{Vec}(M) \), governing the dynamical system:
\[
\dot{q} = V(q).
\]

The diffeomorphism \( \Phi \) induces a pushforward of the vector field \( V \), given by:
\[
(\Phi_* V)(p) = D\Phi(\Phi^{-1}(p)) V(\Phi^{-1}(p)),
\]
where \( D\Phi \) is the differential (Jacobian) of \( \Phi \).  

\paragraph{Transformed Dynamical System}
In the new coordinates \( x = \Phi(q) \), differentiating with respect to time gives:
\[
\dot{x} = \frac{d}{dt} \Phi(q) = D\Phi(q) \dot{q}.
\]

Substituting \( \dot{q} = V(q) \), we obtain:
\[
\dot{x} = D\Phi(q) V(q).
\]

Rewriting in terms of \( x \), where \( q = \Phi^{-1}(x) \), the transformed vector field in the new coordinates is:
\[
\dot{x} = (D\Phi \circ \Phi^{-1}) V(\Phi^{-1}(x)).
\]

Thus, the transformed vector field is:
\[
V' = \Phi_* V = (D\Phi \circ \Phi^{-1}) \cdot (V \circ \Phi^{-1}).
\]
This describes how a diffeomorphism \( \Phi \) acts on the vector field governing the dynamics.




\subsubsection{Normal forms for input-driven dynamics}
Normal form representation of control systems  \citep{gilbert1963controllability}
Brunovsky normal form is a canonical form for controllable linear systems.



\subsection{Coherence in behavior}
\subsubsection{Perfect coherence: Invariant manifolds}%+forward invariance


\subsubsection{Asymptotic behavior}\label{sec:asymptotic}
Infinite time horizon perfect coherence





\subsubsection{Imperfect coherence}%+forward invariance
\paragraph{Almost invariant manifolds}
 Almost-invariant regions are identified via eigenvectors of a transfer operator and are ranked by the corresponding eigenvalues in the order of the sets' invariance or ``leakiness'' \citep{froyland2009almost}.
    
    
    
\paragraph{Metastable behavior}\label{sec:metastable}
    
    
    
\paragraph{Coherent regions}
Regions that remain coherent and relatively nondispersive over finite periods of time \citep{fackeldey2019metastable}.



\subsection{Reconstructing dynamics}\label{sec:rec_dyn}
\subsubsection{NODE}
\citep{finlay2020trainnode}


\section{Interpretability}
Interpretability is ofter used, yet is typically not defined in computational neuroscience\citep{duncker2019learning}.

\subsection{DS}
Why are LDSs interpretable?
- asymptotic behavior is given
- time reparametrization = eigenspectrum of Jacobian.


\subsection{Input-driven}
Why are input-driven systems difficult to characterize? The lesson from our hierarchy of interpretability.
The input acts on an infinite dimensional space in a nonlinear manner.
We cannot easily reduce this (without losing some of the details of the action).



\subsection{Latent variable models}
For these systems the details don't make much sense any ways!
There is already so much abstraction!

\end{document}

