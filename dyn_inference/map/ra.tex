\documentclass{article}
%\usepackage{arxiv}
\usepackage[numbers,sort&compress,super,comma]{natbib} % this must be before neurips_2024
\usepackage[preprint]{neurips_2024}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amssymb, amsmath, amsthm, amsfonts}
\usepackage{thmtools, mathtools, mathrsfs, dsfont}
\usepackage{bbm}
\usepackage{forloop}
\usepackage[pdftex]{graphicx}  %remove demo option in your document
\usepackage{sidecap}
\PassOptionsToPackage{dvipsnames}{xcolor}
\usepackage{xcolor}
\definecolor{ForestGreen}{rgb}{0.13, 0.55, 0.13}
\definecolor{MidnightBlue}{rgb}{0.1, 0.1, 0.44}
\definecolor{BurntOrange}{rgb}{0.8, 0.33, 0.0}
\definecolor{Plum}{rgb}{0.56, 0.27, 0.52}
\usepackage[colorlinks=true,linkcolor=MidnightBlue,citecolor=ForestGreen,filecolor=TealBlue,urlcolor=Plum]{hyperref}
\hypersetup{breaklinks=true}

\graphicspath{{figures}}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}[theorem]
\theoremstyle{definition} \newtheorem{definition}{Definition}
\theoremstyle{remark} \newtheorem{remark}{Remark}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\sep}{\operatorname{sep}}
\newcommand{\cl}{\operatorname{cl}}
\newcommand{\vol}{\operatorname{vol}}
\newcommand{\boa}{\operatorname{BoA}}
\newcommand{\mcN}{\mathcal{N}}
\newcommand{\mcK}{\mathcal{K}}
\newcommand{\mcU}{\mathcal{U}}
\newcommand{\mcX}{\mathcal{X}}
\newcommand{\T}{\operatorname{T}}
\newcommand{\TR}{\T\!\mathbb{R}}
\newcommand{\TM}{\T\!M}
\newcommand{\TpM}{\T_p\!M}
\newcommand{\Diffeo}{\operatorname{Diffeo}}
\newcommand{\Hpert}{H^{\text{pert}}}
\newcommand{\inv}{\operatorname{Inv}}

\newcommand{\defvec}[1]{\expandafter\newcommand\csname v#1\endcsname{{\mathbf{#1}}}}
\newcommand{\dm}[1]{\ensuremath{\mathrm{d}{#1}}} % dx dy dz dmu
\newcounter{ct}
\forLoop{1}{26}{ct}{
    \edef\letter{\alph{ct}}
    \expandafter\defvec\letter
}
% captial \vA
\forLoop{1}{26}{ct}{
    \edef\letter{\Alph{ct}}
    \expandafter\defvec\letter
}

\title{MAP inference of connectivity in a noise-driven RNN with ring attractor dynamics}
\author{\'Abel S\'agodi}
\date{\today}



\begin{document}
\maketitle

\section{Problem setting}
We consider a teacher-student setup, where the teacher has \( D \) neurons and a recurrent weight matrix \( B \), such that the dynamics of its firing rate vector \( z(t) \in \mathbb{R}^D \) is given by
\[
\tau \frac{d z}{dt} = -z + B \, \phi(z) + \boldsymbol{\xi}(t),
\]
where \( \boldsymbol{\xi}(t) \) is uncorrelated Gaussian noise with 
\[
\mathbb{E}[\boldsymbol{\xi}(t)] = 0 \quad \text{and} \quad \mathbb{E}[\boldsymbol{\xi}(t) \boldsymbol{\xi}(s)^\top] = 2 \sigma^2_{\xi} \, \delta(t - s) I_D,
\]
and \( \phi \) is a possibly nonlinear transfer function, which we take to act element-wise.

%Noorman model for RA?


Similarly, we assume a \( d \)-dimensional student with recurrent weights \( A \), such that its rate \( x(t) \in \mathbb{R}^d \) evolves as
\[
\tau \frac{d x}{dt} = -x + A \, \phi(x) + \boldsymbol{\eta}(t),
\]
where \( \boldsymbol{\eta}(t) \) is \( d \)-dimensional white noise with
\[
\mathbb{E}[\boldsymbol{\eta}(t)] = 0 \quad \text{and} \quad \mathbb{E}[\boldsymbol{\eta}(t) \boldsymbol{\eta}(s)^\top] = 2 \sigma^2_{\eta} \, \delta(t - s) I_d.
\]

Assuming \( d < D \), we observe the first \( d \) neurons of the teacher:
\[
x_{\text{obs}}(t) = P z(t) \quad \text{for} \quad t \in [0, T],
\]
where \( P = (I_d, \, 0_{d \times (D - d)}) \).

Our goal is to infer the student’s weight matrix \( A \) given these observations.


\subsection{MAP estimate for the dynamics}

The likelihood of observing a trajectory \(\{x_{\text{obs}}(t) : t \in [0, T]\}\) given a particular weight matrix \( A \), which using the path integral representation of an Itô process can be written non-rigorously as  
\[
p(\{x_{\text{obs}}(t) : t \in [0, T]\} | A) \propto \exp \left( -\frac{1}{2\sigma^2_{\eta}} \int_0^T dt \, \left\| \tau \dot{x}_{\text{obs}}(t) + x_{\text{obs}}(t) - A \phi(x_{\text{obs}}(t)) \right\|^2 \right).
\]


\subsubsection{Prior}
An isotropic Gaussian prior over the elements of \( A \) is given by:  
\[
A_{ij} \sim_{\text{i.i.d.}} \mathcal{N} \left( 0, \frac{\sigma^2_{\eta} T}{\rho} \right),
\]
where \( \rho > 0 \).


\subsubsection{Posterior}
The log-posterior density takes a particularly simple form:
\[
L = -\sigma^2_{\eta} T \log p\left(A \mid \{x_{\text{obs}}(t) : t \in [0, T]\} \right) = \int_{0}^{T} dt \, \left\| \tau \dot{x}_{\text{obs}}(t) + x_{\text{obs}}(t) - A \phi(x_{\text{obs}}(t)) \right\|^2 + \rho \|A\|^2_F.
\]



As the log-posterior density is quadratic, it is easy to read off that the MAP estimate of \( A \) is 

\[
\hat{A}_T = \left( \int_{0}^{T} dt \, \left[ \tau \dot{x}_{\text{obs}}(t) + x_{\text{obs}}(t) \right] \phi(x_{\text{obs}}(t))^\top \right) 
\left( \int_{0}^{T} dt \, \phi(x_{\text{obs}}(t)) \phi(x_{\text{obs}}(t))^\top + \rho I_d \right)^{-1},
\]

where we add a subscript \( T \) to emphasize the observation window. 

Using the dynamics of \( x_{\text{obs}}(t) = P z(t) \), we can re-write this in terms of the teacher’s dynamics as

\[
\hat{A}_T = P \left( B C_T + \int_{0}^{T} dt \, \xi(t) \phi(z(t))^\top \right) P^\top 
\left( P C_T P^\top + \rho I_d \right)^{-1},
\]

where

\[
C_T = \int_{0}^{T} dt \, \phi(z(t)) \phi(z(t))^\top
\]

is the empirical covariance of the teacher network activity.



\section{Calculating the covariance for a symmetric ring attractor network}

Brownian motion on the unit circle \((Y_t , t \geq 0)\) \citep{liu2012stochastic}

On L\'evy’s Brownian Motion and White Noise Space on the Circle \citep{huang2021levy}

Can we consider a continuous attractor (or a fast-slow system more generally) by calculating the correlation in the fast and slow systems separately and then composing the solutions?


\subsection{Time-lagged correlation}
The time-lagged correlation is given by
\[
C(\tau) = \mathbb{E}_s \left[ z(t) z(t + \tau)^\top \right].
\]


\newpage
\bibliographystyle{plain}
\bibliography{../../all_ref.bib}

\end{document}