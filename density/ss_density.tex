\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amssymb, amsmath, amsthm, amsfonts}
\usepackage{thmtools, mathtools, mathrsfs, dsfont}
\usepackage[square,numbers]{natbib}
\bibliographystyle{abbrvnat}
\usepackage{algorithm}
\usepackage{algpseudocode}

\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\theoremstyle{remark}
\newtheorem{remark}{Remark}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\sep}{\operatorname{sep}}
\newcommand{\cl}{\operatorname{cl}}

\title{Approximation of multi-stable dynamical systems on infinite horizons}
\author{\'Abel S\'agodi and Il Memming Park}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
Recurrent Neural Networks (RNNs) are believed to be universal approximators.
However, RNNs are not universal approximators 
Existing results about approximating dynamical systems either focus on finite time horizon approximation or on approximating stable dynamical systems on an infinite horizon.
We seek to extend approximation results for dynamical systems to multi-stable systems (systems without chaos) on an infinite horizon through Neural ODEs (NODEs).



Why do we care about infinite horizon?
- approximation gives us qualitative behavior
- (little) noise doesn't make approximation increase suddenly in error (only smoothly)


It could be that existing theorems could be interpreted that in the limit $N\rightarrow\infty$ the approximation holds.
Here we do it for finite $N$.


\subsection{Approximation of dynamics on compact time intervals}
An RNN with a sufficient number of hidden units and appropriate non-linear activation functions (such as tanh or sigmoid) can approximate any dynamic system or time-dependent function to arbitrary precision, provided the system operates on compact time intervals.


\subsection{Approximation of fading memory systems}
 Jaeger (2001) primarily focused on Echo State Networks (ESNs), demonstrating their universal approximation capabilities specifically for fading memory systems. Fading memory refers to the property where the influence of past inputs diminishes over time, allowing the network to focus more on recent inputs while still retaining some memory of older inputs. Let’s clarify the role of fading memory systems in the context of infinite horizon approximations and the broader discussion on RNNs.



\section{Approximation of dynamical systems}
For an overview of the literature, see [litrev]

\subsection{Targets}
Autonomous
Structurally stable+Fixed points = hyperbolic fixed points
The dynamics is globally attractive / There exist a compact set that with the fixed points inside that is forward invariant 

\begin{equation}
\dot x = f(x)
\end{equation}
$f\in C^1$.

\subsection{Hypothesis class}
The hypothesis class $\mathcal{G}$ is composed of
\begin{equation}
\dot x = g(x) = A\sigma(Bx+\theta)
\end{equation}


\subsection{Metric}

Uniform norm over time (given the same initial starting point)
\begin{equation}
\|\varphi(t,x_0)-\hat \varphi(t,x_0)\|_\infty = \sup_t|\varphi(t,x_0)-\hat \varphi(t,x_0)|
\end{equation}


Expectation of the error over all initial points
\begin{equation}
\|\varphi-\hat \varphi\| = \int_{x_0\in X}\|\varphi(t,x_0)-\hat \varphi(t,x_0)\|_\infty
\end{equation}


\subsection{Separatrices}

Denote the basin of attraction for an attractor $A$ as $BOA(A)$.

\begin{definition}	
Define the separatrix between the basins of attraction of attractors $A_i$ and $A_j$ as 
\begin{equation}
\sep(A_i,A_j) = \cl(BOA(A_i)\cap \cl(BOA(A_j)).
\end{equation}
\end{definition}

\subsection{Theorem}
\begin{theorem}
Let $D$ be an open subset of $\mathbb{R}^n$, $F : D \to \mathbb{R}^m$ be a $C^1$-mapping, and $I_\epsilon$ be a compact subset of $D$.
Suppose that there is a subset $K \subset I_\epsilon$ such that any solution $x(t)$ with initial value $x(0) \in K$ of an ordinary differential equation
\begin{equation}\label{eq:5}
    \dot{x} = F(x), \quad x(0) \in K
\end{equation}
is defined on $I = [0, T]$ $(0 < T < \infty)$ and $x(t)$ is included in $I_\epsilon$ for any $t \in I$.

with flow $\varphi_t$ 


 Then, for an arbitrary $\epsilon > 0$, there exist an MLP recurrent neural network 
 \begin{equation}
\dot x = \hat f(x) = \sigma(A\sigma(Bx+b)+a)
\end{equation}
%such that for a solution $\hat \varphi(t,x_0)$ satisfying Eq~\ref{eq:5} with initial state $x_0$ of the network
such that its flow $\hat \varphi$ satisfies
\begin{equation}
\|\varphi-\hat \varphi\| = \int_{x_0\in X}\|\varphi(t,x_0)-\hat \varphi(t,x_0)\|_\infty<\epsilon.
\end{equation}
\end{theorem}


\begin{proof}
Because of the density of the hypothesis class $\mathcal{G}$ we have that for any $f\in C^1$ and any $\delta_1>0$
there exists $\hat f\in\mathcal{G}$ such that 


Structural stability implies that there exists a $\delta_2>0$ such that for all $g$ that are $\delta_2$ $C^1$ close to $f$ the flow is topologically equivalent to that of $f$.

So we can choose $\delta = \min\{\delta_1,\delta_2\}$ such that the flow of $\hat f\in\mathcal{G}$ is topologically equivalent to that of $f$.



We can distinguish two types of errors:
$x_0\in BOA(A_i^f,A_i^g)$ flow deviates along the trajectories
We can use \citep{vanhandel2007filtering} to bound this


$x_0\in BOA(A_i^f,A_j^g)$ for $i\neq j$
flow error because trajectories go to different attractors
We can bound this by reducing the mismatch between the separatrices (proportional to the distance between the attractors).

(Conj.) For each pair $i\neq j$ the connected component of $\sep(A_i,A_j)$ is a normally hyperbolic backward invariant manifold.

Therefore, by Fenichel's Persistent manifold theorem, there exists a $\delta_{ij}$ such that for all $g$ that are $\delta_{ij}$ $C^1$ close to $f$ such that $g$ has an  normally hyperbolic backward invariant manifold that is diffeomorphic to and at most at $\mathcal{\delta_{ij}}$ Hausdorff distance from $\sep(A_i,A_j)$.


Now we need to sum all the different errors together.
\end{proof}



\subsection{Approximating a limit cycle}


We can use the fact that the tubular neighbourhood around the LC is an inflowing normally hyperbolic invariant manifold.
This implies that there is a 3 layer FNN that approximates the vector field x a bump function around the LC (similar idea as in paper but this would be an actual possible implementation).
Then we know that the this NHIM will persist.
We also know that there is a maximal perturbation size ($\epsilon_{LC}$) for which the LC persists.
So we have to guarantee that the LC approximation is already close enough so that this perturbation does not destroy the LC.
We can now rescale the perturbation so that the period of approximation LC matches the target's. This is the case if 
\[\frac{\ell}{\int_{LC}f(x)} =  \frac{\hat \ell}{\int_{LC}\hat f(x)}\] for the length of the LC $\ell$ and the integrated vector field along the LC $\int_{LC}f(x)$.
We just increase or decrease  the above perturbation by multiplying with a scalar so that this equality holds.
The only thing we now need to worry about is how the length of the LC changes as a result of this perturbation and whether we can indeed get the equality 

Fenichel's PMT guarantees that the perturbed LC lies within $\mathcal{O}(\epsilon)$ the original LC approximation.
The integral will be growing as $\epsilon * \hat \ell(\epsilon)$
Is it true that $\hat \ell(\epsilon) = \hat \ell + g(\epsilon) where |g(\epsilon)|<|\epsilon|$?

Then we still need to worry about whether the fixed point happens at an epsilon that is small enough for the persistence of the other parts


So if it gets longer because of the perturbation (i.e. $\hat \ell(\epsilon)$ is an increasing function) as long as $1/(\hat \ell+g(\epsilon))$ is not decreasing at the same rate there should exist a value $\epsilon$ for which the equality holds


\section{Implications for understanding neural computation}

Use of NODEs \citep{kim2021inferring}




\section{Discussion}


This construction is immensely 

\newpage
\bibliography{../all_ref.bib}


\section{Appendix}
\section{Universal Approximation with Feedforward Neural Networks (FNNs)}

The foundation of our approach lies in leveraging the \textit{universal approximation property} of feedforward neural networks (FNNs), a powerful result that makes neural networks exceptional for a broad array of tasks. This property allows FNNs to approximate complex functions with remarkable flexibility, providing the theoretical basis for their widespread success in machine learning.

\subsection{The Universal Approximation Theorem}

The universal approximation theorem states that a \textit{feedforward neural network} with at least one hidden layer, using a non-linear activation function (such as the sigmoid or ReLU), and equipped with enough neurons, can approximate \textit{any continuous function} defined on a compact subset of \(\mathbb{R}^n\) to arbitrary precision.

Formally, for any continuous function \(f(x)\) and any \(\epsilon > 0\), there exists an FNN such that:
\[
| f(x) - f_{\text{MLP}}(x) | < \epsilon \quad \forall x \in K
\]
where \(K \subset \mathbb{R}^n\) is compact. This guarantees that given sufficient resources (neurons, depth), the FNN can approximate \(f(x)\) as closely as desired within this bounded region.

The importance of the compact domain assumption is that it constrains the input space to a finite region, ensuring the approximation remains well-behaved.
This is where the theorem’s beauty shines: while we often don’t know the explicit form of the target function, neural networks learn it purely from data, making them excellent for tasks like regression, classification, and pattern recognition.



\subsection{Activation functions} %Sigmoidal 
At the heart of the theorem lies the activation function. Any non-linear, continuous, and non-constant function enables the network to approximate continuous functions universally.
A classic example is the \textit{sigmoidal function}, defined by the property:
\[
\lim_{x \to -\infty} \sigma(x) = 0 \quad \text{and} \quad \lim_{x \to +\infty} \sigma(x) = 1
\]
Many popular activation functions, such as the logistic sigmoid and tanh, fall into this category.
These functions introduce the necessary non-linearity, enabling the network to break free from the constraints of purely linear transformations.

%non-sigmoidal activations like the \textit{Rectified Linear Unit (ReLU)} 
Despite ReLU's unbounded nature, it still supports the universal approximation property, as shown in \citep{yarotsky2017error}.



\subsection{The Multi-layer Perceptron (MLP)}

The universal approximation property of FNNs was first proven by \citep{cybenko1989approximation} for sigmoidal activation functions. This was followed by significant generalizations by \citep{hornik1989multilayer} and \citep{funahashi1989approximate}, which extended the result to broader classes of activation functions, including the popular ReLU.
These results serve as the cornerstone of modern neural network theory, supporting the use of deep architectures across diverse machine learning applications.

%all above are 3 layer FNNs? right?

Let’s now focus on the structure of a \textit{multi-layer perceptron (MLP)}, a common form of FNN. For a simple 3-layer MLP, the output can be expressed as:
\[
h^{(1)} = \sigma(W^{(1)} x + b^{(1)})
\]
\[
f_{\text{MLP}}(x) = W^{(2)} h^{(1)} + b^{(2)}
\]
Here, \(h^{(1)}\) represents the hidden layer, while \(W^{(1)}, W^{(2)}\) and \(b^{(1)}, b^{(2)}\) are the weights and biases learned during training. The activation function \(\sigma(\cdot)\) introduces the necessary non-linearity.




In a more general case, for a deeper MLP with \(L\) layers, we have:
\[
h^{(1)} = \sigma(W^{(1)} x + b^{(1)})
\]
\[
h^{(2)} = \sigma(W^{(2)} h^{(1)} + b^{(2)})
\]
\[
\vdots
\]
\[
h^{(L-1)} = \sigma(W^{(L-1)} h^{(L-2)} + b^{(L-1)})
\]
\[
f(x) = W^{(L)} h^{(L-1)} + b^{(L)}
\]

This architecture allows the MLP to progressively build complex representations, layer by layer, transforming raw input data into rich feature hierarchies.






\end{document}