\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amssymb, amsmath, amsthm, amsfonts}
\usepackage{thmtools, mathtools, mathrsfs, dsfont}
\usepackage[square,numbers]{natbib}
\bibliographystyle{abbrvnat}
\usepackage{algorithm}
\usepackage{algpseudocode}

\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\theoremstyle{remark}
\newtheorem{remark}{Remark}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\sep}{\operatorname{sep}}
\newcommand{\cl}{\operatorname{cl}}

\title{Approximation of multi-stable dynamical systems on infinite horizons}
\author{\'Abel S\'agodi and Il Memming Park}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
Recurrent Neural Networks (RNNs) are believed to be universal approximators.
However, RNNs are not universal approximators 
Existing results about approximating dynamical systems either focus on finite time horizon approximation or on approximating stable dynamical systems on an infinite horizon.
We seek to extend approximation results for dynamical systems to multi-stable systems (systems without chaos) on an infinite horizon through Neural ODEs (NODEs).



Why do we care about infinite horizon?
- approximation gives us qualitative behavior
- (little) noise doesn't make approximation increase suddenly in error (only smoothly)




\section{Approximation with FNNs}%Universal approximation with 

\subsection{MLP}%3layer FNN
\citep{cybenko1989approximation,hornik1989multilayer,funahashi1989approximate}



\section{Approximation of dynamical systems}
For an overview of the literature, see [litrev]

\subsection{Targets}
Autonomous
Structurally stable+Fixed points = hyperbolic fixed points
The dynamics is globally attractive / There exist a compact set that with the fixed points inside that is forward invariant 

\begin{equation}
\dot x = f(x)
\end{equation}
$f\in C^1$.

\subsection{Hypothesis class}
The hypothesis class $\mathcal{G}$ is composed of
\begin{equation}
\dot x = g(x) = A\sigma(Bx+\theta)
\end{equation}


\subsection{Metric}

Uniform norm over time (given the same initial starting point)
\begin{equation}
\|\varphi(t,x_0)-\hat \varphi(t,x_0)\|_\infty = \sup_t|\varphi(t,x_0)-\hat \varphi(t,x_0)|
\end{equation}


Expectation of the error over all initial points
\begin{equation}
\|\varphi-\hat \varphi\| = \int_{x_0\in X}\|\varphi(t,x_0)-\hat \varphi(t,x_0)\|_\infty
\end{equation}


\subsection{Separatrices}

Denote the basin of attraction for an attractor $A$ as $BOA(A)$.

\begin{definition}	
Define the separatrix between the basins of attraction of attractors $A_i$ and $A_j$ as 
\begin{equation}
\sep(A_i,A_j) = \cl(BOA(A_i)\cap \cl(BOA(A_j)).
\end{equation}
\end{definition}

\subsection{Theorem}
\begin{theorem}
Let $D$ be an open subset of $\mathbb{R}^n$, $F : D \to \mathbb{R}^m$ be a $C^1$-mapping, and $I_\epsilon$ be a compact subset of $D$.
Suppose that there is a subset $K \subset I_\epsilon$ such that any solution $x(t)$ with initial value $x(0) \in K$ of an ordinary differential equation
\begin{equation}\label{eq:5}
    \dot{x} = F(x), \quad x(0) \in K
\end{equation}
is defined on $I = [0, T]$ $(0 < T < \infty)$ and $x(t)$ is included in $I_\epsilon$ for any $t \in I$.

with flow $\varphi_t$ 


 Then, for an arbitrary $\epsilon > 0$, there exist an MLP recurrent neural network 
 \begin{equation}
\dot x = \hat f(x) = \sigma(A\sigma(Bx+b)+a)
\end{equation}
%such that for a solution $\hat \varphi(t,x_0)$ satisfying Eq~\ref{eq:5} with initial state $x_0$ of the network
such that its flow $\hat \varphi$ satisfies
\begin{equation}
\|\varphi-\hat \varphi\| = \int_{x_0\in X}\|\varphi(t,x_0)-\hat \varphi(t,x_0)\|_\infty<\epsilon.
\end{equation}
\end{theorem}


\begin{proof}
Because of the density of the hypothesis class $\mathcal{G}$ we have that for any $f\in C^1$ and any $\delta_1>0$
there exists $\hat f\in\mathcal{G}$ such that 


Structural stability implies that there exists a $\delta_2>0$ such that for all $g$ that are $\delta_2$ $C^1$ close to $f$ the flow is topologically equivalent to that of $f$.

So we can choose $\delta = \min\{\delta_1,\delta_2\}$ such that the flow of $\hat f\in\mathcal{G}$ is topologically equivalent to that of $f$.



We can distinguish two types of errors:
$x_0\in BOA(A_i^f,A_i^g)$ flow deviates along the trajectories
We can use \citep{vanhandel2007filtering} to bound this


$x_0\in BOA(A_i^f,A_j^g)$ for $i\neq j$
flow error because trajectories go to different attractors
We can bound this by reducing the mismatch between the separatrices (proportional to the distance between the attractors).

(Conj.) For each pair $i\neq j$ the connected component of $\sep(A_i,A_j)$ is a normally hyperbolic backward invariant manifold.

Therefore, by Fenichel's Persistent manifold theorem, there exists a $\delta_{ij}$ such that for all $g$ that are $\delta_{ij}$ $C^1$ close to $f$ such that $g$ has an  normally hyperbolic backward invariant manifold that is diffeomorphic to and at most at $\mathcal{\delta_{ij}}$ Hausdorff distance from $\sep(A_i,A_j)$.


Now we need to sum all the different errors together.
\end{proof}



\subsection{Approximating a limit cycle}


We can use the fact that the tubular neighbourhood around the LC is an inflowing normally hyperbolic invariant manifold.
This implies that there is a 3 layer FNN that approximates the vector field x a bump function around the LC (similar idea as in paper but this would be an actual possible implementation).
Then we know that the this NHIM will persist.
We also know that there is a maximal perturbation size ($\epsilon_{LC}$) for which the LC persists.
So we have to guarantee that the LC approximation is already close enough so that this perturbation does not destroy the LC.
We can now rescale the perturbation so that the period of approximation LC matches the target's. This is the case if 
\[\frac{\ell}{\int_{LC}f(x)} =  \frac{\hat \ell}{\int_{LC}\hat f(x)}\] for the length of the LC $\ell$ and the integrated vector field along the LC $\int_{LC}f(x)$.
We just increase or decrease  the above perturbation by multiplying with a scalar so that this equality holds.
The only thing we now need to worry about is how the length of the LC changes as a result of this perturbation and whether we can indeed get the equality 

Fenichel's PMT guarantees that the perturbed LC lies within $\mathcal{O}(\epsilon)$ the original LC approximation.
The integral will be growing as $\epsilon * \hat \ell(\epsilon)$
Is it true that $\hat \ell(\epsilon) = \hat \ell + g(\epsilon) where |g(\epsilon)|<|\epsilon|$?

Then we still need to worry about whether the fixed point happens at an epsilon that is small enough for the persistence of the other parts


So if it gets longer because of the perturbation (i.e. $\hat \ell(\epsilon)$ is an increasing function) as long as $1/(\hat \ell+g(\epsilon))$ is not decreasing at the same rate there should exist a value $\epsilon$ for which the equality holds


\section{Implications for understanding neural computation}

Use of NODEs \citep{kim2021inferring}




\section{Discussion}


This construction is immensely 

\newpage
\bibliography{../all_ref.bib}

\end{document}