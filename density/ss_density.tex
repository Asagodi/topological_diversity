\documentclass{article}
%\usepackage{arxiv}
\usepackage[numbers,sort&compress,super,comma]{natbib} % this must be before neurips_2024
\usepackage[preprint]{neurips_2024}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amssymb, amsmath, amsthm, amsfonts}
\usepackage{thmtools, mathtools, mathrsfs, dsfont}
\usepackage{bbm}
%\usepackage{bbold} %for mathbb{1}
\usepackage{forloop}
\usepackage[pdftex]{graphicx}  %remove demo option in your document
\usepackage{sidecap}
\usepackage{enumitem}
\PassOptionsToPackage{dvipsnames}{xcolor}
\usepackage{xcolor}
\definecolor{ForestGreen}{rgb}{0.13, 0.55, 0.13}
\definecolor{MidnightBlue}{rgb}{0.1, 0.1, 0.44}
\definecolor{BurntOrange}{rgb}{0.8, 0.33, 0.0}
\definecolor{Plum}{rgb}{0.56, 0.27, 0.52}
\usepackage[colorlinks=true,linkcolor=MidnightBlue,citecolor=ForestGreen,filecolor=TealBlue,urlcolor=Plum]{hyperref}
\hypersetup{breaklinks=true}

%for comments
\newcommand{\ptitle}[1]{\textbf{#1:}\xspace}
\definecolor{mpcolor}{rgb}{1, 0.1, 0.59}
\definecolor{ascolor}{rgb}{1, 0.5, 0.}
\newcommand{\mpcomment}[1]{\textcolor{mpcolor}{(#1)}}
\newcommand{\ascomment}[1]{\textcolor{ascolor}{(#1)}}

\graphicspath{{figures}}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}[theorem]
\theoremstyle{definition} \newtheorem{definition}{Definition}
\theoremstyle{remark} \newtheorem{remark}{Remark}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\sep}{\operatorname{sep}}
\newcommand{\cl}{\operatorname{cl}}
\newcommand{\vol}{\operatorname{vol}}
\newcommand{\boa}{\operatorname{BoA}}
\newcommand{\mcN}{\mathcal{N}}
\newcommand{\mcK}{\mathcal{K}}
\newcommand{\mcU}{\mathcal{U}}
\newcommand{\mcX}{\mathcal{X}}
\newcommand{\T}{\operatorname{T}}
\newcommand{\TR}{\T\!\mathbb{R}}
\newcommand{\TM}{\T\!M}
\newcommand{\TpM}{\T_p\!M}
\newcommand{\Diffeo}{\operatorname{Diffeo}}
\newcommand{\Hpert}{H^{\text{pert}}}
\newcommand{\inv}{\operatorname{Inv}}

\newcommand{\defvec}[1]{\expandafter\newcommand\csname v#1\endcsname{{\mathbf{#1}}}}
\newcommand{\dm}[1]{\ensuremath{\mathrm{d}{#1}}} % dx dy dz dmu
\newcounter{ct}
\forLoop{1}{26}{ct}{
    \edef\letter{\alph{ct}}
    \expandafter\defvec\letter
}

% captial \vA
\forLoop{1}{26}{ct}{
    \edef\letter{\Alph{ct}}
    \expandafter\defvec\letter
}

%\title{Finally, an \emph{actual} universal approximation result for multi-stable dynamical systems}
%\title{RNNs can approximate multistable dynamical systems on an infinite time horizon}
\title{Universal approximation of multistable dynamical systems on an infinite time horizon by RNNs}
\author{\'Abel S\'agodi and Il Memming Park}
\date{\today}



\begin{document}
\maketitle

\section*{Abstract}
Universal approximation results provide rigorous guarantees about the representational power of neural networks and hence are important in evaluating the flexibility of machine learning paradigms. A family of transformations is deemed universal if its elements can approximate, with arbitrary precision, a rich class of transformations, such as all continuous or measurable mappings. Recurrent Neural Networks (RNNs) are often considered universal transformations, and this property is often used to justify their usefulness in models for neural computation. However, existing universal approximation theories for RNNs are limited to addressing either finite time horizon approximation or a subclass of dynamical systems with a unique global attractor, overlooking critical aspects of system behavior such as temporal generalization and multistability.

%Multistability is a critical feature of many neural systems characterized by the ability to transition between multiple stable states depending on context or input. Neural computation is widely regarded as being fundamentally characterized by multistable dynamics, for example, corresponding to the neural activity in the prefrontal cortex during a delayed match-to-sample task, where the system maintains one of several stable states to represent different visual stimuli held in working memory. 

This work addresses the gap in the existing literature by developing the first-of-its-kind universal approximation result for multi-stable dynamical systems over an infinite horizon using RNNs. A key motivation for extending the notion of universal approximation to the infinite time horizon lies in capturing the behavior of dynamical systems over arbitrarily long time intervals. Infinite horizon approximation provides models with temporal generalization properties, ensuring that abrupt errors after the finite time horizon on which the approximation is based do not occur, thereby maintaining consistency between the target system and its approximation over extended periods. Despite its significance, approximation results for more complex systems, such as multi-stable systems with multiple equilibrium states but no chaotic behavior, remain underexplored in this infinite time regime. Furthermore, because dynamical systems cannot be distinguished from each other given finite time horizon data, there are many different topologies and continuations of trajectories that are all valid. However, in this infinite-time regime, these ambiguities go away.

%Our theoretical result provides a framework to analyze and classify approximation errors  by introducing basin-type, trajectory-type, and period-type errors. This new error taxonomy allows researchers working with dynamical systems models of neural computation to identify which aspects of the model need to be adjusted for a theoretical model of neural computation to converge to the true biological implementation. Furthermore, this framework introduces a new way to evaluate how difficult it is to learn a computation (e.g. by calculation of the total length of the separatrices of the system or the difficulty to exactly overlay the shape of the separatrices with an activation function). This framework enables a more comprehensive and unified approach to comparing models for neural computation.



\section{Introduction}
The versatility or flexibility of a given machine learning paradigm is usually established by proving its universality.
We say that a family of transformations is universal when its elements can approximate as accurately as one wants all the elements of a sufficiently rich class containing, for example, all continuous or even all measurable transformations.

%Modeling/Reconstructing dynamics examples. mention?
%gene regulation \citep{paczko2024neural}

Recurrent Neural Networks (RNNs) are believed to be universal approximators.
For example, \citet{durstewitz2023reconstructing} claims that 
%a library of basis functions or
 a neural network may be used to closely approximate the vector field or the trajectories of any given dynamical system.
As another example, in \citep{dinc2025latentcomputing} it is claimed that RNNs are universal approximators of dynamical systems (but the proof they provide only covers universal approximation of vector fields, see also Sec.\ref{sec:uapvfs}). 
%However, universal approximation results based on vector fields are limited in scope, as the behavior of the ODE they support might deviate from the target.
Existing results about approximating dynamical systems either focus on finite time horizon approximation or on approximating stable dynamical systems on an infinite time horizon.
This misses important behavior of the system (e.g. temporal generalization of the approximation, see Fig.~\ref{fig:lc_period_div}).

A key motivation for extending approximation results to the infinite time horizon lies in its ability to capture qualitative behaviors over time.
A model is judged a success when the output of the system fits behavioral or neurophysiologicai data in a coherent and systematic fashion, i.e., when both neurophysiological and behavioral responses are accurately predicted by the model.
 For example, infinite time horizon approximation allows us to better understand how a system's behavior unfolds over extended periods, providing insights into stability, transitions, and equilibrium states.
 Furthermore, an effective infinite time horizon approximation ensures robustness: small perturbations, such as noise, do not result in abrupt increases in approximation error, but instead produce gradual, smooth deviations in the system's trajectory.
%the second point needs to be worked out: with added noise the error would jump for trajectories at separatrices



Our work extends existing approximation results to multi-stable systems (with multiple equilibrium states or limit cycles, but no chaos) over an infinite time horizon. 
To achieve this, we leverage Neural Ordinary Differential Equations (NODEs) as a framework for constructing such approximations.
% By using NODEs, we can model continuous-time dynamical systems while ensuring that key properties, such as stability and smooth error bounds under perturbations, are maintained.
We provide a fist-of-its-kind \textbf{approximation result for multi-stable systems on the infinite time horizon}, laying the groundwork for future advances in both theory and applications of neural approximators in long-term dynamical modeling.
We redefine the previously known landscape of dynamical systems from the perspective of RNNs, see Fig.\ref{fig:ds_land_rnns_fmp}.

%mention that vanilla RNNs are not universal approximators?
%how we will do this
%outlay of paper

%INCLUDE?????
%\paragraph{Formulation: lack of theory on time generalization/forecasting}
%ANNs can generalize. After learning the data presented to them (a sample), ANNs can often correctly infer the unseen part of a population even if the sample data contain noisy information. As forecasting is performed via prediction of future behavior (the unseen part) from examples of past behavior, it is an ideal application area for neural networks, at least in principle. \citep{zhang1998forecasting}


\setlength\belowcaptionskip{-5ex}
\begin{SCfigure}[10][bthp]
  \centering
  \includegraphics[width=0.3\textwidth]{ds_land_rnns_fmp}
  \caption{We show that RNNs are dense in the space of Lipschitz dynamical systems that have a finite set of normally hyperbolic attractors.
  Previously, it was only established that RNNs are dense in systems that have at most one global connected attractor.
  }\label{fig:ds_land_rnns_fmp}
\end{SCfigure}




\section{Approximation of dynamical systems: A vast literature with limited results}
%sota 
Accurately approximating dynamical systems is essential for understanding and predicting complex behaviors across a range of applications, from robotics to climate modeling.
%Recent advances in machine learning have enabled data-driven approaches to approximating these systems, providing new ways to model complex dynamics over extended periods.
Here, we review key approaches in the literature and highlight emerging techniques that address these challenges, before we introduce our method that extends to multi-stable and complex systems over infinite time horizons.
See also \citep{li2022approximation} and \citep{jiang2023brief}.

It is important to realize that a class of models (a hypothesis class) can have the universal approximation property (UAP) \emph{for a specific target class} given \emph{a specific metric to compare} (see also Sec.~\ref{sec:approximationtheory}).
This property implies that the hypothesis class is dense in the target class, hence such results are also referred to as \emph{density results}.
Density results describe the completeness of a hypothesis class relative to a target class, emphasizing the extent to which the hypothesis class can approximate every element in the target class with arbitrary precision.


%SHOULD learning be discussed? move to Discussion?
%\paragraph{Learning sequences}
% A critical review of recurrent neural networks for sequence learning \citep{williams1989learning, pearlmutter1989learning, lipton2015rnn}.
%Approximation of large-scale dynamical systems \citep{antoulas2005approximation}
 %\paragraph{System identification}
 %\citep{werbos1989neural, kuschewski1993application, wang2006fully}
 
 %\paragraph{Operators}%approximation of an operator


\subsection{Universal approximation for mappings}\label{sec:uapmappings}
This is the basic ingredient for many of the UAP results in the literature and will be the starting point for our proof.
See Supp.Sec.~\ref{sec:uniapproxffn} for results on Feedforward Neural Netowkrs (FNN) based UAP.


%vector fields (which is just FNN setting)
\subsubsection{Approximating vector fields}\label{sec:uapvfs}
 FNNs can approximate smooth functions effectively, making them suitable for approximating vector fields \citep{doya1993universality}.
This is a direct consequence of the UAP of FNNs for mappings, see Supp.Sec.~\ref{sec:uniapproxffn}.
%
%\paragraph{Spiking neural network}
%only approximates vector field 
It is possible to approximate vector fields with other architectures.
For example, in limit $N\rightarrow\infty$ spiking network (ODE with jumps) approximates a vector field (for a continuous ODE) \citep{podlaski2024approximating}. 


\subsubsection{Diffeomorphisms}%move to \ref{sec:compacttime}?
The solutions to ODEs can generate smooth transformations on manifolds.
The flow \( \phi_t \) of the vector field \( X \) defines a map from \( M \) to itself for each fixed time \( t \). Under appropriate conditions (such as \( X \) being complete), \( \phi_t \) is a diffeomorphism of \( M \) for each \( t \). In other words, \( \phi_t \) is smooth, bijective, and its inverse \( \phi_{-t} \) is also smooth.
Thus, the solutions (flows) of ODEs associated with smooth vector fields give rise to a one-parameter family of diffeomorphisms.
%
NODEs and invertible neural networks are a sup-universal approximator for a fairly large class of diffeomorphisms, i.e., smooth invertible maps with smooth inverse \citep{huang2018neural, jaini2019sum, teshima2020uap, teshima2020coupling, ishikawa2023uap}.
%rem
For dynamical systems, however, this only guarantees a finite time approximation.
We will now consider compact time interval approximation results in more detail.



\subsection{Approximation of dynamics on compact time intervals}\label{sec:compacttime}
%An RNN with a sufficient number of hidden units and appropriate non-linear activation functions (such as tanh or sigmoid) can approximate any dynamic system or time-dependent function to arbitrary precision, provided the system operates on compact time intervals.
The universal approximation theorem for RNNs, which extends the classical result for feedforward neural networks, guarantees that RNNs can approximate the mapping from a sequence of inputs to outputs for any continuous-time process, including those governed by differential equations.
%
This property has been formally proven for systems operating over finite time intervals \citep{li1992approximation}. %first result 
Specifically, \citet{funahashi1993approximation} showed that RNNs can approximate the time-dependent behaviors of dynamical systems, making them suitable for tasks like time-series prediction, control systems, and the modeling of sequential data.

%rem
Since the results concern a compact time interval, to approximate dynamics it is enough to approximate $f$, i.e., these approaches convert RNN approximation to FNN approximation.
%merge
Existing results have established that recurrent nets are capable of generating simulated trajectories that approximate true system trajectories within arbitrary error tolerance over a finite time interval.
Most of these results rely on Gr\"{o}nwall’s inequality (see Supp.Sec.\ref{sec:gronwall}) to control the difference between the trajectories of the original system and its approximation, which incurs an exponential degradation of approximation accuracy over time
\citep{sontag1992neural, sontag1998learning, funahashi1993approximation, chow2000modeling, li2005approximation}.
This is in general not true for the unbounded time case, as the approximation error can be magnified by the dynamics.


\subsubsection{Discrete sequence-to-sequence mapping}\label{sec:discrete}
\paragraph{Discrete valued tokes: Turing completeness}
%A finite size recurrent net model (with linear and Heaviside transfer functions) is Turing universal (Dynamical Recognizer) \citep{pollack1991induction}.
% Induction by phase transition. A small weight adjustment causes a "bifurcation" in the limit behavior of the network. 
% phase transition corresponds to the onset of the network’s capacity for generalizing to arbitrary-length strings
%A finite-sized recurrent neural network with sigmoidal activation functions can simulate a universal Turing machine \citep{siegelmann1992computational}.
%Sigmoidal neural networks can be used to compute any recursive (Turing) function\citep{kilian1996universality}.

A finite-sized RNN with either linear and Heaviside or sigmoidal activation functions is Turing universal \citep{pollack1991induction, siegelmann1992computational,kilian1996universality}.
More generally, RNNs with an activation function that is for any point Lipschitz continuous in some neighborhood can be simulated by a family of nonuniform Boolean circuits\citep{siegelmann1994analog}.
Furthermore, smooth ODES in $\reals^3$ can simulate arbitrary Turing machines \citep{branicky1995universal}. %DSs are computationally universal in the sense that they can mimic the operations of any computer algorithm
RNNs with polynomially bounded weights can simulate any finite automaton (15-neuron upper bound) and any Turing machine (25-neuron network bound) \citep{indyk1995optimal}.
Finally, any polynomially space-bounded Turing machine can be simulated by a family of polynomial-size continuous time symmetric Hopfield nets \citep{sima2003continuous}.

\ascomment{TODO: organize} 
\citep{moore1990unpredictability,moore1998finite}
\citep{cabessa2019turing}
\citep{perez2019turing}
\citep{chung2021turing}
\citep{perez2021attention}
\citep{giannou2023looped}
\citep{stogin2024provably}
%\paragraph{Probabilistic}
 \citep{nowak2023representational, nowak2024representational}
+Bernstein-type \citep{veeravalli2023nonlinear}


\paragraph{Continuous valued tokens}
RNNs can approximate a state space trajectory (with time-varying inputs) uniformly  produced by either a discrete-time nonlinear system a closed discrete-time interval \citep{jin1995universal}.
Any function can be approximated arbitrarily well in probability with a recurrent neural network\citep{hammer2000approximation}.  %%any function???
% where ... can be computed by one neuron whose activation function $\sigma$ possesses a local linearity.\citep{hammer2000approximation}
RNNs with continuous sigmoidal activation function have the UAP for finite number of time steps \citep{schafer2006recurrent, schafer2007uap}.
Furthermore, locally recurrent networks with two hidden layers is able to approximate a state-space trajectory produced by any Lipschitz continuous function with arbitrary accuracy\citep{patan2008approximation}.
More recently, it was shown that discrete-time RNNs are finite-time horizon universal approximators of discrete-time systems \citep{aguiar2023universal} and the universality of linear recurrences followed by nonlinear projections for finite time was established \citep{orvieto2023universality}.
Finally, it was shown that RNNs are universal approximators of continuous past-dependent sequence-to-sequence functions\citep{song2023minimal}.% RNNs with  a non-degenerate activation function


%\paragraph{RBF neural networks}
%\citep{tan1995efficient}



\subsubsection{Continuous time sequence-to-sequence mapping}
The universal approximation properties of static neural networks \citep{funahashi1989approximate} made them a useful tool for modeling nonlinear systems. 
The problem of nonlinear modeling using static neural networks has been extensively researched \citep{narendra1990identification,warwick1992neural} and many approaches have used multilayer perceptrons and radial basis functions \citep{chen1992neural,choi1996constructive,tan1995efficient}. 
For an overview, see \citep{garces2012strategies}.


\paragraph{Autonomous systems}
The universal approximation capabilities of continuous-time recurrent networks for autonomous systems were first demonstrated by \citet{funahashi1993approximation}. 
These results were expanded on in \citep{kimura1998learning} and were proven for different architectures in \citep{chow2000modeling}.
These networks can approximate the trajectories of autonomous dynamical systems, where the system dynamics are independent of any external input.


\paragraph{Non-autonomous systems}
For non-autonomous systems, where the dynamics explicitly depend on external inputs, it has been shown that nonlinear dynamical systems can be approximated on compact time intervals (Lemma 5.3.2 in \citep{garces2012strategies}, similar to Lemma 5.3.1 in \citep{smale1974differential}).
This result broadens the applicability of continuous-time RNNs to a wide variety of systems influenced by external signals.
A fundamental result in this domain is that any given trajectory of a dynamical system can be approximately realized by a properly configured continuous-time recurrent network on a finite time interval \citep{nakamura2009approximation}.  %u : R → RNu be a C1 mapping, f : W → E be a C1 mapping, and suppose that x′ = f (u(t),x(t)) defines a non-autonomous dynamical system.
Finite-time horizon approximation results for input-driven systems with NODEs have been developed more recently \citep{li2022deep, ko2023homotopy, zakwan2023universal}.
Finally, neural flows have been proposed as an alternative to NODEs with comparable approximation capabilities\citep{bilos2021neural}.
	
%time-variant	
\paragraph{Time variant systems}
Recent advancements in NODEs have expanded these approximation results to include time variant systems. For instance, \citet{li2024universal} established the universal approximation property for semi-autonomous NODEs, demonstrating their ability to approximate a broad class of dynamical systems with time-varying dynamics. Further extensions to control synthesis problems, leveraging continuous-time frameworks, have been proposed by \citet{mei2024controlsynth}, offering new perspectives for integrating control and learning in neural architectures.
%
These developments illustrate the versatility of continuous-time recurrent networks and NODEs in approximating both autonomous and non-autonomous systems, paving the way for new applications in dynamical systems modeling and sequence-to-sequence learning.


\paragraph{Encoder-Decoder}
%\paragraph{Decoder}%part of system approximates
For non-autonomous systems, where the dynamics explicitly depend on external inputs, it has been shown that the states of the output units of a continuous-time RNN, can approximate the solution of systems governed by differential equations of the form $\dot{x}(t) = f(x(t); u(t))$ where $u(t)$ is the input \citep{garces2012strategies}. %5.3.3: interesting result on the error dynamics and its bound!!!
A subpopulation of an RNN's units can approximate a dynamical system with arbitrary precision for finite time\citep{kambhampati2000approximation}. %this is equivalent to setting the decoding mapping to be the identity
 %The first $p$ output neural units of a dynamic $n$-dimensional neural model approximate at a desired proximity a $p$-dimensional dynamic system with $n > p$
RNN encoder-decoders have UAP for linear, continuous and regular functionals\citep{li2021approximationencdec}.
%‖H ˆt‖ decays exponentially as a function of t.
%Regular: limn→∞ Ht(x(n)) = 0.
Neural oscillators have the universal approximation property on compact time intervals (approximation of causal and continuous operators) \citep{lanthaler2023neuraloscillators}.
%
%LTC
Any finite trajectory of an $n$-dimensional continuous dynamical system can be approximated by the internal state of the hidden units and $n$ output units of an Liquid time-constant (LTC) network \citep{hasani2018liquid}.



%\paragraph{FNNs for control}
%%control 
%\citep{psaltis1988multilayered} \citep{li1989control,chen1992adaptive}

%\paragraph{uncat}
%trajectories \citep{toomarian1992learning}



%%FLOW APPROX
\subsubsection{Approximation of flows}%paragraph?
Flow-based approximations for finite time horizon have been established for architectures such as Recurrent High-Order Neural Networks \citep{kosmatopoulos1995structural}.
Any continuous dynamical nonlinear systems can be approximately realized\footnote{The finite time trajectories of a given dynamical continuous system can be approximated by the first $n$ internal state out of the $n + N$ neural units in an RNN.} by RNN on finite time interval \citep{chow2000modeling}.
%fixed initial state!: initial state $x(0)\in X$
%$\dot z = -\alpha z + W_1\sigma(W_2u+z)$
%where $\sigma(\cdot)$ is a $C^1$-sigmoid nonlinear activation function, and $z \in \reals^L$ .
%Euclidian norm
%
This has been extended to time-variant systems but with a fixed initial state \citep{li2005approximation}. %approx of time variant systems with fixed initial state
%
More recently, it was shown that RNNs are universal approximators of flow functions of dynamical systems  with control inputs on finite time intervals  \citep{aguiar2023universal}\footnote{This is direct simulation instead of indirect realization (for definitions, see Sec.~\ref{sec:simulation})}. %not realization, but direct simulation
%periodic inputs



\subsection{Looking beyond the finite-time horizon}\label{sec:beyondfinitetime}
%Why should we care?

For UAP results which consider a finite-time horizon $T$, we can wonder: What goes wrong after $T$?
%two cases: period-matching and separatrix-matching
%
For example, a target limit cycle and its approximation will diverge no matter how close the approximation is (if there is a non-perfect match between the periods, see Fig.~\ref{fig:lc_period_div}).
We will refer to this as P-type error (for Period).

\setlength\belowcaptionskip{-5ex}
\begin{SCfigure}[10][bthp]
  \centering
  \includegraphics[width=0.3\textwidth]{lc_period_div}
  \caption{Divergence of orbits at limit cycles after $T$.
  Even though an approximation (red) might have an orbit that stays $\varepsilon$ close to a target system with a stable limit cycle with period $T$ (blue), if the periods are not exactly matched, the two systems will diverge from each other.
  In this example, the approximation has period $\hat{T} = T+\varepsilon$ and will hence be above the approximation guarantee at time $t=2T$.
  }\label{fig:lc_period_div}
\end{SCfigure}

Another example of divergence after $T$ arises around separatrices.
Points near a separatrix will converge to different attractors if there is a non-perfect match of separatrices (see Fig.~\ref{fig:separatrices}).
We will refer to this as \textbf{B-type error} (for \textbf{B}asin).



%FPS%fading%FMS%FMP
\subsubsection{Approximation of fading memory systems}\label{sec:fadingmemory}
%On the general theory of fading memory \citep{coleman1968general}
The concept of a system having the fading memory property (FMP, see Supp.Sec.~\ref{sec:fading_memory}) has been a cornerstone in the study of dynamical systems\citep{coleman1968general}, providing a framework where the system’s output depends on past inputs with gradually diminishing influence \citep{boyd1985fading}.
That the output of a dynamical relationship should depend only on the recent past of its input and that the effect of the distant past should fade away is intuitively clear  \citep{sepulchre2021fading}.
%-APPROXIMATION BY VOLTERRA SERIES: A simplified interpretation of the N -th Volterra Series expansion is the “N -th Taylor expansion in the sequence variable x”.
%-APPROXIMATION BY FINITE-DIMENSIONAL DYNAMICAL SYSTEMS

Here, we will present some UAP results where target systems have some FM-like property.
However, while these results are promising, they are limited in scope.
The requirement of fading memory inherently restricts the class of dynamical systems that can be effectively approximated.
Many real-world systems exhibit behaviors where the influence of past inputs does not simply fade away over time; instead, they can have long-term dependencies or even cyclical behaviors that are crucial to their dynamics.
A very simple example is a system with two stable fixed points.
We now discuss existing universal approximation results that are confined to fading memory systems and critically assess their limitations.
%
Typically, the FMP is used to reduce the approximation problem to one over a finite, bounded index set, and then appeal to the density of fully connected neural network to obtain approximation \citep{gonon2021fading}. %more examples


\paragraph{Linear RNNs}
\citet{li2020curse} and \citet{li2022approximation} investigate Linear RNNs in continuous time, % and therefore linear functionals
proving that linear RNNs can approximate continuous, linear, causal, regular and time-homogeneous functionals. % on $\mathcal{X}$ .
Of course, regular linear functionals have fading memory: their dynamics converge to zero at infinite time ($\lim_{n\rightarrow\infty} H(x(n)) = 0$) and therefore such results are trivial.
 

\paragraph{Exponential decaying memory}
State Space Models (SSMs) with layer-wise nonlinearity are universal approximators of systems with exponential decaying memory \citep{wang2024state}.


\paragraph{Uniform asymptotic incremental stability}
Uniform asymptotic incremental stability was introduced in \citep{pavlov2006uniform}.
UAP of RNNs for uniformly asymptotically incrementally stable systems on an infinite time horizon\citep{hanson2020universal, hanson2021learning}.


\paragraph{Reservoir computing}
%\paragraph{ESN}
\citet{jaeger2001echo} proposed Echo State Networks (ESNs), demonstrating their universal approximation capabilities specifically for fading memory systems.
This result was extended to include approximation of systems with an input \citep{manjunath2013echo}.
%ESNs are universal uniform approximators  for discrete-time fading memory filters with uniformly bounded inputs \citep{grigoryeva2018echo,grigoryeva2018universal}. 
Reservoir computing (RC), the broader framework that includes ESNs, has also been applied to neuroscience \citep{auslender2024decoding}.

%ADD? ceni2020echo

If the domain of the functional $H$ is restricted to a space of uniformly bounded sequences with the fading memory property then various families of state-space transformations can be used to approximate it uniformly: 
 linear systems with polynomial or neural network readouts \citep{boyd1985fading,grigoryeva2018universal,gonon2019reservoir}, state-affine systems with linear readouts \citep{grigoryeva2018universal}, or echo state networks \citep{grigoryeva2018echo,gonon2019reservoir,gonon2021fading,gonon2023approximation}.
 More recently, Simple Cycle Reservoirs were shown to be universal for this set of systems \citep{li2023simple}.
% In other words, any of these families exhibit universality properties in the fading memory category.
Finally, a new method was developed to approximate Fading Memory Systems (FMS) by using a kernel representation (instead of a state-space representation) of the model \citep{huo2024kernel}. %approximate all fading memory functionals

%Echo state networks are universal \citep{grigoryeva2018echo} [Grig 18a]
%Universal discrete-time reservoir computers with stochastic inputs and linear readouts using non-homogeneous state-affine systems \citep{grigoryeva2018universal} [Grig 18b]
%Risk bounds for reservoir computing \citep{gonon2020risk} [Gono 20a]
%Reservoir computing universality with stochastic inputs \citep{gonon2019reservoir} [Gono 20b]
%Fading memory echo state networks are universal \citep{gonon2021fading} [Gono 21]
%Approximation error estimates for random neural networks and reservoir systems \citep{gonon2023approximation} [Gono 23a]

%Universal Transformer
 \citet{dehghani2018universal} introduced a recurrently-stacked universal transformer and \citet{yun2019transformers} showed that transformer models are universal approximators of continuous permutation equivariant sequence-to-sequence functions with compact support.


% Deep Equilibrium Sequence Model (encapsulates universal transformer) and are themselves an instance of the so-called implicit-depth models such as neural ordinary differential equations
Recently, \citet{bai2019deq} proposed the deep equilibrium model (DEQ) and which was shown to be able to approximate any sequence whose underlying dynamics is determined by a model that converge to a fixed point.
This model was further extended to be able to capture systems with arbitrary invariant sets, but the UAP has not been established \citep{konishi2023stable}. %some remarks: p.397


\subsubsection{FMP makes approximation results trivial} %put in prev section?
 The input forgetting property (also referred to in the literature as the unique steady-state property) is a modeling feature that appears profusely in applications and that can be obtained out of the so-called fading memory property.
In fact, every fading-memory system could be uniformly approximated arbitrarily closely over the set of systems with 'linear dynamics'\citep{matthews1993approximating}. %Theorem 3+4
Using the perceptron to uniformly approximate the external representation of a fading-memory system results in a finite-memory system model, called the perceptron filter \citep{matthews1993approximating}.
%
If  input/output sequence target has a realization\footnote{See Supp.Sec.~\ref{sec:realization}.} as a dynamical system, then the fading memory property is related to the unique steady-state property for dynamical systems \citep{chua1976qualitative}.
Hence, the fading memory assumption implies that the state will be “asymptotically independent” of the initial condition.
Therefore, FPM systems cannot have multi-stability which makes this framework exclude many real-world systems (see also Rem.~\ref{rem:fmp}).
%Stability and memory-loss go hand-in-hand \citep{manjunath2020stability}

\ascomment{TODO:}
Why should we assume that the underlying dynamics in neural networks converge to a fixed point?
Models of neural computation should cover a broader class of dynamics, e.g, oscillations \citep{townley2000existence, kag2020rnns, chang2019antisymmetricrnn,rapp1987periodic}. %could convey preferred capabilities in learning 


%\paragraph{Stability implies FNNs are enough}%crit
%When Recurrent Models Don't Need to Be Recurrent \citep{miller2018stable}.


%%%%%%%%%%%%%%%%%%%
\subsubsection{ESNs and topological equivalency}%+RNNs?
ESNs can be trained to have  topologically conjugate dynamics to structurally stable dynamical systems \citep{hart2020embedding}.
This does guarantee knowledge of the dynamics beyond a finite time horizon.
However, this is trivial for approximation of structurally stable systems with NODEs.
It is true by virtue of Theorem.~\ref{theorem:ss} that vector fields that are close to the vector field of a dynamical system will have dynamics that are topologically conjugate to it.
Because it is well known that vector fields can be approximated to arbitrary precision with FNN, this result immediately follows for NODEs.
Finally, we would like to mention the possibility to construct dynamics with Cohen-Grossberg networks for arbitrary sets of fixed point and limit cycles through realization of dynamics \citep{cohen1992construction}.



\section{Approximation of multi-stable dynamical systems on infinite time intervals}
\paragraph{Roadmap}
%First we describe the target (the systems we can approximate) and hypothesis classes (the systems that we approximate with) and the metric for the density result (way we measure how wrong our approximation is to our target system).
%Then, we prove our universal approximation property for this setting by step-wise constructing the target class:
%starting with autonomous systems with a finite number of hyperbolic fixed points,
%then systems with (additionally) a finite number of hyperbolic limit cycles
%and finally, covering approximation of chaotic orbits.
%We then extend these results to non-autonomous dynamical systems (Sec.~\ref{sec:nonautonomous}).
%
%%mention that the focus is on semiflows? Sec.~\ref{sec:semiflow}
%
%We will discuss what the space is of target dynamical systems that we can approximate (Sec.~\ref{sec:target}).
%Furthermore, these can be approximated in a particular way, i.e. with a specific difference between the approximant and the target system (the metric): measured as the expectation of the uniform norm difference between the flows (Sec.~\ref{sec:metric}). 
%We will show that we can make the probability of finding a trajectory of the approximation to be off by a certain arbitrary threshold arbitrarily small. 
%
%%TODO remove repeat
%We start by showing the UAP for our hypothesis class for structurally stable systems with a finite set of fixed points  (introduced in Sec.~\ref{sec:hypothesis} and proven in Sec.~\ref{sec:fp_theorem}).
%Then we extend this results where we consider systems with a finite set of periodic orbits (Sec.~\ref{sec:lc_theorem}).
%We then also include systems that additionally have a finite set of continuous attractors (Sec.~\ref{sec:ca_theorem}).
%We prove with how these results extend to a class of non-autonomous dynamical systems (Sec.~\ref{sec:nonautonomous}) 
%For remarks about approximation of chaotic orbits (Sec.~\ref{sec:chaos}).
%Each of these results considers approximation of a system on a compact space that is globally attractive. %explain what this entails


First, we define the target class (the dynamical systems we approximate), the hypothesis class (the systems we approximate with), and the metric used to quantify approximation error (Sec.~\ref{sec:target}, \ref{sec:metric}). We then establish our universal approximation property (UAP) by progressively constructing the target class:
\begin{enumerate}
\item Structurally stable systems with a finite number of hyperbolic fixed points (Sec.~\ref{sec:fp_theorem}).
\item Systems with a finite number of hyperbolic limit cycles (Sec.~\ref{sec:lc_theorem}).
\item Systems with a finite set of continuous attractors, including chaotic orbits (Sec.~\ref{sec:ca_theorem}, \ref{sec:chaos}).
\end{enumerate}
Finally, we extend these results to non-autonomous dynamical systems (Sec.~\ref{sec:nonautonomous}). Throughout, we consider approximation in a metric measuring the expectation of the uniform norm difference between flows, ensuring that the probability of large deviations is arbitrarily small. Each result applies to compact, globally attractive spaces.


\subsection{Setup}
\subsubsection{Target class}\label{sec:target}
%Autonomous
%Structurally stable+Fixed points = hyperbolic fixed points
%The dynamics is globally attractive / There exist a compact set that with the fixed points inside that is forward invariant 
For our approximation results, we consider the following dynamics.
Let $\mathcal{X}$ be an open subset of $\mathbb{R}^n$,
 $f\colon \mathcal{X} \to \mathbb{R}^m$ be a $C^1$-mapping,
%  and $I$ be a compact subset of $\mathcal{X}$
 such that any solution $x(t)$ with initial value $x(0) \in \mathcal{X}$ of the ordinary differential equation %replaced from I(0)
\begin{equation}\label{eq:mlp_vf}
    \dot{x} = f(x),
\end{equation}
is defined for $t\in\reals_{+}$ and $x(t)$ is included in $\mathcal{X}$. %replaced from I
This defines a flow $\varphi(\cdot, \cdot)$ by the Picard-Lindel\"of theorem (see Supp.Sec.\ref{sec:solutions}).
For Sec.~\ref{sec:fp_theorem} we assume that the only $\omega$-limit sets of  $f$ are a finite number of hyperbolic fixed points.
For Sec.~\ref{sec:lc_theorem} we assume that $f$ has a finite set of hyperbolic stable periodic orbits.
For Sec.~\ref{sec:ca_theorem} we assume that $f$ has only a finite set of normally hyperbolic continuous attractors.
%
These restrictions of the target class are Bernstein-type results (in the classification of approximation result types made in \citep{jiang2023brief}).
%analogy with realization of transfer functions in state space models

\ascomment{Explanation why not larger target class. We need structurally stable systems so that we know that similar vector fields guarantee topologically equivalent dynamics. The only exception (to our knowledge) for which it is established that nearby vector fields have similar behavior (in epsilon-delta sense) is for normally hyperbolic continuous attractors.}

\subsubsection{Hypothesis class}\label{sec:hypothesis}
%rename f hat?
The hypothesis class $\mathcal{G}$ is composed of $\dot x = g(x)$
where $g$ is an FNN with the universal approximation property. %of arbitrary dimension.
In essence, by setting up $g$ as a neural network, we are equipping the NODE with a highly flexible model for $g$, which can approximate a vast variety of continuous functions. 

%canonical example: 3 layer MLP


\subsubsection{Metric}\label{sec:metric}
First, of all, we compare trajectories through the uniform norm over time (given the same initial starting point \citep{girard2007approximation}):
\begin{equation}
\|\varphi(t,x_0)-\hat \varphi(t,x_0)\|_\infty \coloneqq \sup_t\|\varphi(t,x_0)-\hat \varphi(t,x_0)\|.
\end{equation}

We consider a compact $X$ on which we measure the difference between the target and our approximation.
\footnote{FNN approximation universality is also on compact intervals.
The results can be extended to systems that diverge to infinite in a finite number of orbits (all orbits that diverge converge in uniform norm to one of these divergent orbits).}
%
Expectation of the error over all initial points (similar to the metric in \citep{hammer2000approximation} and \citep{hanson2021learning}):
\begin{equation}\label{eq:initerror}
%\|\varphi-\hat \varphi\|_\varepsilon \coloneqq \frac{1}{\vol X}\mathbb{E}\left[ \int_{x_0\in X}   \mathds{1}[\|\varphi(t,x_0)-\hat \varphi(t,x_0)\|_\infty>\varepsilon]\right],
\|\varphi-\hat \varphi\|_\varepsilon \coloneqq  \mathbb{P}\left(\|\varphi(\cdot,x_0)-\hat \varphi(\cdot,x_0)\|_\infty>\varepsilon\right).
\end{equation}
%where $\mathds{1}[\cdot]$ is the indicator function. 


%why we cannot stricten the metric
For a multistable system it is not possible to find a universal approximation result if we consider a more strict notion of difference between the target and approximant.
For example, if we take the uniform norm of the uniform norm, it can be shown that there is always a system with a separatrix that can not be exactly matched which would lead to an error that is the distance between the two $\omega$-limit sets (see Def.~\ref{def:omega}).

%\subsubsection{Guarantee in $N\rightarrow\infty$ or at finite $N$}
%It could be that existing theorems could be interpreted that in the limit $N\rightarrow\infty$ the approximation holds. %e.g.\citep{podlaski2024approximating}
%ie $\lim_{N\rightarrow\infty} error = 0$
%Here we do it for finite $N$.
%ie for each $\varepsilon>0$ $\exists N$ such that $error(N) < \varepsilon$
%(implies the above limit)




\subsection{Approximating systems with a finite number of hyperbolic stable fixed points}\label{sec:fp_theorem}

%This theorem is phrased for the more general case, move to before? 
\begin{theorem}\label{thrm:approx}
Take $\dot x = f(x)   \ \ \ f\in C^1$, with flow $\psi$ as in Sec.~\ref{sec:target}.
Then, for an arbitrary $\varepsilon, \delta > 0$, there exist a FNN $\hat f: \reals^n\rightarrow\reals^n$ with the UAP (see Sec.~\ref{sec:hypothesis}) such that the NODE defined by it, i.e. $\dot x = g(x)$,
% \begin{equation}
%\dot x = \hat f(x) = A\sigma(Bx+b)+a  %give this as example? 
%\end{equation}
%such that for a solution $\hat \varphi(t,x_0)$ satisfying Eq~\ref{eq:5} with initial state $x_0$ of the network
with flow $\varphi_g$ satisfies $\|\varphi_f- \varphi_g\|_\varepsilon < \delta.$
\end{theorem}

%proof will be at end of section? 
%establish necessary pieces as propositions

We will break down this result in smaller pieces. 
First, we prove that (the location of) fixed points can be arbitrarily closely approximated in Prop.~\ref{prop:fp_approx} (we extend this to limit cycles in Sec.~\ref{sec:lc_theorem}).
This allows us to speak of corresponding attractors between the target and approximant system.
We identify two regions: 1) where the basins of attraction for the same attractor overlap and 2) the remaining state space.
Second, we prove that inside the basin of an attraction the flow can be approximated in Sec.~\ref{sec:inboa_approx} (this is an extension to the infinite time horizon of a standard finite time result). %this itself is composed of finite time approx + global convergence %or just global convergence?
Third, we prove that the remaining state space can be made arbitrarily small in Sec.~\ref{sec:separatrix_approx}.
Finally, we combine these results to prove Theorem \ref{thrm:approx} in Sec.~\ref{sec:proof_fp_approx}.

\begin{proposition}\label{prop:fp_approx}
The fixed points of $C^1$ ODEs can be arbitrarily closely approximated. %Hausdorff distance % in the $\|\varphi-\hat \varphi\|_\varepsilon < \delta$ flow sense.
%and limit cycles later
\end{proposition}

\begin{proof}
%%%top eq
Because of the density of the hypothesis class $\mathcal{G}$ we have that for any $f\in C^1$ and any $\varepsilon>0$
there exists $g\in\mathcal{G}$ such that $\|f-g\|<\varepsilon$.
%
Structural stability implies that there exists a $\delta>0$ such that for all $g\in C^1$ that are $\delta$-$C^1$ close to $f$, the flow is topologically equivalent to that of $f$.
So we can choose $\delta$ such that for all $\|f-g\|<\delta$ the flow of $\hat f$ is topologically equivalent to that of $f$.
Finally, fixed points of $\hat f$ are $\mathcal{O}(\varepsilon)$ close to those of $f$ (from Persistent Manifold Theorem\cite{jones1995gspt}).
%orbits from these initial conditions converge to $\omega$-limit sets that are at most $\varepsilon_i$ Hausdorff distance from each other.
\end{proof}

This establishes that for all $\varepsilon>0$ and $x_0$ initial states in a basin of attraction (for which $\omega (\psi(x_0,T) = x^*$)) there is some time $T$ for which for all $t>T$ we have that $\psi(t)-\hat \psi(t)<\varepsilon$.


\paragraph{A tale of two errors}
Note that the above error bound depends on two separate quantities: the local error 
$\| f(s; \tilde{\varphi}_0, s(x)) - f(s; \tilde{\varphi}_0, s(x)) \|$ 
 and the term 
$\| D\varphi_{s,t}(\tilde{\varphi}_0, s(x)) \|$, 
which bounds the sensitivity of the flow $\varphi_{s,t}(x)$ to an infinitesimal perturbation of its initial condition.
%TODO: more catchy name for type I error: F-type (Flow)
How we can bound these two types of errors will be discussed in the next two subsections.%error^I and error^II

%use \hat f notation?
\setlength\belowcaptionskip{-1ex}
\begin{SCfigure}[10][bthp]
  \centering
  \includegraphics[width=0.5\textwidth]{separatrices}
  \caption{
	Schematic of an approximation of a bistable system with mismatching separatrix. % $A_i$ and $A_j$.
	The target subsystem has two stable fixed points ($A_i^f$ and $A_j^f$) with separation between the basins of attraction by the separatrix (black line).
	The approximation also has two stable fixed points ($A_i^g$ and $A_j^g$) with a separatrix that is close to the target one but is not perfectly on it (dashed line).
	Trajectories initialized in the red region go to $A_i^f$ for the target system (green line) and to $A_i^g$ for the approximation (pink line). % (that is at an $\varepsilon_i$ distance) 
	% Similarly for the other stable fixed point for initial points in the blue region.
	%This target system will have a set of points (in the green region) that if the flow is initialized there it has $A_i^f$ as the $\omega$-limit set (magenta line), however for the approximation the flow will approach $A_i^g$ (green line).
	%The opposite mismatch is true for the dark green region.
	%(such a model can be found in e.g. \citep{wong2007evidence})
  }\label{fig:separatrices}
\end{SCfigure}

\subsubsection{Errors inside the intersection of the basins of attraction of target and approximation}\label{sec:inboa_approx}
%replace this proof with previous result on UAP for FMP systems?
%Introduce A_i^f notation
For an initial value $x_0\in \overline{\boa}_i^{f,g} \coloneqq \boa(A_i^f) \cap \boa(A_i^g)$, the flow deviates along the trajectories 
\[
\|\varphi_{0,t}(x) - \tilde{\varphi}_{0,t}(x)\| \leq \int_0^t \|D\varphi_{s,t}(\tilde{\varphi}_{0,s}(x))\| \|f(s; \tilde{\varphi}_{0,s}(x)) - g(s; \tilde{\varphi}_{0,s}(x))\| \, ds,
\]
from Prop.~\ref{prop:313} from \citep{vanhandel2007filtering}, see Supp.Sec.~\ref{sec:313}.
%
%See Rem.~\ref{rem:313}.
%
The first term \(\|D\varphi_{s,t}(\tilde{\varphi}_{0,s}(x))\|\) is bounded %?
and \(\|D\varphi_{s,t}(\tilde{\varphi}_{0,s}(x))\|\rightarrow 0\) for \(t\rightarrow\infty\).
Define $M\coloneqq \sup \|D\varphi_{s,t}(\tilde{\varphi}_{0,s}(x))\|<\infty$. 
%
This guarantees that the integral is finite.
%
The second term $\|f(s; \tilde{\varphi}_{0,s}(x)) - g(s; \tilde{\varphi}_{0,s}(x))\|$
can be made arbitrarily small because of the UAP for mappings.
%
This guarantees that the integral can be made arbitrarily small.	
%conclusion about Type I errors
Hence, there exists an NODE in which for attractor the error is $\varepsilon_i<\varepsilon$. 



%Consider separately ?
%1 orbits to fixed points: path starting at $x_0$ is finite
%2 orbits to stable limit cycles.



%TODO: more catchy name for type II error: B-type (Basin) 
\subsubsection{Errors at the boundary of a BoA}\label{sec:boa_errors}%type II
\label{sec:separatrix_approx}
The other type of error (basin-type) arises because trajectories go to different attractors, i.e., $x_0\in \boa(A_i^f)$ and $x_0\in \boa(A_j^g)$ for $i\neq j$, see Fig.~\ref{fig:separatrices}.
For such initial points, the error along the trajectories will be higher than $\varepsilon>0$ because the attractors are farther apart than $\varepsilon$.
Eliminating this type of error would constitute perfectly matching the separatrices which typically can not be guaranteed.
We can reduce the volume in of initial points for which this type of error arises, by reducing the mismatch between the separatrices (proportional to the distance between the attractors).

The minimal volume of initial values for which basin-type errors arise along their trajectories is 
\begin{equation}
\operatorname{SMM}(f,g) \coloneqq \bigcup_{i \neq j}\boa(A_i^f)\cap\boa(A_j^g) \ \ \  \vol \left(\operatorname{SMM}(f,g)\right) =  \sum_{i \neq j} \left(\boa(A_i^f)\cap\boa(A_j^g)\right).
\end{equation}
%CHECK: double counting


\begin{proposition}\label{prop:min_sep_bound}
For any $\delta>0$, we can choose $g$ such that $ \vol \left(\operatorname{SMM}(f,g)\right) <\delta$.
\end{proposition}


%focus on separatrices or boundaries of boas?
\paragraph{Boundary of BoA is persistent and can be made arbitrarily close to target}% this depends on the fact that separatrices are persistent
The boundary of the BoA of attractor $A_i^f$ is %BoAs are open sets
\[\partial\boa(A_i^f) = \overline{\boa(A_i^f)} \cap (X - \overline{\boa(A_i^f)}).\]
where \(\overline{\cdot}\) denotes the closure of a set (same as \(\cl\)). %use consistent notation!!!!!!
% =\cl\boa(A_i^f)-\boa(A_i^f)\.\]%this should hold for an open set...
Then for some  $\varepsilon_{\operatorname{sep}}>0$ we have that 
\begin{equation}\label{eq:separatrixoverlap}
\bigcup_{i\neq j} \boa(A_i^f)\cap\boa(A_j^g) \subset \bigcup_i B_{\varepsilon_{\operatorname{sep}}}(\partial\boa(A_i^f)). 
\end{equation}

%The left hand side's volume is at most $\mathcal{O}(\delta)$ with $\delta\leq\varepsilon_{\operatorname{SSM}}$ the Hausdorff distance between the full separatrix submanifolds.
%This $\delta$ can be made arbitrarily low because of the FNN universal approximation results.

%TODO: link: here is how
For each attractor $A_i$ we have that the boundary of the basin of attraction $\partial(\boa(A_i^f))$ is a compact normally hyperbolic invariant manifold (see Supp.Sec.~\ref{sec:boaboundary}).
Consider the connected components $B^f_{ij}$ of $\partial(\boa(A_i^f))$. %NOTE: it is not the case that we have a connected component for each pair of attractors
Then, by Fenichel's Persistent manifold theorem, there exists a $\delta_{ij}$ such that for all $g$ that are $\delta_{ij}$-$C^1$ close to $f$ such that $g$ has a normally hyperbolic invariant manifold  $B^g_{ij}$ that is diffeomorphic to and at most at $\mathcal{O}(\delta_{ij})$ Hausdorff distance from $B^f_{ij}$.
Therefore, by Eq.~\ref{eq:separatrixoverlap}, the overlap of the separatrices is less than $\mathcal{O}(\varepsilon_{\operatorname{SSM}})$. %this is not entirely correct....
%compactness should imply that the volume is at most $\mathcal{\varepsilon_{\operatorname{SSM}}$.
Therefore, there is $k_{ij}\in\reals$ such that the Hausdorff distance between $B^f_{ij}$ and $B^g_{ij}$ is $k_{ij}\delta_{ij}$.
Let  $N_{\operatorname{cc}}^i$ be the number of connected components of $\partial(\boa(A_i^f))$.
Choose $\delta_{ij}<k_{ij}\delta/N_{\operatorname{cc}}^i$. 
Define $\delta_i\coloneqq\sum_{i=1}{N_{\operatorname{cc}}^i}\delta_{ij}$.
% conclude for all attractor (pairs)
Now we need to sum errors coming from all pairs of basins of attraction together.
Take $\delta_i< \frac{\delta}{N_{\operatorname{attractors}}}$ for each attractor $A_i$ and hence $\sum_i\delta_i<\delta$.


\subsubsection{Proof of Theorem~\ref{thrm:approx} for systems with hyperbolic fixed points}\label{sec:proof_fp_approx}
\begin{proof}%[Theorem \ref{thrm:approx}]
Because of Conley's fundamental theorem of dynamical systems\citep{conley1978morse}, we know that every flow of a dynamical system with compact phase portrait admits a decomposition into a chain-recurrent part and a gradient-like flow part.
This implies that the phase portrait consists of basins of attraction and separatrices, both gradient-like, as well as fixed points, which are chain-recurrent.
%
By Sec.\ref{sec:separatrix_approx} (Prop.~\ref{prop:min_sep_bound}) we can make the part of the state space which will have error bigger than $\varepsilon$ arbitrarily small. %for every delta
%for every epsilon
By Sec.\ref{sec:inboa_approx}, we know that in the remaining state space we can make the trajectory-wise error smaller than $\varepsilon$.
\end{proof}

%\paragraph{Bookkeeping}
%\ascomment{Express everything in terms of $\varepsilon$ and $\delta$}
%\begin{enumerate}[start=0]
%\item $\varepsilon_{top.eq.}$ for topological equivalence (structural stability)
%\item Bounding the volume of separatix mismatch regions
%\begin{enumerate}
%\item Rolling back time until $T$ to get isolating neighborhood ($\eta_T$ share of $\mathcal{X}$ not included after this time (call the separatrix mismatch regions $\operatorname{sepmiss}$, which decreases monotonically for $T$)
%\item Approximation of isolating neighborhood: $\mu$ close in vf space implies $\mathcal{O}(\mu)$ closeness of persisting isolating neighbourhood (part of state space not covered in basin of attraction guarantee: $\widehat{\operatorname{sepmis}}$)
%\item $\Gamma(\eta_T, k\mu) <  \delta$ gives a bound on how close our approximation needs to be in vector space
%\item Need at least $\eta_T<k\mu$ (?)
%\item So if we have $\| f - g\|<\tfrac{1}{k}\delta$ then $\vol(\widehat{\operatorname{sepmis}})<\delta$
%\end{enumerate}
%\item Bounding the flow errors inside basins (remaining space $\mathcal{X}-\widehat{\operatorname{sepmis}}$)
%\begin{enumerate}
%\item Per basin: $\Psi_i(\varepsilon)$ so that flow is $\varepsilon$ close (from previous finite time horizon + trajectory closeness to attractor after finite time or from FMP based results)
%\item Take minimum from these bounds $\Psi_{\min}(\varepsilon)$
%\end{enumerate}
%\item Take minimum $\{\tfrac{1}{k}\delta, \Psi_{\min}(\varepsilon),\varepsilon_{top.eq.}\}$ guarantees $\varepsilon$,$\delta$-approximation
%\end{enumerate}


\paragraph{Expressing the maximal error in vector field in terms of $\epsilon$ and $\delta$}  
To express all approximations in terms of $\varepsilon$ and $\delta$, we introduce a few key quantities. First, we define $\varepsilon_{top.eq.}$ as the tolerance ensuring topological equivalence, which guarantees structural stability.  
To bound the volume of separatrix mismatch regions, we consider rolling back time to $T$ to obtain an isolating neighborhood.
Importantly, the volume of these regions decreases monotonically as $T$ increases.
%the separatrix mismatch regions, $\operatorname{SMM}$.

 To approximate the isolating neighborhood, we observe that if two vector fields differ by at most $\mu$, then the resulting isolating neighborhoods remain within $\mathcal{O}(\mu)$ proximity. This leads to an upper bound on the portion of the state space not covered by the basin of attraction guarantee, which we denote as $\widehat{\operatorname{SMM}}$.
  If we have $\Gamma(\eta_T, k\mu) < \delta$, then we obtain a condition on the required accuracy of our vector field approximation.
 In particular, ensuring $\eta_T < k\mu$ implies that if $\| f - g \| < \tfrac{1}{k} \delta$, then the volume of the uncovered region is bounded by $\vol(\widehat{\operatorname{SMM}}) < \delta$.  

Within the remaining portion of the state space, $\mathcal{X} - \widehat{\operatorname{SMM}}$, we control the flow error.
 For each basin, we define a function $\Psi_i(\varepsilon)$ that quantifies how closely the flow remains within $\varepsilon$ of the true trajectory.
  This bound follows either from finite-time trajectory closeness to the attractor or from results based on the Finite-Time Mapping Property (FMP). Taking the minimum across all basins, we obtain a global bound, $\Psi_{\min}(\varepsilon)$.  
Combining these results, we conclude that  a vector field error of $\min \left\{ \frac{1}{k} \delta, \Psi_{\min}(\varepsilon), \varepsilon_{top.eq.} \right\} $
ensures a valid $(\varepsilon, \delta)$-approximation.





%%%%%%%%LC
\subsection{Approximation of stable limit cycles}\label{sec:lc_theorem}
%TODO:  name for type III error: L-type (limit cycle) P-type (period)
As we have seen in Sec.~\ref{sec:beyondfinitetime}, any mismatch in periods in the period of stable limit cycles introduces an error.
We will get convergence onto a limit cycle that is close in Hausdorff distance to the target limit cycle. %$\omega$-limit set?
But, for an approximation of the vector field we will in general have a small discrepancy in the periods between the two limit cycles that will lead to a uniform norm difference between the flows that is the maximal distance between all pairs of points on the two limit cycles.
This would make our method fail!
In our metric, this error is of the size of the volume of the basin of attraction for the limit cycle.
Is it possible to perfectly match target limit cycle periods?
We will describe a recipe for systems with single and multiple limit cycles.


\subsubsection{A single stable limit cycle}
Again, we start out with an approximation of the target vector field with an $\varepsilon_{top.eq.}$ small difference so that topological equivalence is guaranteed ($C^1$ distance between the vector fields).
Fenichel's PMT guarantees that the perturbed limit cycle lies within $\mathcal{O}(\varepsilon)$ the original limit cycle approximation. %Hausdorff
Denote the set of points in the limit cycles by $\gamma_f$ and $\gamma_g$ for the target and approximation, respectively.
%
%However, we can correct this discrepancy.
We typically have a difference between the periods of limit cycle of the target $T_f$ and of the approximation $\hat T$: $\varepsilon_T\coloneqq T_f- T_g>0$
with \[T_f\coloneqq\frac{1}{\int_{\gamma}\|f(x)\|dx} \text{ and } T_g\coloneqq\frac{1}{\int_{\hat{\gamma}}\|g(x)\|dx}.\]
%
We will apply a perturbation $\bar{f}$ to our approximation, $\tilde f = g + \bar f$, that is small enough for the persistence of the limit cycle ($\delta$ needs to be small enough) 
so that 
\begin{equation}\label{eq:lc_constraint}
\int_{\tilde{\gamma}}\|\tilde{f}(x)\| dx = \frac{1}{T}. 	% for the integrated vector field along the limit cycle $\int_{LC}f(x)$.
\end{equation}
The existence of such a perturbation (i.e. that indeed it will be \emph{small} enough) requires that the initial difference between the periods is small:
if the approximation vector field is close enough to the target vector field, then the periods will be guaranteed to be close (see for the precise statement and proof Supp.Sec.~\ref{sec:periodcloseness}).
%this quarantees that top.eq. is not violated

We can use the fact that the tubular neighborhood of diameter $\delta_T$ around the limit cycle is an (inflowing) normally hyperbolic invariant manifold.
We will now construct the correction $\bar{f}$ so that the period of our approximation matches the targets.
This implies that there exists a 3 layer FNN $g_{\operatorname{bump}}$ that approximates the vector field $f_{\operatorname{bump}}$ that is a bump function around the limit cycle (similar idea as Supp.Sec.~5 in \citep{Sagodi2024a}). %but this would be an actual possible implementation
Then we know that this NHIM will persist (from Fenichel's PMT, see Supp.Sec~\ref{sec:fenichel}).
We also know that there is a maximal perturbation size ($\varepsilon_{LC}$) for which the limit cycle persists.
So we have to guarantee that the limit cycle approximation is already close enough so that this perturbation does not destroy the limit cycle (Supp.Sec.~\ref{sec:periodcloseness}). %repetition
We can now rescale the perturbation through a scalar $a_{\varepsilon_T}^*$ (depends on initial $\varepsilon_T$) so that the period of approximation limit cycle matches the target's (Eq.~\ref{eq:lc_correction}). 
We can then increase or decrease the above perturbation by multiplying with a scalar so that the equality in Eq.~\ref{eq:lc_constraint} holds.
The only thing we now need to worry about is how the length of the limit cycle changes as a result of this perturbation and whether we can indeed get the equality.
%CONSTRUCT:
%
The resulting total approximation is %replace a(\varepsilon_T) with just an optimal constant?
\begin{equation}\label{eq:lc_correction}
\tilde{f}= g+a_{\varepsilon_T}^*g_{\operatorname{bump}} = 
g + a_{\varepsilon_T}^*(f_{\operatorname{bump}}+ \hat{\varepsilon}\bar{f}_{\operatorname{bump}}),
 \end{equation}
where $\hat{\varepsilon}\bar{f}_{\operatorname{bump}} = f_{\operatorname{bump}} - g_{\operatorname{bump}}$ with $\|\bar f\|=1$ and $\hat \varepsilon <\max(\varepsilon_T,\varepsilon_{LC})$. %actually this should be a function of epsilon T so that closeness in Supp.Sec.~\ref{sec:periodcloseness} is guaranteed
%no this is not the bound that belongs to hat epsilon!!!!!!!!!!!
%it should be \|f_{\operatorname{bump}\| < some epsilon...
%
%So if the period gets longer because of the perturbation (i.e. $\hat \ell(\varepsilon)$ is an increasing function) as long as $1/(\hat \ell+g(\varepsilon))$ is not decreasing at the same rate there should exist a value $\varepsilon$ for which the equality holds.
See further Supp.Sec.~\ref{sec:lc_extras}.


%\paragraph{Expressing the maximal error in vector field in terms of $\epsilon$ and $\delta$}  
%\begin{enumerate}
%\item How small needs the tubular neighbourhood to be for the bump? 
%\begin{enumerate}
%\item Small enough so that it is included inside the basin of attraction
%\item Should not change flow to introduce more than $\varepsilon$ error flow type error.
%\end{enumerate}
%\item How big should $a_{\varepsilon_T}$ be: Initial period mismatch $\varepsilon_T$, then $a_{\varepsilon_T}< \Lambda(\varepsilon_T)$ %TODO: specify function $\Lambda$...
%\item How big can the bump approximation error be?
%\begin{enumerate}
%\item Should not shift location of limit cycle by more than $\varepsilon$.
%\item See below.
%\end{enumerate}
%\item How small does the initial period error need to be and how is this expressed in vector field space distance?
%\begin{enumerate}
%\item We need that the adjustment still has at most $\varepsilon$ error in basins and that only $\delta$ part of the state space is in the problematic separatrix mismatch region
%\item Separatrices are persistent so they will be shifted $\mathcal{O}(a_{\varepsilon_T}\hat \varepsilon)$. So we need $ka_{\varepsilon_T}\hat \varepsilon < \tfrac{1}{2}\delta$ and if we require for the initial mismatch error to be $\kappa<\tfrac{1}{2}\delta$ we get the total to be less than $\delta$. 
%\item The vector field outside the tubular neighborhood is $f - \eta\bar f  + a_{\varepsilon_T}\hat \varepsilon\bar f_{\operatorname{bump}}$. 
%We have that $\|\eta\bar f  + a_{\varepsilon_T}\hat \varepsilon\bar f_{\operatorname{bump}}\| < \eta + a_{\varepsilon_T}\hat \varepsilon$ (triangle inequality and absolute homogeneity of norm). Therefore, we can take $\hat f$ to be  $\Psi_{\min}(\eta + a_{\varepsilon_T}\hat \varepsilon)$ close to $f$ to guarantee $\varepsilon$,$\delta$-approximation
%\end{enumerate}
%\end{enumerate}

\paragraph{Bounding the Approximation Error in Terms of $\varepsilon$ and $\delta$}  

To ensure the approximation remains within the desired accuracy, the tubular neighborhood for the bump function must be small enough to stay inside the basin of attraction while limiting flow errors to at most $\varepsilon$. The rescaling factor $a_{\varepsilon_T}$, which corrects the period mismatch $\varepsilon_T$, must remain within a bound $\Lambda(\varepsilon_T)$ to preserve the structure of the vector field. Additionally, the perturbation must not shift the limit cycle by more than $\varepsilon$, ensuring topological equivalence.
 Since separatrices persist under small perturbations, their displacement scales with $a_{\varepsilon_T} \hat{\varepsilon}$ and must satisfy $k a_{\varepsilon_T} \hat{\varepsilon} < \frac{1}{2} \delta$.
  Likewise, the initial period mismatch should be below $\frac{1}{2} \delta$ to keep the total error within acceptable bounds.
  Outside the tubular neighborhood, the perturbed vector field satisfies $\| \eta \bar{f} + a_{\varepsilon_T} \hat{\varepsilon} \bar{f}_{\operatorname{bump}} \| < \eta + a_{\varepsilon_T} \hat{\varepsilon}$, ensuring that the deviation from the target remains controlled.  

Combining these results, we conclude that a vector field error of $\min \left\{ \frac{1}{k} \delta, \Psi_{\min}(\varepsilon), \varepsilon_{top.eq.} \right\}$ ensures a valid $(\varepsilon, \delta)$-approximation.
 The same bound applies to the limit cycle case, where the perturbation must ensure the cycle remains within $\mathcal{O}(\varepsilon)$ of the target in Hausdorff distance, while the period discrepancy remains at most $\varepsilon_T$.
 This guarantees that both the vector field and the limit cycle retain their structural properties under the approximation.  



\paragraph{Summary}
We can choose the initial approximation $\hat{f}$ such that the period difference is smaller than $\varepsilon_T$.
We can then choose a correction to the vector field in a tubular neighborhood around the limit cycle such that the difference in periods between target and approximation is zero.



%%%%%%%%%%%%%%%%
\subsubsection{Multiple limit cycles}
%%%QPTA
Systems with multiple limit cycles are ubiquitous: e.g., quasi-periodic toroidal attractors (QPTA)\citep{Park2023a}.
%
If $f$ has $N>1$ limit cycles, we can perform a similar procedure.
We have to make sure that all the $\hat{\varepsilon}_i$ are small enough so that $\varepsilon_{T_i}$ is small enough:
\begin{equation}\label{eq:nlc_vfs}
\tilde f_i = \hat{f}_i + \sum_{j}a_{\varepsilon_{T_j}}\hat{\varepsilon}_j\bar{f}_j \text{ for } i=1,\dots, N.
\end{equation}
%TODO: replace a(\varepsilon_{T_j}) with a_j^*? 
%\paragraph{Finding local perturbations to simultaneously satisfy Eq.\ref{eq:nlc_vfs} for $i=1,\dots, N$.}
We can find $a_i^* = a_{\varepsilon_{T_i}}$ and $\hat\varepsilon_i$ for $i=1,\dots,N$ such that Eq.~\ref{eq:lc_constraint} is satisfied for each limit cycle (Eq.~\ref{eq:nlc_constraints}).
For a proof , see Supp.Sec.~\ref{sec:mlc_extras}.
%can we really do this sequentially? no, we have to do it in parallel
%when we change a_T_i we have to adjust the other a_T_j s
%we have to guarantee that a(\varepsilon_{T_i})




%%%%%%%%%%%%%%%%
%%%CA
\subsection{Approximating continuous attractors}\label{sec:ca_theorem}
%TODO:  name for type V error: D-type (discretization) 
%for the extension of continuous attractors we need to assume that all maximally invariant sets are normally hyperbolic


If $f$ has a normally hyperbolic continuous attractor, we can tile the continuous attractor with a uniform grid of stable fixed points \citep{Sagodi2024a}.
Each of the neighboring stable fixed points can be at most $\varepsilon$ apart. %needs to be smaller because of $\mathcal{O}(\varepsilon)$ dispacement of stable FP of approx?
Because this is a structurally stable dynamical system,  we can approximate its vector field with $\varepsilon$ accuracy and we are guaranteed to have a topologically equivalent system to it with the fixed points being moved at most $\mathcal{O}(\varepsilon)$ by Theorem~\ref{thrm:approx}. 
Finally, for $f$ with a continuum of attractors (nonzero flow in at least one dimension): e.g. torus of limit cycles, we can tile the continuous attractor with a uniform grid of stable limit cycles, see Section S5 in \citep{Sagodi2024a}.


%\subsection{Center manifolds}
%???





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Non-autonomous dynamics}\label{sec:nonautonomous}
%We will consider input-driven systems (Sec.~\ref{sec:inputdriven}).  %general non-autonomous systems (Sec.~\ref{sec:generalnonautonomous}) 


\subsection{Target and hypothesis class for input-driven systems}\label{sec:inputdriven}
In many dynamical systems, external inputs influence the system's evolution over time. Such systems can be described by differential equations of the form
\begin{equation}\label{eq:inputdriven}
\dot x(t) = f(x(t)) + g(x(t),u(t)),
\end{equation}
where $x(t)\in\reals^n$  represents the system state and $u(t)\in\reals^m$ is an input function.
%
The solutions of this system, denoted as $\psi(t,x_0,u)$, depend on the initial condition $x(0) = x_0$  and the input $u(t)$.
We assume the solutions to be unique (see Sec.~\ref{sec:inputunique}).




%we can extend \mathcal{U} for a set epsilon and delta

%for systems with FPs we can allow for more leniency (?) % we can allow for small inputs as long as it doesnt bring the system across separatrices (but this is all defined probabilistically in the PDE picture) %or if it does then it does it only a finite number of times
% but for now we just consider inputs converging to a periodic function.

%for systems with LCs small inputs will desynchronize approximation from target
%

%Bounded functions?
%$\mathcal{U} = \mathcal{L}_2$ (square-integrable functions) % $\int\vert f(x)\vert^2 dx<+\infty$
%\mathcal{L}_1? % $\int\vert f(x)\vert dx<+\infty$
%But also functions that converge to a time constant function or that converge to a periodic function.
%Sobolev space? % $f\in L^2$ and $f'\in L^2$

%Input doesn't need to be periodic! %but in the appropriate norm these functions will be on a period function...
%As long as the maximal size (at a given time point) is bounded (this bound is determined by the BoA)
%And as long as the proportion of trajectories that remain around the separatrices falls of fast enough
%How to characterize this?




\subsubsection{Targets: Constraints on $g$}\label{sec:input_target}
%Similarly, as for the autonous case, $g$ needs to be smooth enough as a function of $x$ and $u$ (at least Lipschitz or $C^1$). %C^1 for persistence!
%%
%Needs to be hyperbolic (in the limit $t\rightarrow\infty$) for each $u(\infty)$. %u(\infty) only makes sense for convergent u ? or we need to take equivalence classes that are invariant wrt changes in u.
%%
%Can only have a finite set of fixed points (in the limit $t\rightarrow\infty$) for each $u(\infty)$. %so that we can use the autonomous UAP theory.


We assume that $g(x,u(t))$ is exponentially asymptotically autonomous dynamics for each $u$ (Def.~\ref{def:eaa_input} with $\sigma,\gamma=1$). %Supp.Sec.~\ref{sec:eea_input}
%
Furthermore, we assume that $g$ converges to a finite number of topological equivalence classes of asymptotically autonomous vector fields.
%can we increase it to countable?


\paragraph{How does this relate to the input class? Constraints on $u(t)$}
This would guarantee exponential asymptotic autonomy for minimal assumptions about $g$. %SPECIFY

We will now characterize the input space 
$u\in \mathcal{U}$ with $u:\reals_+\rightarrow\reals^m$.
We assume that $u(t)$ converges (exponentially) to a periodic function, i.e., %or g(\dot,u) needs to (?)
%rate of convergence?
\begin{equation}
\mathcal{U} \coloneqq \left\{ u(t) \mid \exists p(t) \text{ periodic} \text{ or } c \in \mathbb{R}^n, \ \lim_{t \to \infty} \| u(t) - p(t) \| = 0 \text{ or } \lim_{t \to \infty} \| u(t) - c \| = 0 \right\},
\end{equation}
such that only a measure-zero set of input sequences that lead to non-hyperbolic asymptotically autonomous vector fields.
%include more general u class?



%Alternative assumptions:
%epsilon, delta dependent input space

%-or: forward attractors of non-autonomous dynamical systems \citep{ju2019approximate} \citep{crauel2015nonautonomous} \citep{cheban2002relationship} \citep{cui2018invariant} \citep{kloeden2016construction} \citep{koksch2002pullback} \citep{giesl2012numerical} \citep{cheban2002relationship}


\subsubsection{Hypothesis space: Approximating $g(x,u)$}\label{sec:input_hypothesis} %with input-driven MLP-NODEs
MLP for autonomous vector field $f$ as in Eq.~\ref{eq:mlp_vf}.
For the input-driven part of the vector field we have an MLP:
 $\hat g\colon \reals^{n+m} \rightarrow \reals^{n}$ with $m$ the dimensionality of the input.



\subsubsection{Metric}\label{sec:input_metric} 
For the metric to compare input-driven systems with their approximations, see Sec.\ref{sec:inputdriven_metric}.

\subsection{Theorem}
\begin{theorem}
For an input-driven system as in Sec.~\ref{sec:input_target} % Eq.~\ref{eq:inputdriven} %+assymptions...
with solutions  $\psi(t,x_0,u)$
and $\varepsilon, \delta>0$, there exists an input-driven MLP style neural ODE %...
with solutions  $\hat \psi(t,x_0,u)$ such that 
$d_\varepsilon(\psi,\hat\psi)<\delta$.
\end{theorem}


\subsubsection{Proof}
%prove: %use this for input-driven
\paragraph{Finite-time horizon approximation + exponentially asymptotically autonomous = Infinite-time horizon approximation}
What is the maximal error $h-\hat h$ we can allow for the $\varepsilon,\delta$ distance in flow fields to hold?

%TODO
\begin{enumerate}
\item Let non-aut vf converge up to some time $T$
\item Pick asy aut vf $h_u$ for approximation (so we have $\hat h_u = h_u + \eta\ \bar h_u$)
\item Small error in vf
\begin{enumerate}
\item from input-driven vf (quantify...)
\item from approximation (quantify...)
\item Small displacement in attractors of the autonomous vf (why?) in topologically equivalent regions in input space (what does this mean?)
\end{enumerate}
\item Error in trajectories
\begin{enumerate}
\item From sep mismatch (how does this look at $T$?)
\item From input-driven vf mismatch
\item from aut and asy-aut vf mismatch
\item Quantify above in PDE terms
\end{enumerate}
\end{enumerate}

\begin{itemize}
\item probability density around asymptotic orbits can be made arbitrarily high
\item probability density in separatrix mismatch regions can be made arbitrarily low
\item what to do with which input functions lead to bad approximation?
\begin{itemize}
\item Calculate what proportion of initial states ends up in problematic separatrix mismatch regions
\item Reduce volume of these regions appropriately
\item Input functions that are problematic? Separatrices in input space?
\end{itemize}
\end{itemize}

Step 1+2+3 (apply autonomous UAP theory to vector field dynamics)
\paragraph{Apply autonomous UAP theory to vector field dynamics} %+exponentially
%up to T
We can approximate the changing vector field $g$ up to a finite $T$ by applying the existing finite time horizon approximation results.

%after T
We have that for a finite time we will make at most $\Psi(\eta,T)$ error (for a vector field that is $\eta$ close). $T$ depends on asymptotic exponent for the vector field $\alpha$ (Def.~\ref{def:eaa_input}). %$g$ is exponentially asymptotically autonomous
%We need to consider this until $T$ for which we are $\eta$ close to the asymptotic, autonomous vector field.
Eq.~\ref{eq:nonautonomous} so that 
\begin{equation}\label{eq:inputdriven_asymp}
g(x(t), u(t)) \rightarrow h_{\operatorname{aut}}(x(t))
\end{equation}
%for every epsilon there is a T
that is for $t>T$ for some $T$ %...
\begin{equation}\label{eq:inputdriven_asymp}
g(x(t), u(t)) =  h_{\operatorname{aut}}(x(t)) + \eta h_{\operatorname{inp}}(x(t), u(t))
\end{equation}
with $\|h_{\operatorname{inp}}(\cdot, u(t))\| \leq 1$ and $0<\eta\ll1$.

%At some $T$ the vector field is $\eta$ away from the asymptotic vector field.
We consider Eq.~\ref{eq:init_path_density} with $T$ for which the convergence of the non-autonomous vector field is at most $\eta$ away from the asymptotic autonomous vector field $h_{\operatorname{aut}}(x)$. %which is a combination of f(x) and the asymptotic part of g(x)
We can now consider the dynamics of Eq.~\ref{eq:nonautonomous} as an small perturbation from $g(x)$ ($\eta$ away).
We can approximate that with $\kappa$ accuracy with our vector field.
%So we are at most $\eta+\kappa$ away from the actual target dynamics at each time point after $T$.
%Rather: 
The approximation asymptotic autonomous vector field converges at most  $\eta+\kappa$ away from the actual target dynamics at each time point after $T$.
%At some $T>0$ stop changing $\hat h.$ %related to the convergence of nonautonomous vf to autonomous vf
%There is a $T$ for which for all $t>T$ we have that $\|h(t,x)-h_\infty(x)\|<\eta$ and therefore $\|g(t,x)-\hat g(T,x)\|<\eta+\kappa$.






\paragraph{Step 4}
\begin{enumerate}
\item Characterize proportion of “bad” initial positions for each input sequence and measure the proportion in terms of the probability measure for $\mathcal{U}$ (Supp.Sec.~\ref{sec:inputspace_measure})
\begin{enumerate}
\item Input brings state to separatrix mismatch: 
	Can be controlled by size of mismatch region (and hence autonomous part of the vf (or the asymptotically autonomous vf))
\item  Input brings state to wrong side of separatrix:
 	Can be controlled by mismatch in non-autonomous/input-driven factor of vf
\end{enumerate}
\item Characterize proportion of “bad” input sequences through asymptotic behavior (e.g., non-hyperbolic dynamics)
\end{enumerate}


\paragraph{Integrating over all inputs}
See Supp.Sec.~\ref{sec:inputdriven_metric}.

%using the probability density to measure the proportion of initial points that have trajectories that will have bigger than epsilon error:
We can set $p(x_0)$ to be the uniform distribution over the compact state space $\mathcal{X}$ we consider. 
Then the probability density after $t$ time has passed is
\begin{equation}\label{eq:init_path_density}
p(x_1,t) = p(x_0)p(x_0,t).
\end{equation}
%$p(x_{i+1},t+s) = p(x_i,t)p(x_{i+1},s)$


Approximation $\hat p(x_1,t)$

Up to finite $T$: $d(p(x_1,t),\hat p(x_1,t))<\mu$.

Asymptotically ($t\rightarrow\infty$): $d(p(x_1,\infty),\hat p(x_1,\infty))$ is given by initial separatrix mismatch + contribution from aut asym separatrix mismatch(es).


%\subsection{General}\label{sec:generalnonautonomous}%includes time-varying 

%%and of course includes input-driven systems
%Unlike autonomous systems, where the evolution is determined solely by the state variable $x$, non-autonomous systems incorporate an explicit time dependence in the function $h(x,t)$:
%\begin{equation}
%\dot x = h(x,t).
%\end{equation}

%\subsubsection{Proposed constraints on $h$}
%%Needs to be smooth enough.
%
%For non-autonomous we need another Bernstein-type result:
%we can only approximate input driven dynamics (in our metric) if a forward attractor exists for each input sequence.
%This allows us to formulate approximation in terms of attractors and separatrices (but now in input space)
%
%%Important concepts: %?
%%Input-to-state stability \citep{sontag1995iss, sontag1996iss, sontag2008iss} introduce in Appendix?



%for a general non-autonomous system.
%\subsubsection{Approximating $h(x,t)$}
%Same as before but now $\hat h\colon \reals^{n+1} \rightarrow \reals^{n}$. %x'\rightarrow x\oplus t


    
\subsection{Concluding remarks on nonautonomus systems}

\paragraph{Bookkeping}
\ascomment{What is the maximal non-autonomous vector field error $g-\hat g$ we can allow for the $\varepsilon$-$\delta$ distance in flow fields to hold?}



\subsubsection{meta-FMP and vf-ISS}
%\ascomment{Alternative strategy?}
We can consider the vector field as a dynamical system.
We can then apply the autonomous theory to it.
This would lead to the requirement ``$g$ needs to converge to a finite number of topological equivalence classes of asymptotically autonomous vector fields'' (Sec.~\ref{sec:input_target}).

\begin{remark}
Exponentially asymptotically autonomous dynamics could be seen as a form of meta-FMP (FMP for the \emph{vector field}).
\end{remark}

\begin{remark}
Exponentially asymptotically autonomous dynamics a form of input to state stability (ISS\citep{sontag1995iss, sontag1996iss, sontag2008iss}) but for the vector field dynamics.
\end{remark}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
%\paragraph{Recap}	
This is the first proof of the universal approximation property of neural ODEs for multistable dynamical systems on an infinite time horizon.


basin stability\citep{menck2013basinstability}

\subsection{Limitations}
\paragraph{Limit cycles}
Stable limit cycles are approximable but their approximation is not robust! (period will change under almost all perturbations, only a measure zero set of perturbations preserves the period).


\paragraph{Asymptotically autonomous assumption}
It is often [and in neural network models of working memory (WM), typically] assumed that inputs are only non-zero for a brief initial period (the cue period of a WM trial) \citep{hopfield1982neural, stroud2023infromationloading, stroud2024computational, schimel2024preparation}.

\paragraph{Chaotic dynamics}
the constraints of computing with biological wet-ware may make chaotic dynamics unavoidable in complex nervous systems\citep{hirsch1995computing}
\citep{hoppensteadt2013analysis}

%also pdes?
\paragraph{Stochastic dynamics}
\citep{gonon2019reservoir}
\citep{hart2021echo}
\citep{chen2022universal} 
\citep{wei2022statistically}
Recurrent Neural Networks and Universal Approximation of Bayesian Filters \citep{bishop2023recurrent}
\citep{plate2024optimal}
%\paragraph{Flow Matching}
\citep{mishne2024elucidating}
\citep{lipman2024flow}

\paragraph{PDEs}
\citep{richter2022neural}

\subsection{Practicalities}
\paragraph{Performance after learning and noise}
learning related generalization \citep{marion2023generalization}\citep{bleistein2023generalization}

\paragraph{The price of universality}
This construction is immensely inefficient.
%Better to just look at RNNs (?)
%Expressivity paper?
%%
Relation to mean squared error and total integrated error, see Supp.Sec.~\ref{sec:error_remarks}.


%%
\subsection{NODEs vs RNNs}
NODEs simulate RNNs (see definition for simulation in Sec.~\ref{sec:simulation})
- Each NODE is state space equivalent to an RNN that is split into a part for which the equivalence holds and a hidden part (Boltzmann Machine-like)
-Each RNN is state space equivalent to an RNN

This equivalence does not hold for input-driven systems (?)  (according to \citep{dinc2025latentcomputing} they are?)

% NNs as controllers \citep{sontag1992neural}


%%
\subsection{Implications for understanding neural computation} 
Reconstructing Computational Dynamics from Neural Measurements with Recurrent Neural Networks \citep{durstewitz2023reconstructing}.

Use of NODEs in neuroscience \citep{kim2021inferring}.

system identification in neuroscience \citep{han2023si} 

Temporal generalization

Seperatrices are robust hence decision-making is robust to D-type noise \citep{Park2023a}.

%\paragraph{Robust neural computation}
%Idea: Two dynamical phenomena are isolated into primitive architectural components which perform the operations of continuous nonlinear transformation and autoassociative recall \citep{pineda1988dynamics}.

%TODO: choose most relevant topic
%\paragraph{Models that generalize over time}

%\paragraph{Convergence in model space}

%\paragraph{Convergent realism}

%\paragraph{Interpretability}%point to anti-truncated koopman operator paper?







\newpage
\bibliographystyle{unsrtnat_IMP_v1}
\bibliography{../all_ref.bib,../catniplab.bib}



\newpage
\appendix 

\section{Universal Approximation with Feedforward Neural Networks (FNNs)}\label{sec:uniapproxffn}
The foundation of our approach lies in leveraging the \textit{universal approximation property} of feedforward neural networks (FNNs) \citep{poggio1990networks}, a powerful result that makes neural networks exceptional for a broad array of tasks. This property allows FNNs to approximate complex functions with remarkable flexibility, providing the theoretical basis for their widespread success in machine learning.
For an overview see \citep{blum1991approximation,scarselli1998universal,augustine2024survey}.


\subsection{The Universal Approximation Theorem}
The universal approximation theorem states that a \textit{feedforward neural network} with at least one hidden layer, using a non-linear activation function (such as the sigmoid or ReLU), and equipped with enough neurons, can approximate \textit{any continuous function} defined on a compact subset of \(\mathbb{R}^n\) to arbitrary precision.

Formally, for any continuous function \(f(x)\) and any \(\varepsilon > 0\), there exists an FNN such that:
\begin{equation}
| f(x) - f_{\text{MLP}}(x) | < \varepsilon \quad \forall x \in K,
\end{equation}
where \(K \subset \mathbb{R}^n\) is compact. This guarantees that given sufficient resources (neurons, depth), the FNN can approximate \(f(x)\) as closely as desired within this bounded region.

The importance of the compact domain assumption is that it constrains the input space to a finite region, ensuring the approximation remains well-behaved.	
This is where the theorem’s beauty shines: while we often don’t know the explicit form of the target function, neural networks learn it purely from data, making them excellent for tasks like regression, classification, and pattern recognition.

The universal approximation property of FNNs was first proven by \citep{cybenko1989approximation} for sigmoidal activation functions. This was followed by significant generalizations by \citep{hornik1989multilayer}, \citep{funahashi1989approximate} and \citep{hechtnielsen1992backpropagation}, which extended the result to broader classes of activation functions, including the popular ReLU.
These results serve as the cornerstone of modern neural network theory, supporting the use of deep architectures across diverse machine learning applications.

For example, sums of polynomial functions are known to have the UAP \citep{llavona1986approximation}.


\subsection{Activation functions} %Sigmoidal 
\paragraph{Sigmoid} %general form?
\begin{theorem}[Universal Approximation Theorem for Feedforward Neural Networks\citep{hornik1989multilayer,schafer2007uap}]
For any sigmoid activation function \( f \), any dimension \( I \), and any probability measure \( \mu \) on \( (\mathbb{R}^I, \mathcal{B}^I) \), \( \Sigma_I(f) \) is uniformly dense on a compact domain in \( C^I \) and \( \rho_{\mu} \)-dense in \( M^I \).
\end{theorem}

%\begin{corollary}%[in \citep{schafer2007uap}]
%Theorem 1 holds for the approximation of functions in \( C^{I,N} \) and \( M^{I,N} \) by the extended function class \( \Sigma^{I,N} \). Thereby, the metric \( \rho_{\mu} \) is replaced by \( \rho_{\mu}^N := \sum_{k=1}^{N} \rho_{\mu}(f_k, g_k) \).
%\end{corollary}


At the heart of the theorem lies the activation function.
Any non-linear, continuous, and non-constant function enables the network to approximate continuous functions universally\citep{csaji2001approximation}.
A classic example is the \textit{sigmoidal function}, defined by the property:
\[
\lim_{x \to -\infty} \sigma(x) = 0 \quad \text{and} \quad \lim_{x \to +\infty} \sigma(x) = 1.
\]
Many popular activation functions, such as the logistic sigmoid and tanh, fall into this category.
Despite ReLU's unbounded nature, it still supports the universal approximation property \citep{yarotsky2017error}.
These functions introduce the necessary non-linearity, enabling the network to break free from the constraints of purely linear transformations.
%non-sigmoidal activations like the \textit{Rectified Linear Unit (ReLU)} 

The universal approximation property is equivalent to having a nonpolynomial activation function \citep{pinkus1999approximation}.


\subsection{The Multi-layer Perceptron (MLP)}
Let’s now focus on the structure of a \textit{multi-layer perceptron (MLP)}, a common form of FNN with the UAP.
 For a simple 3-layer MLP, the output can be expressed as:
\[
h^{(1)} = \sigma(W^{(1)} x + b^{(1)})
\]
\[
f_{\text{MLP}}(x) = W^{(2)} h^{(1)} + b^{(2)}
\]
Here, \(h^{(1)}\) represents the hidden layer, while \(W^{(1)}, W^{(2)}\) and \(b^{(1)}, b^{(2)}\) are the weights and biases learned during training. The activation function \(\sigma(\cdot)\) introduces the necessary non-linearity.
Three-layered multi-output Feedforward Networks are universal approximators for vector-valued functions \citep{irie1988capabilities}.
In these networks there is a single hidden layer feedforward network that approximates any measurable function to any desired degree of accuracy on some compact set \citep{hornik1989multilayer}.

It is not the specific choice of the activation function but rather the multilayer feed-forward architecture itself that gives neural networks the potential of being universal approximators\citep{hornik1991approximation}.

%In a more general case, for a deeper MLP with \(L\) layers, we have:
%\[
%h^{(1)} = \sigma(W^{(1)} x + b^{(1)})
%\]
%\[
%h^{(2)} = \sigma(W^{(2)} h^{(1)} + b^{(2)})
%\]
%\[
%\vdots
%\]
%\[
%h^{(L-1)} = \sigma(W^{(L-1)} h^{(L-2)} + b^{(L-1)})
%\]
%\[
%f(x) = W^{(L)} h^{(L-1)} + b^{(L)}
%\]

%This architecture allows the MLP to progressively build complex representations, layer by layer, transforming raw input data into rich feature hierarchies.%
%For the approximations, we either need to let the width or depth of the MLP grow.



\subsection{Efficiency}
The blessing of compositionality mitigates the curse of dimensionality \citep{mhaskar2019function}.


\subsection{Other results on UAP for mappings}

 
NODEs are $L^p$-universal approximators \citep{li2022deep,li2022deeparxiv}.
However, the universal approximation property with respect to the $L^p$-norm can be insufficient as it does not guarantee an approximation for the entire input domain:
 $L^p$ approximation may still hold even if the approximator largely differs from the target function on a small region of the input space.



Universal approximation of 
\begin{itemize}
\item Homeomorphisms \citep{zhang2020approximation}, with normalizing flows \citep{papamakarios2021normalizing}
\item Invariant functions\citep{li2022deep}
\item Monotone analytic functions homotopic to the identity\citep{tabuada2020universal}, \citep{tabuada2022universal}
\end{itemize}


\ascomment{TODO: organize}
RBF\citep{park1991universal}
a\citep{marchi2021training}
\citep{lowe1988multivariable}
\citep{mhaskar1997neural}
\citep{leshno1993multilayer}
UA with Deep narrow \citep{kidger2020universal}
with WTA networks \citep{maass2000wta}
Function+Derivatives \citep{hornik1990universal}
function and derivative \citep{li1996simultaneous}
ReLU\citep{huang2020relu}
ResNet\citep{lin2018resnet}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{UAP for RNNs overview}

\begin{table}[h]
\begin{tabular}{|l|l|l|l|l|l|}
%\hline
\cline{2-6}
    \multicolumn{1}{c|}{}							%leave out size?
 						&  Time interval & FMP	& Finite size 		&  Input     	&     Time   \\\cline{2-6}\hline
\citet{cybenko1989approximation} 	& Finite	& No 	& Yes 			&  Yes		&	Continuous\\\hline  %vf
\citet{funahashi1989approximate} 	& Finite	& No		& Yes			&  Yes		&	Continuous\\\hline  %vf
\citet{hornik1989multilayer} 		& Finite	& No 	& Yes			& Yes		&	Continuous\\\hline  %vf
\citet{funahashi1993approximation} 	& Finite	& No		& Yes			&  Yes		&	Continuous\\\hline
\citet{jin1995approximation}		& Finite	& No		& Yes			& Yes		&	Continuous	\\\hline
\citet{hammer2000approximation}	& Finite     & No 	& Yes			& Yes		& 	Discrete	\\\hline
\citet{jin1995universal}			& Finite	& No		& Yes			& Yes		&	Discrete	\\\hline
\citet{bai2019deq} 				& Infinite	& Yes	& No		 	& Yes 			&	Discrete		\\\hline
\citet{schafer2007uap} 			& Infinite 	& Yes	& No			& Yes 			&	Discrete\\\hline
\citet{wang2024state}			& Infinite   & Yes 	& No 		& Yes			& 	Continuous\\\hline % FMP type: (exp. decay) 
\dots&\dots &\dots&\dots&\dots&\dots\\
\end{tabular}
\end{table}



%%%%%%%%%%%%%%%%%
\newpage
\section{Neural ODEs (NODEs)}
%\subsection{}
Neural ODEs \citep{chen2018neural} represent the latest instance of continuous deep learning model for time series, first developed in the context of continuous recurrent networks \citep{cohen1983absolute}.
%
Neural ODEs model the continuous trajectory of the hidden state \( \mathbf{h}(t) \) as the solution to an ordinary differential equation parameterized by a neural network. Mathematically, the dynamics are defined by:

\[
\frac{d \mathbf{h}(t)}{dt} = f(\mathbf{h}(t), t, \theta)
\]

where \( \mathbf{h}(t) \) is the hidden state at time \( t \), \( f \) is a neural network with parameters \( \theta \), and \( t \) represents time. Given an initial condition \( \mathbf{h}(t_0) = \mathbf{h}_0 \), the evolution of the hidden state over time is computed by solving the ODE using numerical solvers, allowing the model to capture complex, continuous transformations.

%example of f
\textbf{Fully Connected Neural Network (Feedforward Network)}:

\[
f(\mathbf{h}(t), t, \theta) = W_2 \sigma(W_1 \mathbf{h}(t) + \mathbf{b}_1) + \mathbf{b}_2
\]

Here, \( \sigma \) is an activation function (e.g., ReLU, tanh, or sigmoid), \( W_1, W_2 \) are weight matrices, and \( \mathbf{b}_1, \mathbf{b}_2 \) are bias vectors.
 This form models the ODE dynamics using a simple two-layer feedforward network.

Universal approximation properties of neural operator architectures \citep{lu2021learning, kissas2022learning, kovachki2021universal}.


\paragraph{Applications}
%NODEs in neuroscience%
Neural ordinary differential equations (NODEs) are being increasingly adopted in computational and systems neuroscience, showing improved performance compared to current approaches \citep{kim2021inferring,geenjaar2023learning,sedler2023expressive,elgazzar2024universal,rubanova2019latent,coelho2024enhancing}.


%\paragraph{Latent}
%Latent ODE-RNN  \citep{rubanova2019latent}
%Latent ODE-LSTM \citep{coelho2024enhancing}

\paragraph{Time variant}
As continuous limits of ResNets, it is natural to take the coefficients of NODEs to be time-dependent.

semiautonomous NODEs (SA-NODEs) \citep{li2024universal}


\paragraph{Analysis}
\citep{massaroli2020nodes}


%
%\newpage
%\section{Topology}
%\subsection{Homeomorphism and homotopy}
%
%\subsubsection{Homeomorphism}
%A homeomorphism \( f \) between  two limit cycles is a continuous, bijective map that transforms points on \( \gamma_1(t) \) to points on \( \gamma_2(s) \) while preserving the topological structure. Mathematically, this is expressed as:
%\[
%f: \gamma_1(t) \rightarrow \gamma_2(s), \quad \text{where} \quad s = h(t)
%\]
%Here, \( h(t) \) is a continuous, bijective, and periodic function that maps the time variable \( t \) in System 1 to the time variable \( s \) in System 2, i.e.,
%\[
%h: [0, T_1] \rightarrow [0, T_2]
%\]
%with the condition that \( h(0) = 0 \) and \( h(T_1) = T_2 \).
%
%The map \( f \) in terms of the coordinates can be written as:
%\[
%f(x_1(t), y_1(t)) = (x_2(h(t)), y_2(h(t)))
%\]
%
%\subsubsection{Homotopy}
%A homotopy \( H \) between two limit cycles \( \gamma_1(t) \) and \( \gamma_2(t) \) is expressed as:
%\[
%H: [0,1] \times [0, T] \rightarrow \mathbb{R}^n
%\]
%where \( H(\tau, t) \) gives the position of a point on the limit cycle at time \( t \) and for a given \( \tau \). The transformation satisfies:
%\[
%H(0, t) = \gamma_1(t), \quad H(1, t) = \gamma_2(t)
%\]
%This means that at \( \tau = 0 \), the homotopy corresponds to the first limit cycle \( \gamma_1 \), and at \( \tau = 1 \), the homotopy corresponds to the second limit cycle \( \gamma_2 \). For intermediate values of \( \tau \), \( H(\tau, t) \) represents a continuous deformation between \( \gamma_1 \) and \( \gamma_2 \).





%%%%%%%%%%%
\newpage
\section{Dynamical systems concepts}
Overview: 
Types of systems: FPS Sec.~\ref{sec:fading_memory}

Implementing dynamics through different means: Realization\label{sec:realization}, Simulation\label{sec:simulation}

DS definitions and terms 
Semiflow \ref{sec:semiflow}
Existence and uniqueness of flow of an ODE \ref{sec:solutions}

Fundamental theorem of DSs \ref{sec:fundamental}
Conley Index theory\ref{sec:cit}
Structural stability \ref{sec:ss}

Fenichel \ref{sec:fenichel}

We include the basic definitions and some fundamental theorems about attractors (Sec.~\ref{sec:attractors})
their basins of attraction (Sec.~\ref{sec:boa}) and separatrices between them (Sec.~\ref{sec:sep}).

Asymptocically autonomous\ref{sec:asyauto}

Contractive dynamics\ref{sec:contractive}



\subsection{Fading memory}\label{sec:fading_memory}

\begin{definition}[Fading memory Property (FMP)]\label{def:fmp}
Let $x_1, x_2$ be bounded sequences indexed by $\mathbb{R}$, and let $H$ be a sequence of causal, shift-equivariant (also called time-homogeneous) functionals.
Here, causal means $H_t(x) = H_t(x(-\infty,t])$ for all $t$.
We say that $H$ has the \textbf{FMP} if there is a monotonically decreasing function $w : \mathbb{R}^+ \to (0, 1]$ such that for any $\varepsilon > 0$ there exists $\delta > 0$ with 
\[
|H_t(x_1) - H_t(x_2)| < \varepsilon \quad \text{whenever} \quad \sup_{s \in (-\infty, t]} |x_1(s) - x_2(s)| w(t - s) < \delta.
\]
\end{definition}
The notion of fading memory is strictly an input/output property, that is, it refers only to the operator $\mathcal{H}$ which maps inputs into outputs; the realization of $\mathcal{H}$ (there need not even be one) is irrelevant.

\begin{remark}\label{rem:fmp}%this is just a repeat
The FMP quantitatively captures the idea that perturbations to the initial condition have asymptotically negligible influence on the long-term behavior of the system trajectory.
This implies that imperfect system models may still be able of generating outputs that uniformly approximate the outputs of the original system over infinite time intervals.
In contrast, for systems not satisfying this stability condition, a sharp bound on the approximation error degrades exponentially with time \citep{hirsch1974nonautonomous, sontag2013mathematical}.
\end{remark}


%introduce ISS here?


\subsection{Realization}\label{sec:realization}
%TODO: intro
Infinite sequences \( y = (y_t)_{t \in \mathbb{Z}} \), where \( y_t \in \mathbb{R}^d \) for all \( t \in \mathbb{Z} \), for which there exists a functional \( H : (\mathbb{R}^d)^{\mathbb{Z}^-} \rightarrow \mathbb{R}^d \) such that 
\begin{equation}\label{eq:filter}
y_t = H(\ldots, y_{t-2}, y_{t-1}) = H\left( y_{t-1} \right), \quad \text{for all } t \in \mathbb{Z},
\end{equation}
and where the symbol \( y_{t-1} \in (\mathbb{R}^d)^{\mathbb{Z}^-} \) denotes the semi-infinite sequence \( y_{t-1} = (\ldots, y_{t-2}, y_{t-1}) \).

A canonical causal and time-invariant filter \( U_H : (\mathbb{R}^d)^{\mathbb{Z}} \rightarrow (\mathbb{R}^d)^{\mathbb{Z}} \) is associated and determined by the equality 
\[
U_H(y)_t := H\left( y_{t-1} \right), \quad \text{for all } y \in (\mathbb{R}^d)^{\mathbb{Z}} \text{ and } t \in \mathbb{Z}.
\]

A classical problem in control and systems theory is the realization of filters like \ref{eq:filter} as the unique solution (when it exists) of a state-space transformation:
\begin{equation}
x_t = F(x_{t-1}, y_{t-1}),
\end{equation}
\begin{equation}
y_t = h(x_t),
\end{equation}
where \( F : \mathbb{R}^L \times \mathbb{R}^d \rightarrow \mathbb{R}^L \) and \( h : \mathbb{R}^L \rightarrow \mathbb{R}^d \) are a state and a readout map, respectively. \( \mathbb{R}^L \) is, in this case, the state space and, typically, \( L \) is much bigger than \( d \).


%%%%
\subsection{Simulation}\label{sec:simulation}
\citep{sontag1992neural}

\begin{definition}[Sec.2.3 in \citep{sontag1992neural}]\label{def:simulation}
Consider two systems $\Sigma$ and $\tilde\Sigma$ described by the following dynamics 
\begin{equation}
    \Sigma: \begin{cases}
        \dot x = f (x, u)\\ y = Hx
    \end{cases}
    \ \ \ 
    \tilde\Sigma: \begin{cases}
        \dot{\tilde x} = \tilde f (\tilde x, u)\\ \tilde y = \tilde H\tilde x
    \end{cases}
\end{equation} 
with inputs $u(t) \in \mathbb{R}^m$, outputs $y(t) \in \mathbb{R}^p$, and states $x(t) \in \mathbb{R}^n$ and $\tilde{x}(t) \in \mathbb{R}^{\tilde{n}}$. Suppose we are given a compact set $K \subset \mathbb{R}^n$, a set $U$ of admissible inputs, and a time interval $T \subseteq \mathbb{R}^+$.

$\Sigma$ \emph{simulates} $\tilde{\Sigma}$ on sets $K$ and $U$ up to accuracy $\varepsilon$ for times $t \in \mathbb{T}$ if there exist two continuous maps $\alpha : \mathbb{R}^{\tilde{n}} \to \mathbb{R}^n$ and $\gamma : \mathbb{R}^n \to \mathbb{R}^{\tilde{n}}$ such that, when $\Sigma$ is initialized at $x(s) = \xi \in K$, $\tilde{\Sigma}$ is initialized at $\tilde{x}(s) = \gamma(\xi)$, where $s := \inf \mathbb{T}$, and any common input $u(\cdot) \in U$ is supplied to both $\Sigma$ and $\tilde{\Sigma}$, then
\[
|x(t) - \alpha(\tilde{x}(t))| < \varepsilon \quad \text{and} \quad |y(t) - \tilde{y}(t)| < \varepsilon \quad \text{for all } t \in \mathbb{T}.
\]
\end{definition}


%%%
\subsection{Semiflows\citep{sell2013dynamics}}\label{sec:semiflow}
Let \( M \) be a subset of a complete metric space \( W \), and let \( \mathbb{R} = (-\infty, \infty) \) 
and \( \mathbb{R}_+ = [0, \infty) \). A mapping \( \alpha = \alpha(w,t) \), where 
\( \alpha: M \times [0,\infty) \to M \), is said to be a \textit{semiflow} on \( M \), provided the following hold:

\begin{enumerate}
    \item Identity property: 
    \[
    \alpha(w,0) = w, \quad \text{for all } w \in M.
    \]
    \item Semigroup property: 
    \[
    \alpha(\alpha(w,s),t) = \alpha(w,s+t),
    \]
    for all \( w \in M \) and \( s, t \in \mathbb{R}_+ \).
    \item The mapping \( \alpha: M \times (0,\infty) \to M \) is continuous.
\end{enumerate}


 Existence of Global Attractors for compact semiflows: Theorem 23.11 in \citep{sell2013dynamics}
 


%%%
\subsection{Uniqueness of solutions}\label{sec:solutions}
To construct the infinite time horizon approximation theory of the NODEs, some preliminaries will be provided in this section.


%TODO: intro to basic cont reqs
\begin{definition}[Lipschitz]
 Let both $S \subset \mathbb{S}^n$ and $U \subset \mathbb{R}^m$ be open subsets.
 A mapping $f: S \times U \rightarrow \mathbb{R}^n$ is said to be Lipschitz in $x$ on $S \times U$ if there exists a constant $L$ such that
\[
|f(x_1, u) - f(x_2, u)| \leq L |x_1 - x_2|
\]
for all $x_1, x_2 \in S$, and any $u \in U$, and $L$ is a Lipschitz constant of $f(x, u)$ in $x$. $f$ is locally Lipschitz in $x$ if each point of $S$ has a neighborhood $S_0 \subset S$ such that the restriction $f$ is Lipschitz in $x$ on $S_0 \times U$.
\end{definition}


%\begin{lemma}
%Let a mapping \( F : D \to \mathbb{R}^n \) be \( C^t \). 
%Then \( F \) is locally Lipschitz. Moreover, if \( A \subset D \) is compact, then the restriction \( F|_A \) is Lipschitz.
%(For proof see Hirsch \& Smale, 1974, chap. 8, §3. Lemma and §6. Lemma.)
%\end{lemma}	


\subsubsection{Autonomous solutions}\label{sec:autonomousunique}
\begin{lemma}
%LEMMA 3.
Let \( D \) be an open subset of \( \mathbb{R}^n \) and \( F : D \to \mathbb{R}^n \) be a \( C^t \)-mapping. 
Let \( x(t) \) be a solution on a maximal open interval \( J = (a, \beta) \subset \mathbb{R} \) with \( \beta < \infty \). 
Then for any given compact subset \( K \subset D \), there is some \( t \in (a, \beta) \) with \( x(t) \notin K \). 
\end{lemma}	
For proof see \citep{hirsch1974nonautonomous}[chap. 8, §5]).


\begin{lemma}[Lemma 4 in \citep{funahashi1993approximation}]
Let \( F: \mathbb{R}^n \to \mathbb{R}^n \) be a bounded \( C^t \)-mapping. Then, the differential equation
\[
\dot{x} = -\frac{1}{r} x + F(x), \quad r > 0
\]
has a unique solution on \([0, \infty)\).
\end{lemma}	


%TODO: uniqueness for non-autonomous dynamics?

\subsubsection{Input-driven solutions}\label{sec:inputunique}
If \( f(x) \) and \( g(x,u) \) are Lipschitz continuous in \( x \) and bounded for a given \( u(t) \), then for any initial condition \( x_0 \), there exists a unique local solution on some interval \( [0,T) \).  

A sufficient condition for the solutions to exist globally is that \( f(x) \) and \( g(x,u) \) satisfy a linear growth bound:  
\[
\| f(x) + g(x,u) \| \leq C (1 + \| x \|), \quad \text{for some } C > 0.
\]
%This prevents solutions from blowing up in finite time and guarantees global existence.



%%%%
\subsubsection{Gronwall's lemma}\label{sec:gronwall}
\begin{lemma}\label{lemma:gronwall}

\end{lemma}

%%%%
\subsection{The Fundamental Theorem of Dynamical Systems}\label{sec:fundamental}
See, for example\citep{conley1978morse, norton1995fundamental}, it is also referred to as Conley's Decomposition Theorem \citep{mischaikow1999cit}.

\begin{theorem}[\citep{conley1978morse}]
 Any flow on a compact metric space decomposes into a chain recurrent part and a gradient-like part.
\end{theorem}

%TODO: takeaway


%%%%
\subsection{Conley Index Theory}\label{sec:cit}
From \citet{mischaikow1999cit}.

Let $\varphi$ be a flow. A compact set $N$ is called an \emph{isolating neighborhood} if 
\[
\inv(N,\varphi) \coloneqq \{x\in N\ | \ \varphi(\reals,x)\subset N\} \subset \operatorname{int} N,
\]
where $ \operatorname{int} N$ denoted the interior of $N$. 
$S$ is an \emph{isolated invariant set} if $S=\inv(N)$ for some isolating neighborhood $N$.

The important property of the isolating neighborhood that we will need is that it is robust with respect to perturbations. To state this more precisely, consider a continuous family of dynamical systems
\begin{equation}
\varphi_\lambda\colon \reals\times X\rightarrow X, \ \ \ \lambda\in[-1,1].
\end{equation}

\begin{proposition}[Proposition 1.1 in \citep{mischaikow1999cit}]
Let $N$ be an isolating neighborhood for the flow $\varphi_0$. Then, for sufficiently small $\delta>0$, $N$ is an isolating neighborhood for all $\varphi_\lambda, |\lambda|<\delta$.
\end{proposition}

%Def 1.2: continuation
\begin{definition}
Let $N\subset X$ be a compact set. 
Let $S_\lambda = \inv(N,\varphi_\lambda).$
Two isolated invariant sets $S_{\lambda_0}$ and $S_{\lambda_1}$ are \emph{related by continuation} if $N$ is an isolating neighborhood for all $\varphi_\lambda, \lambda\in[\lambda_0,\lambda_1$.
\end{definition}

%Continuation Property
\begin{theorem}[Continuation Proprty, Theorem 3.10 in \citep{mischaikow1999cit}]
Let $S_{\lambda_0}$ and $S_{\lambda_1}$ be isolated invaraint sets that are related by continuation. 
Then they have the same Conley Index, i.e.,
\[
\operatorname{CH}_*(S_{\lambda_0})\approx \operatorname{CH}_*(S_{\lambda_1}).
\]
\end{theorem}


%%%%
\subsection{Structural stability}\label{sec:ss}
\citep{peixoto1959ss, mane1987ss, hu1994ss, hayashi1997invariant, robbin1971ss, robinson1974ss, palis1970ss}

Structural stability is a fundamental concept in the study of dynamical systems. A structurally stable system is one for which small perturbations in the system's parameters do not lead to drastic qualitative changes in the system's behavior.
% In other words, if a system is structurally stable, its behavior remains robust under perturbations.

\begin{definition}[Flow of an autonomous system of ODEs]
Let $\vF: \reals^n\rightarrow \reals^n$ be a (time-independent) vector field and $\vx\colon\reals\rightarrow\reals^n$  the solution of the initial value problem
\[
\dot\vx(t) =  \vF(\vx(t)), \ \ \ \vx(0) = \vx_0.
\]
Then $ \phi (\vx_{0},t)=\vx(t)$ is the flow of the vector field $\vF$.
We will use the notation $\mathcal {O}(x,\phi)\coloneqq \{\phi(x,t)\ |\ t\in\reals\}$ to denote an orbit.
\end{definition}


\begin{definition}[Closed orbit]
A \emph{closed orbit} of a flow is a solution $\vx(t)$ such that there exists $t_1, t_2\in\reals$ with  $t_1\neq t_2$ for which $\vx(t_1) = \vx(t_2)$.
\end{definition}

\begin{definition}[Topological equivalence]
Let $X,Y$ be two topological spaces. Let $\phi$ be a flow on $X$, and $\psi$ be a flow on $Y$.
Then $\phi$ and $\psi$ are \emph{topologically equivalent} if there is a homeomorphism $h\colon X\to Y$ mapping orbits of  $\phi$  to orbits of $\psi$  homeomorphically, and preserving the orientation of the orbits. In other words, there must be  an increasing map $\tau\colon X\times\reals\rightarrow\reals$ such that 
\begin{equation}\label{eq:topeq}
h(\phi_{\tau(t,x)}(x)) = \psi_t(h(x)).
\end{equation}
\end{definition}


\subsubsection{Dynamical systems on manifolds}
For a more general setting one might study flows on manifolds.
\begin{definition}
Let $M$ be a compact Riemannian manifold without boundary. For each $r\geq 1$, let $\mcX^r(M)$ denote the set of $C^r$ vector fields of $M$, endowed with the $C^r$ topology. Every $S\in \mcX^r(M)$ generates a $C^r$ flow $\phi=\phi^S\colon M\times \reals\rightarrow M$.
Two flows are topologically equivalent if there is a homeomorphism $h\colon M\rightarrow M$ that maps the orbits of one flow onto those of the other flow while preserving the orientation. 
 We say $S$ is $C^r$ structurally stable if $S$ has a $C^r$ neighborhood $\mcU$ in $\mcX^r(M)$ such that every $X \in \mcU$ generates a flow $\phi^X$ that is topologically equivalent to $\phi^S$ (i.e. it sends the orbits of $X$ to the orbits of $Y$, preserving the orientation of the orbits.
\end{definition}

\begin{remark}
For every manifold, structurally stable flows form non-empty open subsets of $\mcX^r(M)$ \citep{palis1970ss}.
\end{remark}

\subsubsection{Hyperbolicity}
The fixed point $x$ of the flow $\phi_t\colon M\rightarrow M$ is hyperbolic if $x$ is a hyperbolic fixed point of the diffeomorphism $\phi_1: M\rightarrow M$.
 An alternate way of saying this is as follows: If $x$ is a fixed point of the flow $\phi_t\colon M\rightarrow M$, then the derivative $D\phi_t(x)\colon T_x(M)\rightarrow T_x(M)$ defines a linear representation of the real line and so can be written in the form $D\phi_t(x)=e^{tA}$ where $A$ is a linear endomorphism of $T_x(M)$.


Hyperbolicity for $M=\reals^n$ is an eigenvalue condition.
The eigenvalues of the linear part of a vector field at a hyperbolic singularity have nonzero real part.
In the case of a closed orbit $\gamma=\mathcal{O}(x,\phi)$, we choose a point $p\in\gamma$ and a transversal $\tau$ to $\gamma$ at $p$.
The vector field's flow defines a local diffeomorphism (the first-return map) of $\tau$ to itself having $p$ as a fixed point.
Hyperbolicity of $\gamma$ means that the eigenvalues of the linear part of the first return map at $p$ have nonzero real part.
   
Hyperbolicity of an invariant manifold can be defined in terms of stable and unstable subspaces. Let $M$ be an invariant manifold, and let $\TM$  denote the tangent space at $M$. Hyperbolicity implies that there exist complementary subspaces $E^s$  and $E^u$  of $\T_M$  such that for all $x\in M$, the linearization of the flow at $x$ has the following properties:
\begin{itemize}
\item Stable Subspace: For all $v\in E^s$, the linearized flow contracts in the $E^s$  direction, i.e., \[\lim_{t \to \infty} \frac{\|D\Phi_t(x)v\|}{\|v\|} = 0.\]
\item Unstable Subspace: For all $u\in E^u$ , the linearized flow expands in the $E^u$ direction, i.e., \[\lim_{t \to -\infty} \frac{\|D\Phi_t(x)w\|}{\|w\|} = 0.\]
\end{itemize}


%%%%
\subsubsection{Structural stability in dimensions greater than two}
\begin{theorem}[\cite{robbin1971ss}, \cite{robinson1974ss}]\label{theorem:ss}
A vector field $S$ is structurally stable if it satisfies Axiom A and the Strong Transversality conditions.
\end{theorem}

\paragraph{Axiom A}
A point $x\in M$ is \emph{nonwandering} of the vector field $S$ if for any neighborhood $V$ of $x$ in $M$, there is $t\geq 1$ such that $\phi_t(V) \cap V\neq \emptyset$. The set of \emph{nonwandering} points of $S$ is the nonwandering set of $S$, and denoted by $\Omega(S)$. 

\begin{definition}
$S$ is an axiom A flow if the following two conditions hold:
\begin{enumerate}
\item The nonwandering set of $S, \Omega(S)$, is a hyperbolic set and compact.
\item The set of periodic points of $S$ is dense in $\Omega(S)$.
\end{enumerate}
\end{definition}

Singularities (i.e. points where $S(x)=0$) and points of periodic orbits are all nonwandering.
% An $S$-invariant set $\Lambda$ is \emph{hyperbolic} if the restricted tangent bundle $\T_\Lambda\! M$ has a continuous $S$-invariant splitting $E^s\oplus S \oplus E^u$ such that for some constants $\lambda> 0$ and $T>0$, the inequalities

\begin{definition}
 Suppose $M$ is a manifold, $\phi_t\colon M\rightarrow M$ is a flow.
  We say that $\phi_t$ is \emph{hyperbolic} %(also called an Anosov flow)
   if for every $p\in M$ there is a splitting of the tangent space $\TpM=E^s(x)\oplus E^0(x)\oplus E^u(x)$, where $E^0=\langle \dot\phi_t\rangle$ is the flow direction and there are constants $C>0$ and $\lambda\in(0,1)$ such that for every $t>0$ one has \[\|D\phi_t(v)\|\leq C\lambda^t\|v\|\]
for $v\in E^s(x)$ and \[\|D\phi_{-t}(v)\|\leq C\lambda^t\|v\|\] for $v\in E^u(x)$ . 
\end{definition}

\paragraph{Strong Transversality Condition}
%An axiom A diffeomorphism satisfies the strong transversality condition if for any non-wandering points $x$ and $y$, the stable manifold of $x$ is transverse to the unstable manifold of $y$.

If a vector field $S$ satisfies Axiom A, then for any $x \in M$, the stable manifold 
\[W^s(x)=\left\{ y \in M \colon \lim_{t\rightarrow+\infty} d(\phi_t( y), \phi_t(x))=0\right\}\]
of $x$ and the unstable manifold
\[W^u(x)=\left\{ y \in M \colon \lim_{t\rightarrow-\infty} d(\phi_t( y), \phi_t(x))=0\right\}\]
of $x$ are each an injectively immersed $C^r$ submanifold of $M$, if $S$ is $C^r$.

An Axiom A system $S$ satisfies the strong transversality condition if $W^s(x)$ is transverse to $W^u(x)$ at all $x\in M$.
Roughly, this requires that the stable and unstable manifolds cross when they intersect. 

Two submanifolds $M_1$ and $M_2$ are transverse if their tangent spaces span $\mathbb{R}^n$.

\begin{remark}
The stability and genericity of transversality make it a very powerful condition, and give rise to a number of applications, in many branches of science which might not initially seem related to
differentiable manifolds. If a data set can be represented as a manifold, which is transverse to some condition that we care about, then we know that any (small) perturbations of the data set will not effect its relation to this important condition.
\end{remark}


\begin{remark}
For higher than 2 dimensional systems saddle-to-saddle connections can be structurally stable, and can be hence used for robust neural computation with transients, see also \cite{rabinovich2008transient}.
\end{remark}








%%%%
\subsubsection{Fenichel's Persistent Manifold Theorem}\label{sec:fenichel}
%TODO: intro

%%%%small intro for PMT? move there as subsub?
\subsubsection{Invariant manifolds}\label{sec:invman}
\citep{roberts1989invariant,
kalitin2021attractors}

\citep{hirsch1970invariant}
\citep{wiggins1994nhim}
\citep{jones1995gspt}
\citep{kuehn2015multipletimescale}


\subsubsection{Normally hyperbolic invariant manifolds are persistent}
\begin{theorem}[\citep{fenichel1971persistence}; see also \citep{kuehn2015multipletimescale}]
%\textbf{Theorem 2.2.2} ().%
 Consider
\begin{equation}\label{eq:fenichelode}
\dot{z} = H(z) \quad \text{with } H \in C^r \text{ and } z \in \mathbb{R}^N. 
\end{equation}
Let \( M \) be a \( C^r \) compact connected manifold that is overflowing invariant under the flow \( \phi_t \) defined by Eq.~\ref{eq:fenichelode}. Assume that
\begin{equation}
\nu(p) < 1 \quad \text{and} \quad \sigma(p) < \frac{1}{r} \quad \text{for all } p \in M. 
\end{equation}
Then for every \( C^r \) vector field \( H_{\text{pert}} \) that is \( C^1 \theta \)-close to \( H \), with \( \theta \) sufficiently small, there is a manifold \( M_{\text{pert}} \) that is overflowing invariant under \( H_{\text{pert}} \) and \( C^r \)-diffeomorphic to \( M \).
\end{theorem}




%%%%
\subsection{Attractors}\label{sec:attractors}
Consider the autonomous system
\begin{equation}\label{eq:attractorode}
\dot x = f(x)
\end{equation}
with \(f \in C^1(E)\) where E is an open subset of \(\reals^n\). 

\begin{definition}
The \textit{global attractor} of a system \( \dot{x} = f(x) \) (relative to the state space \( X \)) is the set  
\[
\mathbb{A} = \bigcap_{t \geq 0} \phi(t,X).
\]
\end{definition}

\begin{definition}[\(\omega\)- and \(\alpha\)-limit sets]\label{def:omega}
A point \( p \in E \) is a \(\omega\)-limit point of the trajectory \( \varphi(t, x) \) of the system Eq.~\ref{eq:attractorode} if there exists a sequence \( t_n \to +\infty \) such that
\[
\varphi(t_n, x) = p.
\]
Similarly, if there exists a sequence \( t_n \to -\infty \) such that
\[
\varphi(t_n, x) = q,
\]
and the point \( q \in E \), then the point \( q \) is called an \(\alpha\)-limit point of the trajectory \( \varphi(t, x) \) of  Eq.~\ref{eq:attractorode}. The set of all \(\omega\)-limit points of a trajectory \( r \) is called the \(\omega\)-limit set of \( r \) and is denoted by \( \omega(r) \). The set of all \(\alpha\)-limit points of a trajectory \( r \) is called the \(\alpha\)-limit set of \( r \) and is denoted by \( \alpha(r) \). The set of all limit points of \( r \), \( \alpha(r) \cup \omega(r) \), is called the limit set of \( r \).
\end{definition}

\subsubsection{Nonexpansivity}
\begin{definition}\label{def:nonexpansive}
A system \( \dot{x} = f(x) \) is \textit{nonexpansive} with respect to a norm \( \|\cdot\| \) if for all \( t > 0 \) and all \( x, y \) in the system’s state space, we have  
\[
\|\phi_t(x) - \phi_t(y)\| \leq \|x - y\|.
\]
\end{definition}


\textbf{Corollary 2. in \citep{duvall2024remark}} Suppose we have a \( C^1 \) system \( \dot{x} = f(x) \) with a compact forward-invariant state space \( X \). Then, for any real number \( t \geq 0 \), the time evolution operator \( \phi_t \) is an isometry on the set  
\[
A = \bigcap_{t \geq 0} \phi_t(X)
\]  
(i.e., the global attractor of the system).


\subsubsection{Compact dynamics}
The semiflow $\sigma(u, t) = S(t)u$ on $W$ is said to be compact, if for every bounded set $B$ in $W$ there is a $r = r(B)$, $0 \leq r < \infty$, such that for every $t> r$, the set $S(t)B$ lies in a compact subset of $W$, i.e., $\cl_WS(t)B$ is compact. 

%\subsubsection{$\kappa$-contracting}%for noncompact dynamics
%\citep{sell2013dynamics}

%%%%move to attractors as subsub?
\subsection{Basins of attraction}\label{sec:boa}
\citep{milnor1985attractor}
\citep{hirsch1995computing}

Also called the Region of attraction, e.g., in \citep{garces2012strategies} (Definition 2.2.5).
\begin{definition}[Basin of attraction]
%for equilibrium poin
Suppose that $x=0$ is an attractive equilibrium point of a system $\dot x = f(x)$.
The basin of attraction $BOA(0)$ is defined as the set of initial states $x(0)=x_0$ for which the resulting trajectory $x(t), t\geq 0$ tends to the equilibrium point:
\begin{equation}
\boa(0) \coloneqq \{x_0\in \reals^n|x(t)\rightarrow0\text{ as } t\rightarrow\infty\}.
\end{equation}

%for LC
\end{definition}

%Denote the basin of attraction for an attractor $A$ as $BOA(A)$.

%is this important?
\begin{proposition}
\[\dim \boa(A_i) = \dim X\] for each attractor $A_i$.
\end{proposition}
\begin{proposition}
\[\dim\cl \boa(A_i) \cap \boa(A_i) = \dim X - 1 \] for each attractor $A_i$.
\end{proposition}



\begin{proposition}
Each connected component of $\ \bigcup_i\cl \boa(A_i) \cap \boa(A_i) $ is a connected, compact normally hyperbolic invariant manifold. %repelling/outflowing?
\end{proposition}


%%%%
\subsection{Separatrices}\label{sec:sep}
The boundaries between basins of attraction are exactly the stable manifolds of dimension $n-1$ (e.g.originating from a saddle point with $n-1$ stable directions 
or from a closed orbit with $n-2$ stable directions) \citep{gruemm1975stable}.
These separatrices are invariant, so without  perturbation the system cannot cross them.
When the system under some perturbations (S-type) crosses the separatrices, it moves into another basin and its long-range behavior changes drastically.

\begin{definition}[Separatrix]\label{def:separatrix}
Define the separatrix between the basins of attraction of attractors $A_i$ and $A_j$ as 
\begin{equation}
\sep(A_i,A_j) = \cl(\boa(A_i)\cap \cl(\boa(A_j)).
\end{equation}
\end{definition}










%%%%
%\subsection{Non-autonomous dynamical systems}

\subsection{Asymptotically autonomous dynamics}\label{sec:asyauto}
introduced in \citep{markus1956asyauto}

The vector ordinary differential equation  
\begin{equation}\label{eq:nonautonomous}
 \dot y = f(y) + g(t, y),
\end{equation}
is commonly said to be asymptotically autonomous if \( g(t, y) \) becomes small in some sense as \( t \to \infty \). One then hopes that the solutions of Eq.~\ref{eq:nonautonomous} behave in some sense like those of the autonomous equation  
\begin{equation}\label{eq:autonomous}
\quad x' = f(x)
\end{equation}
for large \( t \). If the solutions of Eq.~\ref{eq:autonomous} are uniquely determined by initial conditions,   then it is well-known \cite{} that all positive limit sets \( \omega(x_0) \) of Eq.~\ref{eq:autonomous} are invariant with respect to Eq.~\ref{eq:autonomous}.

More on aads: \citep{wieczorek2021compactification}

\subsubsection{Exponentially asymptotically autonomous (EAA)}
\begin{definition}\label{def:eaa}
Let \(\sigma \in \mathbb{N}\). Consider the exponentially asymptotically autonomous equation  
\[
\dot{x} = g(t,x), \quad x \in \mathbb{R}^n,
\]
where \( g, h \in C^\sigma \) and \( g(t,x) \to h(x) \) in the following sense: for each compact set \( K \subset \mathbb{R}^n \) and each \( \varepsilon > 0 \), there exists \( T \) such that  
\[
\|\partial^\gamma g(t,x) - \partial^\gamma h(x)\| e^{\alpha t} < \varepsilon
\]
for all multi-indices \( \gamma \) with \( |\gamma| \leq \sigma \), for all \( x \in K \), and for all \( t > T \).
\end{definition}

\paragraph{EAA for each input}\label{sec:eea_input}
\begin{definition}\label{def:eaa_input}
Let \(\sigma \in \mathbb{N}\). Consider the exponentially asymptotically autonomous equation  
\[
\dot{x} = g(x,u(t)), \quad x \in \mathbb{R}^n,
\]
where \( g, h \in C^\sigma \) and \( g(u(t),x) \to h(x) \) in the following sense: for each compact set \( K \subset \mathbb{R}^n \) and each \( \varepsilon > 0 \), there exists \( T \) such that  
\[
\|\partial^\gamma g(u(t),x) - \partial^\gamma h_u(x)\| e^{\alpha t} < \varepsilon
\]
for all multi-indices \( \gamma \) with \( |\gamma| \leq \sigma \), for all \( x \in K \), and for all \( t > T \).
\end{definition}

\paragraph{Class of EEA}
We take $\sigma\geq 1$.


\subsubsection{BoA}
Exponentially asymptotically autonomous \citep{giesl2011numerical}
non-expo \citep{giesl2012numerical}

Non-autonomous approximation can be done by also calculating in the probability of making an error on a given input.
So the approximation probability guarantee will need to be formulated in terms of the probability of making an error bigger than a threshold calculated over the function of input sequences and initial points.




\subsection{Contractive dynamics}\label{sec:contractive}

Overview \citep{aminzare2014contraction, tsukamoto2021contraction}

The concept of a \textit{generalized Jacobian} was introduced for analyzing contracting systems \citep{lohmiller1998contraction}.
Consider a nonsingular change of differential coordinates \( \delta z = \Theta(x) \delta x \), then the dynamics in the new coordinates are given by
\[
\delta\dot{ z} = F(x) \delta z
\]
where the generalized Jacobian is defined as
\[
F(x) := \Theta(x) A(x) \Theta(x)^{-1} + \dot{\Theta}(x) \Theta(x)^{-1}.
\]

If such a change of coordinates exists such that
\[
F(x) + F(x)' \leq -\lambda I,
\]
then the system is contracting with rate \( \lambda \). Furthermore, the metric
\[
M(x) = \Theta(x)^T \Theta(x)
\]
is a valid contraction metric.

\begin{definition}
Given the system equations \(\dot{x} = f(x, t)\), a region of the state space is called a \textit{contraction region} if the Jacobian \(\frac{\partial f}{\partial x}\) is uniformly negative definite in that region.
\end{definition}

\begin{theorem}[Theorem 1 in \citep{lohmiller1998contraction}]\label{thrm:contractive_ball}
Given the system equations  $\dot x = f(x, t)$, any trajectory, which starts in a ball of constant radius centered about a given trajectory and contained at all times in a contraction region, remains in that ball and converges exponentially to this trajectory.
\end{theorem}

Theorem 1 can be vastly extended simply by using a more general definition of differential length.


\begin{definition}
Given the system equations \(\dot{x} = f(x, t)\), a region of the state space is called a \textit{contraction region} with respect to a uniformly positive definite metric \( M(x, t) = \Theta^T \Theta \), if equivalently \( F \) in (7) is uniformly negative definite or  
\[
\frac{\partial f}{\partial x}^T M + M \frac{\partial f}{\partial x} + \dot{M} \leq -\beta_M M
\]
(with constant \( \beta_M > 0 \)) in that region.
\end{definition}

\begin{theorem}[Theorem 2 in \citep{lohmiller1998contraction}]\label{thrm:contractive}
Given the system equations $\dot x = f(x,t)$, any trajectory, which starts in a ball of constant radius with respect to the metric $M(x,t)$, centered at a given trajectory and contained at all times in a contraction region with respect to $M(x,t)$,  remains in that ball and converges exponentially to this trajectory.
\end{theorem}

\begin{remark}
Theorem \ref{thrm:contractive} corresponds to a necessary and sufficient condition for exponential convergence of a system. 
\end{remark}

\begin{remark}
A convex contraction region contains at most one equilibrium point, since any length between two trajectories is shrinking exponentially in that region.
\end{remark}

\subsubsection{Transverse contraction}
\begin{definition}[Transverse contraction] 
A system is said to be \textit{transversely contracting} if there exists a differential change of coordinates such that  

\[
\delta z^T (F + F') \delta z < 0
\]

for all \( \delta z \) satisfying  

\[
\delta z^T \Theta(x) f(x) = 0,
\]

where the latter condition follows from  

\[
\dot{z} = \Theta(x) \dot{x} = \Theta(x) f(x).
\]

\end{definition}

%Alternative def:
Consider a dynamical system:
\begin{equation}
    \dot{x} = f(x), \quad x \in \mathbb{R}^n.
\end{equation}

The system is said to be \textit{transversely contracting} if there exists an orthogonal projection $P(x)$ that removes motion along a particular subspace (such as an invariant manifold), and a metric $M(x)$ under which the reduced system exhibits contraction. Mathematically, this means:
\begin{equation}
    P(x) (J(x) + J(x)^T) P(x) \leq -\lambda I, \quad \lambda > 0,
\end{equation}
where:
\begin{itemize}
    \item $J(x) = \frac{\partial f}{\partial x}$ is the Jacobian of the system,
    \item $P(x)$ is a projection operator defining the transverse direction
\end{itemize}


\paragraph{Limit cycles and contractivity}%
\citep{manchester2014transverse} and refs therein: e.g. 16-20


\subsubsection{Robustness of contractive dynamics}\label{sec:contractive_robustness}

%contractive
Given a time-varying metric \( M : \mathbb{R}^n \times \mathbb{R} \to \text{Sym}_{n \times n}^{>0} \), a function \( f(x,t) \) is said to be contracting in the metric \( M(x,t) \) at rate \( \lambda \) if for all \( x \) and \( t \):

\[
\frac{\partial f}{\partial x} (x,t)^{\top} M(x,t) + M(x,t) \frac{\partial f}{\partial x} (x,t) + \dot{M} (x,t) \preceq -2\lambda M(x,t).
\]


%geodesic
Given a Riemannian metric \( M: \mathbb{R}^n \to \text{Sym}_{+}^{n \times n} \), the geodesic distance associated with \( M \) is defined as:
\begin{equation}
    d_M(x, y) := \inf_{\gamma \in \Gamma(x,y)} \int_0^1 \sqrt{\dot{\gamma}(s)^T M(\gamma(s)) \dot{\gamma}(s)} \, ds,
\end{equation}
where \( \Gamma(x,y) \) denotes the set of smooth curves \( \gamma: [0,1] \to \mathbb{R}^n \) with boundary conditions \( \gamma(0) = x \) and \( \gamma(1) = y \).


\begin{proposition}[A.2 in \citep{zhang2022adversarially}]
Consider two nonlinear systems:
\[
\dot{x}_p = f(x_p, t) + d(x_p, t)
\]
and 
\[
\dot{x} = f(x, t)
\]
where \( f(x, t) \) is contracting in the metric \( M(x,t) \).
 Then the geodesic distance \( d_{M(\cdot,t)}(x_p(t), x(t)) \) satisfies the differential inequality:
\[
\frac{d}{dt} d_{M(\cdot,t)}(x_p(t), x(t)) \leq -\lambda d_{M(\cdot,t)}(x_p(t), x(t)) + \|\Theta(x_p(t),t)d(x_p(t), t)\|_M
\]
where \( M(x,t) = \Theta(x,t)^\top \Theta(x,t) \).
\end{proposition}



\subsubsection{Contractivity and ISS}
Contraction implies E-ISS \citep{zhang2022adversarially}

\begin{definition}[Exponential Incrementally Input-to-State Stability (E-$\delta$ISS)]
Let \( \beta, \rho, \gamma > 0 \) be positive constants.
A continuous-time dynamical system  
\(\dot{x} = f(x, t)\)
is said to be \((\beta, \rho, \gamma)\)-exponential-incrementally-input-to-state stable (\((\beta, \rho, \gamma)\)-E-$\delta$ISS) if, for any pair of initial conditions \( (x_0, y_0) \) and input signal \( u(t) \) – which can depend causally on \( x, y \) – the trajectories \( x(t) \) and \( y(t) \), satisfying  
\[
\dot{x} = f(x, t), \quad \dot{y} = f(y, t) + u(t),
\]
obey the bound for all \( t \geq 0 \):
\[
\| x(t) - y(t) \|^2 \leq  \beta \| x_0 - y_0 \|^2 e^{- \rho t} + \gamma \int_0^t e^{-\rho (t-s)} \| u(s) \|^2 ds.
\]
\end{definition}


\subsubsection{How to use contractivity for UAP}
\paragraph{Per basin}
We can fill the basin of attraction with a metric, as in Sec.~\ref{sec:boa_errors}.
This is robust (Sec.~\ref{sec:contractive_robustness}).
Therefore, 


\paragraph{Globally}
\ascomment{TODO:}
If a system is contractive (with rate $\lambda$) then there exists a change of differential coordinates % δz = Θ(x)δx
such that for the generalized Jacobian $F$ such that $F(x)+F (x)' \leq  -\lambda I$.

If a system is transverse contracting then there exists a differential change of coordinates such that $\delta z(F + F')\delta z < 0$ for all $\delta z$ satisfying $\delta' z \Theta(x)f (x) = 0$, where the latter condition follows from  $\dot z= \Theta(x) \dot x = \Theta(x)f (x)$.

Q: can we consider the maximally invariant subspace of the system to be the subspace onto which we have transverse contractivity?
Qi: what is the projection operator?
Qii: 
R: this only proves contractivity onto this subspace and robustness for it in the sense of trajectories being close after a small perturbation but it cannot track what happens inside the subspace
%is this already helpful?
R: what we want to know is that which perturbations (inputs) lead us to a different attractor asymptotically and which proportion of all possible perturbations this is 






%move to start?
%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Support for the proof}
General intro into UAP Sec.~\ref{sec:approximationtheory}

Bound on approximation error inside a basin of attraction Sec.~\ref{sec:313} \ascomment{move section on this in main text to this sec?}

Separatrices/boundaries of BoAs are persistent Sec.~\ref{sec:boaboundary}

Perturbing limit cycles: period closeness Sec.~\ref{sec:lc_pert}

Chaos Sec.~\ref{sec:chaos} 

Non-autonomous/input-driven Sec.~\ref{sec:input_driven_app} 

\subsection{The problem of approximation}\label{sec:approximationtheory} %{Problem statement} 
%this can be cut shorter
For behaving agents we can formalize their behavior as sequence-to-sequence or input-output mappings.
We will call $\mathcal{X}$ the space of input sequences and $\mathcal{Y}$ the space of output sequences.
We consider a family of target functions, or simply targets, which is a subset \(\mathcal{C} \) of all mappings \( \mathcal{X} \rightarrow \mathcal{Y} \), i.e., \( \mathcal{C} \subset \mathcal{Y}^\mathcal{X} \). 
These are the relationships we wish to approximate (or ``learn"), by some (simpler or at least parameterized) candidate functions.
Let us denote this set of \textbf{candidates} by \( \mathcal{H} \subset \mathcal{Y}^\mathcal{X} \).
In learning theory, this is often called a \textbf{hypothesis space}.
The problem of approximation concerns how well functions in \( \mathcal{H} \) can resolve functions in \( \mathcal{C} \) (by comparing elements through a metric).
This is typically formulated in terms of a bound on the (maximal) error between the approximation and the target mapping.

\subsection{In basin bound}\label{sec:313}
\begin{proposition}[Proposition 3.1.3. in \citep{vanhandel2007filtering}]\label{prop:313}
Define 
\[
\frac{d x(t)}{d t} = f(t; x_t), \quad x(t) \in \mathbb{R}^n.
\]
We denote by $\varphi_{s,t}(x)$ the solution $x_t$ of this equation when it is started at the initial condition $x(s) = x$ ($s \leq t$). We will suppose that $f$ is sufficiently regular so that the flow $\varphi_{s,t}(x)$ exists, is unique, and is a diffeomorphism for all $0 \leq s \leq t < \infty$.

Now consider another differential equation 
\[
\frac{d \tilde{x}_t}{d t} = \tilde{f}(t; \tilde{x}_t), \quad \tilde{x}_t \in \mathbb{R}^n,
\]
where again we assume that $\tilde{f}$ is sufficiently regular so that this equation generates a unique flow $\tilde{\varphi}_{s,t}(x)$ as above.


The following error bound holds:
\[
\|\varphi_{0,t}(x) - \tilde{\varphi}_{0,t}(x)\| \leq \int_0^t \|D\varphi_{s,t}(\tilde{\varphi}_{0,s}(x))\| \|f(s; \tilde{\varphi}_{0,s}(x)) - f(s; \tilde{\varphi}_{0,s}(x))\| \, ds.
\]
\end{proposition}


\begin{remark}\label{rem:313}
Note that the above error bound depends on two separate quantities: 
the local error \( \| f(s; \tilde{\phi}_{0,s}(x)) - f(s; \phi_{0,s}(x)) \| \) 
and 
the term \( \| D\phi_{s,t}(\tilde{\phi}_{0,s}(x)) \| \), which bounds the sensitivity of the flow \( \phi_{s,t}(x) \) to an infinitesimal perturbation of its initial condition.
\end{remark}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Boundaries of basins of attraction}\label{sec:boaboundary}

\begin{proposition}
The BoA of an stable attractor is an connected open set of dimension $n$. %forward invariant manifold
\end{proposition}


\begin{proposition}
The boundary of a BoA of an stable attractor is an compact normally hyperbolic invariant manifold of dimension $n-1$. %doesnt need to be connected, e.g. 
\end{proposition}

\begin{proof}
\textbf{Invariance}
The boundary of a basin of attraction is composed of points that do not belong to the basin of the periodic orbit but are arbitrarily close to it.
 Any trajectory starting exactly on this boundary will not enter the basin of the periodic orbit nor escape to another basin.
This implies that if a trajectory starts on the boundary, it will remain on the boundary for all future times. Therefore, the boundary is invariant under the flow of the system.


\textbf{Corollary. p.194 in \citep{perko2013differential}} \( \alpha(r) \) and \( \omega(r) \) are invariant with respect to the flow \( \phi_t \) of \(\dot x = f(X)\). %(1) \dot x = f(X) f \in C1(E) where E is an open subset of Rn.

\textbf{Compactness}
\textit{Bounded} Because we consider bounded dynamics.

\textit{Closed} 
The boundary of a BoA of an stable attractor:
\[\partial(BoA) = \cl(BoA) -  BoA\]
is closed.

The boundary of a set \( A \) is defined as 
\[
\partial A = \overline{A} \cap (X - \overline{A}).
\]
It is the intersection of two closed sets and hence is closed.
\footnote{
The boundary is made up of 
$\alpha$-limit sets 
and
connecting orbits between unstable periodic orbits.
(unstable manifolds of unstable periodic orbits).
}


\textbf{Normal hyperbolicity}
Implied by structural stability.

\end{proof}

\subsubsection{Nontrivial examples}

\paragraph{The basin of attraction for a stable limit cycle}%irrelevant?
Regions of attraction to periodic orbits have an interesting structure: they must be a continuous deformation of a torus: the Cartesian product of an open unit disc of dimension $n-1$, with a scalar circle coordinate \citep{wilson1967structure}.
But they can be have complex structures, see for example Fig.~\ref{fig:unstab_lc_3fps}.

\setlength\belowcaptionskip{-5ex}
\begin{SCfigure}[10][bthp]
  \centering
  \includegraphics[width=0.5\textwidth]{unstab_lc_3fps}
  \caption{Separatrix between two stable fixed points inside an unstable limit cycle.
  }\label{fig:unstab_lc_3fps}
\end{SCfigure}



\subsubsection{Separatrices are persistent with isolating neighborhoods that fill the basin of attraction}
We will prove Prop.~\ref{prop:min_sep_bound} by showing that separatrices are persistent.
Isolating neighborhood with boundary that is transversal to flow.
The boundary can be taken to be arbitrarily close to the boundary of the BoA.
By taking the isolating neighborhood to be closer and closer to the BoA, we can prove persistence of separatrix.

%construction
%\textbf{Step 1:} 
We start by creating an inflowing isolated neighborhood.
%N-dim DS
 Let $I_0$ denote a section within the tubular neighborhood of the fixed point.
For each $t\in\reals$, the set $\{\varphi(t,x_0) \mid x_0\in I_0\}$ defines a connected $N-1$-dimensional manifold, which evolves smoothly over time. 
For each \( t \in \reals \), the image \( \{ \varphi(t, s) \mid s \in I_0 \} \) remains a connected $N-1$ dimensional manifold because the map \( \phi(t, \cdot) \) is \( C^1 \) and thus an immersion and the image of a connected set under a continuous map remains connected.
% The initial family \( \gamma(I) \) is a $N-1$ dimensional manifold.
This forms the boundary of the isolating neighborhood (see further Supp.Sec.~\ref{sec:isonbhd}).

%Forward Invariance
For each $t\in\reals$ the set $I_t = \{\varphi(t,x_0) \mid x_0 \in I_0 \}$ is a manifold.
Furthermore, the union of these sets up to any time  $\bigcup_{s<t}I_s$, indeed forms an inflowing isolating neighborhood.
%
%fills up basin
Then, as $t\rightarrow\infty$, the isolating neighborhood expands to fill the basin of attraction of the fixed point.
 This illustrates the neighborhood’s role in characterizing the asymptotic behavior of trajectories and the fixed point’s influence on the global dynamics.

%4:
Finally, because isolating neighborhoods of target and approximation have same (trivial) Conley index  for each $t$ (if vector fields are close to each other, see Sec.\ref{sec:cit}),
 %5
the separatrix must be in remaining part of the state space and, therefore it is persistent (i.e., it is $\mathcal{O}(\varepsilon)$ close), and hence normally hyperbolic \citep{mane1978persistent}).

\ascomment{Mention alternative proofs?}
%alternative proof with
%% 1 the above isolating nbhd being a normally hyperbolic invariant manifold (flow is tanget to boundary by construction)
%% 2 contractivity see \ref{sec:contractive_robustness}

This seems related: Theorem 3.1 in \citep{butler1986persistence}.


\subsubsection{Isolating neighbourhood for the basin}\label{sec:isonbhd}
\begin{remark}%move to app?
In the case of limit cycles, the isolating neighborhood takes on a distinctive topology. For instance, in a 3D state space, it forms a toroidal 2D manifold surrounding the cycle. In 2D systems, the neighborhood reduces to a pair of connected circles. These structures are particularly relevant for the analysis of periodic orbits and their stability.
 \end{remark}


%\subsubsection{Alternative: stable and unstable manifolds as persistent}
%stable and unstable manifolds of a normally hyperbolic manifold persist under perturbation Theorem 2.3.6 in \citep{kuehn2015multipletimescale}.
%
%Laminations: The collection of global stable/unstable manifolds. The same for the local stable/unstable manifolds or the local unstable manifolds for the geodesic flow on a compact manifolds with nonzero Lyapunov exponents almost everywhere. 
%Persistence of laminations \citep{berger2010persistence}


%simplifying:%we can consider $\cl\boa(A_i) to be an invariant manifold with boundary

%\footnote{For example,  all invariant manifolds of ReLU RNNs are either continuous attractors (in the direction of non-normal hyperbolicity) or normally hyperbolic.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Perturbing limit cycles}\label{sec:lc_pert}
If the limit cycle is hyperbolic, the Hausdorff distance between the original and perturbed limit cycles tends to scale linearly with \(\varepsilon\), the size of the perturbation. That is,

\[
d_H(\gamma_0, \gamma_\varepsilon) = O(\varepsilon),
\]

where \(d_H\) denotes the Hausdorff distance. The linear dependence arises because small changes in the vector field result in small changes in the trajectories of the system, which leads to a nearby limit cycle.


\subsubsection{Period closeness}\label{sec:periodcloseness}
%TODO: 
\begin{proposition}\label{prop:periodcloseness}%For a limit cycle from
Take a vector field $f\in C^1$.
If $\dot x = f(x)$ has a hyperbolic limit cycle with period $T_f$ then there is $\varepsilon$ for which all $g$ in a $\varepsilon$ $C^1$ neighbourhood will have a limit cycle (with the same stability) with period $T_g$ such that  $|T_f-T_g| = \mathcal{O}(\varepsilon)$.
\end{proposition}

\begin{proof}
Existence and Hausdorff distance of $\mathcal{O}(\varepsilon)$ follows from PMT.
Parametrize the limit cycle orbit by $\theta$.
Then $T_f = \frac{1}{\int_{\theta=0}^{2\pi}f(\gamma_f(\theta))d\theta}$.
There if a $k\in\reals$ such that $\gamma_{\text{Arc Length}} - \gamma_{\text{Arc Length}}<k\varepsilon$.

\[\text{Arc Length} = \int_a^b \sqrt{1 + [f'(x)]^2} \, dx.\]
%\begin{align}
%\int_{\gamma_f}\sqrt{1 + [f(x)]^2}dx\\
%\int_{\gamma_g}\sqrt{1 + [g(x)]^2}dx
%\end{align}

$\gamma_f(\theta)-\gamma_g(\theta) = \mathcal{O}(\varepsilon) \leq m\varepsilon$ %use homeomorphism for mapping! from PMT


USE LIPSCHITZ! 
$\|f(x)-f(y)\|\leq L_f\|x-y\|$
$\|g(x)-g(y)\|\leq L_g\|x-y\|$


\begin{align}
f(\gamma_f(\theta)) - g(\gamma_g(\theta)) \leq  L_fL_gm\varepsilon.
\end{align}

So \[\int_{\theta=0}^{2\pi}f(\gamma_f(\theta))d\theta-g(\gamma_g(\theta))d\theta\leq L_fL_gm\varepsilon\theta.\]


Use Davis-Kahan theorem. 
This states that the largest eigenvalues (0 for a stable limit cycle) are at most 
\end{proof}


\begin{theorem}[Davis-Kahan]
Let \( A \) and \( B \) be \( n \times n \) Hermitian matrices with eigenvalues \( \lambda_1 \leq \lambda_2 \leq \dots \leq \lambda_n \) for \( A \), and \( \mu_1 \leq \mu_2 \leq \dots \leq \mu_n \) for \( B \). Let \( \Delta = A - B \), and let \( \delta = \| \Delta \| \) be the operator norm of \( \Delta \). Then, for any eigenvalue \( \lambda_i \) of \( A \) and \( \mu_j \) of \( B \), the Davis-Kahan theorem gives the following bound on the difference between the eigenvalues of \( A \) and \( B \):
\[
|\lambda_i - \mu_i| \leq \frac{\| \Delta \|}{\sin \theta_i}
\]
where \( \theta_i \) is the angle between the eigenvectors corresponding to \( \lambda_i \) and \( \mu_i \). That is, if \( v_i \) is the eigenvector of \( A \) corresponding to \( \lambda_i \) and \( w_i \) is the eigenvector of \( B \) corresponding to \( \mu_i \), then
\[
\cos \theta_i = | \langle v_i, w_i \rangle |.
\]
\end{theorem}

For a discussion and proofs of this theorem see for example Section V.3 of \citet{stewart1990matrix}.


\begin{theorem}[Wedin's Theorem]
Let \( A \in \mathbb{C}^{m \times n} \) and \( \tilde{A} \in \mathbb{C}^{m \times n} \) be two matrices such that \( \| A - \tilde{A} \| \) is small, where \( \| \cdot \| \) denotes a matrix norm. Then, for the singular values \( \sigma_i(A) \) and \( \sigma_i(\tilde{A}) \), and corresponding singular vectors \( \mathbf{u}_i(A) \), \( \mathbf{v}_i(A) \), \( \mathbf{u}_i(\tilde{A}) \), \( \mathbf{v}_i(\tilde{A}) \), the following bounds hold:
\[
\left| \sigma_i(A) - \sigma_i(\tilde{A}) \right| \leq \| A - \tilde{A} \| \cdot \max_{j \neq i} \frac{\sigma_j(A)}{\sigma_i(A)}
\]
and for the singular vectors:
\[
\| \mathbf{u}_i(A) - \mathbf{u}_i(\tilde{A}) \| \leq \frac{\| A - \tilde{A} \|}{\sigma_i(A)} \cdot \max_{j \neq i} \frac{\sigma_j(A)}{\sigma_i(A)}
\]
\[
\| \mathbf{v}_i(A) - \mathbf{v}_i(\tilde{A}) \| \leq \frac{\| A - \tilde{A} \|}{\sigma_i(A)} \cdot \max_{j \neq i} \frac{\sigma_j(A)}{\sigma_i(A)}
\]
\end{theorem}



\subsubsection{Period matching}
%obsolete?
\subsubsection{Applying Melnikov’s Method to Limit Cycles}

To understand the period change due to a perturbation of a limit cycle, let’s consider the following:

Suppose we have an unperturbed system
\[
\dot{x} = f(x)
\]
with a limit cycle \(\gamma_0\) of period \(T_0\).

Now we apply a small perturbation to the system:
\[
\dot{x} = f(x) + \varepsilon g(x),
\]
where \(g(x)\) represents the perturbation, and \(\varepsilon\) is a small parameter.

The Melnikov function \(M(t)\) measures the first-order effect of the perturbation on the limit cycle, and it is given by:
\[
M(t) = \int_0^{T_0} \langle \nabla H(x_0(t)), g(x_0(t)) \rangle \, dt,
\]
where:
\begin{itemize}
    \item \(x_0(t)\) is the trajectory along the limit cycle \(\gamma_0\),
    \item \(H(x)\) is the Hamiltonian (or energy function) associated with the unperturbed system,
    \item \(g(x)\) is the perturbation vector field,
    \item \(\langle \cdot, \cdot \rangle\) denotes the dot product.
\end{itemize}

The Melnikov function tells us how the perturbation influences the distance between stable and unstable manifolds in the phase space. It is closely related to the splitting of separatrices, but in the context of limit cycles, it helps predict changes in the period and persistence of the limit cycle.

For small perturbations:

\(M(\gamma_0)\) represents the sensitivity of the period or structure of the limit cycle to the perturbation.

If \(M(\gamma_0) \neq 0\), the limit cycle may shift slightly, but it will persist, and the change in the period can be approximated as:
\[
\Delta T \approx \varepsilon M(\gamma_0),
\]
meaning that the period change is proportional to \(\varepsilon\), the size of the perturbation, with \(M(\gamma_0)\) determining the proportionality constant.

If \(M(\gamma_0) = 0\), the system is at a critical point where a bifurcation may occur, and the limit cycle may undergo a qualitative change (e.g., it could disappear or bifurcate).


\subsection{Approximating limit cycles}\label{sec:lc_extras}
\ascomment{TODO: prove that the fixed point happens at an epsilon that is small enough for the persistence of the other parts}
Follows from taking initial $\varepsilon_T$ to be small enough.

%\ascomment{TODO: trajectories next to LC have same period Chapter 8.1 in \citep{ermentrout2010mathematical}}

\subsubsection{Isochrons}%move to DS section?
This section if based on Section 8.1 in \citep{ermentrout2010mathematical}.
Consider the differential equation in $\reals^n$
\begin{equation}\label{eq:isochron_ode}
\dot x = f(x)
\end{equation}
and suppose $\Gamma$ is a $T$-periodic asymptotically stable limit cycle.
We parametrize $\Gamma$ by time with respect to the period $T$ and thus define a phase $\theta\in[0,T]$ along the limit cycle.
Let $\Theta(z)$ denote the phase of the oscillator for a point $z$ on $\Gamma$.
For an asymptotically stable limit cycle it is possible to define a phase for points $y$ in a neighborhood of the cycle.
Let $x(t,y)$ be the solution to Eq.~\ref{eq:isochron_ode} with initial condition $y$. 
Suppose $y$ is a point in the neighborhood of the limit cycle such that $\|x(t,z) - x(t,y)\|\rightarrow 0$ as $t\rightarrow \infty$.
Then we define $\Theta(y) = \Theta(z)$ (as $t\rightarrow\infty$, the solutions are indistinguishable.
The set of points which have the same asymptotic phase is called the \emph{isochron} of the limit cycle.
We denote the isochron of a point $z\in \Gamma$ as $N(z) $.
%The map $y\rightarrow y'$ is a Poincar\'e map for the limit cycle which takes exatly $T$ to return.
The existence of isochrons was proven in \citep{guckenheimer1975isochrons}.

%\textbf{Knobs}
%\begin{itemize}
%\item $\varepsilon_T$: small enough so that limit cycle is structurally stable
%\item $\hat\varepsilon$: needs to be small enough so that effect of correction outside of the tubular neighbourhood doesn't affect attractors and separatrices
%\item $\delta_T$: needs to be small enough so that no other attractor or separatrix is included (will this have an effect on the inside-basin flow error?) %choosing the size of the tubular neighbourhood
%\end{itemize}

%%effects: incorporated into Bookkeeping 
%\paragraph{The effect of the correction $\tilde \varepsilon\coloneqq a(\varepsilon_T)\hat\varepsilon$}
%- Tubular neighbourhood: $\|\hat f_{\operatorname{bump}}\|$ effect (relevant for $\overline\boa(A_i)$). 
%
%- Outside $\tilde\varepsilon$ effect for a non-compact activation function (e.g., affects location of fixed points and limit cycles)
%
%-  $\tilde\varepsilon$ needs to be small enough so that effect of correction outside of the tubular neighborhood doesn't affect attractors and separatrices too much.
%
%-- These should be also taken into account when choosing an initial approximation $\hat f$ to which to apply the correction.


\subsubsection{Multiple}\label{sec:mlc_extras}
\begin{equation}\label{eq:nlc_constraints}
\int_{\tilde{\gamma}}\|\tilde{f}_i(x)\| dx = \frac{1}{T_i} \ \  \text{for} \ i=1,\dots, N. 	% for the integrated vector field along the limit cycle $\int_{LC}f(x)$.
\end{equation}


\paragraph{For non-compact support activation functions}
\begin{enumerate}
\item We need that the adjustment still has at most $\varepsilon$ error in basins and that only $\delta$ part of the state space is in the problematic separatrix mismatch region
\item Separatrices are persistent so they will be shifted $\mathcal{O}(a_{\varepsilon_{T_i}}\hat \varepsilon_i)$. So we need $\sum_{i=1}^{N}k_i a_{\varepsilon_{T_i}}\hat \varepsilon_i < \tfrac{1}{2}\delta$ and if we require for the initial mismatch error to be $\kappa<\tfrac{1}{2}\delta$ we get the total to be less than $\delta$. 
\item The vector field outside the tubular neighborhood is $f - \eta\bar f  + \sum_{i=1}^Na_{\varepsilon_{T_i}}\hat \varepsilon_i\bar f_{i,\operatorname{bump}}$. 
%We have that $\|\eta\bar f  + a_{\varepsilon_T}\hat \varepsilon\bar f_{\operatorname{bump}}\| < \eta + a_{\varepsilon_T}\hat \varepsilon$ (triangle inequality and absolute homogeneity of norm). 
Therefore, we can take $\hat f$ to be  $\Psi_{\min}(\eta + \sum_{i=1}^Na_{\varepsilon_{T_i}}\hat \varepsilon_i)$ close to $f$ to guarantee $\varepsilon$,$\delta$-approximation.
\item Because we can make the correction vector field term dominate through $a_{\varepsilon_{T_i}}$ by choosing $\hat \varepsilon_i$ small enough, we can always adjust the periods of all stable limit cycles simultaneously (if there is a finite number of them).
\end{enumerate}

\paragraph{Using compact support activation functions}
\ascomment{Which ones are really compact?}
 ELU/CELU/GeLU/TaLU networks\citep{clevert2015elu,barron2017celu,elfwing2018sigmoid,hasan2023talu} which can be made to be zero outside of the tubular neighborhood as a method to make things work for multiple stable limit cycles.
See also \citep{duch1999survey,dubey2022activation,jagtap2023activation,ramachandran2017activation,hayou2019activation} for a survey of neural activation functions.

With a compact support activation function we can create a bump function that is truly zero outside of (an approximation of) the tubular neighborhood.
%instead of approximating zero.


%%%%%%%%%%%%%%%%%%%%%
\subsection{Approximating a chaotic orbit}\label{sec:chaos} 
\citep{pathak2017using}
\paragraph{Why consider chaos} 
Forecasting
\citep{jaeger2004harnessing}
\citep{fan2020long}
\citep{vlachas2020backpropagation}
\citep{grigoryeva2024forecasting}

%TODO:  name for type IV error: C-type (chaos)
\paragraph{The issue with our method} 
We cannot approximate according to our above criteria.
Every small discrepancy in the vector field will lead to orbits starting at same position to diverge over time, analogous to perturbing initial start of the chaotic orbit.


\paragraph{How to proceed?}
Prior work:
- Attractor reconstruction, reconstructing the full Lyapunov spectrum of a dynamical system using reservoir computing \citep{hart2024attractor}.
- teacher forcing \citep{hess2023teacherforcing}.

Different metric: Chaotic orbits
- mention: we can guarantee finite time approximation (prove: we can never do infinite time approximation)
- approximation of maximal attractor in Hausdorff sense possible \citep{hess2023teacherforcing}

What we can do is minimize the Hausdorff distance between the chaotic invariant manifolds and match the Lyapunov exponents of the target system
The first because of structural stability.
%%The second because of ???
%
Alternatively, we can guarantee that the shift representation of the dynamics is persistent.
% Symbolic dynamics: Mapping onto the same shift dynamical system



\subsection{Input-driven extras}\label{sec:input_driven_app} 
Unlike autonomous systems, where the evolution is determined solely by the state variable $x$, non-autonomous systems incorporate an explicit time dependence in the function $h(x,t)$:
\begin{equation}
\dot x = h(x,t).
\end{equation}

\subsubsection{Measure for input functions}\label{sec:inputspace_measure}
Uniform Lebesgue Measure on a Bounded Subset of \( C([0,T], \reals^m) \)
If \( u(t) \) is continuous and satisfies a uniform bound
\[
\|u(t)\| \leq C \quad \text{for all } t,
\]
the natural measure is the uniform measure on a subset of the Banach space \( C([0,T], \reals^m) \):
\[
\mathcal{P}[u] \propto \mathds{1}_{\|u\|_\infty \leq C} \mathcal{D} u.
\]
Here, \( \|u\|_\infty = \sup_{t \in [0,T]} \|u(t)\| \), and the measure is supported on the Banach space of continuous functions with bounded norm.

With this we can capture what proportion of inputs lead the system to the separatrix mismatch regions $\boa(A_i^f)\cap\boa(A_j^{\hat{f}})$. %TODO introduce some nice notation

\subsubsection{Defining the metric for the approximation of non-autonomous systems}\label{sec:inputdriven_metric}
%Reconsider $\delta$:
We now also keep track of which proportion of input space leads to bigger than $\varepsilon$ trajectory error.
So we define 
\begin{equation}%indicate X,U spaces?
d_\varepsilon(\varphi,\hat \varphi) \coloneqq  \mathbb{P}_{x_0}^{u}\left(\|\varphi(\cdot,x_0,u)-\hat \varphi(\cdot,x_0,u)\|_\infty>\varepsilon\right).
\end{equation}


Path integral representation of the distribution of $x(t,u)$ when integrating over all possible input functions $u(t)$:
\begin{equation}
    p(x,t) = \int \delta(x - x(t, u)) \mathcal{P}[u] \mathcal{D}u
\end{equation}
where $\mathcal{P}[u]$ is a probability measure over the function space of $u(t)$.

Any stochastic and even deterministic system can be expressed in terms of a path integral for which asymptotic methods can be systematically applied \citep{chow2015path}.


Consideration of $p(x,t)$ after some time passed %calculating path integral from initial distribution p(x,t)
%use dynamics until then to get a prior on the initial states
%then calculate path integral over inputs 
\begin{equation}
    p(x,t+s) = \int_{x_t} \int \delta(x - x(s, x_t, u)) \mathcal{P}[u] \mathcal{D}u dx_t.
    %    p(x,t+s) = \int_{x(t)} \int \delta(x - x(s, x(t), u)) \mathcal{P}[u] \mathcal{D}u dx(t) %which form is better?
\end{equation}


\subsubsection{Why this target class?}
%\paragraph{In terms of general non-autonomous dynamics}
%mention? convergence depends on x_0? this allows us to formulate coupled system with hyperbolic system for parameters?
If an sequence doesn't converge (i.e. it has a mixing property) we get the same issue with approximating the system as with chaotic orbits.
%and the input is not in the kernel of the input function
If it converges to a fixed point: then it is Asymptotically autonomous. Necessary if $f(x)$ has a continuous attractor.
If it converges to a limit cycle: then we have periodic forcing. 
If it doesn't converge, we cannot approximate.  %?
%except systems that have the inputs in some nullspaces
%In any case we cannot have a continuous attractor in the asymptotic vector field. %or can we?


%%with convergent u, we can inlude LCs and CAs!
%%What goes wrong with LCs: basically lack of robustness
%%What goes wrong with CAs: lack of robustness




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Implications}

\subsection{Remarks on error}\label{sec:error_remarks}
\paragraph{Link to Total integrated error (TIE)}
We can look at the total integrated error between the target and the approximant and derive a similar UAP theorem.

Error is then defined as 
\begin{equation}
\operatorname{TIE}(\varphi, \hat{\varphi}) = \int_{x_0\in X}\|\varphi(t,x_0) - \hat{\varphi}(t, x_0)\|_\infty.
\end{equation}

It is then easy to show that $\forall\varepsilon>0$ there exists a NODE such that $\operatorname{TIE}(\varphi, \hat{\varphi})<\varepsilon$.

\paragraph{Link to MSE loss}
In the limit that $\delta\rightarrow 0$ and $\varepsilon\rightarrow 0$  we have that 
\[\operatorname{MSE}(\varphi, \hat{\varphi}) \coloneqq \int_{x_0\in X} \|\varphi_t(x_0) - \hat{\varphi}_t(x_0)\|_\infty \rightarrow 0.\]


Limit $\delta\rightarrow 0$: error of type II goes to zero, i.e., perfectly matching separatrices


Limit $\varepsilon\rightarrow 0$: error of type I goes to zero (apart from small mismatch around separatrices), i.e., perfectly matching in-basin dynamics

\end{document}
