\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amssymb, amsmath, amsthm, amsfonts}
\usepackage{thmtools, mathtools, mathrsfs, dsfont}
\usepackage[square,numbers]{natbib}
\bibliographystyle{abbrvnat}
\usepackage{algorithm}
\usepackage{algpseudocode}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\theoremstyle{remark}
\newtheorem{remark}{Remark}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\sep}{\operatorname{sep}}
\newcommand{\cl}{\operatorname{cl}}
\newcommand{\vol}{\operatorname{vol}}

\title{Approximation of multi-stable dynamical systems on infinite horizons}
\author{\'Abel S\'agodi and Il Memming Park}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
Recurrent Neural Networks (RNNs) are believed to be universal approximators.
However, RNNs are not universal approximators 
Existing results about approximating dynamical systems either focus on finite time horizon approximation or on approximating stable dynamical systems on an infinite horizon.
We seek to extend approximation results for dynamical systems to multi-stable systems (systems without chaos) on an infinite horizon through Neural ODEs (NODEs).



Why do we care about infinite horizon?
- approximation gives us qualitative behavior
- (little) noise doesn't make approximation increase suddenly in error (only smoothly)


It could be that existing theorems could be interpreted that in the limit $N\rightarrow\infty$ the approximation holds.
ie $\lim_{N\rightarrow\infty} error = 0$
Here we do it for finite $N$.
ie for each $\epsilon>0$ $\exists N$ such that $error(N) < \epsilon$
(implies the above limit)


\section{Approximation of dynamical systems}
For an overview of the literature, see [litrev]


\subsection{Approximation of dynamics on compact time intervals}
%An RNN with a sufficient number of hidden units and appropriate non-linear activation functions (such as tanh or sigmoid) can approximate any dynamic system or time-dependent function to arbitrary precision, provided the system operates on compact time intervals.

The universal approximation theorem for RNNs, which extends the classical result for feedforward neural networks, guarantees that RNNs can approximate the mapping from a sequence of inputs to outputs for any continuous-time process, including those governed by differential equations.

% \citep{funahashi1993approximation}
This property has been formally proven for systems operating over finite time intervals. Specifically, \citet{funahashi1993approximation} showed that RNNs can approximate the time-dependent behaviors of dynamical systems, making them suitable for tasks like time-series prediction, control systems, and the modeling of sequential data.


\subsection{Approximation of fading memory systems}
Fading memory \citep{boyd1985fading}

\begin{definition}[Fading memory Property (FMP)]
Let $x_1, x_2$ be bounded sequences indexed by $\mathbb{R}$, and let $H$ be a sequence of causal, shift-equivariant (also called time-homogeneous) functionals.
Here, causal means $H_t(x) = H_t(x(-\infty,t])$ for all $t$.
We say that $H$ has the \textbf{FMP} if there is a monotonically decreasing function $w : \mathbb{R}^+ \to (0, 1]$ such that for any $\varepsilon > 0$ there exists $\delta > 0$ with 
\[
|H_t(x_1) - H_t(x_2)| < \varepsilon \quad \text{whenever} \quad \sup_{s \in (-\infty, t]} |x_1(s) - x_2(s)| w(t - s) < \delta.
\]
\end{definition}

 Typically, the FMP is used to reduce the approximation problem to one over a finite, bounded index set, and then appeal to the density of fully connected neural network to obtain approximation \citep{gonon2021fading}. %more examples


\begin{remark}
Every fading-memory system could be uniformly approximated arbitrarily closely over the set of systems with 'linear dynamics'\citep{matthews1993approximating}. %Theorem 3+4
Using the perceptron to uniformly approximate the external representation of a fading-memory system results in a finite-memory system model, called the perceptron filter \citep{matthews1993approximating}.
\end{remark}
 

\paragraph{ESN}
 \citet{jaeger2001echo} proposed Echo State Networks (ESNs), demonstrating their universal approximation capabilities specifically for fading memory systems.
Fading memory refers to the property where the influence of past inputs diminishes over time, allowing the network to focus more on recent inputs while still retaining some memory of older inputs.
 Let’s clarify the role of fading memory systems in the context of infinite horizon approximations and the broader discussion on RNNs.
 

\subsection{Topological equivalency}
Topological equivalency \citep{hart2020recurrent}
This is trivial for structurally stable systems!


\subsection{Neural ODEs (NODEs)}
Neural ODEs\citep{chen2018neural} model the continuous trajectory of the hidden state \( \mathbf{h}(t) \) as the solution to an ordinary differential equation parameterized by a neural network. Mathematically, the dynamics are defined by:

\[
\frac{d \mathbf{h}(t)}{dt} = f(\mathbf{h}(t), t, \theta)
\]

where \( \mathbf{h}(t) \) is the hidden state at time \( t \), \( f \) is a neural network with parameters \( \theta \), and \( t \) represents time. Given an initial condition \( \mathbf{h}(t_0) = \mathbf{h}_0 \), the evolution of the hidden state over time is computed by solving the ODE using numerical solvers, allowing the model to capture complex, continuous transformations.


NODEs in neuroscience\citep{kim2021inferring, sedler2023expressive}




Theoretical Guarantees for Dynamical Systems
 Proofs of Universal Approximation for Dynamical Systems
 
 
 Any finite trajectory of an $n$-dimensional continuous dynamical system can be approximated by the internal state of the hidden units and $n$ output units of an Liquid time-constant (LTC) network \citep{hasani2018liquid}.
 
 
\citep{tabuada2020universal}



\paragraph{Latent}
Latent ODE-RNN\citep{rubanova2019latent}

Latent ODE-LSTM \citep{coelho2024enhancing}


\subsection{Flows}
RNNs are universal approximators of flow  functions of dynamical systems  with control inputs on finite time intervals  \citep{aguiar2023}.

\subsection{Misc.}

\subparagraph{Neural oscillators}
\citep{lanthaler2023neuraloscillators}




\section{Approximation of dynamical systems on infinite time intervals}
Let's get started.
We will need to consider what the space is of target dynamical systems that we can approximate.
Furthermore, these can be approximated in a particular way, i.e. with a specific difference between the approximant and the target system.
This difference is measured as the expectation of the uniform norm difference between the flows. We can make the probability of finding a 



\subsection{Targets}
Autonomous
Structurally stable+Fixed points = hyperbolic fixed points
The dynamics is globally attractive / There exist a compact set that with the fixed points inside that is forward invariant 

\begin{equation}
\dot x = f(x)
\end{equation}
$f\in C^1$.



Later: Limit cycles (or just one?)



Different metric: Chaotic orbits
mention: we can guarantee finite time approximation
prove: we can never do infinite time approximation 


\subsection{Hypothesis class}
The hypothesis class $\mathcal{G}$ is composed of
\begin{equation}
\dot x = g(x) = A\sigma(Bx+\theta)
\end{equation}


\subsection{Metric}

Uniform norm over time (given the same initial starting point)
\begin{equation}
\|\varphi(t,x_0)-\hat \varphi(t,x_0)\|_\infty = \sup_t|\varphi(t,x_0)-\hat \varphi(t,x_0)|
\end{equation}

We consider a compact $X$ on which we measure the difference between the target and our approximation.
\footnote{FNN approximation universality is also on compact intervals.}

Expectation of the error over all initial points
\begin{equation}
\|\varphi-\hat \varphi\| = \frac{1}{\vol X}\mathbb{E}[ \int_{x_0\in X}   \mathbb{1}[\|\varphi(t,x_0)-\hat \varphi(t,x_0)\|_\infty>\epsilon]
\end{equation}




\subsection{Theorem}
\begin{theorem}
Let $D$ be an open subset of $\mathbb{R}^n$, $F : D \to \mathbb{R}^m$ be a $C^1$-mapping, and $I_\epsilon$ be a compact subset of $D$.
Suppose that there is a subset $K \subset I_\epsilon$ such that any solution $x(t)$ with initial value $x(0) \in K$ of an ordinary differential equation
\begin{equation}\label{eq:5}
    \dot{x} = F(x), \quad x(0) \in K
\end{equation}
is defined on $I = [0, T]$ $(0 < T < \infty)$ and $x(t)$ is included in $I_\epsilon$ for any $t \in I$.

with flow $\varphi_t$ 


 Then, for an arbitrary $\epsilon > 0$, there exist an MLP recurrent neural network 
 \begin{equation}
\dot x = \hat f(x) = \sigma(A\sigma(Bx+b)+a)
\end{equation}
%such that for a solution $\hat \varphi(t,x_0)$ satisfying Eq~\ref{eq:5} with initial state $x_0$ of the network
such that its flow $\hat \varphi$ satisfies
\begin{equation}
\|\varphi-\hat \varphi\| = \int_{x_0\in X}\|\varphi(t,x_0)-\hat \varphi(t,x_0)\|_\infty<\epsilon.
\end{equation}
\end{theorem}


\begin{proof}
Because of the density of the hypothesis class $\mathcal{G}$ we have that for any $f\in C^1$ and any $\delta_1>0$
there exists $\hat f\in\mathcal{G}$ such that 


Structural stability implies that there exists a $\delta_2>0$ such that for all $g$ that are $\delta_2$ $C^1$ close to $f$ the flow is topologically equivalent to that of $f$.

So we can choose $\delta = \min\{\delta_1,\delta_2\}$ such that the flow of $\hat f\in\mathcal{G}$ is topologically equivalent to that of $f$.



We can distinguish two types of errors:
$x_0\in BOA(A_i^f,A_i^g)$ flow deviates along the trajectories
We can use \citep{vanhandel2007filtering} to bound this


\begin{proposition}[Proposition 3.1.3. in \citep{vanhandel2007filtering}]
Define 
\[
\frac{d x_t}{d t} = f(t; x_t), \quad x_t \in \mathbb{R}^n.
\]
We denote by $\varphi_{s,t}(x)$ the solution $x_t$ of this equation when it is started at the initial condition $x_s = x$ ($s \leq t$). We will suppose that $f$ is sufficiently regular so that the flow $\varphi_{s,t}(x)$ exists, is unique, and is a diffeomorphism for all $0 \leq s \leq t < \infty$.

Now consider another differential equation 
\[
\frac{d \tilde{x}_t}{d t} = \tilde{f}(t; \tilde{x}_t), \quad \tilde{x}_t \in \mathbb{R}^n,
\]
where again we assume that $\tilde{f}$ is sufficiently regular so that this equation generates a nice flow $\tilde{\varphi}_{s,t}(x)$ as above.


The following error bound holds:
\[
\|\varphi_{0,t}(x) - \tilde{\varphi}_{0,t}(x)\| \leq \int_0^t \|D\varphi_{s,t}(\tilde{\varphi}_{0,s}(x))\| \|f(s; \tilde{\varphi}_{0,s}(x)) - f(s; \tilde{\varphi}_{0,s}(x))\| \, ds.
\]
\end{proposition}



Note that the above error bound depends on two separate quantities: the local error 
$\| f(s; \tilde{\varphi}_0, s(x)) - f(s; \tilde{\varphi}_0, s(x)) \|$ 
(compare with the quantity $\epsilon(y_n; \varphi p_n)$ in section 0.1.5) and the term 
$\| D\varphi_{s,t}(\tilde{\varphi}_0, s(x)) \|$, 
which bounds the sensitivity of the flow $\varphi_{s,t}(x)$ to an infinitesimal perturbation of its initial condition.



$x_0\in BOA(A_i^f,A_j^g)$ for $i\neq j$
flow error because trajectories go to different attractors
We can bound this by reducing the mismatch between the separatrices (proportional to the distance between the attractors).

(Conj.) For each pair $i\neq j$ the connected component of $\sep(A_i,A_j)$ is a normally hyperbolic backward invariant manifold.

- For ReLU NNs all invariant manifolds are either continuous attractors (in the direction of non-normal hyperbolicity) or normally hyperbolic.

Therefore, by Fenichel's Persistent manifold theorem, there exists a $\delta_{ij}$ such that for all $g$ that are $\delta_{ij}$ $C^1$ close to $f$ such that $g$ has an  normally hyperbolic backward invariant manifold that is diffeomorphic to and at most at $\mathcal{\delta_{ij}}$ Hausdorff distance from $\sep(A_i,A_j)$.





Now we need to sum all the different errors together.
\end{proof}



\subsection{Approximating a limit cycle}


We can use the fact that the tubular neighbourhood around the LC is an inflowing normally hyperbolic invariant manifold.
This implies that there is a 3 layer FNN that approximates the vector field x a bump function around the LC (similar idea as in paper but this would be an actual possible implementation).
Then we know that the this NHIM will persist.
We also know that there is a maximal perturbation size ($\epsilon_{LC}$) for which the LC persists.
So we have to guarantee that the LC approximation is already close enough so that this perturbation does not destroy the LC.
We can now rescale the perturbation so that the period of approximation LC matches the target's. This is the case if 
\[\frac{\ell}{\int_{LC}f(x)} =  \frac{\hat \ell}{\int_{LC}\hat f(x)}\] for the length of the LC $\ell$ and the integrated vector field along the LC $\int_{LC}f(x)$.
We just increase or decrease  the above perturbation by multiplying with a scalar so that this equality holds.
The only thing we now need to worry about is how the length of the LC changes as a result of this perturbation and whether we can indeed get the equality 

Fenichel's PMT guarantees that the perturbed LC lies within $\mathcal{O}(\epsilon)$ the original LC approximation.
The integral will be growing as $\epsilon * \hat \ell(\epsilon)$
Is it true that $\hat \ell(\epsilon) = \hat \ell + g(\epsilon)$ where $|g(\epsilon)|<|\epsilon|$?

Then we still need to worry about whether the fixed point happens at an epsilon that is small enough for the persistence of the other parts


So if it gets longer because of the perturbation (i.e. $\hat \ell(\epsilon)$ is an increasing function) as long as $1/(\hat \ell+g(\epsilon))$ is not decreasing at the same rate there should exist a value $\epsilon$ for which the equality holds

Conj: we can keep g(epsilon) small so that the speed changes more quickly and we can adjust the period
For this we might need an additional constraint on the flow normal to the approximate limit cycle
Or we can perhaps achieve this by choosing the size of the tubular neighbourhood!




\subsection{Approximating a chaotic orbit}
We cannot approximate according to our above criteria.
Every small discrepancy in the vector field will lead to 
(analogous to perturbing initial start of the chaotic orbit 


What we can do is minimize the Hausdorff distance between the chaotic invariant manifolds and match the Lyapunov exponents of the target system
The first because of structural stability
The second because of ??


Symbolic dynamics
Mapping onto the same discrete dynamical shift dynamical system?





\subsection{Input driven dynamics}

This can be only done by also calculating in the probability of making an error on a given input.
So the approximation probability guarantee will need to be formulated in terms of the probability of making an error bigger than a threshold calculated over the function of input sequences and initial points.


\section{Implications for understanding neural computation}

Use of NODEs \citep{kim2021inferring}




\section{Discussion}


\paragraph{Limit cycles}
Approximable but approximation is not robust! (period will change under almost all perturbations)

\paragraph{The price of universality}
This construction is immensely inefficient

Better to just look at RNNs. 



\newpage
\bibliography{../all_ref.bib}



\newpage
\section*{Appendix}
\section{Universal Approximation with Feedforward Neural Networks (FNNs)}

The foundation of our approach lies in leveraging the \textit{universal approximation property} of feedforward neural networks (FNNs), a powerful result that makes neural networks exceptional for a broad array of tasks. This property allows FNNs to approximate complex functions with remarkable flexibility, providing the theoretical basis for their widespread success in machine learning.

\subsection{The Universal Approximation Theorem}

The universal approximation theorem states that a \textit{feedforward neural network} with at least one hidden layer, using a non-linear activation function (such as the sigmoid or ReLU), and equipped with enough neurons, can approximate \textit{any continuous function} defined on a compact subset of \(\mathbb{R}^n\) to arbitrary precision.

Formally, for any continuous function \(f(x)\) and any \(\epsilon > 0\), there exists an FNN such that:
\[
| f(x) - f_{\text{MLP}}(x) | < \epsilon \quad \forall x \in K
\]
where \(K \subset \mathbb{R}^n\) is compact. This guarantees that given sufficient resources (neurons, depth), the FNN can approximate \(f(x)\) as closely as desired within this bounded region.

The importance of the compact domain assumption is that it constrains the input space to a finite region, ensuring the approximation remains well-behaved.
This is where the theorem’s beauty shines: while we often don’t know the explicit form of the target function, neural networks learn it purely from data, making them excellent for tasks like regression, classification, and pattern recognition.



\subsection{Activation functions} %Sigmoidal 
At the heart of the theorem lies the activation function. Any non-linear, continuous, and non-constant function enables the network to approximate continuous functions universally.
A classic example is the \textit{sigmoidal function}, defined by the property:
\[
\lim_{x \to -\infty} \sigma(x) = 0 \quad \text{and} \quad \lim_{x \to +\infty} \sigma(x) = 1
\]
Many popular activation functions, such as the logistic sigmoid and tanh, fall into this category.
These functions introduce the necessary non-linearity, enabling the network to break free from the constraints of purely linear transformations.

%non-sigmoidal activations like the \textit{Rectified Linear Unit (ReLU)} 
Despite ReLU's unbounded nature, it still supports the universal approximation property, as shown in \citep{yarotsky2017error}.



\subsection{The Multi-layer Perceptron (MLP)}

The universal approximation property of FNNs was first proven by \citep{cybenko1989approximation} for sigmoidal activation functions. This was followed by significant generalizations by \citep{hornik1989multilayer} and \citep{funahashi1989approximate}, which extended the result to broader classes of activation functions, including the popular ReLU.
These results serve as the cornerstone of modern neural network theory, supporting the use of deep architectures across diverse machine learning applications.

%all above are 3 layer FNNs? right?

Let’s now focus on the structure of a \textit{multi-layer perceptron (MLP)}, a common form of FNN. For a simple 3-layer MLP, the output can be expressed as:
\[
h^{(1)} = \sigma(W^{(1)} x + b^{(1)})
\]
\[
f_{\text{MLP}}(x) = W^{(2)} h^{(1)} + b^{(2)}
\]
Here, \(h^{(1)}\) represents the hidden layer, while \(W^{(1)}, W^{(2)}\) and \(b^{(1)}, b^{(2)}\) are the weights and biases learned during training. The activation function \(\sigma(\cdot)\) introduces the necessary non-linearity.




In a more general case, for a deeper MLP with \(L\) layers, we have:
\[
h^{(1)} = \sigma(W^{(1)} x + b^{(1)})
\]
\[
h^{(2)} = \sigma(W^{(2)} h^{(1)} + b^{(2)})
\]
\[
\vdots
\]
\[
h^{(L-1)} = \sigma(W^{(L-1)} h^{(L-2)} + b^{(L-1)})
\]
\[
f(x) = W^{(L)} h^{(L-1)} + b^{(L)}
\]

This architecture allows the MLP to progressively build complex representations, layer by layer, transforming raw input data into rich feature hierarchies.


\section{Support for the proof}
\subsection{Separatrices}

Denote the basin of attraction for an attractor $A$ as $BOA(A)$.

\begin{definition}	
Define the separatrix between the basins of attraction of attractors $A_i$ and $A_j$ as 
\begin{equation}
\sep(A_i,A_j) = \cl(BOA(A_i)\cap \cl(BOA(A_j)).
\end{equation}
\end{definition}



\end{document}